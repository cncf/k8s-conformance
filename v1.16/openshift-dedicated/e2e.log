Mar  2 21:27:21.472: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I0302 21:27:21.472389   56340 e2e.go:92] Starting e2e run "6c4f358f-4985-4af2-96bb-db5141279b0c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583184440 - Will randomize all specs
Will run 15 of 4897 specs

Mar  2 21:27:21.490: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:27:21.492: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Mar  2 21:27:21.670: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 21:27:21.721: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 21:27:21.721: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Mar  2 21:27:21.721: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 21:27:21.738: INFO: e2e test version: v1.16.3-beta.0.38+c3aac8e00076fd
Mar  2 21:27:21.749: INFO: kube-apiserver version: v1.16.2
Mar  2 21:27:21.749: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:27:21.764: INFO: Cluster IP family: ipv4
SSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:27:21.765: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename taint-multiple-pods
Mar  2 21:27:21.882: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Mar  2 21:27:21.894: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 21:28:22.465: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:28:22.490: INFO: Starting informer...
STEP: Starting pods...
Mar  2 21:28:22.538: INFO: Pod1 is running on ip-10-0-130-27.ec2.internal. Tainting Node
Mar  2 21:28:32.609: INFO: Pod2 is running on ip-10-0-130-27.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  2 21:28:45.789: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 21:29:05.792: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:29:05.930: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5626" for this suite.
Mar  2 21:29:12.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:29:15.723: INFO: namespace taint-multiple-pods-5626 deletion completed in 9.618995997s

• [SLOW TEST:113.959 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:29:15.726: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  2 21:29:15.837: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 21:29:16.120: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:29:16.146: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-224.ec2.internal before test
Mar  2 21:29:16.470: INFO: tuned-cvjkf from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:16.470: INFO: kibana-85fc498c95-z56md from openshift-logging started at 2020-02-24 14:53:23 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container kibana ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 	Container kibana-proxy ready: true, restart count 0
Mar  2 21:29:16.470: INFO: fluentd-vgqp4 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:29:16.470: INFO: dns-default-78jzw from openshift-dns started at 2020-02-10 19:36:41 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:16.470: INFO: splunkforwarder-ds-2j9mv from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:16.470: INFO: node-exporter-s8c2n from openshift-monitoring started at 2020-02-10 19:21:13 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:16.470: INFO: ovs-m9222 from openshift-sdn started at 2020-02-10 19:28:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:16.470: INFO: rbac-permissions-operator-7ddb7b95c-tf5qr from openshift-rbac-permissions started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Mar  2 21:29:16.470: INFO: sre-dns-latency-exporter-sds5m from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.470: INFO: sdn-qrnvj from openshift-sdn started at 2020-02-10 19:25:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:16.470: INFO: multus-c92b7 from openshift-multus started at 2020-02-10 19:33:07 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:16.470: INFO: elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:29:16.470: INFO: node-ca-5lj4v from openshift-image-registry started at 2020-02-10 19:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:16.470: INFO: machine-config-daemon-xfc2n from openshift-machine-config-operator started at 2020-02-10 19:37:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.470: INFO: validation-webhook-6f8dcdd9fb-94rfx from openshift-validation-webhook started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.470: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:29:16.470: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-27.ec2.internal before test
Mar  2 21:29:16.591: INFO: ovs-bjftz from openshift-sdn started at 2020-02-10 19:28:52 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.591: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:16.591: INFO: node-exporter-n5znc from openshift-monitoring started at 2020-02-10 19:21:51 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.591: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:16.591: INFO: splunk-forwarder-operator-catalog-xmff9 from openshift-splunk-forwarder-operator started at 2020-02-24 18:15:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.591: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:29:16.591: INFO: node-ca-zkxpc from openshift-image-registry started at 2020-03-02 21:29:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:16.592: INFO: fluentd-xsxth from openshift-logging started at 2020-03-02 21:29:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:29:16.592: INFO: sre-dns-latency-exporter-8df72 from openshift-monitoring started at 2020-03-02 21:29:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container main ready: false, restart count 0
Mar  2 21:29:16.592: INFO: rbac-permissions-operator-registry-qdnz8 from openshift-rbac-permissions started at 2020-02-10 20:09:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:29:16.592: INFO: sdn-bg7s7 from openshift-sdn started at 2020-02-10 19:26:18 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:16.592: INFO: tuned-4q8sm from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:16.592: INFO: configure-alertmanager-operator-registry-c4mtq from openshift-monitoring started at 2020-02-28 15:20:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:29:16.592: INFO: machine-config-daemon-txz8t from openshift-machine-config-operator started at 2020-03-02 21:29:05 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:16.592: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.592: INFO: splunkforwarder-ds-qkm6f from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:16.592: INFO: multus-2m27b from openshift-multus started at 2020-02-10 19:29:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:16.592: INFO: dns-default-xpn8q from openshift-dns started at 2020-03-02 21:29:05 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.592: INFO: 	Container dns ready: false, restart count 0
Mar  2 21:29:16.592: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:16.592: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-159.ec2.internal before test
Mar  2 21:29:16.758: INFO: machine-config-daemon-b6c6q from openshift-machine-config-operator started at 2020-02-10 19:38:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: grafana-58f6c99667-jb9f4 from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: telemeter-client-78c68b58db-2fl8t from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:29:16.758: INFO: fluentd-rzd9d from openshift-logging started at 2020-02-24 14:53:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:29:16.758: INFO: node-exporter-w56xf from openshift-monitoring started at 2020-02-10 19:22:24 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:16.758: INFO: multus-blrhd from openshift-multus started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:16.758: INFO: dns-default-95h6x from openshift-dns started at 2020-02-10 19:37:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:16.758: INFO: kube-state-metrics-67bcd5775b-t85wt from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container kube-state-metrics ready: true, restart count 2
Mar  2 21:29:16.758: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-10 19:52:07 +0000 UTC (7 container statuses recorded)
Mar  2 21:29:16.758: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:29:16.758: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:29:16.759: INFO: ovs-r2phg from openshift-sdn started at 2020-02-10 19:26:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:16.759: INFO: sdn-xpc4d from openshift-sdn started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container sdn ready: true, restart count 1
Mar  2 21:29:16.759: INFO: tuned-7cwm9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:16.759: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-10 19:52:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:29:16.759: INFO: splunkforwarder-ds-vg9wk from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:16.759: INFO: sre-dns-latency-exporter-b7brb from openshift-monitoring started at 2020-02-05 15:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.759: INFO: node-ca-2c7vd from openshift-image-registry started at 2020-02-10 19:23:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:16.759: INFO: image-registry-776d8848f-4dkxq from openshift-image-registry started at 2020-02-10 19:52:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.759: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:29:16.759: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-248.ec2.internal before test
Mar  2 21:29:16.882: INFO: sre-build-test-1583176260-jdk6t from openshift-build-test started at 2020-03-02 19:11:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.882: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:29:16.882: INFO: dns-default-rj656 from openshift-dns started at 2020-02-10 19:38:43 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.882: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:16.882: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:16.882: INFO: sre-ebs-iops-reporter-1-smq2g from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.883: INFO: splunk-forwarder-operator-f6d6c9c57-vv9mk from openshift-splunk-forwarder-operator started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Mar  2 21:29:16.883: INFO: prometheus-operator-6d474cc98b-pkw6b from openshift-monitoring started at 2020-02-27 16:36:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:29:16.883: INFO: router-default-595f5b77b4-kz6pp from openshift-ingress started at 2020-02-19 15:08:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container router ready: true, restart count 0
Mar  2 21:29:16.883: INFO: configure-alertmanager-operator-57f5b56487-jp6j4 from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sre-build-test-1583179860-8bmsn from openshift-build-test started at 2020-03-02 20:11:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:29:16.883: INFO: node-exporter-lqkf8 from openshift-monitoring started at 2020-02-10 19:23:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.883: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sdn-cqj65 from openshift-sdn started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:16.883: INFO: machine-config-daemon-hk5js from openshift-machine-config-operator started at 2020-02-10 19:38:33 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:16.883: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.883: INFO: prometheus-adapter-56d646d4cd-bs2fc from openshift-monitoring started at 2020-02-21 10:30:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:29:16.883: INFO: fluentd-2mc9b from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:29:16.883: INFO: splunkforwarder-ds-f2c94 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sre-build-test-1583183460-z47qg from openshift-build-test started at 2020-03-02 21:11:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:29:16.883: INFO: multus-zh5hw from openshift-multus started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sre-machine-api-status-exporter-1-j9zvt from openshift-monitoring started at 2020-02-10 20:00:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container main ready: true, restart count 1
Mar  2 21:29:16.883: INFO: tuned-m4fk9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sre-stuck-ebs-vols-1-d7lg9 from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.883: INFO: managed-velero-operator-589fbb966f-4gwmw from openshift-velero started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container managed-velero-operator ready: true, restart count 0
Mar  2 21:29:16.883: INFO: sre-dns-latency-exporter-qksxc from openshift-monitoring started at 2020-02-05 21:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.883: INFO: node-ca-m4bjs from openshift-image-registry started at 2020-02-10 19:21:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:16.883: INFO: ovs-lnrws from openshift-sdn started at 2020-02-10 19:28:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.883: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:16.883: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-145.ec2.internal before test
Mar  2 21:29:16.972: INFO: multus-fmzkv from openshift-multus started at 2020-02-10 19:27:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:16.972: INFO: machine-config-daemon-522l4 from openshift-machine-config-operator started at 2020-02-10 19:38:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: splunkforwarder-ds-56rrh from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:16.972: INFO: elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 from openshift-logging started at 2020-02-24 14:53:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: sre-dns-latency-exporter-bk7vm from openshift-monitoring started at 2020-02-10 19:12:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:16.972: INFO: osd-curated-redhat-operators-7b9b69f76c-7tp57 from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Mar  2 21:29:16.972: INFO: velero-878675ff4-6wmq2 from openshift-velero started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container velero ready: true, restart count 0
Mar  2 21:29:16.972: INFO: node-ca-lsnwz from openshift-image-registry started at 2020-02-10 19:24:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:16.972: INFO: ovs-8ptlb from openshift-sdn started at 2020-02-10 19:25:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:16.972: INFO: thanos-querier-64f49bdcf5-rkr2t from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (4 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:29:16.972: INFO: osd-curated-community-operators-7b689d68dc-x2sqj from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Mar  2 21:29:16.972: INFO: node-exporter-gxpxf from openshift-monitoring started at 2020-02-10 19:22:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:16.972: INFO: fluentd-p2gn2 from openshift-logging started at 2020-02-24 14:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:29:16.972: INFO: downloads-5fdf68d856-tsdl7 from openshift-console started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:29:16.972: INFO: validation-webhook-6f8dcdd9fb-tb8jg from openshift-validation-webhook started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:29:16.972: INFO: dns-default-9wkv6 from openshift-dns started at 2020-02-10 19:37:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:16.972: INFO: tuned-tskk8 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:16.972: INFO: openshift-state-metrics-b6755756-hzdgv from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:29:16.972: INFO: sdn-w4cmx from openshift-sdn started at 2020-02-10 19:25:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:16.972: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:16.972: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-199.ec2.internal before test
Mar  2 21:29:17.060: INFO: multus-rxxrb from openshift-multus started at 2020-02-10 19:31:44 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:17.060: INFO: tuned-npnfx from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:17.060: INFO: sre-dns-latency-exporter-s2fkj from openshift-monitoring started at 2020-02-05 15:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:17.060: INFO: dns-default-2qsk9 from openshift-dns started at 2020-02-10 19:35:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:17.060: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-10 19:45:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:29:17.060: INFO: router-default-595f5b77b4-b4hxs from openshift-ingress started at 2020-02-19 15:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container router ready: true, restart count 0
Mar  2 21:29:17.060: INFO: node-ca-8qmpk from openshift-image-registry started at 2020-02-10 19:21:58 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:17.060: INFO: node-exporter-bxmbt from openshift-monitoring started at 2020-02-10 19:22:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:17.060: INFO: sdn-hxmmm from openshift-sdn started at 2020-02-10 19:25:14 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:17.060: INFO: machine-config-daemon-rcprb from openshift-machine-config-operator started at 2020-02-10 19:37:58 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: splunkforwarder-ds-bq459 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:17.060: INFO: ovs-km945 from openshift-sdn started at 2020-02-10 19:26:34 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:17.060: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-10 19:45:02 +0000 UTC (3 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:29:17.060: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-10 19:45:03 +0000 UTC (7 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:29:17.060: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:29:17.060: INFO: prometheus-adapter-56d646d4cd-9hzn5 from openshift-monitoring started at 2020-02-21 10:30:38 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:29:17.060: INFO: fluentd-xbdx7 from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.060: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:29:17.060: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-141-17.ec2.internal before test
Mar  2 21:29:17.149: INFO: tuned-xtm2h from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:29:17.149: INFO: downloads-5fdf68d856-5x2xq from openshift-console started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:29:17.149: INFO: sre-dns-latency-exporter-wdtkb from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container main ready: true, restart count 0
Mar  2 21:29:17.149: INFO: validation-webhook-6f8dcdd9fb-kzqnf from openshift-validation-webhook started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:29:17.149: INFO: splunkforwarder-ds-c8jp7 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:29:17.149: INFO: node-exporter-vzwkp from openshift-monitoring started at 2020-02-10 19:21:31 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:29:17.149: INFO: node-ca-tvvm8 from openshift-image-registry started at 2020-02-10 19:22:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:29:17.149: INFO: sdn-7dgr4 from openshift-sdn started at 2020-02-10 19:25:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:29:17.149: INFO: elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl from openshift-logging started at 2020-02-24 14:53:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: ovs-hrkkz from openshift-sdn started at 2020-02-10 19:27:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:29:17.149: INFO: multus-7rlww from openshift-multus started at 2020-02-10 19:30:19 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:29:17.149: INFO: dns-default-cv5xk from openshift-dns started at 2020-02-10 19:35:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:29:17.149: INFO: fluentd-h9qzb from openshift-logging started at 2020-02-24 14:53:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:29:17.149: INFO: machine-config-daemon-95wk2 from openshift-machine-config-operator started at 2020-02-10 19:37:26 +0000 UTC (2 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: osd-curated-certified-operators-65b5f55f88-rc57s from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Mar  2 21:29:17.149: INFO: cluster-logging-operator-758c4648b-rcdxn from openshift-logging started at 2020-02-24 14:50:53 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Mar  2 21:29:17.149: INFO: thanos-querier-64f49bdcf5-ldl9b from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (4 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:29:17.149: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:29:17.149: INFO: elasticsearch-operator-594788b869-hkp9l from openshift-logging started at 2020-02-24 14:50:24 +0000 UTC (1 container statuses recorded)
Mar  2 21:29:17.149: INFO: 	Container elasticsearch-operator ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-762dcc57-c375-465c-b31c-6ad95f68159a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-762dcc57-c375-465c-b31c-6ad95f68159a off the node ip-10-0-130-27.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-762dcc57-c375-465c-b31c-6ad95f68159a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:34:37.491: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-995" for this suite.
Mar  2 21:34:59.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:35:02.956: INFO: namespace sched-pred-995 deletion completed in 25.354969841s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:347.230 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:35:02.962: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Mar  2 21:35:03.088: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 21:36:03.685: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:36:03.709: INFO: Starting informer...
STEP: Starting pod...
Mar  2 21:36:03.749: INFO: Pod is running on ip-10-0-130-27.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  2 21:36:03.794: INFO: Pod wasn't evicted. Proceeding
Mar  2 21:36:03.794: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  2 21:37:18.857: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:37:18.861: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4896" for this suite.
Mar  2 21:37:30.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:37:34.269: INFO: namespace taint-single-pod-4896 deletion completed in 15.35747314s

• [SLOW TEST:151.308 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:37:34.283: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:37:40.677: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-1677" for this suite.
Mar  2 21:37:46.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:37:50.209: INFO: namespace namespaces-1677 deletion completed in 9.354576401s
STEP: Destroying namespace "nsdeletetest-5602" for this suite.
Mar  2 21:37:50.222: INFO: Namespace nsdeletetest-5602 was already deleted
STEP: Destroying namespace "nsdeletetest-5422" for this suite.
Mar  2 21:37:56.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:37:59.579: INFO: namespace nsdeletetest-5422 deletion completed in 9.356922059s

• [SLOW TEST:25.297 seconds]
[sig-api-machinery] Namespaces [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:37:59.582: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  2 21:37:59.686: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 21:37:59.975: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:38:00.001: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-224.ec2.internal before test
Mar  2 21:38:00.069: INFO: elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:38:00.069: INFO: node-ca-5lj4v from openshift-image-registry started at 2020-02-10 19:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.069: INFO: machine-config-daemon-xfc2n from openshift-machine-config-operator started at 2020-02-10 19:37:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.069: INFO: validation-webhook-6f8dcdd9fb-94rfx from openshift-validation-webhook started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:38:00.069: INFO: tuned-cvjkf from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.069: INFO: kibana-85fc498c95-z56md from openshift-logging started at 2020-02-24 14:53:23 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container kibana ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 	Container kibana-proxy ready: true, restart count 0
Mar  2 21:38:00.069: INFO: fluentd-vgqp4 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:38:00.069: INFO: dns-default-78jzw from openshift-dns started at 2020-02-10 19:36:41 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.069: INFO: splunkforwarder-ds-2j9mv from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.069: INFO: node-exporter-s8c2n from openshift-monitoring started at 2020-02-10 19:21:13 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.069: INFO: ovs-m9222 from openshift-sdn started at 2020-02-10 19:28:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.069: INFO: rbac-permissions-operator-7ddb7b95c-tf5qr from openshift-rbac-permissions started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Mar  2 21:38:00.069: INFO: sre-dns-latency-exporter-sds5m from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.069: INFO: sdn-qrnvj from openshift-sdn started at 2020-02-10 19:25:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.069: INFO: multus-c92b7 from openshift-multus started at 2020-02-10 19:33:07 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.069: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.069: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-27.ec2.internal before test
Mar  2 21:38:00.123: INFO: sre-dns-latency-exporter-9hb4h from openshift-monitoring started at 2020-03-02 21:36:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.123: INFO: rbac-permissions-operator-registry-qdnz8 from openshift-rbac-permissions started at 2020-02-10 20:09:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:38:00.123: INFO: sdn-bg7s7 from openshift-sdn started at 2020-02-10 19:26:18 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.123: INFO: tuned-4q8sm from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.123: INFO: configure-alertmanager-operator-registry-c4mtq from openshift-monitoring started at 2020-02-28 15:20:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:38:00.123: INFO: dns-default-t8vhr from openshift-dns started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.123: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.123: INFO: splunkforwarder-ds-qkm6f from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.123: INFO: multus-2m27b from openshift-multus started at 2020-02-10 19:29:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.123: INFO: machine-config-daemon-r8rp4 from openshift-machine-config-operator started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.123: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.123: INFO: ovs-bjftz from openshift-sdn started at 2020-02-10 19:28:52 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.123: INFO: node-ca-8tlsw from openshift-image-registry started at 2020-03-02 21:36:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.123: INFO: node-exporter-n5znc from openshift-monitoring started at 2020-02-10 19:21:51 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.123: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.123: INFO: fluentd-lxcgg from openshift-logging started at 2020-03-02 21:36:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:38:00.123: INFO: splunk-forwarder-operator-catalog-xmff9 from openshift-splunk-forwarder-operator started at 2020-02-24 18:15:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.123: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:38:00.123: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-159.ec2.internal before test
Mar  2 21:38:00.254: INFO: ovs-r2phg from openshift-sdn started at 2020-02-10 19:26:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.254: INFO: sdn-xpc4d from openshift-sdn started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container sdn ready: true, restart count 1
Mar  2 21:38:00.254: INFO: tuned-7cwm9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.254: INFO: kube-state-metrics-67bcd5775b-t85wt from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container kube-state-metrics ready: true, restart count 2
Mar  2 21:38:00.254: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-10 19:52:07 +0000 UTC (7 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:38:00.254: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-10 19:52:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.254: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:38:00.254: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:38:00.254: INFO: sre-dns-latency-exporter-b7brb from openshift-monitoring started at 2020-02-05 15:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.255: INFO: node-ca-2c7vd from openshift-image-registry started at 2020-02-10 19:23:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.255: INFO: image-registry-776d8848f-4dkxq from openshift-image-registry started at 2020-02-10 19:52:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:38:00.255: INFO: splunkforwarder-ds-vg9wk from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.255: INFO: node-exporter-w56xf from openshift-monitoring started at 2020-02-10 19:22:24 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.255: INFO: multus-blrhd from openshift-multus started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.255: INFO: dns-default-95h6x from openshift-dns started at 2020-02-10 19:37:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.255: INFO: machine-config-daemon-b6c6q from openshift-machine-config-operator started at 2020-02-10 19:38:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.255: INFO: grafana-58f6c99667-jb9f4 from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:38:00.255: INFO: telemeter-client-78c68b58db-2fl8t from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:38:00.255: INFO: fluentd-rzd9d from openshift-logging started at 2020-02-24 14:53:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.255: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:38:00.255: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-248.ec2.internal before test
Mar  2 21:38:00.354: INFO: fluentd-2mc9b from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.354: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:38:00.354: INFO: splunkforwarder-ds-f2c94 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.354: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.354: INFO: sre-build-test-1583183460-z47qg from openshift-build-test started at 2020-03-02 21:11:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.354: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:38:00.355: INFO: multus-zh5hw from openshift-multus started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-machine-api-status-exporter-1-j9zvt from openshift-monitoring started at 2020-02-10 20:00:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container main ready: true, restart count 1
Mar  2 21:38:00.355: INFO: prometheus-adapter-56d646d4cd-bs2fc from openshift-monitoring started at 2020-02-21 10:30:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-stuck-ebs-vols-1-d7lg9 from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.355: INFO: managed-velero-operator-589fbb966f-4gwmw from openshift-velero started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container managed-velero-operator ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-dns-latency-exporter-qksxc from openshift-monitoring started at 2020-02-05 21:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.355: INFO: node-ca-m4bjs from openshift-image-registry started at 2020-02-10 19:21:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.355: INFO: ovs-lnrws from openshift-sdn started at 2020-02-10 19:28:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.355: INFO: tuned-m4fk9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.355: INFO: dns-default-rj656 from openshift-dns started at 2020-02-10 19:38:43 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.355: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-ebs-iops-reporter-1-smq2g from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.355: INFO: splunk-forwarder-operator-f6d6c9c57-vv9mk from openshift-splunk-forwarder-operator started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-build-test-1583176260-jdk6t from openshift-build-test started at 2020-03-02 19:11:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:38:00.355: INFO: router-default-595f5b77b4-kz6pp from openshift-ingress started at 2020-02-19 15:08:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container router ready: true, restart count 0
Mar  2 21:38:00.355: INFO: configure-alertmanager-operator-57f5b56487-jp6j4 from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sre-build-test-1583179860-8bmsn from openshift-build-test started at 2020-03-02 20:11:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:38:00.355: INFO: node-exporter-lqkf8 from openshift-monitoring started at 2020-02-10 19:23:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.355: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.355: INFO: sdn-cqj65 from openshift-sdn started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.355: INFO: machine-config-daemon-hk5js from openshift-machine-config-operator started at 2020-02-10 19:38:33 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.355: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.355: INFO: prometheus-operator-6d474cc98b-pkw6b from openshift-monitoring started at 2020-02-27 16:36:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.355: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:38:00.355: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-145.ec2.internal before test
Mar  2 21:38:00.432: INFO: node-ca-lsnwz from openshift-image-registry started at 2020-02-10 19:24:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.432: INFO: velero-878675ff4-6wmq2 from openshift-velero started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container velero ready: true, restart count 0
Mar  2 21:38:00.432: INFO: node-exporter-gxpxf from openshift-monitoring started at 2020-02-10 19:22:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.432: INFO: ovs-8ptlb from openshift-sdn started at 2020-02-10 19:25:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.432: INFO: thanos-querier-64f49bdcf5-rkr2t from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (4 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:38:00.432: INFO: osd-curated-community-operators-7b689d68dc-x2sqj from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Mar  2 21:38:00.432: INFO: downloads-5fdf68d856-tsdl7 from openshift-console started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:38:00.432: INFO: fluentd-p2gn2 from openshift-logging started at 2020-02-24 14:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:38:00.432: INFO: validation-webhook-6f8dcdd9fb-tb8jg from openshift-validation-webhook started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:38:00.432: INFO: sdn-w4cmx from openshift-sdn started at 2020-02-10 19:25:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.432: INFO: dns-default-9wkv6 from openshift-dns started at 2020-02-10 19:37:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.432: INFO: tuned-tskk8 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.432: INFO: openshift-state-metrics-b6755756-hzdgv from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:38:00.432: INFO: sre-dns-latency-exporter-bk7vm from openshift-monitoring started at 2020-02-10 19:12:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.432: INFO: multus-fmzkv from openshift-multus started at 2020-02-10 19:27:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.432: INFO: machine-config-daemon-522l4 from openshift-machine-config-operator started at 2020-02-10 19:38:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: splunkforwarder-ds-56rrh from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.432: INFO: elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 from openshift-logging started at 2020-02-24 14:53:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:38:00.432: INFO: osd-curated-redhat-operators-7b9b69f76c-7tp57 from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.432: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Mar  2 21:38:00.432: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-199.ec2.internal before test
Mar  2 21:38:00.509: INFO: ovs-km945 from openshift-sdn started at 2020-02-10 19:26:34 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.509: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-10 19:45:02 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:38:00.509: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-10 19:45:03 +0000 UTC (7 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:38:00.509: INFO: prometheus-adapter-56d646d4cd-9hzn5 from openshift-monitoring started at 2020-02-21 10:30:38 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:38:00.509: INFO: fluentd-xbdx7 from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:38:00.509: INFO: multus-rxxrb from openshift-multus started at 2020-02-10 19:31:44 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.509: INFO: tuned-npnfx from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.509: INFO: sre-dns-latency-exporter-s2fkj from openshift-monitoring started at 2020-02-05 15:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.509: INFO: dns-default-2qsk9 from openshift-dns started at 2020-02-10 19:35:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.509: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.509: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.510: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-10 19:45:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:38:00.510: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:38:00.510: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:38:00.510: INFO: router-default-595f5b77b4-b4hxs from openshift-ingress started at 2020-02-19 15:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container router ready: true, restart count 0
Mar  2 21:38:00.510: INFO: node-ca-8qmpk from openshift-image-registry started at 2020-02-10 19:21:58 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.510: INFO: node-exporter-bxmbt from openshift-monitoring started at 2020-02-10 19:22:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.510: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.510: INFO: sdn-hxmmm from openshift-sdn started at 2020-02-10 19:25:14 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.510: INFO: machine-config-daemon-rcprb from openshift-machine-config-operator started at 2020-02-10 19:37:58 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.510: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.510: INFO: splunkforwarder-ds-bq459 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.510: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.510: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-141-17.ec2.internal before test
Mar  2 21:38:00.598: INFO: thanos-querier-64f49bdcf5-ldl9b from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (4 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:38:00.598: INFO: elasticsearch-operator-594788b869-hkp9l from openshift-logging started at 2020-02-24 14:50:24 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Mar  2 21:38:00.598: INFO: tuned-xtm2h from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:38:00.598: INFO: downloads-5fdf68d856-5x2xq from openshift-console started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:38:00.598: INFO: sre-dns-latency-exporter-wdtkb from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container main ready: true, restart count 0
Mar  2 21:38:00.598: INFO: validation-webhook-6f8dcdd9fb-kzqnf from openshift-validation-webhook started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:38:00.598: INFO: splunkforwarder-ds-c8jp7 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:38:00.598: INFO: node-exporter-vzwkp from openshift-monitoring started at 2020-02-10 19:21:31 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:38:00.598: INFO: node-ca-tvvm8 from openshift-image-registry started at 2020-02-10 19:22:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:38:00.598: INFO: sdn-7dgr4 from openshift-sdn started at 2020-02-10 19:25:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:38:00.598: INFO: elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl from openshift-logging started at 2020-02-24 14:53:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: ovs-hrkkz from openshift-sdn started at 2020-02-10 19:27:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:38:00.598: INFO: multus-7rlww from openshift-multus started at 2020-02-10 19:30:19 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:38:00.598: INFO: dns-default-cv5xk from openshift-dns started at 2020-02-10 19:35:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:38:00.598: INFO: fluentd-h9qzb from openshift-logging started at 2020-02-24 14:53:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:38:00.598: INFO: machine-config-daemon-95wk2 from openshift-machine-config-operator started at 2020-02-10 19:37:26 +0000 UTC (2 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:38:00.598: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:38:00.598: INFO: osd-curated-certified-operators-65b5f55f88-rc57s from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Mar  2 21:38:00.598: INFO: cluster-logging-operator-758c4648b-rcdxn from openshift-logging started at 2020-02-24 14:50:53 +0000 UTC (1 container statuses recorded)
Mar  2 21:38:00.598: INFO: 	Container cluster-logging-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f89a713e324397], Reason = [FailedScheduling], Message = [0/10 nodes are available: 3 node(s) were unschedulable, 7 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f89a713eb3a938], Reason = [FailedScheduling], Message = [0/10 nodes are available: 3 node(s) were unschedulable, 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:38:01.764: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-2550" for this suite.
Mar  2 21:38:07.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:38:11.200: INFO: namespace sched-pred-2550 deletion completed in 9.357550394s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.618 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:38:11.204: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:38:11.524: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 21:38:11.670: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:11.670: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:11.670: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:11.684: INFO: Number of nodes with available pods: 0
Mar  2 21:38:11.684: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:12.752: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:12.752: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:12.752: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:12.766: INFO: Number of nodes with available pods: 0
Mar  2 21:38:12.766: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:13.752: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:13.752: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:13.752: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:13.765: INFO: Number of nodes with available pods: 0
Mar  2 21:38:13.765: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:14.789: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:14.789: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:14.789: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:14.802: INFO: Number of nodes with available pods: 0
Mar  2 21:38:14.802: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:15.752: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:15.752: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:15.752: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:15.766: INFO: Number of nodes with available pods: 0
Mar  2 21:38:15.766: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:16.785: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:16.785: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:16.785: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:16.809: INFO: Number of nodes with available pods: 0
Mar  2 21:38:16.809: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:17.762: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:17.762: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:17.762: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:17.776: INFO: Number of nodes with available pods: 0
Mar  2 21:38:17.776: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:18.752: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:18.752: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:18.752: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:18.765: INFO: Number of nodes with available pods: 0
Mar  2 21:38:18.765: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:38:19.763: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:19.763: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:19.763: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:19.778: INFO: Number of nodes with available pods: 3
Mar  2 21:38:19.778: INFO: Node ip-10-0-130-27.ec2.internal is running more than one daemon pod
Mar  2 21:38:20.753: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:20.753: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:20.753: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:20.768: INFO: Number of nodes with available pods: 7
Mar  2 21:38:20.768: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:20.964: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:21.023: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:21.023: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:21.023: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:22.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:22.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:22.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.051: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:23.107: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:23.107: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:23.107: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:24.096: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:24.096: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:24.096: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:26.112: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:26.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:26.112: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.056: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:27.114: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:27.114: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:27.114: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:28.105: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:28.105: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:28.105: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:29.136: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:29.136: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:29.136: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:30.142: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:30.142: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:30.142: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-js5tg. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Pod daemon-set-js5tg is not available
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:31.127: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:31.127: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:31.127: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.048: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:32.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:32.105: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:32.105: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:32.105: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.048: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:33.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:33.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:33.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:33.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.047: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:34.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:34.115: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:34.115: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:34.115: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.048: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:35.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:35.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:35.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:35.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.049: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:36.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:36.106: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:36.106: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:36.106: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.047: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:37.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:37.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:37.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:37.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.048: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:38.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:38.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:38.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:38.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.049: INFO: Pod daemon-set-xn5qd is not available
Mar  2 21:38:39.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:39.098: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:39.098: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:39.098: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:40.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.049: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.049: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:40.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:40.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:40.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:41.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:41.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:41.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-pklpd. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.049: INFO: Pod daemon-set-pklpd is not available
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:42.086: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:42.086: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:42.086: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:43.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:43.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:43.047: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:43.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:43.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:43.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:43.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:43.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:43.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:44.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:44.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:44.048: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:44.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:44.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:44.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:44.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:44.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:44.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:45.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:45.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:45.047: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:45.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:45.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:45.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:45.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:45.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:45.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:46.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:46.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:46.048: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:46.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:46.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:46.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:46.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:46.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:46.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:47.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:47.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:47.048: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:47.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:47.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:47.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:47.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:47.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:47.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:48.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:48.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:48.047: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:48.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:48.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:48.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:48.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:48.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:48.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:49.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:49.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:49.048: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:49.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:49.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:49.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:49.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:49.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:49.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:50.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:50.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:50.047: INFO: Pod daemon-set-kcxv8 is not available
Mar  2 21:38:50.047: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:50.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:50.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:50.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:50.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:50.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:51.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:51.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:51.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:51.048: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:51.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:51.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:51.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:51.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:51.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:52.058: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:52.058: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:52.058: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:52.058: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:52.058: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:52.058: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:52.137: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:52.138: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:52.138: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:53.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:53.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:53.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:53.048: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:53.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:53.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:53.126: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:53.126: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:53.126: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:54.260: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:54.260: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:54.260: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:54.260: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:54.260: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:54.260: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:54.328: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:54.328: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:54.328: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:55.058: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:55.058: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:55.058: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:55.058: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:55.058: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:55.058: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:55.147: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:55.147: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:55.147: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:56.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:56.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:56.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:56.048: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:56.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:56.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:56.115: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:56.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:56.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:57.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:57.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:57.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:57.048: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:57.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:57.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:57.105: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:57.105: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:57.105: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:58.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:58.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:58.048: INFO: Wrong image for pod: daemon-set-qnn5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:58.048: INFO: Pod daemon-set-qnn5w is not available
Mar  2 21:38:58.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:58.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:58.105: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:58.105: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:58.105: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:59.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:59.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:59.048: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:38:59.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:59.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:38:59.176: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:59.176: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:38:59.176: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:00.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:00.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:00.047: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:00.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:00.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:00.148: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:00.148: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:00.148: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:01.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:01.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:01.047: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:01.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:01.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:01.157: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:01.157: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:01.157: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:02.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:02.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:02.049: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:02.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:02.049: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:02.159: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:02.159: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:02.159: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:03.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:03.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:03.047: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:03.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:03.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:03.125: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:03.125: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:03.125: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:04.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:04.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:04.048: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:04.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:04.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:04.147: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:04.147: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:04.147: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:05.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:05.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:05.048: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:05.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:05.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:05.340: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:05.340: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:05.340: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:06.053: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:06.053: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:06.053: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:06.053: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:06.053: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:06.143: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:06.143: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:06.143: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:07.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:07.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:07.048: INFO: Pod daemon-set-qcgc5 is not available
Mar  2 21:39:07.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:07.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:07.148: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:07.148: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:07.148: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:08.054: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:08.054: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:08.054: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:08.054: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:08.185: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:08.186: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:08.186: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:09.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:09.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:09.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:09.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:09.049: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:09.248: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:09.248: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:09.248: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:10.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:10.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:10.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:10.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:10.048: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:10.140: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:10.140: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:10.140: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:11.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:11.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:11.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:11.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:11.048: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:11.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:11.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:11.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:12.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:12.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:12.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:12.047: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:12.047: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:12.125: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:12.125: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:12.125: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:13.060: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:13.060: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:13.060: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:13.060: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:13.060: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:13.148: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:13.148: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:13.149: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:14.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:14.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:14.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:14.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:14.048: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:14.126: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:14.127: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:14.127: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:15.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:15.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:15.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:15.048: INFO: Wrong image for pod: daemon-set-xqs7g. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:15.048: INFO: Pod daemon-set-xqs7g is not available
Mar  2 21:39:15.142: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:15.142: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:15.142: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:16.048: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:16.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:16.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:16.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:16.137: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:16.137: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:16.137: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:17.048: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:17.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:17.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:17.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:17.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:17.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:17.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:18.068: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:18.068: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:18.068: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:18.068: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:18.253: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:18.254: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:18.254: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:19.076: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:19.076: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:19.076: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:19.076: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:19.208: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:19.208: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:19.208: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:20.048: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:20.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:20.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:20.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:20.128: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:20.128: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:20.128: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:21.049: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:21.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:21.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:21.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:21.106: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:21.106: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:21.106: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:22.048: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:22.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:22.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:22.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:22.106: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:22.106: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:22.106: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:23.047: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:23.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:23.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:23.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:23.106: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:23.107: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:23.107: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:24.048: INFO: Pod daemon-set-62xk8 is not available
Mar  2 21:39:24.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:24.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:24.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:24.093: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:24.093: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:24.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:25.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:25.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:25.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:25.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:26.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:26.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:26.049: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:26.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:26.096: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:26.096: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:26.096: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:27.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:27.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:27.048: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:27.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:27.116: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:27.116: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:27.116: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:28.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:28.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:28.047: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:28.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:28.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:28.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:28.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:29.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:29.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:29.047: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:29.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:29.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:29.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:29.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:30.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:30.049: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:30.049: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:30.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:30.095: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:30.096: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:30.096: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:31.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:31.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:31.047: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:31.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:31.093: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:31.093: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:31.093: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:32.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:32.047: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:32.047: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:32.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:32.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:32.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:32.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:33.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:33.048: INFO: Wrong image for pod: daemon-set-jvwqw. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:33.048: INFO: Pod daemon-set-jvwqw is not available
Mar  2 21:39:33.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:33.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:33.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:33.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:34.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:34.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:34.047: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:34.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:34.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:34.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:35.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:35.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:35.048: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:35.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:35.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:35.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:36.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:36.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:36.048: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:36.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:36.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:36.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:37.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:37.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:37.047: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:37.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:37.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:37.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:38.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:38.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:38.047: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:38.103: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:38.103: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:38.103: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:39.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:39.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:39.048: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:39.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:39.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:39.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:40.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:40.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:40.048: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:40.115: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:40.115: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:40.115: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:41.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:41.047: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:41.047: INFO: Pod daemon-set-tvwvp is not available
Mar  2 21:39:41.115: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:41.115: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:41.115: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:42.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:42.048: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:42.126: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:42.126: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:42.126: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:43.049: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:43.049: INFO: Wrong image for pod: daemon-set-sl262. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:43.049: INFO: Pod daemon-set-sl262 is not available
Mar  2 21:39:43.127: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:43.128: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:43.128: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:44.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:44.048: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:44.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:44.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:44.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:45.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:45.047: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:45.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:45.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:45.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:46.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:46.048: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:46.097: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:46.097: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:46.097: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:47.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:47.048: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:47.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:47.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:47.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:48.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:48.047: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:49.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:49.048: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:49.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:49.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:49.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:50.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:50.047: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:50.093: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:50.093: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:50.093: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:51.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:51.047: INFO: Pod daemon-set-g4crm is not available
Mar  2 21:39:51.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:51.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:51.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:52.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:52.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:52.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:52.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:53.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:53.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:53.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:53.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:54.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:54.048: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:54.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:54.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:54.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:55.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:55.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:55.082: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:55.082: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:55.082: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:56.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:56.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:56.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:56.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:56.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:57.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:57.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:57.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:57.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:57.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:58.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:58.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:58.082: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:58.082: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:58.082: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:59.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:39:59.048: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:39:59.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:59.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:39:59.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:00.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:40:00.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:40:00.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:00.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:00.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:01.047: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:40:01.047: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:40:01.084: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:01.084: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:01.084: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:02.048: INFO: Wrong image for pod: daemon-set-b8cpq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 21:40:02.048: INFO: Pod daemon-set-b8cpq is not available
Mar  2 21:40:02.085: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:02.085: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:02.085: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:03.047: INFO: Pod daemon-set-n4c7l is not available
Mar  2 21:40:03.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:03.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:03.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.047: INFO: Pod daemon-set-n4c7l is not available
Mar  2 21:40:04.083: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.083: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.083: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  2 21:40:04.108: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.108: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.108: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:04.122: INFO: Number of nodes with available pods: 6
Mar  2 21:40:04.122: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:05.168: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:05.168: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:05.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:05.182: INFO: Number of nodes with available pods: 6
Mar  2 21:40:05.182: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:06.169: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:06.169: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:06.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:06.182: INFO: Number of nodes with available pods: 6
Mar  2 21:40:06.182: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:07.169: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:07.169: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:07.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:07.182: INFO: Number of nodes with available pods: 6
Mar  2 21:40:07.182: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:08.168: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:08.168: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:08.168: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:08.182: INFO: Number of nodes with available pods: 6
Mar  2 21:40:08.182: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:09.169: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:09.169: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:09.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:09.182: INFO: Number of nodes with available pods: 6
Mar  2 21:40:09.182: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:10.169: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:10.169: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:10.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:10.183: INFO: Number of nodes with available pods: 6
Mar  2 21:40:10.183: INFO: Node ip-10-0-135-248.ec2.internal is running more than one daemon pod
Mar  2 21:40:11.169: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:11.169: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:11.169: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:40:11.182: INFO: Number of nodes with available pods: 7
Mar  2 21:40:11.182: INFO: Number of running nodes: 7, number of available pods: 7
[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2266, will wait for the garbage collector to delete the pods
Mar  2 21:40:11.329: INFO: Deleting DaemonSet.extensions daemon-set took: 17.198569ms
Mar  2 21:40:11.529: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.290894ms
Mar  2 21:40:23.543: INFO: Number of nodes with available pods: 0
Mar  2 21:40:23.543: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:40:23.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2266/daemonsets","resourceVersion":"15414820"},"items":null}

Mar  2 21:40:23.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2266/pods","resourceVersion":"15414820"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:40:23.695: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-2266" for this suite.
Mar  2 21:40:31.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:40:35.077: INFO: namespace daemonsets-2266 deletion completed in 11.356553719s

• [SLOW TEST:143.872 seconds]
[sig-apps] Daemon set [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:40:35.079: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:40:35.340: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  2 21:40:35.369: INFO: Number of nodes with available pods: 0
Mar  2 21:40:35.369: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  2 21:40:35.463: INFO: Number of nodes with available pods: 0
Mar  2 21:40:35.463: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:36.478: INFO: Number of nodes with available pods: 0
Mar  2 21:40:36.478: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:37.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:37.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:38.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:38.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:39.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:39.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:40.478: INFO: Number of nodes with available pods: 0
Mar  2 21:40:40.478: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:41.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:41.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:42.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:42.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:43.477: INFO: Number of nodes with available pods: 0
Mar  2 21:40:43.477: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:44.477: INFO: Number of nodes with available pods: 1
Mar  2 21:40:44.477: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  2 21:40:44.558: INFO: Number of nodes with available pods: 0
Mar  2 21:40:44.558: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  2 21:40:44.586: INFO: Number of nodes with available pods: 0
Mar  2 21:40:44.586: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:45.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:45.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:46.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:46.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:47.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:47.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:48.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:48.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:49.599: INFO: Number of nodes with available pods: 0
Mar  2 21:40:49.599: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:50.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:50.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:51.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:51.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:52.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:52.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:53.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:53.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:54.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:54.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:55.642: INFO: Number of nodes with available pods: 0
Mar  2 21:40:55.642: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:56.600: INFO: Number of nodes with available pods: 0
Mar  2 21:40:56.600: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:40:57.600: INFO: Number of nodes with available pods: 1
Mar  2 21:40:57.600: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-288, will wait for the garbage collector to delete the pods
Mar  2 21:40:57.706: INFO: Deleting DaemonSet.extensions daemon-set took: 17.551043ms
Mar  2 21:40:57.806: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.216891ms
Mar  2 21:41:07.821: INFO: Number of nodes with available pods: 0
Mar  2 21:41:07.821: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:41:07.834: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-288/daemonsets","resourceVersion":"15415336"},"items":null}

Mar  2 21:41:07.847: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-288/pods","resourceVersion":"15415336"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:41:08.127: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-288" for this suite.
Mar  2 21:41:14.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:41:17.562: INFO: namespace daemonsets-288 deletion completed in 9.35580291s

• [SLOW TEST:42.483 seconds]
[sig-apps] Daemon set [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:41:17.563: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:41:17.948: INFO: Create a RollingUpdate DaemonSet
Mar  2 21:41:17.963: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 21:41:18.030: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:18.030: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:18.030: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:18.046: INFO: Number of nodes with available pods: 0
Mar  2 21:41:18.046: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:19.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:19.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:19.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:19.118: INFO: Number of nodes with available pods: 0
Mar  2 21:41:19.118: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:20.103: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:20.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:20.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:20.117: INFO: Number of nodes with available pods: 0
Mar  2 21:41:20.117: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:21.104: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:21.104: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:21.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:21.118: INFO: Number of nodes with available pods: 0
Mar  2 21:41:21.118: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:22.103: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:22.103: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:22.103: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:22.117: INFO: Number of nodes with available pods: 0
Mar  2 21:41:22.117: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:23.093: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:23.093: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:23.093: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:23.107: INFO: Number of nodes with available pods: 0
Mar  2 21:41:23.107: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:24.127: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:24.128: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:24.128: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:24.141: INFO: Number of nodes with available pods: 0
Mar  2 21:41:24.141: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:25.124: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:25.124: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:25.124: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:25.138: INFO: Number of nodes with available pods: 0
Mar  2 21:41:25.138: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:26.121: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:26.121: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:26.122: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:26.136: INFO: Number of nodes with available pods: 1
Mar  2 21:41:26.136: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:41:27.103: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:27.103: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:27.104: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:27.118: INFO: Number of nodes with available pods: 7
Mar  2 21:41:27.118: INFO: Number of running nodes: 7, number of available pods: 7
Mar  2 21:41:27.118: INFO: Update the DaemonSet to trigger a rollout
Mar  2 21:41:27.159: INFO: Updating DaemonSet daemon-set
Mar  2 21:41:31.247: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 21:41:31.280: INFO: Updating DaemonSet daemon-set
Mar  2 21:41:31.280: INFO: Make sure DaemonSet rollback is complete
Mar  2 21:41:31.296: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:31.296: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:31.343: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:31.343: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:31.343: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:32.367: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:32.367: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:32.415: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:32.415: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:32.415: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:33.368: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:33.368: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:33.404: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:33.404: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:33.404: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:34.368: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:34.368: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:34.438: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:34.438: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:34.438: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:35.368: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:35.368: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:35.424: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:35.424: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:35.425: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:36.368: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:36.368: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:36.414: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:36.414: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:36.414: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:37.369: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:37.369: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:37.415: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:37.415: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:37.416: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:38.368: INFO: Wrong image for pod: daemon-set-jmwb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 21:41:38.368: INFO: Pod daemon-set-jmwb8 is not available
Mar  2 21:41:38.414: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:38.414: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:38.414: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:39.368: INFO: Pod daemon-set-wl9ht is not available
Mar  2 21:41:39.414: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:39.414: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:41:39.414: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6287, will wait for the garbage collector to delete the pods
Mar  2 21:41:39.520: INFO: Deleting DaemonSet.extensions daemon-set took: 17.257479ms
Mar  2 21:41:39.720: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.36471ms
Mar  2 21:41:52.035: INFO: Number of nodes with available pods: 0
Mar  2 21:41:52.035: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:41:52.048: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6287/daemonsets","resourceVersion":"15415843"},"items":null}

Mar  2 21:41:52.062: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6287/pods","resourceVersion":"15415843"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:41:52.236: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-6287" for this suite.
Mar  2 21:41:58.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:42:01.638: INFO: namespace daemonsets-6287 deletion completed in 9.356006525s

• [SLOW TEST:44.076 seconds]
[sig-apps] Daemon set [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:42:01.639: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:42:37.054: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-3199" for this suite.
Mar  2 21:42:43.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:42:46.622: INFO: namespace namespaces-3199 deletion completed in 9.372574504s
STEP: Destroying namespace "nsdeletetest-462" for this suite.
Mar  2 21:42:46.636: INFO: Namespace nsdeletetest-462 was already deleted
STEP: Destroying namespace "nsdeletetest-5308" for this suite.
Mar  2 21:42:52.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:42:56.006: INFO: namespace nsdeletetest-5308 deletion completed in 9.369878822s

• [SLOW TEST:54.367 seconds]
[sig-api-machinery] Namespaces [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:42:56.015: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  2 21:42:56.291: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 21:42:56.748: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:42:56.804: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-224.ec2.internal before test
Mar  2 21:42:57.014: INFO: node-ca-5lj4v from openshift-image-registry started at 2020-02-10 19:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.014: INFO: machine-config-daemon-xfc2n from openshift-machine-config-operator started at 2020-02-10 19:37:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.014: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.014: INFO: validation-webhook-6f8dcdd9fb-94rfx from openshift-validation-webhook started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:42:57.014: INFO: elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:42:57.014: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:42:57.014: INFO: tuned-cvjkf from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.014: INFO: kibana-85fc498c95-z56md from openshift-logging started at 2020-02-24 14:53:23 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.014: INFO: 	Container kibana ready: true, restart count 0
Mar  2 21:42:57.015: INFO: 	Container kibana-proxy ready: true, restart count 0
Mar  2 21:42:57.015: INFO: fluentd-vgqp4 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:42:57.015: INFO: node-exporter-s8c2n from openshift-monitoring started at 2020-02-10 19:21:13 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.015: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.015: INFO: ovs-m9222 from openshift-sdn started at 2020-02-10 19:28:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.015: INFO: dns-default-78jzw from openshift-dns started at 2020-02-10 19:36:41 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.015: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.015: INFO: splunkforwarder-ds-2j9mv from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.015: INFO: rbac-permissions-operator-7ddb7b95c-tf5qr from openshift-rbac-permissions started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Mar  2 21:42:57.015: INFO: sre-dns-latency-exporter-sds5m from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.015: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.015: INFO: sdn-qrnvj from openshift-sdn started at 2020-02-10 19:25:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.016: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.016: INFO: multus-c92b7 from openshift-multus started at 2020-02-10 19:33:07 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.016: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.016: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-27.ec2.internal before test
Mar  2 21:42:57.101: INFO: fluentd-lxcgg from openshift-logging started at 2020-03-02 21:36:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:42:57.101: INFO: splunk-forwarder-operator-catalog-xmff9 from openshift-splunk-forwarder-operator started at 2020-02-24 18:15:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:42:57.101: INFO: node-exporter-n5znc from openshift-monitoring started at 2020-02-10 19:21:51 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.101: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.101: INFO: sre-dns-latency-exporter-9hb4h from openshift-monitoring started at 2020-03-02 21:36:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.101: INFO: rbac-permissions-operator-registry-qdnz8 from openshift-rbac-permissions started at 2020-02-10 20:09:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:42:57.101: INFO: tuned-4q8sm from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.101: INFO: configure-alertmanager-operator-registry-c4mtq from openshift-monitoring started at 2020-02-28 15:20:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:42:57.101: INFO: sdn-bg7s7 from openshift-sdn started at 2020-02-10 19:26:18 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.101: INFO: splunkforwarder-ds-qkm6f from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.101: INFO: dns-default-t8vhr from openshift-dns started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.101: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.101: INFO: multus-2m27b from openshift-multus started at 2020-02-10 19:29:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.101: INFO: machine-config-daemon-r8rp4 from openshift-machine-config-operator started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.101: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.101: INFO: node-ca-8tlsw from openshift-image-registry started at 2020-03-02 21:36:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.101: INFO: ovs-bjftz from openshift-sdn started at 2020-02-10 19:28:52 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.101: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.101: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-159.ec2.internal before test
Mar  2 21:42:57.198: INFO: sre-dns-latency-exporter-b7brb from openshift-monitoring started at 2020-02-05 15:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.198: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.198: INFO: node-ca-2c7vd from openshift-image-registry started at 2020-02-10 19:23:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.198: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.198: INFO: image-registry-776d8848f-4dkxq from openshift-image-registry started at 2020-02-10 19:52:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.198: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:42:57.198: INFO: splunkforwarder-ds-vg9wk from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.198: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.199: INFO: node-exporter-w56xf from openshift-monitoring started at 2020-02-10 19:22:24 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.199: INFO: multus-blrhd from openshift-multus started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.199: INFO: dns-default-95h6x from openshift-dns started at 2020-02-10 19:37:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.199: INFO: machine-config-daemon-b6c6q from openshift-machine-config-operator started at 2020-02-10 19:38:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.199: INFO: grafana-58f6c99667-jb9f4 from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:42:57.199: INFO: telemeter-client-78c68b58db-2fl8t from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:42:57.199: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:42:57.199: INFO: fluentd-rzd9d from openshift-logging started at 2020-02-24 14:53:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.199: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:42:57.199: INFO: ovs-r2phg from openshift-sdn started at 2020-02-10 19:26:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.200: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.200: INFO: sdn-xpc4d from openshift-sdn started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.200: INFO: 	Container sdn ready: true, restart count 1
Mar  2 21:42:57.200: INFO: tuned-7cwm9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.200: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.200: INFO: kube-state-metrics-67bcd5775b-t85wt from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.200: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:42:57.200: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:42:57.200: INFO: 	Container kube-state-metrics ready: true, restart count 2
Mar  2 21:42:57.200: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-10 19:52:07 +0000 UTC (7 container statuses recorded)
Mar  2 21:42:57.200: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.200: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:42:57.200: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:42:57.200: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:42:57.201: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-10 19:52:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.201: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:42:57.201: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-248.ec2.internal before test
Mar  2 21:42:57.279: INFO: multus-zh5hw from openshift-multus started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-machine-api-status-exporter-1-j9zvt from openshift-monitoring started at 2020-02-10 20:00:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container main ready: true, restart count 1
Mar  2 21:42:57.280: INFO: prometheus-adapter-56d646d4cd-bs2fc from openshift-monitoring started at 2020-02-21 10:30:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:42:57.280: INFO: fluentd-2mc9b from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:42:57.280: INFO: splunkforwarder-ds-f2c94 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-build-test-1583183460-z47qg from openshift-build-test started at 2020-03-02 21:11:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:42:57.280: INFO: node-ca-m4bjs from openshift-image-registry started at 2020-02-10 19:21:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.280: INFO: ovs-lnrws from openshift-sdn started at 2020-02-10 19:28:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.280: INFO: tuned-m4fk9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-stuck-ebs-vols-1-d7lg9 from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.280: INFO: managed-velero-operator-589fbb966f-4gwmw from openshift-velero started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container managed-velero-operator ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-dns-latency-exporter-qksxc from openshift-monitoring started at 2020-02-05 21:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-ebs-iops-reporter-1-smq2g from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.280: INFO: splunk-forwarder-operator-f6d6c9c57-vv9mk from openshift-splunk-forwarder-operator started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sre-build-test-1583176260-jdk6t from openshift-build-test started at 2020-03-02 19:11:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:42:57.280: INFO: dns-default-rj656 from openshift-dns started at 2020-02-10 19:38:43 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.280: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.280: INFO: sdn-cqj65 from openshift-sdn started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.280: INFO: machine-config-daemon-hk5js from openshift-machine-config-operator started at 2020-02-10 19:38:33 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.280: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.281: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.281: INFO: prometheus-operator-6d474cc98b-pkw6b from openshift-monitoring started at 2020-02-27 16:36:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.281: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:42:57.281: INFO: router-default-595f5b77b4-kz6pp from openshift-ingress started at 2020-02-19 15:08:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.281: INFO: 	Container router ready: true, restart count 0
Mar  2 21:42:57.281: INFO: configure-alertmanager-operator-57f5b56487-jp6j4 from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.281: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Mar  2 21:42:57.281: INFO: sre-build-test-1583179860-8bmsn from openshift-build-test started at 2020-03-02 20:11:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.281: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:42:57.281: INFO: node-exporter-lqkf8 from openshift-monitoring started at 2020-02-10 19:23:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.281: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.281: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-145.ec2.internal before test
Mar  2 21:42:57.346: INFO: sre-dns-latency-exporter-bk7vm from openshift-monitoring started at 2020-02-10 19:12:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.346: INFO: multus-fmzkv from openshift-multus started at 2020-02-10 19:27:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.346: INFO: machine-config-daemon-522l4 from openshift-machine-config-operator started at 2020-02-10 19:38:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.346: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.346: INFO: splunkforwarder-ds-56rrh from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.346: INFO: elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 from openshift-logging started at 2020-02-24 14:53:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:42:57.346: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:42:57.346: INFO: osd-curated-redhat-operators-7b9b69f76c-7tp57 from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Mar  2 21:42:57.346: INFO: node-ca-lsnwz from openshift-image-registry started at 2020-02-10 19:24:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.346: INFO: velero-878675ff4-6wmq2 from openshift-velero started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container velero ready: true, restart count 0
Mar  2 21:42:57.346: INFO: node-exporter-gxpxf from openshift-monitoring started at 2020-02-10 19:22:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.346: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.346: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.346: INFO: ovs-8ptlb from openshift-sdn started at 2020-02-10 19:25:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.347: INFO: thanos-querier-64f49bdcf5-rkr2t from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (4 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:42:57.347: INFO: osd-curated-community-operators-7b689d68dc-x2sqj from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Mar  2 21:42:57.347: INFO: downloads-5fdf68d856-tsdl7 from openshift-console started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:42:57.347: INFO: fluentd-p2gn2 from openshift-logging started at 2020-02-24 14:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:42:57.347: INFO: validation-webhook-6f8dcdd9fb-tb8jg from openshift-validation-webhook started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:42:57.347: INFO: sdn-w4cmx from openshift-sdn started at 2020-02-10 19:25:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.347: INFO: dns-default-9wkv6 from openshift-dns started at 2020-02-10 19:37:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.347: INFO: tuned-tskk8 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.347: INFO: openshift-state-metrics-b6755756-hzdgv from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.347: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:42:57.347: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-199.ec2.internal before test
Mar  2 21:42:57.416: INFO: node-exporter-bxmbt from openshift-monitoring started at 2020-02-10 19:22:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.416: INFO: sdn-hxmmm from openshift-sdn started at 2020-02-10 19:25:14 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.416: INFO: machine-config-daemon-rcprb from openshift-machine-config-operator started at 2020-02-10 19:37:58 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: splunkforwarder-ds-bq459 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.416: INFO: node-ca-8qmpk from openshift-image-registry started at 2020-02-10 19:21:58 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.416: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-10 19:45:02 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:42:57.416: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-10 19:45:03 +0000 UTC (7 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:42:57.416: INFO: prometheus-adapter-56d646d4cd-9hzn5 from openshift-monitoring started at 2020-02-21 10:30:38 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:42:57.416: INFO: fluentd-xbdx7 from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:42:57.416: INFO: ovs-km945 from openshift-sdn started at 2020-02-10 19:26:34 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.416: INFO: multus-rxxrb from openshift-multus started at 2020-02-10 19:31:44 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.416: INFO: tuned-npnfx from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.416: INFO: dns-default-2qsk9 from openshift-dns started at 2020-02-10 19:35:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.416: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-10 19:45:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:42:57.416: INFO: router-default-595f5b77b4-b4hxs from openshift-ingress started at 2020-02-19 15:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container router ready: true, restart count 0
Mar  2 21:42:57.416: INFO: sre-dns-latency-exporter-s2fkj from openshift-monitoring started at 2020-02-05 15:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.416: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.416: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-141-17.ec2.internal before test
Mar  2 21:42:57.508: INFO: tuned-xtm2h from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:42:57.508: INFO: downloads-5fdf68d856-5x2xq from openshift-console started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:42:57.508: INFO: sre-dns-latency-exporter-wdtkb from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container main ready: true, restart count 0
Mar  2 21:42:57.508: INFO: validation-webhook-6f8dcdd9fb-kzqnf from openshift-validation-webhook started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:42:57.508: INFO: splunkforwarder-ds-c8jp7 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:42:57.508: INFO: node-exporter-vzwkp from openshift-monitoring started at 2020-02-10 19:21:31 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:42:57.508: INFO: node-ca-tvvm8 from openshift-image-registry started at 2020-02-10 19:22:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:42:57.508: INFO: sdn-7dgr4 from openshift-sdn started at 2020-02-10 19:25:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:42:57.508: INFO: elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl from openshift-logging started at 2020-02-24 14:53:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: ovs-hrkkz from openshift-sdn started at 2020-02-10 19:27:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:42:57.508: INFO: multus-7rlww from openshift-multus started at 2020-02-10 19:30:19 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:42:57.508: INFO: dns-default-cv5xk from openshift-dns started at 2020-02-10 19:35:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:42:57.508: INFO: fluentd-h9qzb from openshift-logging started at 2020-02-24 14:53:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:42:57.508: INFO: machine-config-daemon-95wk2 from openshift-machine-config-operator started at 2020-02-10 19:37:26 +0000 UTC (2 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: osd-curated-certified-operators-65b5f55f88-rc57s from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Mar  2 21:42:57.508: INFO: cluster-logging-operator-758c4648b-rcdxn from openshift-logging started at 2020-02-24 14:50:53 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Mar  2 21:42:57.508: INFO: thanos-querier-64f49bdcf5-ldl9b from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (4 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:42:57.508: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:42:57.508: INFO: elasticsearch-operator-594788b869-hkp9l from openshift-logging started at 2020-02-24 14:50:24 +0000 UTC (1 container statuses recorded)
Mar  2 21:42:57.508: INFO: 	Container elasticsearch-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5f417fc3-86e0-4341-942b-82617ef5be5a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5f417fc3-86e0-4341-942b-82617ef5be5a off the node ip-10-0-130-27.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5f417fc3-86e0-4341-942b-82617ef5be5a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:43:17.764: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-92" for this suite.
Mar  2 21:43:39.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:43:43.195: INFO: namespace sched-pred-92 deletion completed in 25.354629597s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:47.180 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:43:43.204: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  2 21:43:43.318: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 21:43:43.466: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:43:43.491: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-224.ec2.internal before test
Mar  2 21:43:43.543: INFO: dns-default-78jzw from openshift-dns started at 2020-02-10 19:36:41 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.543: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.543: INFO: splunkforwarder-ds-2j9mv from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.543: INFO: node-exporter-s8c2n from openshift-monitoring started at 2020-02-10 19:21:13 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.543: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.543: INFO: ovs-m9222 from openshift-sdn started at 2020-02-10 19:28:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.543: INFO: rbac-permissions-operator-7ddb7b95c-tf5qr from openshift-rbac-permissions started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Mar  2 21:43:43.543: INFO: sre-dns-latency-exporter-sds5m from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.543: INFO: sdn-qrnvj from openshift-sdn started at 2020-02-10 19:25:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.543: INFO: multus-c92b7 from openshift-multus started at 2020-02-10 19:33:07 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.543: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.543: INFO: elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:43:43.544: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:43:43.544: INFO: node-ca-5lj4v from openshift-image-registry started at 2020-02-10 19:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.544: INFO: machine-config-daemon-xfc2n from openshift-machine-config-operator started at 2020-02-10 19:37:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.544: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.544: INFO: validation-webhook-6f8dcdd9fb-94rfx from openshift-validation-webhook started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:43:43.544: INFO: tuned-cvjkf from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.544: INFO: kibana-85fc498c95-z56md from openshift-logging started at 2020-02-24 14:53:23 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container kibana ready: true, restart count 0
Mar  2 21:43:43.544: INFO: 	Container kibana-proxy ready: true, restart count 0
Mar  2 21:43:43.544: INFO: fluentd-vgqp4 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.544: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:43:43.544: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-27.ec2.internal before test
Mar  2 21:43:43.586: INFO: ovs-bjftz from openshift-sdn started at 2020-02-10 19:28:52 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.586: INFO: node-ca-8tlsw from openshift-image-registry started at 2020-03-02 21:36:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.586: INFO: node-exporter-n5znc from openshift-monitoring started at 2020-02-10 19:21:51 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.586: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.586: INFO: fluentd-lxcgg from openshift-logging started at 2020-03-02 21:36:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:43:43.586: INFO: splunk-forwarder-operator-catalog-xmff9 from openshift-splunk-forwarder-operator started at 2020-02-24 18:15:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:43:43.586: INFO: sre-dns-latency-exporter-9hb4h from openshift-monitoring started at 2020-03-02 21:36:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.586: INFO: rbac-permissions-operator-registry-qdnz8 from openshift-rbac-permissions started at 2020-02-10 20:09:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:43:43.586: INFO: configure-alertmanager-operator-registry-c4mtq from openshift-monitoring started at 2020-02-28 15:20:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:43:43.586: INFO: sdn-bg7s7 from openshift-sdn started at 2020-02-10 19:26:18 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.586: INFO: tuned-4q8sm from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.586: INFO: dns-default-t8vhr from openshift-dns started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.586: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.586: INFO: splunkforwarder-ds-qkm6f from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.586: INFO: multus-2m27b from openshift-multus started at 2020-02-10 19:29:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.586: INFO: machine-config-daemon-r8rp4 from openshift-machine-config-operator started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.586: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.586: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.586: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-159.ec2.internal before test
Mar  2 21:43:43.638: INFO: dns-default-95h6x from openshift-dns started at 2020-02-10 19:37:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.638: INFO: machine-config-daemon-b6c6q from openshift-machine-config-operator started at 2020-02-10 19:38:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: grafana-58f6c99667-jb9f4 from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: telemeter-client-78c68b58db-2fl8t from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:43:43.638: INFO: fluentd-rzd9d from openshift-logging started at 2020-02-24 14:53:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:43:43.638: INFO: node-exporter-w56xf from openshift-monitoring started at 2020-02-10 19:22:24 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.638: INFO: multus-blrhd from openshift-multus started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.638: INFO: tuned-7cwm9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.638: INFO: kube-state-metrics-67bcd5775b-t85wt from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container kube-state-metrics ready: true, restart count 2
Mar  2 21:43:43.638: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-10 19:52:07 +0000 UTC (7 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:43:43.638: INFO: ovs-r2phg from openshift-sdn started at 2020-02-10 19:26:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.638: INFO: sdn-xpc4d from openshift-sdn started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container sdn ready: true, restart count 1
Mar  2 21:43:43.638: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-10 19:52:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:43:43.638: INFO: image-registry-776d8848f-4dkxq from openshift-image-registry started at 2020-02-10 19:52:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:43:43.638: INFO: splunkforwarder-ds-vg9wk from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.638: INFO: sre-dns-latency-exporter-b7brb from openshift-monitoring started at 2020-02-05 15:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.638: INFO: node-ca-2c7vd from openshift-image-registry started at 2020-02-10 19:23:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.638: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.638: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-248.ec2.internal before test
Mar  2 21:43:43.690: INFO: sdn-cqj65 from openshift-sdn started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.690: INFO: machine-config-daemon-hk5js from openshift-machine-config-operator started at 2020-02-10 19:38:33 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.690: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.690: INFO: prometheus-operator-6d474cc98b-pkw6b from openshift-monitoring started at 2020-02-27 16:36:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:43:43.690: INFO: router-default-595f5b77b4-kz6pp from openshift-ingress started at 2020-02-19 15:08:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container router ready: true, restart count 0
Mar  2 21:43:43.690: INFO: configure-alertmanager-operator-57f5b56487-jp6j4 from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Mar  2 21:43:43.690: INFO: sre-build-test-1583179860-8bmsn from openshift-build-test started at 2020-03-02 20:11:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:43:43.690: INFO: node-exporter-lqkf8 from openshift-monitoring started at 2020-02-10 19:23:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.690: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.691: INFO: multus-zh5hw from openshift-multus started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-machine-api-status-exporter-1-j9zvt from openshift-monitoring started at 2020-02-10 20:00:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container main ready: true, restart count 1
Mar  2 21:43:43.691: INFO: prometheus-adapter-56d646d4cd-bs2fc from openshift-monitoring started at 2020-02-21 10:30:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:43:43.691: INFO: fluentd-2mc9b from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:43:43.691: INFO: splunkforwarder-ds-f2c94 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-build-test-1583183460-z47qg from openshift-build-test started at 2020-03-02 21:11:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:43:43.691: INFO: node-ca-m4bjs from openshift-image-registry started at 2020-02-10 19:21:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.691: INFO: ovs-lnrws from openshift-sdn started at 2020-02-10 19:28:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.691: INFO: tuned-m4fk9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-stuck-ebs-vols-1-d7lg9 from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.691: INFO: managed-velero-operator-589fbb966f-4gwmw from openshift-velero started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container managed-velero-operator ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-dns-latency-exporter-qksxc from openshift-monitoring started at 2020-02-05 21:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-ebs-iops-reporter-1-smq2g from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.691: INFO: splunk-forwarder-operator-f6d6c9c57-vv9mk from openshift-splunk-forwarder-operator started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Mar  2 21:43:43.691: INFO: sre-build-test-1583176260-jdk6t from openshift-build-test started at 2020-03-02 19:11:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:43:43.691: INFO: dns-default-rj656 from openshift-dns started at 2020-02-10 19:38:43 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.691: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.691: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.691: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-145.ec2.internal before test
Mar  2 21:43:43.742: INFO: downloads-5fdf68d856-tsdl7 from openshift-console started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:43:43.742: INFO: fluentd-p2gn2 from openshift-logging started at 2020-02-24 14:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:43:43.742: INFO: validation-webhook-6f8dcdd9fb-tb8jg from openshift-validation-webhook started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:43:43.742: INFO: openshift-state-metrics-b6755756-hzdgv from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:43:43.742: INFO: sdn-w4cmx from openshift-sdn started at 2020-02-10 19:25:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.742: INFO: dns-default-9wkv6 from openshift-dns started at 2020-02-10 19:37:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.742: INFO: tuned-tskk8 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.742: INFO: splunkforwarder-ds-56rrh from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.742: INFO: elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 from openshift-logging started at 2020-02-24 14:53:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: sre-dns-latency-exporter-bk7vm from openshift-monitoring started at 2020-02-10 19:12:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.742: INFO: multus-fmzkv from openshift-multus started at 2020-02-10 19:27:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.742: INFO: machine-config-daemon-522l4 from openshift-machine-config-operator started at 2020-02-10 19:38:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: osd-curated-redhat-operators-7b9b69f76c-7tp57 from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Mar  2 21:43:43.742: INFO: node-ca-lsnwz from openshift-image-registry started at 2020-02-10 19:24:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.742: INFO: velero-878675ff4-6wmq2 from openshift-velero started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container velero ready: true, restart count 0
Mar  2 21:43:43.742: INFO: osd-curated-community-operators-7b689d68dc-x2sqj from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Mar  2 21:43:43.742: INFO: node-exporter-gxpxf from openshift-monitoring started at 2020-02-10 19:22:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.742: INFO: ovs-8ptlb from openshift-sdn started at 2020-02-10 19:25:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.742: INFO: thanos-querier-64f49bdcf5-rkr2t from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (4 container statuses recorded)
Mar  2 21:43:43.742: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:43:43.742: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-199.ec2.internal before test
Mar  2 21:43:43.784: INFO: sre-dns-latency-exporter-s2fkj from openshift-monitoring started at 2020-02-05 15:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.784: INFO: dns-default-2qsk9 from openshift-dns started at 2020-02-10 19:35:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.784: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-10 19:45:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:43:43.784: INFO: router-default-595f5b77b4-b4hxs from openshift-ingress started at 2020-02-19 15:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container router ready: true, restart count 0
Mar  2 21:43:43.784: INFO: node-ca-8qmpk from openshift-image-registry started at 2020-02-10 19:21:58 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.784: INFO: node-exporter-bxmbt from openshift-monitoring started at 2020-02-10 19:22:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.784: INFO: sdn-hxmmm from openshift-sdn started at 2020-02-10 19:25:14 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.784: INFO: machine-config-daemon-rcprb from openshift-machine-config-operator started at 2020-02-10 19:37:58 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: splunkforwarder-ds-bq459 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.784: INFO: ovs-km945 from openshift-sdn started at 2020-02-10 19:26:34 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.784: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-10 19:45:02 +0000 UTC (3 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:43:43.784: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-10 19:45:03 +0000 UTC (7 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:43:43.784: INFO: prometheus-adapter-56d646d4cd-9hzn5 from openshift-monitoring started at 2020-02-21 10:30:38 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:43:43.784: INFO: fluentd-xbdx7 from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:43:43.784: INFO: multus-rxxrb from openshift-multus started at 2020-02-10 19:31:44 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.784: INFO: tuned-npnfx from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.784: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.784: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-141-17.ec2.internal before test
Mar  2 21:43:43.857: INFO: sre-dns-latency-exporter-wdtkb from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container main ready: true, restart count 0
Mar  2 21:43:43.857: INFO: validation-webhook-6f8dcdd9fb-kzqnf from openshift-validation-webhook started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:43:43.857: INFO: splunkforwarder-ds-c8jp7 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:43:43.857: INFO: node-exporter-vzwkp from openshift-monitoring started at 2020-02-10 19:21:31 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:43:43.857: INFO: node-ca-tvvm8 from openshift-image-registry started at 2020-02-10 19:22:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:43:43.857: INFO: sdn-7dgr4 from openshift-sdn started at 2020-02-10 19:25:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:43:43.857: INFO: elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl from openshift-logging started at 2020-02-24 14:53:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: ovs-hrkkz from openshift-sdn started at 2020-02-10 19:27:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:43:43.857: INFO: multus-7rlww from openshift-multus started at 2020-02-10 19:30:19 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:43:43.857: INFO: dns-default-cv5xk from openshift-dns started at 2020-02-10 19:35:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:43:43.857: INFO: fluentd-h9qzb from openshift-logging started at 2020-02-24 14:53:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:43:43.857: INFO: machine-config-daemon-95wk2 from openshift-machine-config-operator started at 2020-02-10 19:37:26 +0000 UTC (2 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: osd-curated-certified-operators-65b5f55f88-rc57s from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Mar  2 21:43:43.857: INFO: cluster-logging-operator-758c4648b-rcdxn from openshift-logging started at 2020-02-24 14:50:53 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Mar  2 21:43:43.857: INFO: thanos-querier-64f49bdcf5-ldl9b from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (4 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:43:43.857: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:43:43.857: INFO: elasticsearch-operator-594788b869-hkp9l from openshift-logging started at 2020-02-24 14:50:24 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Mar  2 21:43:43.857: INFO: tuned-xtm2h from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:43:43.857: INFO: downloads-5fdf68d856-5x2xq from openshift-console started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:43:43.857: INFO: 	Container download-server ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-672b0ef7-fc1a-4eb9-943f-5acb7a4bd4b4 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-672b0ef7-fc1a-4eb9-943f-5acb7a4bd4b4 off the node ip-10-0-130-27.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-672b0ef7-fc1a-4eb9-943f-5acb7a4bd4b4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:44:33.162: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-3527" for this suite.
Mar  2 21:44:49.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:44:52.652: INFO: namespace sched-pred-3527 deletion completed in 19.379444682s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:69.448 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:44:52.664: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:44:53.772: INFO: Pod name wrapped-volume-race-cd99213f-a7f1-4700-9be0-d8cbaa2d426d: Found 1 pods out of 5
Mar  2 21:44:58.887: INFO: Pod name wrapped-volume-race-cd99213f-a7f1-4700-9be0-d8cbaa2d426d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cd99213f-a7f1-4700-9be0-d8cbaa2d426d in namespace emptydir-wrapper-2413, will wait for the garbage collector to delete the pods
Mar  2 21:45:03.093: INFO: Deleting ReplicationController wrapped-volume-race-cd99213f-a7f1-4700-9be0-d8cbaa2d426d took: 17.925726ms
Mar  2 21:45:03.294: INFO: Terminating ReplicationController wrapped-volume-race-cd99213f-a7f1-4700-9be0-d8cbaa2d426d pods took: 200.469394ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:45:47.976: INFO: Pod name wrapped-volume-race-5fda5fbf-d547-4366-8d4f-059278497371: Found 0 pods out of 5
Mar  2 21:45:53.013: INFO: Pod name wrapped-volume-race-5fda5fbf-d547-4366-8d4f-059278497371: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5fda5fbf-d547-4366-8d4f-059278497371 in namespace emptydir-wrapper-2413, will wait for the garbage collector to delete the pods
Mar  2 21:45:57.179: INFO: Deleting ReplicationController wrapped-volume-race-5fda5fbf-d547-4366-8d4f-059278497371 took: 17.547953ms
Mar  2 21:45:57.279: INFO: Terminating ReplicationController wrapped-volume-race-5fda5fbf-d547-4366-8d4f-059278497371 pods took: 100.500754ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:46:37.948: INFO: Pod name wrapped-volume-race-d183f0be-b187-4b12-bdab-17b5f0e5cfe4: Found 0 pods out of 5
Mar  2 21:46:42.976: INFO: Pod name wrapped-volume-race-d183f0be-b187-4b12-bdab-17b5f0e5cfe4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d183f0be-b187-4b12-bdab-17b5f0e5cfe4 in namespace emptydir-wrapper-2413, will wait for the garbage collector to delete the pods
Mar  2 21:46:49.156: INFO: Deleting ReplicationController wrapped-volume-race-d183f0be-b187-4b12-bdab-17b5f0e5cfe4 took: 18.383227ms
Mar  2 21:46:49.356: INFO: Terminating ReplicationController wrapped-volume-race-d183f0be-b187-4b12-bdab-17b5f0e5cfe4 pods took: 200.309821ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:47:28.815: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2413" for this suite.
Mar  2 21:47:36.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:47:40.217: INFO: namespace emptydir-wrapper-2413 deletion completed in 11.354898456s

• [SLOW TEST:167.553 seconds]
[sig-storage] EmptyDir wrapper volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:47:40.225: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 21:47:40.563: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:40.563: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:40.563: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:40.578: INFO: Number of nodes with available pods: 0
Mar  2 21:47:40.578: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:41.636: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:41.636: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:41.636: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:41.650: INFO: Number of nodes with available pods: 0
Mar  2 21:47:41.650: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:42.636: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:42.636: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:42.636: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:42.650: INFO: Number of nodes with available pods: 0
Mar  2 21:47:42.650: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:43.626: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:43.626: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:43.626: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:43.640: INFO: Number of nodes with available pods: 0
Mar  2 21:47:43.640: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:44.626: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:44.626: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:44.626: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:44.639: INFO: Number of nodes with available pods: 0
Mar  2 21:47:44.639: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:45.628: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:45.628: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:45.628: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:45.642: INFO: Number of nodes with available pods: 0
Mar  2 21:47:45.642: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:46.627: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:46.627: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:46.627: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:46.641: INFO: Number of nodes with available pods: 0
Mar  2 21:47:46.641: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:47.625: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:47.625: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:47.625: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:47.639: INFO: Number of nodes with available pods: 0
Mar  2 21:47:47.639: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:48.626: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:48.626: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:48.626: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:48.639: INFO: Number of nodes with available pods: 1
Mar  2 21:47:48.639: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:47:49.626: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.626: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.626: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.667: INFO: Number of nodes with available pods: 7
Mar  2 21:47:49.667: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 21:47:49.835: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.835: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.835: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:49.860: INFO: Number of nodes with available pods: 6
Mar  2 21:47:49.860: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:50.938: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:50.938: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:50.938: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:50.952: INFO: Number of nodes with available pods: 6
Mar  2 21:47:50.952: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:51.929: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:51.929: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:51.929: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:51.943: INFO: Number of nodes with available pods: 6
Mar  2 21:47:51.943: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:52.930: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:52.930: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:52.930: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:52.944: INFO: Number of nodes with available pods: 6
Mar  2 21:47:52.944: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:53.959: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:53.960: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:53.960: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:53.984: INFO: Number of nodes with available pods: 6
Mar  2 21:47:53.984: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:54.939: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:54.939: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:54.939: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:54.953: INFO: Number of nodes with available pods: 6
Mar  2 21:47:54.953: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:55.929: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:55.929: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:55.929: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:55.943: INFO: Number of nodes with available pods: 6
Mar  2 21:47:55.943: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:56.919: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:56.919: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:56.919: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:56.933: INFO: Number of nodes with available pods: 6
Mar  2 21:47:56.933: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:57.918: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:57.919: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:57.919: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:57.933: INFO: Number of nodes with available pods: 6
Mar  2 21:47:57.933: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:47:58.918: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:58.918: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:58.919: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:47:58.932: INFO: Number of nodes with available pods: 7
Mar  2 21:47:58.932: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3719, will wait for the garbage collector to delete the pods
Mar  2 21:47:59.043: INFO: Deleting DaemonSet.extensions daemon-set took: 19.991912ms
Mar  2 21:47:59.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.324372ms
Mar  2 21:48:08.957: INFO: Number of nodes with available pods: 0
Mar  2 21:48:08.957: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:48:08.970: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3719/daemonsets","resourceVersion":"15419253"},"items":null}

Mar  2 21:48:08.983: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3719/pods","resourceVersion":"15419253"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:48:09.249: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-3719" for this suite.
Mar  2 21:48:15.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:48:18.680: INFO: namespace daemonsets-3719 deletion completed in 9.354071369s

• [SLOW TEST:38.456 seconds]
[sig-apps] Daemon set [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:48:18.683: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  2 21:48:18.784: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Mar  2 21:48:18.934: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 21:48:19.188: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-224.ec2.internal before test
Mar  2 21:48:19.254: INFO: rbac-permissions-operator-7ddb7b95c-tf5qr from openshift-rbac-permissions started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Mar  2 21:48:19.254: INFO: sre-dns-latency-exporter-sds5m from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.254: INFO: sdn-qrnvj from openshift-sdn started at 2020-02-10 19:25:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.254: INFO: multus-c92b7 from openshift-multus started at 2020-02-10 19:33:07 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.254: INFO: elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:48:19.254: INFO: node-ca-5lj4v from openshift-image-registry started at 2020-02-10 19:21:13 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.254: INFO: machine-config-daemon-xfc2n from openshift-machine-config-operator started at 2020-02-10 19:37:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.254: INFO: validation-webhook-6f8dcdd9fb-94rfx from openshift-validation-webhook started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:48:19.254: INFO: tuned-cvjkf from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.254: INFO: kibana-85fc498c95-z56md from openshift-logging started at 2020-02-24 14:53:23 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container kibana ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 	Container kibana-proxy ready: true, restart count 0
Mar  2 21:48:19.254: INFO: fluentd-vgqp4 from openshift-logging started at 2020-02-24 14:53:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:48:19.254: INFO: dns-default-78jzw from openshift-dns started at 2020-02-10 19:36:41 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.254: INFO: splunkforwarder-ds-2j9mv from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.254: INFO: node-exporter-s8c2n from openshift-monitoring started at 2020-02-10 19:21:13 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.254: INFO: ovs-m9222 from openshift-sdn started at 2020-02-10 19:28:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.254: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.254: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-130-27.ec2.internal before test
Mar  2 21:48:19.307: INFO: dns-default-t8vhr from openshift-dns started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.307: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.307: INFO: splunkforwarder-ds-qkm6f from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.307: INFO: multus-2m27b from openshift-multus started at 2020-02-10 19:29:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.307: INFO: machine-config-daemon-r8rp4 from openshift-machine-config-operator started at 2020-03-02 21:36:15 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.307: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.307: INFO: ovs-bjftz from openshift-sdn started at 2020-02-10 19:28:52 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.307: INFO: node-ca-8tlsw from openshift-image-registry started at 2020-03-02 21:36:15 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.307: INFO: node-exporter-n5znc from openshift-monitoring started at 2020-02-10 19:21:51 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.307: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.307: INFO: fluentd-lxcgg from openshift-logging started at 2020-03-02 21:36:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:48:19.307: INFO: splunk-forwarder-operator-catalog-xmff9 from openshift-splunk-forwarder-operator started at 2020-02-24 18:15:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:48:19.307: INFO: sre-dns-latency-exporter-9hb4h from openshift-monitoring started at 2020-03-02 21:36:35 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.307: INFO: rbac-permissions-operator-registry-qdnz8 from openshift-rbac-permissions started at 2020-02-10 20:09:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:48:19.307: INFO: sdn-bg7s7 from openshift-sdn started at 2020-02-10 19:26:18 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.307: INFO: tuned-4q8sm from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.307: INFO: configure-alertmanager-operator-registry-c4mtq from openshift-monitoring started at 2020-02-28 15:20:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.307: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 21:48:19.307: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-159.ec2.internal before test
Mar  2 21:48:19.385: INFO: sdn-xpc4d from openshift-sdn started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container sdn ready: true, restart count 1
Mar  2 21:48:19.385: INFO: tuned-7cwm9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.385: INFO: kube-state-metrics-67bcd5775b-t85wt from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container kube-state-metrics ready: true, restart count 2
Mar  2 21:48:19.385: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-10 19:52:07 +0000 UTC (7 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:48:19.385: INFO: ovs-r2phg from openshift-sdn started at 2020-02-10 19:26:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.385: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-10 19:52:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:48:19.385: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:48:19.385: INFO: node-ca-2c7vd from openshift-image-registry started at 2020-02-10 19:23:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.385: INFO: image-registry-776d8848f-4dkxq from openshift-image-registry started at 2020-02-10 19:52:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container registry ready: true, restart count 0
Mar  2 21:48:19.385: INFO: splunkforwarder-ds-vg9wk from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.385: INFO: sre-dns-latency-exporter-b7brb from openshift-monitoring started at 2020-02-05 15:49:59 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.385: INFO: multus-blrhd from openshift-multus started at 2020-02-10 19:26:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.385: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.385: INFO: dns-default-95h6x from openshift-dns started at 2020-02-10 19:37:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.386: INFO: machine-config-daemon-b6c6q from openshift-machine-config-operator started at 2020-02-10 19:38:46 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.386: INFO: grafana-58f6c99667-jb9f4 from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container grafana ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar  2 21:48:19.386: INFO: telemeter-client-78c68b58db-2fl8t from openshift-monitoring started at 2020-02-10 19:52:01 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container reload ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 21:48:19.386: INFO: fluentd-rzd9d from openshift-logging started at 2020-02-24 14:53:28 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:48:19.386: INFO: node-exporter-w56xf from openshift-monitoring started at 2020-02-10 19:22:24 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.386: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.386: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-135-248.ec2.internal before test
Mar  2 21:48:19.450: INFO: dns-default-rj656 from openshift-dns started at 2020-02-10 19:38:43 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.450: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.450: INFO: sre-ebs-iops-reporter-1-smq2g from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.450: INFO: splunk-forwarder-operator-f6d6c9c57-vv9mk from openshift-splunk-forwarder-operator started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Mar  2 21:48:19.450: INFO: sre-build-test-1583176260-jdk6t from openshift-build-test started at 2020-03-02 19:11:04 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:48:19.450: INFO: configure-alertmanager-operator-57f5b56487-jp6j4 from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Mar  2 21:48:19.450: INFO: sre-build-test-1583179860-8bmsn from openshift-build-test started at 2020-03-02 20:11:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:48:19.450: INFO: node-exporter-lqkf8 from openshift-monitoring started at 2020-02-10 19:23:03 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.450: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.450: INFO: sdn-cqj65 from openshift-sdn started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.450: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.450: INFO: machine-config-daemon-hk5js from openshift-machine-config-operator started at 2020-02-10 19:38:33 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.451: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.451: INFO: prometheus-operator-6d474cc98b-pkw6b from openshift-monitoring started at 2020-02-27 16:36:05 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 21:48:19.451: INFO: router-default-595f5b77b4-kz6pp from openshift-ingress started at 2020-02-19 15:08:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container router ready: true, restart count 0
Mar  2 21:48:19.451: INFO: splunkforwarder-ds-f2c94 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.451: INFO: sre-build-test-1583183460-z47qg from openshift-build-test started at 2020-03-02 21:11:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container sre-build-test ready: false, restart count 0
Mar  2 21:48:19.451: INFO: multus-zh5hw from openshift-multus started at 2020-02-10 19:25:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.451: INFO: sre-machine-api-status-exporter-1-j9zvt from openshift-monitoring started at 2020-02-10 20:00:33 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container main ready: true, restart count 1
Mar  2 21:48:19.451: INFO: prometheus-adapter-56d646d4cd-bs2fc from openshift-monitoring started at 2020-02-21 10:30:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:48:19.451: INFO: fluentd-2mc9b from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:48:19.451: INFO: managed-velero-operator-589fbb966f-4gwmw from openshift-velero started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container managed-velero-operator ready: true, restart count 0
Mar  2 21:48:19.451: INFO: sre-dns-latency-exporter-qksxc from openshift-monitoring started at 2020-02-05 21:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.451: INFO: node-ca-m4bjs from openshift-image-registry started at 2020-02-10 19:21:43 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.451: INFO: ovs-lnrws from openshift-sdn started at 2020-02-10 19:28:10 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.451: INFO: tuned-m4fk9 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.451: INFO: sre-stuck-ebs-vols-1-d7lg9 from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.451: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.451: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-145.ec2.internal before test
Mar  2 21:48:19.505: INFO: sdn-w4cmx from openshift-sdn started at 2020-02-10 19:25:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.505: INFO: dns-default-9wkv6 from openshift-dns started at 2020-02-10 19:37:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.505: INFO: tuned-tskk8 from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.505: INFO: openshift-state-metrics-b6755756-hzdgv from openshift-monitoring started at 2020-02-10 20:04:57 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 21:48:19.505: INFO: sre-dns-latency-exporter-bk7vm from openshift-monitoring started at 2020-02-10 19:12:49 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.505: INFO: multus-fmzkv from openshift-multus started at 2020-02-10 19:27:27 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.505: INFO: machine-config-daemon-522l4 from openshift-machine-config-operator started at 2020-02-10 19:38:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: splunkforwarder-ds-56rrh from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.505: INFO: elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 from openshift-logging started at 2020-02-24 14:53:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: osd-curated-redhat-operators-7b9b69f76c-7tp57 from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Mar  2 21:48:19.505: INFO: node-ca-lsnwz from openshift-image-registry started at 2020-02-10 19:24:39 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.505: INFO: velero-878675ff4-6wmq2 from openshift-velero started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container velero ready: true, restart count 0
Mar  2 21:48:19.505: INFO: node-exporter-gxpxf from openshift-monitoring started at 2020-02-10 19:22:17 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.505: INFO: ovs-8ptlb from openshift-sdn started at 2020-02-10 19:25:37 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.505: INFO: thanos-querier-64f49bdcf5-rkr2t from openshift-monitoring started at 2020-02-10 20:09:03 +0000 UTC (4 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:48:19.505: INFO: osd-curated-community-operators-7b689d68dc-x2sqj from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Mar  2 21:48:19.505: INFO: downloads-5fdf68d856-tsdl7 from openshift-console started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container download-server ready: true, restart count 0
Mar  2 21:48:19.505: INFO: fluentd-p2gn2 from openshift-logging started at 2020-02-24 14:53:31 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:48:19.505: INFO: validation-webhook-6f8dcdd9fb-tb8jg from openshift-validation-webhook started at 2020-02-10 20:13:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.505: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:48:19.505: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-137-199.ec2.internal before test
Mar  2 21:48:19.558: INFO: multus-rxxrb from openshift-multus started at 2020-02-10 19:31:44 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.558: INFO: tuned-npnfx from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:42 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.558: INFO: sre-dns-latency-exporter-s2fkj from openshift-monitoring started at 2020-02-05 15:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.558: INFO: dns-default-2qsk9 from openshift-dns started at 2020-02-10 19:35:40 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.558: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-10 19:45:06 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:48:19.558: INFO: router-default-595f5b77b4-b4hxs from openshift-ingress started at 2020-02-19 15:09:03 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container router ready: true, restart count 0
Mar  2 21:48:19.558: INFO: node-ca-8qmpk from openshift-image-registry started at 2020-02-10 19:21:58 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.558: INFO: node-exporter-bxmbt from openshift-monitoring started at 2020-02-10 19:22:01 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.558: INFO: sdn-hxmmm from openshift-sdn started at 2020-02-10 19:25:14 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.558: INFO: machine-config-daemon-rcprb from openshift-machine-config-operator started at 2020-02-10 19:37:58 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: splunkforwarder-ds-bq459 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.558: INFO: ovs-km945 from openshift-sdn started at 2020-02-10 19:26:34 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.558: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-10 19:45:02 +0000 UTC (3 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 21:48:19.558: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-10 19:45:03 +0000 UTC (7 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  2 21:48:19.558: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 21:48:19.558: INFO: prometheus-adapter-56d646d4cd-9hzn5 from openshift-monitoring started at 2020-02-21 10:30:38 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 21:48:19.558: INFO: fluentd-xbdx7 from openshift-logging started at 2020-02-24 14:53:32 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.558: INFO: 	Container fluentd ready: true, restart count 2
Mar  2 21:48:19.558: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-141-17.ec2.internal before test
Mar  2 21:48:19.611: INFO: sre-dns-latency-exporter-wdtkb from openshift-monitoring started at 2020-02-05 15:45:08 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container main ready: true, restart count 0
Mar  2 21:48:19.611: INFO: validation-webhook-6f8dcdd9fb-kzqnf from openshift-validation-webhook started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container validation-webhook ready: true, restart count 0
Mar  2 21:48:19.611: INFO: splunkforwarder-ds-c8jp7 from openshift-security started at 2020-02-24 18:15:40 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container splunk-uf ready: true, restart count 0
Mar  2 21:48:19.611: INFO: node-exporter-vzwkp from openshift-monitoring started at 2020-02-10 19:21:31 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 21:48:19.611: INFO: node-ca-tvvm8 from openshift-image-registry started at 2020-02-10 19:22:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 21:48:19.611: INFO: sdn-7dgr4 from openshift-sdn started at 2020-02-10 19:25:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container sdn ready: true, restart count 0
Mar  2 21:48:19.611: INFO: elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl from openshift-logging started at 2020-02-24 14:53:29 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: ovs-hrkkz from openshift-sdn started at 2020-02-10 19:27:01 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container openvswitch ready: true, restart count 0
Mar  2 21:48:19.611: INFO: multus-7rlww from openshift-multus started at 2020-02-10 19:30:19 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 21:48:19.611: INFO: dns-default-cv5xk from openshift-dns started at 2020-02-10 19:35:11 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container dns ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 21:48:19.611: INFO: fluentd-h9qzb from openshift-logging started at 2020-02-24 14:53:30 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container fluentd ready: true, restart count 0
Mar  2 21:48:19.611: INFO: machine-config-daemon-95wk2 from openshift-machine-config-operator started at 2020-02-10 19:37:26 +0000 UTC (2 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container machine-config-daemon ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: osd-curated-certified-operators-65b5f55f88-rc57s from openshift-marketplace started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Mar  2 21:48:19.611: INFO: cluster-logging-operator-758c4648b-rcdxn from openshift-logging started at 2020-02-24 14:50:53 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container cluster-logging-operator ready: true, restart count 0
Mar  2 21:48:19.611: INFO: thanos-querier-64f49bdcf5-ldl9b from openshift-monitoring started at 2020-03-02 18:39:25 +0000 UTC (4 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 21:48:19.611: INFO: 	Container thanos-querier ready: true, restart count 0
Mar  2 21:48:19.611: INFO: elasticsearch-operator-594788b869-hkp9l from openshift-logging started at 2020-02-24 14:50:24 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container elasticsearch-operator ready: true, restart count 0
Mar  2 21:48:19.611: INFO: tuned-xtm2h from openshift-cluster-node-tuning-operator started at 2020-02-10 19:51:41 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container tuned ready: true, restart count 0
Mar  2 21:48:19.611: INFO: downloads-5fdf68d856-5x2xq from openshift-console started at 2020-03-02 18:39:25 +0000 UTC (1 container statuses recorded)
Mar  2 21:48:19.611: INFO: 	Container download-server ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-10-0-130-224.ec2.internal
STEP: verifying the node has the label node ip-10-0-130-27.ec2.internal
STEP: verifying the node has the label node ip-10-0-132-159.ec2.internal
STEP: verifying the node has the label node ip-10-0-135-248.ec2.internal
STEP: verifying the node has the label node ip-10-0-137-145.ec2.internal
STEP: verifying the node has the label node ip-10-0-137-199.ec2.internal
STEP: verifying the node has the label node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-4q8sm requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-7cwm9 requesting resource cpu=10m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-cvjkf requesting resource cpu=10m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-m4fk9 requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-npnfx requesting resource cpu=10m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-tskk8 requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod tuned-xtm2h requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod downloads-5fdf68d856-5x2xq requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod downloads-5fdf68d856-tsdl7 requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-2qsk9 requesting resource cpu=110m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-78jzw requesting resource cpu=110m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-95h6x requesting resource cpu=110m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-9wkv6 requesting resource cpu=110m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-cv5xk requesting resource cpu=110m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-rj656 requesting resource cpu=110m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod dns-default-t8vhr requesting resource cpu=110m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod image-registry-776d8848f-4dkxq requesting resource cpu=100m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-2c7vd requesting resource cpu=10m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-5lj4v requesting resource cpu=10m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-8qmpk requesting resource cpu=10m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-8tlsw requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-lsnwz requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-m4bjs requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-ca-tvvm8 requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod router-default-595f5b77b4-b4hxs requesting resource cpu=100m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod router-default-595f5b77b4-kz6pp requesting resource cpu=100m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod cluster-logging-operator-758c4648b-rcdxn requesting resource cpu=0m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod elasticsearch-cdm-85dgpaaj-1-77dbddc454-9wmcl requesting resource cpu=200m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2 requesting resource cpu=200m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod elasticsearch-cdm-85dgpaaj-3-8cdd6b57b-578p2 requesting resource cpu=200m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod elasticsearch-operator-594788b869-hkp9l requesting resource cpu=0m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-2mc9b requesting resource cpu=100m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-h9qzb requesting resource cpu=100m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-lxcgg requesting resource cpu=100m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-p2gn2 requesting resource cpu=100m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-rzd9d requesting resource cpu=100m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-vgqp4 requesting resource cpu=100m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod fluentd-xbdx7 requesting resource cpu=100m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod kibana-85fc498c95-z56md requesting resource cpu=200m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-522l4 requesting resource cpu=20m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-95wk2 requesting resource cpu=20m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-b6c6q requesting resource cpu=20m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-hk5js requesting resource cpu=20m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-r8rp4 requesting resource cpu=20m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-rcprb requesting resource cpu=20m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod machine-config-daemon-xfc2n requesting resource cpu=20m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod osd-curated-certified-operators-65b5f55f88-rc57s requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod osd-curated-community-operators-7b689d68dc-x2sqj requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod osd-curated-redhat-operators-7b9b69f76c-7tp57 requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod configure-alertmanager-operator-57f5b56487-jp6j4 requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod configure-alertmanager-operator-registry-c4mtq requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod grafana-58f6c99667-jb9f4 requesting resource cpu=100m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod kube-state-metrics-67bcd5775b-t85wt requesting resource cpu=30m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-bxmbt requesting resource cpu=10m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-gxpxf requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-lqkf8 requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-n5znc requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-s8c2n requesting resource cpu=10m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-vzwkp requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod node-exporter-w56xf requesting resource cpu=10m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod openshift-state-metrics-b6755756-hzdgv requesting resource cpu=120m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod prometheus-adapter-56d646d4cd-9hzn5 requesting resource cpu=10m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod prometheus-adapter-56d646d4cd-bs2fc requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod prometheus-operator-6d474cc98b-pkw6b requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-9hb4h requesting resource cpu=0m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-b7brb requesting resource cpu=0m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-bk7vm requesting resource cpu=0m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-qksxc requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-s2fkj requesting resource cpu=0m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-sds5m requesting resource cpu=0m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-dns-latency-exporter-wdtkb requesting resource cpu=0m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-ebs-iops-reporter-1-smq2g requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-machine-api-status-exporter-1-j9zvt requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod sre-stuck-ebs-vols-1-d7lg9 requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:19.999: INFO: Pod telemeter-client-78c68b58db-2fl8t requesting resource cpu=10m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:19.999: INFO: Pod thanos-querier-64f49bdcf5-ldl9b requesting resource cpu=40m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:19.999: INFO: Pod thanos-querier-64f49bdcf5-rkr2t requesting resource cpu=40m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:19.999: INFO: Pod multus-2m27b requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:19.999: INFO: Pod multus-7rlww requesting resource cpu=10m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.000: INFO: Pod multus-blrhd requesting resource cpu=10m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:20.000: INFO: Pod multus-c92b7 requesting resource cpu=10m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod multus-fmzkv requesting resource cpu=10m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.000: INFO: Pod multus-rxxrb requesting resource cpu=10m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:20.000: INFO: Pod multus-zh5hw requesting resource cpu=10m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod rbac-permissions-operator-7ddb7b95c-tf5qr requesting resource cpu=0m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod rbac-permissions-operator-registry-qdnz8 requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-8ptlb requesting resource cpu=200m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-bjftz requesting resource cpu=200m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-hrkkz requesting resource cpu=200m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-km945 requesting resource cpu=200m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-lnrws requesting resource cpu=200m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-m9222 requesting resource cpu=200m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod ovs-r2phg requesting resource cpu=200m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-7dgr4 requesting resource cpu=100m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-bg7s7 requesting resource cpu=100m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-cqj65 requesting resource cpu=100m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-hxmmm requesting resource cpu=100m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-qrnvj requesting resource cpu=100m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-w4cmx requesting resource cpu=100m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.000: INFO: Pod sdn-xpc4d requesting resource cpu=100m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-2j9mv requesting resource cpu=0m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-56rrh requesting resource cpu=0m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-bq459 requesting resource cpu=0m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-c8jp7 requesting resource cpu=0m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-f2c94 requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-qkm6f requesting resource cpu=0m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunkforwarder-ds-vg9wk requesting resource cpu=0m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunk-forwarder-operator-catalog-xmff9 requesting resource cpu=10m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.000: INFO: Pod splunk-forwarder-operator-f6d6c9c57-vv9mk requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod validation-webhook-6f8dcdd9fb-94rfx requesting resource cpu=0m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.000: INFO: Pod validation-webhook-6f8dcdd9fb-kzqnf requesting resource cpu=0m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.000: INFO: Pod validation-webhook-6f8dcdd9fb-tb8jg requesting resource cpu=0m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.000: INFO: Pod managed-velero-operator-589fbb966f-4gwmw requesting resource cpu=0m on Node ip-10-0-135-248.ec2.internal
Mar  2 21:48:20.000: INFO: Pod velero-878675ff4-6wmq2 requesting resource cpu=0m on Node ip-10-0-137-145.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Mar  2 21:48:20.000: INFO: Creating a pod which consumes cpu=1428m on Node ip-10-0-137-145.ec2.internal
Mar  2 21:48:20.022: INFO: Creating a pod which consumes cpu=1183m on Node ip-10-0-137-199.ec2.internal
Mar  2 21:48:20.042: INFO: Creating a pod which consumes cpu=1519m on Node ip-10-0-141-17.ec2.internal
Mar  2 21:48:20.062: INFO: Creating a pod which consumes cpu=1421m on Node ip-10-0-130-224.ec2.internal
Mar  2 21:48:20.082: INFO: Creating a pod which consumes cpu=1680m on Node ip-10-0-130-27.ec2.internal
Mar  2 21:48:20.102: INFO: Creating a pod which consumes cpu=1162m on Node ip-10-0-132-159.ec2.internal
Mar  2 21:48:20.121: INFO: Creating a pod which consumes cpu=1617m on Node ip-10-0-135-248.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9.15f89b0172ccaddf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9 to ip-10-0-130-27.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9.15f89b0349a4d665], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9.15f89b0357018b56], Reason = [Created], Message = [Created container filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9.15f89b03581a9f42], Reason = [Started], Message = [Started container filler-pod-022febad-770f-493a-b9f4-942e1eaa71f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f.15f89b0170608dc8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f to ip-10-0-141-17.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f.15f89b035cfe7c7d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f.15f89b03662469fb], Reason = [Created], Message = [Created container filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f.15f89b03674e4430], Reason = [Started], Message = [Started container filler-pod-68cefcc3-db78-49e8-99ab-59aa0f271b7f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371.15f89b016e0d10be], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371 to ip-10-0-137-145.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371.15f89b0320cf7062], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371.15f89b0327c1aeac], Reason = [Created], Message = [Created container filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371.15f89b03291eb20c], Reason = [Started], Message = [Started container filler-pod-994ad9ba-0afd-4227-a19b-6c622aeab371]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564.15f89b01751b9790], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564 to ip-10-0-135-248.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564.15f89b0328a37953], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564.15f89b0330018c40], Reason = [Created], Message = [Created container filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564.15f89b03313d3072], Reason = [Started], Message = [Started container filler-pod-a6ecdc53-97d6-44b1-8c98-ed5bc6882564]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570.15f89b016f1e9a78], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570 to ip-10-0-137-199.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570.15f89b0342be9480], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570.15f89b034a375de6], Reason = [Created], Message = [Created container filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570.15f89b034b7baae0], Reason = [Started], Message = [Started container filler-pod-b25f3b6f-8811-4504-b2dc-e18eadf9d570]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9.15f89b0173e048f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9 to ip-10-0-132-159.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9.15f89b03414ea2dc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9.15f89b034975f5b6], Reason = [Created], Message = [Created container filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9.15f89b034a8f97bf], Reason = [Started], Message = [Started container filler-pod-c229a407-f469-4c7d-973a-b50e4e7d31c9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2a89640-266f-4a9e-b121-97412c37f172.15f89b017186e8ce], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1311/filler-pod-c2a89640-266f-4a9e-b121-97412c37f172 to ip-10-0-130-224.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2a89640-266f-4a9e-b121-97412c37f172.15f89b034710f305], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2a89640-266f-4a9e-b121-97412c37f172.15f89b035628b8e6], Reason = [Created], Message = [Created container filler-pod-c2a89640-266f-4a9e-b121-97412c37f172]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2a89640-266f-4a9e-b121-97412c37f172.15f89b03574677c2], Reason = [Started], Message = [Started container filler-pod-c2a89640-266f-4a9e-b121-97412c37f172]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f89b03d38b2d2e], Reason = [FailedScheduling], Message = [0/10 nodes are available: 3 node(s) were unschedulable, 7 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-137-199.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-141-17.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-130-224.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-130-27.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-132-159.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-135-248.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-137-145.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:48:31.702: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1311" for this suite.
Mar  2 21:48:37.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:48:41.127: INFO: namespace sched-pred-1311 deletion completed in 9.357241976s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:22.444 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:48:41.130: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 21:48:41.624: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:41.624: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:41.624: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:41.639: INFO: Number of nodes with available pods: 0
Mar  2 21:48:41.639: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:42.708: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:42.709: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:42.709: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:42.722: INFO: Number of nodes with available pods: 0
Mar  2 21:48:42.722: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:43.989: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:43.989: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:43.989: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:44.013: INFO: Number of nodes with available pods: 0
Mar  2 21:48:44.013: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:44.761: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:44.761: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:44.761: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:44.786: INFO: Number of nodes with available pods: 0
Mar  2 21:48:44.786: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:45.783: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:45.783: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:45.783: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:45.807: INFO: Number of nodes with available pods: 0
Mar  2 21:48:45.807: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:46.740: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:46.740: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:46.740: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:46.754: INFO: Number of nodes with available pods: 0
Mar  2 21:48:46.754: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:47.708: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:47.709: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:47.709: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:47.723: INFO: Number of nodes with available pods: 0
Mar  2 21:48:47.723: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:48.707: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:48.707: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:48.707: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:48.721: INFO: Number of nodes with available pods: 0
Mar  2 21:48:48.721: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:49.697: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:49.697: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:49.697: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:49.710: INFO: Number of nodes with available pods: 1
Mar  2 21:48:49.710: INFO: Node ip-10-0-130-224.ec2.internal is running more than one daemon pod
Mar  2 21:48:50.697: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:50.697: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:50.698: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:50.711: INFO: Number of nodes with available pods: 6
Mar  2 21:48:50.711: INFO: Node ip-10-0-137-145.ec2.internal is running more than one daemon pod
Mar  2 21:48:51.686: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.686: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.686: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.699: INFO: Number of nodes with available pods: 7
Mar  2 21:48:51.700: INFO: Number of running nodes: 7, number of available pods: 7
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 21:48:51.790: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.790: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.791: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:51.804: INFO: Number of nodes with available pods: 6
Mar  2 21:48:51.804: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:52.852: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:52.852: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:52.853: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:52.867: INFO: Number of nodes with available pods: 6
Mar  2 21:48:52.867: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:53.873: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:53.873: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:53.873: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:53.887: INFO: Number of nodes with available pods: 6
Mar  2 21:48:53.887: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:54.862: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:54.862: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:54.862: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:54.876: INFO: Number of nodes with available pods: 6
Mar  2 21:48:54.876: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:55.861: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:55.861: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:55.861: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:55.874: INFO: Number of nodes with available pods: 6
Mar  2 21:48:55.874: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:56.861: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:56.861: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:56.861: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:56.874: INFO: Number of nodes with available pods: 6
Mar  2 21:48:56.874: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:57.856: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:57.857: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:57.857: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:57.870: INFO: Number of nodes with available pods: 6
Mar  2 21:48:57.870: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:58.851: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:58.851: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:58.851: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:58.865: INFO: Number of nodes with available pods: 6
Mar  2 21:48:58.865: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:48:59.852: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:59.852: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:59.852: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:48:59.866: INFO: Number of nodes with available pods: 6
Mar  2 21:48:59.866: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:49:00.852: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:00.852: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:00.852: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:00.865: INFO: Number of nodes with available pods: 6
Mar  2 21:49:00.865: INFO: Node ip-10-0-132-159.ec2.internal is running more than one daemon pod
Mar  2 21:49:01.857: INFO: DaemonSet pods can't tolerate node ip-10-0-136-100.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:01.857: INFO: DaemonSet pods can't tolerate node ip-10-0-138-218.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:01.857: INFO: DaemonSet pods can't tolerate node ip-10-0-142-206.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-03-02 16:40:02 +0000 UTC}], skip checking this node
Mar  2 21:49:01.871: INFO: Number of nodes with available pods: 7
Mar  2 21:49:01.871: INFO: Number of running nodes: 7, number of available pods: 7
[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-952, will wait for the garbage collector to delete the pods
Mar  2 21:49:01.965: INFO: Deleting DaemonSet.extensions daemon-set took: 17.67587ms
Mar  2 21:49:02.166: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.350746ms
Mar  2 21:49:13.080: INFO: Number of nodes with available pods: 0
Mar  2 21:49:13.080: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 21:49:13.094: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-952/daemonsets","resourceVersion":"15420053"},"items":null}

Mar  2 21:49:13.106: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-952/pods","resourceVersion":"15420053"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:13.461: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-952" for this suite.
Mar  2 21:49:19.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:49:22.862: INFO: namespace daemonsets-952 deletion completed in 9.355228664s

• [SLOW TEST:41.732 seconds]
[sig-apps] Daemon set [Serial]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMar  2 21:49:22.865: INFO: Running AfterSuite actions on all nodes
Mar  2 21:49:22.865: INFO: Running AfterSuite actions on node 1
Mar  2 21:49:22.865: INFO: Dumping logs locally to: /osd_43_conformance/origin/_output/scripts/conformance-k8s/artifacts
Mar  2 21:49:22.866: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 15 of 4897 Specs in 1321.381 seconds
SUCCESS! -- 15 Passed | 0 Failed | 0 Pending | 4882 Skipped
PASS

Ginkgo ran 1 suite in 22m2.51295974s
Test Suite Passed
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583185762 - Will randomize all specs
Will run 4897 specs

Running in parallel across 4 nodes

Mar  2 21:49:24.153: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:49:24.156: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Mar  2 21:49:24.542: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 21:49:24.613: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 21:49:24.613: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Mar  2 21:49:24.613: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 21:49:24.638: INFO: e2e test version: v1.16.3-beta.0.38+c3aac8e00076fd
Mar  2 21:49:24.655: INFO: kube-apiserver version: v1.16.2
Mar  2 21:49:24.655: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:49:24.678: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSS
------------------------------
Mar  2 21:49:24.659: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:49:24.714: INFO: Cluster IP family: ipv4

SSSSSSSSS
------------------------------
Mar  2 21:49:24.659: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:49:24.721: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
Mar  2 21:49:24.668: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:49:24.757: INFO: Cluster IP family: ipv4

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:24.720: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
Mar  2 21:49:24.828: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:49:25.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d" in namespace "downward-api-5346" to be "success or failure"
Mar  2 21:49:25.915: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.386938ms
Mar  2 21:49:27.943: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048183263s
Mar  2 21:49:29.996: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10131873s
Mar  2 21:49:32.010: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.115145534s
Mar  2 21:49:34.024: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.129340212s
STEP: Saw pod success
Mar  2 21:49:34.025: INFO: Pod "downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d" satisfied condition "success or failure"
Mar  2 21:49:34.039: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d container client-container: <nil>
STEP: delete the pod
Mar  2 21:49:34.082: INFO: Waiting for pod downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d to disappear
Mar  2 21:49:34.094: INFO: Pod downwardapi-volume-0bcf700d-2e7d-40ef-a501-73201281d99d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:34.095: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5346" for this suite.
Mar  2 21:49:40.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:49:43.541: INFO: namespace downward-api-5346 deletion completed in 9.352486676s


• [SLOW TEST:18.822 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:24.762: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename deployment
Mar  2 21:49:24.969: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:49:24.981: INFO: Creating deployment "test-recreate-deployment"
Mar  2 21:49:24.997: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 21:49:25.022: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 21:49:25.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-68fc85c7bb\""}}, CollisionCount:(*int32)(nil)}
Mar  2 21:49:27.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:49:29.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:49:31.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:49:33.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782564, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:49:35.066: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 21:49:35.093: INFO: Updating deployment test-recreate-deployment
Mar  2 21:49:35.093: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  2 21:49:35.173: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9280 /apis/apps/v1/namespaces/deployment-9280/deployments/test-recreate-deployment 339e0a39-bb27-49b9-820e-3ab835d52e3c 15420482 2 2020-03-02 21:49:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000059018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-02 21:49:34 +0000 UTC,LastTransitionTime:2020-03-02 21:49:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-02 21:49:34 +0000 UTC,LastTransitionTime:2020-03-02 21:49:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 21:49:35.186: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-9280 /apis/apps/v1/namespaces/deployment-9280/replicasets/test-recreate-deployment-5f94c574ff 51fbf4dc-1e6f-41a9-add7-1d19ad166481 15420481 1 2020-03-02 21:49:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 339e0a39-bb27-49b9-820e-3ab835d52e3c 0xc000059c97 0xc000059c98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000059df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 21:49:35.186: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 21:49:35.186: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-9280 /apis/apps/v1/namespaces/deployment-9280/replicasets/test-recreate-deployment-68fc85c7bb 85ed049f-627e-48d7-80a5-ef27d0a5495e 15420470 2 2020-03-02 21:49:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 339e0a39-bb27-49b9-820e-3ab835d52e3c 0xc000059ea7 0xc000059ea8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000059f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 21:49:35.199: INFO: Pod "test-recreate-deployment-5f94c574ff-pwvmh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-pwvmh test-recreate-deployment-5f94c574ff- deployment-9280 /api/v1/namespaces/deployment-9280/pods/test-recreate-deployment-5f94c574ff-pwvmh c35e1b02-cf2d-4624-957c-0a8bdb94dec8 15420480 0 2020-03-02 21:49:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 51fbf4dc-1e6f-41a9-add7-1d19ad166481 0xc0004b85f7 0xc0004b85f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-55clv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-55clv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-55clv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-djx6d,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 21:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 21:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 21:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 21:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 21:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:35.199: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-9280" for this suite.
Mar  2 21:49:41.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:49:44.625: INFO: namespace deployment-9280 deletion completed in 9.352133676s


• [SLOW TEST:19.863 seconds]
[sig-apps] Deployment
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:24.751: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
Mar  2 21:49:25.065: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  2 21:49:35.758: INFO: Successfully updated pod "annotationupdate3e0aef5b-911d-49a9-a12b-5fa1c8065eb4"
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:39.830: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-9305" for this suite.
Mar  2 21:49:51.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:49:55.316: INFO: namespace downward-api-9305 deletion completed in 15.378672263s


• [SLOW TEST:30.565 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:44.678: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:44.818: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-7576" for this suite.
Mar  2 21:49:56.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:00.233: INFO: namespace pods-7576 deletion completed in 15.352589999s


• [SLOW TEST:15.556 seconds]
[k8s.io] [sig-node] Pods Extended
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:24.763: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
Mar  2 21:49:25.083: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 21:49:25.102: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2181'
Mar  2 21:49:29.449: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  2 21:49:29.449: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Mar  2 21:49:29.515: INFO: scanned /root for discovery docs: <nil>
Mar  2 21:49:29.515: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2181'
Mar  2 21:49:50.658: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  2 21:49:50.658: INFO: stdout: "Created e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055\nScaling up e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar  2 21:49:50.658: INFO: stdout: "Created e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055\nScaling up e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar  2 21:49:50.658: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2181'
Mar  2 21:49:50.816: INFO: stderr: ""
Mar  2 21:49:50.816: INFO: stdout: "e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055-l765j "
Mar  2 21:49:50.816: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055-l765j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2181'
Mar  2 21:49:50.935: INFO: stderr: ""
Mar  2 21:49:50.935: INFO: stdout: "true"
Mar  2 21:49:50.935: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055-l765j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2181'
Mar  2 21:49:51.057: INFO: stderr: ""
Mar  2 21:49:51.057: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar  2 21:49:51.057: INFO: e2e-test-httpd-rc-c02778170a2b89fba8eb601b8d235055-l765j is verified up and running
[AfterEach] Kubectl rolling-update
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Mar  2 21:49:51.057: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete rc e2e-test-httpd-rc --namespace=kubectl-2181'
Mar  2 21:49:51.196: INFO: stderr: ""
Mar  2 21:49:51.196: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:49:51.196: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2181" for this suite.
Mar  2 21:49:57.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:00.716: INFO: namespace kubectl-2181 deletion completed in 9.380357586s


• [SLOW TEST:35.954 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:55.323: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  2 21:50:06.113: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9503 pod-service-account-51fbd50b-ed3f-436b-bdc6-e499f439dda9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  2 21:50:06.397: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9503 pod-service-account-51fbd50b-ed3f-436b-bdc6-e499f439dda9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  2 21:50:06.716: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9503 pod-service-account-51fbd50b-ed3f-436b-bdc6-e499f439dda9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:07.005: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-9503" for this suite.
Mar  2 21:50:13.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:16.473: INFO: namespace svcaccounts-9503 deletion completed in 9.381724947s


• [SLOW TEST:21.149 seconds]
[sig-auth] ServiceAccounts
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:00.238: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-032a7797-eabb-4d66-9b72-555ef577dc30
STEP: Creating a pod to test consume configMaps
Mar  2 21:50:00.419: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d" in namespace "projected-2967" to be "success or failure"
Mar  2 21:50:00.432: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.215949ms
Mar  2 21:50:02.446: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026256589s
Mar  2 21:50:04.459: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039786365s
Mar  2 21:50:06.472: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052818208s
Mar  2 21:50:08.486: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066430134s
Mar  2 21:50:10.500: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.080700301s
STEP: Saw pod success
Mar  2 21:50:10.501: INFO: Pod "pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d" satisfied condition "success or failure"
Mar  2 21:50:10.513: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 21:50:10.550: INFO: Waiting for pod pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d to disappear
Mar  2 21:50:10.563: INFO: Pod pod-projected-configmaps-6bd6f3b9-00ab-4e44-bc9c-b5eb5106fd9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:10.563: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2967" for this suite.
Mar  2 21:50:16.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:19.991: INFO: namespace projected-2967 deletion completed in 9.355406442s


• [SLOW TEST:19.754 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:49:43.556: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8238
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8238
STEP: creating replication controller externalsvc in namespace services-8238
I0302 21:49:43.726872   56391 runners.go:184] Created replication controller with name: externalsvc, namespace: services-8238, replica count: 2
I0302 21:49:46.778424   56391 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:49:49.778905   56391 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:49:52.779451   56391 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  2 21:49:52.829: INFO: Creating new exec pod
Mar  2 21:50:00.877: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-8238 execpodqtfc7 -- /bin/sh -x -c nslookup nodeport-service'
Mar  2 21:50:01.189: INFO: stderr: "+ nslookup nodeport-service\n"
Mar  2 21:50:01.189: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-8238.svc.cluster.local\tcanonical name = externalsvc.services-8238.svc.cluster.local.\nName:\texternalsvc.services-8238.svc.cluster.local\nAddress: 172.30.199.148\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8238, will wait for the garbage collector to delete the pods
Mar  2 21:50:01.269: INFO: Deleting ReplicationController externalsvc took: 16.658111ms
Mar  2 21:50:01.370: INFO: Terminating ReplicationController externalsvc pods took: 100.504259ms
Mar  2 21:50:15.911: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:15.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-8238" for this suite.
Mar  2 21:50:22.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:25.626: INFO: namespace services-8238 deletion completed in 9.553456985s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:42.070 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:00.740: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:50:01.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:03.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:05.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:07.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:09.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782600, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:50:12.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:12.751: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1667" for this suite.
Mar  2 21:50:21.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:24.720: INFO: namespace webhook-1667 deletion completed in 11.58299403s
STEP: Destroying namespace "webhook-1667-markers" for this suite.
Mar  2 21:50:30.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:34.302: INFO: namespace webhook-1667-markers deletion completed in 9.582084651s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.646 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:25.651: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 21:50:25.768: INFO: Waiting up to 5m0s for pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e" in namespace "emptydir-6849" to be "success or failure"
Mar  2 21:50:25.780: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.250874ms
Mar  2 21:50:27.793: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025394955s
Mar  2 21:50:29.806: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038657927s
Mar  2 21:50:31.838: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070333076s
Mar  2 21:50:33.853: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.085608907s
STEP: Saw pod success
Mar  2 21:50:33.854: INFO: Pod "pod-377b72d4-7053-4e2d-b7c2-c00913957a6e" satisfied condition "success or failure"
Mar  2 21:50:33.866: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-377b72d4-7053-4e2d-b7c2-c00913957a6e container test-container: <nil>
STEP: delete the pod
Mar  2 21:50:33.900: INFO: Waiting for pod pod-377b72d4-7053-4e2d-b7c2-c00913957a6e to disappear
Mar  2 21:50:33.912: INFO: Pod pod-377b72d4-7053-4e2d-b7c2-c00913957a6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:33.912: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6849" for this suite.
Mar  2 21:50:40.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:50:43.409: INFO: namespace emptydir-6849 deletion completed in 9.403914964s


• [SLOW TEST:17.758 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:43.435: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:50:44.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:46.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:48.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:50.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:50:52.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782643, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:50:55.424: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:55.591: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6432" for this suite.
Mar  2 21:51:01.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:05.338: INFO: namespace webhook-6432 deletion completed in 9.602942957s
STEP: Destroying namespace "webhook-6432-markers" for this suite.
Mar  2 21:51:13.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:16.790: INFO: namespace webhook-6432-markers deletion completed in 11.451532711s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.407 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:34.397: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:50:34.665: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ce8f4d14-e08c-4ccf-8b83-3e185798ea95
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ce8f4d14-e08c-4ccf-8b83-3e185798ea95
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:50:46.868: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2006" for this suite.
Mar  2 21:51:15.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:18.894: INFO: namespace projected-2006 deletion completed in 31.918566682s


• [SLOW TEST:44.496 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:16.494: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  2 21:50:16.779: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  2 21:50:43.077: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:50:50.551: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:16.956: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6808" for this suite.
Mar  2 21:51:23.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:26.812: INFO: namespace crd-publish-openapi-6808 deletion completed in 9.360955567s


• [SLOW TEST:70.319 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:16.845: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Mar  2 21:51:16.962: INFO: Asynchronously running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:17.109: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5264" for this suite.
Mar  2 21:51:23.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:26.817: INFO: namespace kubectl-5264 deletion completed in 9.512154076s


• [SLOW TEST:9.972 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:50:20.018: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:50:20.116: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Creating first CR 
Mar  2 21:50:20.351: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:19Z generation:1 name:name1 resourceVersion:15421478 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ecc6206a-4cd1-490e-9905-953e1df48f99] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  2 21:50:30.376: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:29Z generation:1 name:name2 resourceVersion:15421678 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a7104fdd-25a8-48f0-8ffa-c44249ec5538] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  2 21:50:40.392: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:19Z generation:2 name:name1 resourceVersion:15421836 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ecc6206a-4cd1-490e-9905-953e1df48f99] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  2 21:50:50.407: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:29Z generation:2 name:name2 resourceVersion:15422030 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a7104fdd-25a8-48f0-8ffa-c44249ec5538] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  2 21:51:00.656: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:19Z generation:2 name:name1 resourceVersion:15422154 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ecc6206a-4cd1-490e-9905-953e1df48f99] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  2 21:51:10.683: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-02T21:50:29Z generation:2 name:name2 resourceVersion:15422299 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a7104fdd-25a8-48f0-8ffa-c44249ec5538] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:21.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-watch-5827" for this suite.
Mar  2 21:51:29.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:32.840: INFO: namespace crd-watch-5827 deletion completed in 11.456204146s


• [SLOW TEST:72.823 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:26.825: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  2 21:51:27.133: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:38.921: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-6207" for this suite.
Mar  2 21:51:45.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:48.531: INFO: namespace init-container-6207 deletion completed in 9.464122409s


• [SLOW TEST:21.707 seconds]
[k8s.io] InitContainer [NodeConformance]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:18.905: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:51:20.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:22.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:24.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:26.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782679, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:51:29.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  2 21:51:29.348: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:29.398: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4928" for this suite.
Mar  2 21:51:35.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:39.071: INFO: namespace webhook-4928 deletion completed in 9.481771801s
STEP: Destroying namespace "webhook-4928-markers" for this suite.
Mar  2 21:51:45.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:48.615: INFO: namespace webhook-4928-markers deletion completed in 9.544198215s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:29.853 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:32.859: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:51:33.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc" in namespace "projected-771" to be "success or failure"
Mar  2 21:51:33.030: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Pending", Reason="", readiness=false. Elapsed: 26.962171ms
Mar  2 21:51:35.045: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041926559s
Mar  2 21:51:37.072: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069543071s
Mar  2 21:51:39.103: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100481936s
Mar  2 21:51:41.116: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113235772s
Mar  2 21:51:43.129: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.126250968s
STEP: Saw pod success
Mar  2 21:51:43.129: INFO: Pod "downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc" satisfied condition "success or failure"
Mar  2 21:51:43.142: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc container client-container: <nil>
STEP: delete the pod
Mar  2 21:51:43.177: INFO: Waiting for pod downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc to disappear
Mar  2 21:51:43.189: INFO: Pod downwardapi-volume-67c899f0-f61a-4e02-a590-6b9d398890bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:43.189: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-771" for this suite.
Mar  2 21:51:51.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:51:54.767: INFO: namespace projected-771 deletion completed in 11.352951976s


• [SLOW TEST:21.908 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:26.881: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:51:27.347: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  2 21:51:33.049: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 create -f -'
Mar  2 21:51:37.796: INFO: stderr: ""
Mar  2 21:51:37.796: INFO: stdout: "e2e-test-crd-publish-openapi-9346-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 21:51:37.796: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 delete e2e-test-crd-publish-openapi-9346-crds test-foo'
Mar  2 21:51:37.968: INFO: stderr: ""
Mar  2 21:51:37.969: INFO: stdout: "e2e-test-crd-publish-openapi-9346-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 21:51:37.969: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 apply -f -'
Mar  2 21:51:39.590: INFO: stderr: ""
Mar  2 21:51:39.590: INFO: stdout: "e2e-test-crd-publish-openapi-9346-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 21:51:39.590: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 delete e2e-test-crd-publish-openapi-9346-crds test-foo'
Mar  2 21:51:39.736: INFO: stderr: ""
Mar  2 21:51:39.736: INFO: stdout: "e2e-test-crd-publish-openapi-9346-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  2 21:51:39.736: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 create -f -'
Mar  2 21:51:40.227: INFO: rc: 1
Mar  2 21:51:40.231: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 apply -f -'
Mar  2 21:51:40.904: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  2 21:51:40.904: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 create -f -'
Mar  2 21:51:41.483: INFO: rc: 1
Mar  2 21:51:41.484: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-1346 apply -f -'
Mar  2 21:51:42.354: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  2 21:51:42.354: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9346-crds'
Mar  2 21:51:42.890: INFO: stderr: ""
Mar  2 21:51:42.890: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9346-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  2 21:51:42.891: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9346-crds.metadata'
Mar  2 21:51:43.572: INFO: stderr: ""
Mar  2 21:51:43.572: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9346-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 21:51:43.573: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9346-crds.spec'
Mar  2 21:51:44.722: INFO: stderr: ""
Mar  2 21:51:44.722: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9346-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 21:51:44.722: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9346-crds.spec.bars'
Mar  2 21:51:45.342: INFO: stderr: ""
Mar  2 21:51:45.342: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9346-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  2 21:51:45.342: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9346-crds.spec.bars2'
Mar  2 21:51:45.960: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:51.921: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1346" for this suite.
Mar  2 21:51:58.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:01.828: INFO: namespace crd-publish-openapi-1346 deletion completed in 9.62617097s


• [SLOW TEST:34.948 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:54.823: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Mar  2 21:51:54.935: INFO: Asynchronously running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix824133572/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:54.991: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6366" for this suite.
Mar  2 21:52:01.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:04.453: INFO: namespace kubectl-6366 deletion completed in 9.352156506s


• [SLOW TEST:9.630 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:48.774: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 21:51:49.027: INFO: Waiting up to 5m0s for pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b" in namespace "emptydir-8495" to be "success or failure"
Mar  2 21:51:49.046: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.677036ms
Mar  2 21:51:51.069: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041935272s
Mar  2 21:51:53.089: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061993894s
Mar  2 21:51:55.109: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082120806s
Mar  2 21:51:57.130: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103000115s
Mar  2 21:51:59.150: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.123270751s
STEP: Saw pod success
Mar  2 21:51:59.150: INFO: Pod "pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b" satisfied condition "success or failure"
Mar  2 21:51:59.170: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b container test-container: <nil>
STEP: delete the pod
Mar  2 21:51:59.217: INFO: Waiting for pod pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b to disappear
Mar  2 21:51:59.236: INFO: Pod pod-cf12dc2f-64e3-4a7b-a0a5-0331c831cd9b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:51:59.236: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8495" for this suite.
Mar  2 21:52:07.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:10.863: INFO: namespace emptydir-8495 deletion completed in 11.384926052s


• [SLOW TEST:22.089 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:51:48.542: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:51:49.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:51.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:53.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:55.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:51:57.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782708, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:52:00.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:52:00.733: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5744-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:01.446: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-3834" for this suite.
Mar  2 21:52:09.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:12.943: INFO: namespace webhook-3834 deletion completed in 11.373213165s
STEP: Destroying namespace "webhook-3834-markers" for this suite.
Mar  2 21:52:18.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:22.299: INFO: namespace webhook-3834-markers deletion completed in 9.355547997s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:04.482: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 21:52:04.654: INFO: Waiting up to 5m0s for pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4" in namespace "emptydir-1680" to be "success or failure"
Mar  2 21:52:04.672: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.27946ms
Mar  2 21:52:06.690: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036003249s
Mar  2 21:52:08.703: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04914481s
Mar  2 21:52:10.717: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062594228s
Mar  2 21:52:12.731: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077030278s
Mar  2 21:52:14.745: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.091008336s
STEP: Saw pod success
Mar  2 21:52:14.745: INFO: Pod "pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4" satisfied condition "success or failure"
Mar  2 21:52:14.758: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4 container test-container: <nil>
STEP: delete the pod
Mar  2 21:52:14.797: INFO: Waiting for pod pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4 to disappear
Mar  2 21:52:14.810: INFO: Pod pod-9b41e7cd-5ae1-4d84-b8bd-37e5455bbab4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:14.810: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1680" for this suite.
Mar  2 21:52:20.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:24.306: INFO: namespace emptydir-1680 deletion completed in 9.387426754s


• [SLOW TEST:19.824 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:10.876: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:52:11.038: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92" in namespace "security-context-test-9916" to be "success or failure"
Mar  2 21:52:11.057: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800592ms
Mar  2 21:52:13.078: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039969949s
Mar  2 21:52:15.098: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060074988s
Mar  2 21:52:17.119: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080833894s
Mar  2 21:52:19.140: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101334337s
Mar  2 21:52:21.160: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121344876s
Mar  2 21:52:21.160: INFO: Pod "busybox-readonly-false-5762be7a-138d-42a9-9224-5257f41e5f92" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:21.160: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-9916" for this suite.
Mar  2 21:52:27.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:30.692: INFO: namespace security-context-test-9916 deletion completed in 9.408186767s


• [SLOW TEST:19.817 seconds]
[k8s.io] Security Context
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:24.391: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-469b1d4f-9c73-4af6-a073-f07a66266eb2
STEP: Creating a pod to test consume configMaps
Mar  2 21:52:24.568: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919" in namespace "configmap-6074" to be "success or failure"
Mar  2 21:52:24.580: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Pending", Reason="", readiness=false. Elapsed: 11.961136ms
Mar  2 21:52:26.594: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025369268s
Mar  2 21:52:28.607: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038771756s
Mar  2 21:52:30.620: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051899818s
Mar  2 21:52:32.634: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065348629s
Mar  2 21:52:34.648: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.079947668s
STEP: Saw pod success
Mar  2 21:52:34.649: INFO: Pod "pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919" satisfied condition "success or failure"
Mar  2 21:52:34.662: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 21:52:34.701: INFO: Waiting for pod pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919 to disappear
Mar  2 21:52:34.713: INFO: Pod pod-configmaps-ae4cc594-75cc-439a-ab32-244363f06919 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:34.714: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6074" for this suite.
Mar  2 21:52:40.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:44.192: INFO: namespace configmap-6074 deletion completed in 9.353494408s


• [SLOW TEST:19.802 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:30.705: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 21:52:37.252997   56389 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 21:52:37.253: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:37.253: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-3252" for this suite.
Mar  2 21:52:43.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:46.743: INFO: namespace gc-3252 deletion completed in 9.382145139s


• [SLOW TEST:16.038 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:44.195: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0302 21:52:44.466303   56390 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 21:52:44.466: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:44.466: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-4752" for this suite.
Mar  2 21:52:50.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:52:53.874: INFO: namespace gc-4752 deletion completed in 9.354027159s


• [SLOW TEST:9.679 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:22.371: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-5914da58-ece3-4958-a9aa-b3c582aab15d in namespace container-probe-2386
Mar  2 21:52:32.535: INFO: Started pod liveness-5914da58-ece3-4958-a9aa-b3c582aab15d in namespace container-probe-2386
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 21:52:32.548: INFO: Initial restart count of pod liveness-5914da58-ece3-4958-a9aa-b3c582aab15d is 0
Mar  2 21:52:54.727: INFO: Restart count of pod container-probe-2386/liveness-5914da58-ece3-4958-a9aa-b3c582aab15d is now 1 (22.178669622s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:52:54.748: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2386" for this suite.
Mar  2 21:53:00.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:04.194: INFO: namespace container-probe-2386 deletion completed in 9.352278841s


• [SLOW TEST:41.823 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:01.836: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-bcf7ad3f-0797-4d1e-aa04-0606b6841fe6 in namespace container-probe-4986
Mar  2 21:52:12.086: INFO: Started pod busybox-bcf7ad3f-0797-4d1e-aa04-0606b6841fe6 in namespace container-probe-4986
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 21:52:12.115: INFO: Initial restart count of pod busybox-bcf7ad3f-0797-4d1e-aa04-0606b6841fe6 is 0
Mar  2 21:53:00.654: INFO: Restart count of pod container-probe-4986/busybox-bcf7ad3f-0797-4d1e-aa04-0606b6841fe6 is now 1 (48.539260783s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:00.683: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-4986" for this suite.
Mar  2 21:53:06.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:10.273: INFO: namespace container-probe-4986 deletion completed in 9.38827192s


• [SLOW TEST:68.437 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:53.877: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-68b48616-7e48-4789-bbb5-2fb3a6680a5e
STEP: Creating a pod to test consume configMaps
Mar  2 21:52:54.035: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426" in namespace "projected-852" to be "success or failure"
Mar  2 21:52:54.048: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Pending", Reason="", readiness=false. Elapsed: 13.621004ms
Mar  2 21:52:56.062: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027213803s
Mar  2 21:52:58.076: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0411807s
Mar  2 21:53:00.089: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054615528s
Mar  2 21:53:02.103: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067925336s
Mar  2 21:53:04.116: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.081472514s
STEP: Saw pod success
Mar  2 21:53:04.116: INFO: Pod "pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426" satisfied condition "success or failure"
Mar  2 21:53:04.129: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 21:53:04.165: INFO: Waiting for pod pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426 to disappear
Mar  2 21:53:04.176: INFO: Pod pod-projected-configmaps-2f66cb77-79b4-4290-b846-155b7dc49426 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:04.177: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-852" for this suite.
Mar  2 21:53:10.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:13.610: INFO: namespace projected-852 deletion completed in 9.360816642s


• [SLOW TEST:19.733 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:04.204: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 21:53:13.575: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:13.607: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-2876" for this suite.
Mar  2 21:53:19.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:23.100: INFO: namespace container-runtime-2876 deletion completed in 9.358522968s


• [SLOW TEST:18.896 seconds]
[k8s.io] Container Runtime
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:10.327: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:53:10.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c" in namespace "downward-api-4260" to be "success or failure"
Mar  2 21:53:10.523: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.130695ms
Mar  2 21:53:12.543: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041886912s
Mar  2 21:53:14.566: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064015864s
Mar  2 21:53:16.587: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.085475773s
Mar  2 21:53:18.608: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106757787s
Mar  2 21:53:20.630: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.128795677s
STEP: Saw pod success
Mar  2 21:53:20.631: INFO: Pod "downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c" satisfied condition "success or failure"
Mar  2 21:53:20.651: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c container client-container: <nil>
STEP: delete the pod
Mar  2 21:53:20.707: INFO: Waiting for pod downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c to disappear
Mar  2 21:53:20.728: INFO: Pod downwardapi-volume-22e4b84d-8849-4b3d-880b-a667cfb72d7c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:20.728: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4260" for this suite.
Mar  2 21:53:28.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:32.657: INFO: namespace downward-api-4260 deletion completed in 11.745032003s


• [SLOW TEST:22.329 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:13.618: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-45vx7 in namespace proxy-7797
I0302 21:53:13.780167   56390 runners.go:184] Created replication controller with name: proxy-service-45vx7, namespace: proxy-7797, replica count: 1
I0302 21:53:14.830934   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:15.831531   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:16.832015   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:17.832361   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:18.832915   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:19.833385   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:20.833752   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:21.834125   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:53:22.834584   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 21:53:23.835128   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 21:53:24.835604   56390 runners.go:184] proxy-service-45vx7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 21:53:24.849: INFO: setup took 11.109694235s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 21:53:24.869: INFO: (0) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 20.236941ms)
Mar  2 21:53:24.869: INFO: (0) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 20.239334ms)
Mar  2 21:53:24.869: INFO: (0) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 20.092196ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 29.997056ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 30.109194ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 30.041517ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 30.066873ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 30.075149ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 30.141695ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 30.247311ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 30.150971ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 30.10505ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 30.193947ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 30.202377ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 30.067213ms)
Mar  2 21:53:24.879: INFO: (0) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 30.496811ms)
Mar  2 21:53:24.894: INFO: (1) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 14.654229ms)
Mar  2 21:53:24.895: INFO: (1) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 15.308543ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 25.110074ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 25.200466ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.04406ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 25.082499ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.019128ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 25.137686ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.202679ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 25.133759ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 25.064864ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 25.056741ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 25.091543ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 25.130681ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 25.068375ms)
Mar  2 21:53:24.905: INFO: (1) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.134324ms)
Mar  2 21:53:24.921: INFO: (2) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 15.549029ms)
Mar  2 21:53:24.921: INFO: (2) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 16.01264ms)
Mar  2 21:53:24.921: INFO: (2) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 16.069985ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 36.484254ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 36.608975ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 36.688744ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 36.588703ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 36.654459ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 36.39708ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 36.595984ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 36.646003ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 36.6978ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 36.643217ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 36.671249ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 36.661149ms)
Mar  2 21:53:24.942: INFO: (2) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 36.696794ms)
Mar  2 21:53:24.959: INFO: (3) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 16.332416ms)
Mar  2 21:53:24.959: INFO: (3) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 16.369685ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 26.19818ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 26.250193ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 26.275067ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 26.287929ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 26.315391ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 26.296033ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 26.362935ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 26.383242ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 26.41645ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 26.329314ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 26.356396ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 26.336059ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 26.337451ms)
Mar  2 21:53:24.969: INFO: (3) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 26.390078ms)
Mar  2 21:53:24.985: INFO: (4) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 15.80361ms)
Mar  2 21:53:24.985: INFO: (4) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 15.792641ms)
Mar  2 21:53:24.985: INFO: (4) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 16.004345ms)
Mar  2 21:53:24.985: INFO: (4) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 15.722025ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.50022ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 25.933617ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.528886ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 25.672327ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 25.966995ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.849066ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 25.765244ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 25.615674ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 25.744409ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 25.856782ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 25.566354ms)
Mar  2 21:53:24.995: INFO: (4) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 25.520881ms)
Mar  2 21:53:25.010: INFO: (5) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 14.147167ms)
Mar  2 21:53:25.010: INFO: (5) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 14.491247ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.151818ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 24.214013ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.292661ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 24.337605ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 24.236443ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 24.297242ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.296904ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 24.240484ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 24.290879ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 24.357754ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 24.365801ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 24.403678ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 24.30812ms)
Mar  2 21:53:25.020: INFO: (5) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 24.421347ms)
Mar  2 21:53:25.035: INFO: (6) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 14.955944ms)
Mar  2 21:53:25.036: INFO: (6) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 15.301804ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 25.372279ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 25.425025ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 25.330454ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 25.404797ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 25.486258ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 25.850547ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 25.788646ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 25.801877ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 25.770644ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.808086ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.775354ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 25.918821ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 25.847914ms)
Mar  2 21:53:25.046: INFO: (6) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 25.84425ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 29.772691ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 29.969312ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 29.872938ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 29.880974ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.880728ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 29.979613ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.834012ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 29.950494ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 29.885573ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 29.86ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 29.953008ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 29.871874ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 29.973089ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 29.902358ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 30.008142ms)
Mar  2 21:53:25.077: INFO: (7) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 29.916528ms)
Mar  2 21:53:25.093: INFO: (8) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 15.413286ms)
Mar  2 21:53:25.093: INFO: (8) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 15.34152ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 25.542448ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.629995ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 25.680753ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.645421ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 25.6641ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 25.599992ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 25.637079ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.638289ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 25.701873ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 25.825348ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 25.849957ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 25.69524ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 25.813224ms)
Mar  2 21:53:25.103: INFO: (8) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 25.760491ms)
Mar  2 21:53:25.118: INFO: (9) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 14.675584ms)
Mar  2 21:53:25.118: INFO: (9) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 14.701323ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 24.632958ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.666146ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 24.624726ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 24.713628ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.736173ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 24.71393ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 24.769183ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.665958ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 24.667748ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 24.90941ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 24.789902ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 24.815504ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 24.794976ms)
Mar  2 21:53:25.128: INFO: (9) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 24.836579ms)
Mar  2 21:53:25.143: INFO: (10) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 13.800358ms)
Mar  2 21:53:25.143: INFO: (10) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 13.933285ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 23.788424ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 23.80757ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 23.783314ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 23.88462ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 23.825683ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 23.90951ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 23.852605ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 23.865786ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 23.949116ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 23.905228ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 23.979781ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 23.934515ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 23.893733ms)
Mar  2 21:53:25.153: INFO: (10) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 24.023008ms)
Mar  2 21:53:25.167: INFO: (11) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 13.507921ms)
Mar  2 21:53:25.167: INFO: (11) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 13.662003ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 74.307155ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 74.28623ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 74.278568ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 74.176606ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 74.422089ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 74.231807ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 74.351066ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 74.280708ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 74.383288ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 74.379005ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 74.505843ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 74.25455ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 74.33364ms)
Mar  2 21:53:25.228: INFO: (11) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 74.279474ms)
Mar  2 21:53:25.242: INFO: (12) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 13.87934ms)
Mar  2 21:53:25.243: INFO: (12) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 14.511286ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 23.811587ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 23.820342ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 23.763762ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 24.107255ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 23.892969ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 23.882471ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 23.927552ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 23.986246ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.047756ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 23.895883ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 23.888121ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 23.946321ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 23.986495ms)
Mar  2 21:53:25.252: INFO: (12) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 23.999394ms)
Mar  2 21:53:25.267: INFO: (13) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 14.145859ms)
Mar  2 21:53:25.267: INFO: (13) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 14.3698ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 24.412715ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 24.378219ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 24.408029ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 24.384618ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 24.428509ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.473205ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 24.381452ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 24.428053ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 24.435222ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.467188ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 24.409679ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.419211ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 24.50012ms)
Mar  2 21:53:25.277: INFO: (13) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.516099ms)
Mar  2 21:53:25.292: INFO: (14) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 14.120338ms)
Mar  2 21:53:25.292: INFO: (14) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 14.149632ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 24.07926ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 24.031705ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 24.156409ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 24.139558ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 24.094632ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 24.186931ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 24.210336ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 24.287333ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 24.192197ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 24.151246ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 24.228735ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 24.290532ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 24.125978ms)
Mar  2 21:53:25.302: INFO: (14) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 24.176509ms)
Mar  2 21:53:25.316: INFO: (15) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 13.767712ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.59337ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 29.847511ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 29.692836ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 29.88507ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.877533ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 29.874068ms)
Mar  2 21:53:25.332: INFO: (15) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 29.831988ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 29.821902ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 29.728662ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 29.738939ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 29.982443ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 29.849933ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 29.808248ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 29.978686ms)
Mar  2 21:53:25.333: INFO: (15) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 29.905918ms)
Mar  2 21:53:25.349: INFO: (16) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 15.812386ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 25.728974ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.701299ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.77867ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.80762ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 25.777883ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 25.731419ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 25.863668ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 25.679055ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 25.800002ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 25.760329ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 25.907968ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 25.847298ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 25.818706ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 25.812452ms)
Mar  2 21:53:25.359: INFO: (16) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 25.821817ms)
Mar  2 21:53:25.405: INFO: (17) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 45.98366ms)
Mar  2 21:53:25.406: INFO: (17) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 46.328273ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 55.977958ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 56.178911ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 56.279136ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 56.10963ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 56.28993ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 56.079172ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 56.109874ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 56.177026ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 56.462444ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 56.362204ms)
Mar  2 21:53:25.416: INFO: (17) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 56.476612ms)
Mar  2 21:53:25.456: INFO: (17) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 96.020041ms)
Mar  2 21:53:25.456: INFO: (17) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 96.033073ms)
Mar  2 21:53:25.456: INFO: (17) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 95.994936ms)
Mar  2 21:53:25.475: INFO: (18) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 19.277896ms)
Mar  2 21:53:25.475: INFO: (18) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 19.175445ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 29.112517ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 29.257571ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 29.162687ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 29.240239ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 29.22835ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.365791ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 29.203523ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 29.22732ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 29.24857ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 29.310352ms)
Mar  2 21:53:25.485: INFO: (18) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 29.245973ms)
Mar  2 21:53:25.486: INFO: (18) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 29.859336ms)
Mar  2 21:53:25.495: INFO: (18) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 38.998511ms)
Mar  2 21:53:25.495: INFO: (18) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 38.960819ms)
Mar  2 21:53:25.521: INFO: (19) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:160/proxy/: foo (200; 25.802043ms)
Mar  2 21:53:25.523: INFO: (19) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:1080/proxy/rewriteme">... (200; 28.142732ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:162/proxy/: bar (200; 35.649852ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname1/proxy/: tls baz (200; 35.901025ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname2/proxy/: bar (200; 35.962487ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:160/proxy/: foo (200; 35.654386ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/pods/http:proxy-service-45vx7-24zl6:162/proxy/: bar (200; 35.809754ms)
Mar  2 21:53:25.531: INFO: (19) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:460/proxy/: tls baz (200; 35.617757ms)
Mar  2 21:53:25.533: INFO: (19) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:462/proxy/: tls qux (200; 38.46075ms)
Mar  2 21:53:25.533: INFO: (19) /api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/https:proxy-service-45vx7-24zl6:443/proxy/tlsrewritem... (200; 37.98388ms)
Mar  2 21:53:25.533: INFO: (19) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6:1080/proxy/rewriteme">test<... (200; 38.193605ms)
Mar  2 21:53:25.533: INFO: (19) /api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/: <a href="/api/v1/namespaces/proxy-7797/pods/proxy-service-45vx7-24zl6/proxy/rewriteme">test</a> (200; 38.385405ms)
Mar  2 21:53:25.542: INFO: (19) /api/v1/namespaces/proxy-7797/services/https:proxy-service-45vx7:tlsportname2/proxy/: tls qux (200; 46.420593ms)
Mar  2 21:53:25.547: INFO: (19) /api/v1/namespaces/proxy-7797/services/proxy-service-45vx7:portname1/proxy/: foo (200; 51.726639ms)
Mar  2 21:53:25.547: INFO: (19) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname1/proxy/: foo (200; 51.878142ms)
Mar  2 21:53:25.554: INFO: (19) /api/v1/namespaces/proxy-7797/services/http:proxy-service-45vx7:portname2/proxy/: bar (200; 58.319076ms)
STEP: deleting ReplicationController proxy-service-45vx7 in namespace proxy-7797, will wait for the garbage collector to delete the pods
Mar  2 21:53:25.646: INFO: Deleting ReplicationController proxy-service-45vx7 took: 23.1045ms
Mar  2 21:53:25.847: INFO: Terminating ReplicationController proxy-service-45vx7 pods took: 200.310432ms
[AfterEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:28.048: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-7797" for this suite.
Mar  2 21:53:34.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:53:35.578: INFO: namespace proxy-7797 deletion completed in 7.35448396s


• [SLOW TEST:21.960 seconds]
[sig-network] Proxy
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:23.103: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  2 21:53:23.290: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:53:29.775: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:54.149: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8001" for this suite.
Mar  2 21:54:00.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:03.577: INFO: namespace crd-publish-openapi-8001 deletion completed in 9.353741376s


• [SLOW TEST:40.475 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:52:46.751: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Mar  2 21:52:46.878: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-492'
Mar  2 21:52:47.853: INFO: stderr: ""
Mar  2 21:52:47.853: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 21:52:47.854: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Mar  2 21:52:48.002: INFO: stderr: ""
Mar  2 21:52:48.002: INFO: stdout: "update-demo-nautilus-7cz84 update-demo-nautilus-lsscs "
Mar  2 21:52:48.002: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-7cz84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:48.120: INFO: stderr: ""
Mar  2 21:52:48.120: INFO: stdout: ""
Mar  2 21:52:48.120: INFO: update-demo-nautilus-7cz84 is created but not running
Mar  2 21:52:53.120: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Mar  2 21:52:53.243: INFO: stderr: ""
Mar  2 21:52:53.243: INFO: stdout: "update-demo-nautilus-7cz84 update-demo-nautilus-lsscs "
Mar  2 21:52:53.243: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-7cz84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:53.394: INFO: stderr: ""
Mar  2 21:52:53.394: INFO: stdout: ""
Mar  2 21:52:53.394: INFO: update-demo-nautilus-7cz84 is created but not running
Mar  2 21:52:58.394: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Mar  2 21:52:58.547: INFO: stderr: ""
Mar  2 21:52:58.547: INFO: stdout: "update-demo-nautilus-7cz84 update-demo-nautilus-lsscs "
Mar  2 21:52:58.547: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-7cz84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:58.668: INFO: stderr: ""
Mar  2 21:52:58.669: INFO: stdout: "true"
Mar  2 21:52:58.669: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-7cz84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:58.817: INFO: stderr: ""
Mar  2 21:52:58.817: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:52:58.817: INFO: validating pod update-demo-nautilus-7cz84
Mar  2 21:52:58.840: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:52:58.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:52:58.840: INFO: update-demo-nautilus-7cz84 is verified up and running
Mar  2 21:52:58.840: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-lsscs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:58.986: INFO: stderr: ""
Mar  2 21:52:58.986: INFO: stdout: "true"
Mar  2 21:52:58.986: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-lsscs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:52:59.106: INFO: stderr: ""
Mar  2 21:52:59.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:52:59.106: INFO: validating pod update-demo-nautilus-lsscs
Mar  2 21:52:59.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:52:59.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:52:59.129: INFO: update-demo-nautilus-lsscs is verified up and running
STEP: rolling-update to new replication controller
Mar  2 21:52:59.163: INFO: scanned /root for discovery docs: <nil>
Mar  2 21:52:59.164: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-492'
Mar  2 21:53:32.628: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  2 21:53:32.628: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 21:53:32.628: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-492'
Mar  2 21:53:32.798: INFO: stderr: ""
Mar  2 21:53:32.798: INFO: stdout: "update-demo-kitten-cbfdl update-demo-kitten-rxfpg "
Mar  2 21:53:32.798: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-kitten-cbfdl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:53:32.919: INFO: stderr: ""
Mar  2 21:53:32.919: INFO: stdout: "true"
Mar  2 21:53:32.919: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-kitten-cbfdl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:53:33.038: INFO: stderr: ""
Mar  2 21:53:33.039: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  2 21:53:33.039: INFO: validating pod update-demo-kitten-cbfdl
Mar  2 21:53:33.061: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  2 21:53:33.061: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  2 21:53:33.061: INFO: update-demo-kitten-cbfdl is verified up and running
Mar  2 21:53:33.061: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-kitten-rxfpg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:53:33.207: INFO: stderr: ""
Mar  2 21:53:33.207: INFO: stdout: "true"
Mar  2 21:53:33.207: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-kitten-rxfpg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-492'
Mar  2 21:53:33.327: INFO: stderr: ""
Mar  2 21:53:33.327: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar  2 21:53:33.327: INFO: validating pod update-demo-kitten-rxfpg
Mar  2 21:53:33.360: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  2 21:53:33.360: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  2 21:53:33.360: INFO: update-demo-kitten-rxfpg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:33.360: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-492" for this suite.
Mar  2 21:54:01.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:04.900: INFO: namespace kubectl-492 deletion completed in 31.381491905s


• [SLOW TEST:78.149 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:32.689: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  2 21:53:41.102: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:41.179: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-4954" for this suite.
Mar  2 21:54:09.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:12.781: INFO: namespace replicaset-4954 deletion completed in 31.383691478s


• [SLOW TEST:40.092 seconds]
[sig-apps] ReplicaSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:53:35.583: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  2 21:53:46.328: INFO: Successfully updated pod "labelsupdatec18b3660-17e4-4c29-99b7-ba2077808fca"
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:53:48.370: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1446" for this suite.
Mar  2 21:54:18.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:21.857: INFO: namespace projected-1446 deletion completed in 33.362101139s


• [SLOW TEST:46.274 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:04.910: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 21:54:13.332: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:13.408: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-1649" for this suite.
Mar  2 21:54:19.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:23.001: INFO: namespace container-runtime-1649 deletion completed in 9.382016715s


• [SLOW TEST:18.091 seconds]
[k8s.io] Container Runtime
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:03.587: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:54:04.236: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 21:54:06.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:08.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:10.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:12.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782843, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:54:15.311: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:54:15.324: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:16.237: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-1474" for this suite.
Mar  2 21:54:22.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:25.757: INFO: namespace crd-webhook-1474 deletion completed in 9.367692764s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:22.225 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:21.865: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Mar  2 21:54:22.021: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig api-versions'
Mar  2 21:54:22.486: INFO: stderr: ""
Mar  2 21:54:22.486: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nlogging.openshift.io/v1\nlogging.openshift.io/v1alpha1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nsplunkforwarder.managed.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:22.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8320" for this suite.
Mar  2 21:54:28.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:32.029: INFO: namespace kubectl-8320 deletion completed in 9.389054342s


• [SLOW TEST:10.165 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:12.815: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:54:13.687: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:15.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:17.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:19.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:54:21.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782853, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:54:24.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:25.292: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9869" for this suite.
Mar  2 21:54:31.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:34.826: INFO: namespace webhook-9869 deletion completed in 9.385229157s
STEP: Destroying namespace "webhook-9869-markers" for this suite.
Mar  2 21:54:40.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:44.212: INFO: namespace webhook-9869-markers deletion completed in 9.385938117s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:31.483 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:32.100: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1675/secret-test-d7e95eab-5fe1-40e6-98c6-33562391c2c4
STEP: Creating a pod to test consume secrets
Mar  2 21:54:32.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366" in namespace "secrets-1675" to be "success or failure"
Mar  2 21:54:32.257: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366": Phase="Pending", Reason="", readiness=false. Elapsed: 12.282996ms
Mar  2 21:54:34.270: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025784861s
Mar  2 21:54:36.284: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039310953s
Mar  2 21:54:38.298: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053729884s
Mar  2 21:54:40.314: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069340998s
STEP: Saw pod success
Mar  2 21:54:40.314: INFO: Pod "pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366" satisfied condition "success or failure"
Mar  2 21:54:40.328: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366 container env-test: <nil>
STEP: delete the pod
Mar  2 21:54:40.366: INFO: Waiting for pod pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366 to disappear
Mar  2 21:54:40.378: INFO: Pod pod-configmaps-fb44d00b-017c-4a6e-b088-d645c7205366 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:40.378: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1675" for this suite.
Mar  2 21:54:46.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:54:49.804: INFO: namespace secrets-1675 deletion completed in 9.352793692s


• [SLOW TEST:17.704 seconds]
[sig-api-machinery] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:25.854: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Mar  2 21:54:25.952: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  2 21:54:25.952: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:26.679: INFO: stderr: ""
Mar  2 21:54:26.679: INFO: stdout: "service/redis-slave created\n"
Mar  2 21:54:26.679: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  2 21:54:26.679: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:27.592: INFO: stderr: ""
Mar  2 21:54:27.592: INFO: stdout: "service/redis-master created\n"
Mar  2 21:54:27.592: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 21:54:27.593: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:28.166: INFO: stderr: ""
Mar  2 21:54:28.166: INFO: stdout: "service/frontend created\n"
Mar  2 21:54:28.166: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  2 21:54:28.166: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:28.838: INFO: stderr: ""
Mar  2 21:54:28.839: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 21:54:28.839: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 21:54:28.839: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:29.322: INFO: stderr: ""
Mar  2 21:54:29.322: INFO: stdout: "deployment.apps/redis-master created\n"
Mar  2 21:54:29.322: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  2 21:54:29.322: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-3248'
Mar  2 21:54:30.104: INFO: stderr: ""
Mar  2 21:54:30.104: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar  2 21:54:30.104: INFO: Waiting for all frontend pods to be Running.
Mar  2 21:54:55.155: INFO: Waiting for frontend to serve content.
Mar  2 21:54:55.181: INFO: Trying to add a new entry to the guestbook.
Mar  2 21:54:55.202: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  2 21:55:00.233: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:00.380: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:00.380: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 21:55:00.380: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:00.558: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:00.558: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 21:55:00.558: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:00.742: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:00.742: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 21:55:00.742: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:00.875: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:00.875: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 21:55:00.875: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:01.006: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:01.006: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 21:55:01.007: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3248'
Mar  2 21:55:01.169: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:55:01.169: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:01.169: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3248" for this suite.
Mar  2 21:55:13.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:16.647: INFO: namespace kubectl-3248 deletion completed in 15.353611553s


• [SLOW TEST:50.793 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:49.807: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:54:50.047: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-143ac111-efd1-4ff9-be22-fba5f2ee4dad
STEP: Creating configMap with name cm-test-opt-upd-d06962f9-da2d-4a3a-91b6-014f59b5c081
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-143ac111-efd1-4ff9-be22-fba5f2ee4dad
STEP: Updating configmap cm-test-opt-upd-d06962f9-da2d-4a3a-91b6-014f59b5c081
STEP: Creating configMap with name cm-test-opt-create-42e2ab38-5ed8-49dc-b643-7ec369fd2419
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:02.291: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6727" for this suite.
Mar  2 21:55:30.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:33.857: INFO: namespace projected-6727 deletion completed in 31.391289629s


• [SLOW TEST:44.051 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:23.012: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4134
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4134
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4134
Mar  2 21:54:23.439: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 21:54:33.460: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 21:54:33.480: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 21:54:33.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 21:54:33.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 21:54:33.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 21:54:33.867: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 21:54:43.887: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 21:54:43.887: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 21:54:43.968: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 21:54:43.968: INFO: ss-0  ip-10-0-130-27.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  }]
Mar  2 21:54:43.969: INFO: ss-1  ip-10-0-137-145.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:54:43.969: INFO: 
Mar  2 21:54:43.969: INFO: StatefulSet ss has not reached scale 3, at 2
Mar  2 21:54:45.006: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97922961s
Mar  2 21:54:46.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.941783728s
Mar  2 21:54:47.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.903041167s
Mar  2 21:54:48.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.865704427s
Mar  2 21:54:49.140: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.829013477s
Mar  2 21:54:50.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.808174637s
Mar  2 21:54:51.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.78767237s
Mar  2 21:54:52.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.762568126s
Mar  2 21:54:53.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 741.313391ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4134
Mar  2 21:54:54.247: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 21:54:54.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 21:54:54.589: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 21:54:54.589: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 21:54:54.589: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 21:54:54.910: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 21:54:54.910: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 21:54:54.910: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 21:54:54.910: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 21:54:55.101: INFO: rc: 1
Mar  2 21:54:55.102: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc0033c78c0 exit status 1 <nil> <nil> true [0xc0025c2198 0xc0025c21b0 0xc0025c21d0] [0xc0025c2198 0xc0025c21b0 0xc0025c21d0] [0xc0025c21a8 0xc0025c21c8] [0x10efd60 0x10efd60] 0xc003379620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 21:55:05.102: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 21:55:05.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 21:55:05.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 21:55:05.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 21:55:05.408: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:55:05.408: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:55:05.408: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  2 21:55:05.429: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 21:55:05.758: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 21:55:05.758: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 21:55:05.758: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 21:55:05.758: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 21:55:06.048: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 21:55:06.048: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 21:55:06.048: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 21:55:06.048: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-4134 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 21:55:06.343: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 21:55:06.343: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 21:55:06.343: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 21:55:06.343: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 21:55:06.363: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 21:55:16.404: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 21:55:16.404: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 21:55:16.404: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 21:55:16.467: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 21:55:16.468: INFO: ss-0  ip-10-0-130-27.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  }]
Mar  2 21:55:16.468: INFO: ss-1  ip-10-0-137-145.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:16.468: INFO: ss-2  ip-10-0-141-17.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:16.468: INFO: 
Mar  2 21:55:16.468: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 21:55:17.489: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Mar  2 21:55:17.489: INFO: ss-0  ip-10-0-130-27.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:22 +0000 UTC  }]
Mar  2 21:55:17.489: INFO: ss-1  ip-10-0-137-145.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:17.489: INFO: ss-2  ip-10-0-141-17.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:17.489: INFO: 
Mar  2 21:55:17.489: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 21:55:18.509: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 21:55:18.509: INFO: ss-2  ip-10-0-141-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:18.509: INFO: 
Mar  2 21:55:18.509: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 21:55:19.530: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 21:55:19.531: INFO: ss-2  ip-10-0-141-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:19.531: INFO: 
Mar  2 21:55:19.531: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 21:55:20.551: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Mar  2 21:55:20.551: INFO: ss-2  ip-10-0-141-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:55:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-02 21:54:43 +0000 UTC  }]
Mar  2 21:55:20.551: INFO: 
Mar  2 21:55:20.551: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 21:55:21.571: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.894673543s
Mar  2 21:55:22.592: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.874183939s
Mar  2 21:55:23.612: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.854002397s
Mar  2 21:55:24.633: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.83368849s
Mar  2 21:55:25.654: INFO: Verifying statefulset ss doesn't scale past 0 for another 812.934602ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4134
Mar  2 21:55:26.676: INFO: Scaling statefulset ss to 0
Mar  2 21:55:26.735: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 21:55:26.756: INFO: Deleting all statefulset in ns statefulset-4134
Mar  2 21:55:26.775: INFO: Scaling statefulset ss to 0
Mar  2 21:55:26.835: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 21:55:26.855: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:26.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-4134" for this suite.
Mar  2 21:55:33.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:36.526: INFO: namespace statefulset-4134 deletion completed in 9.381049617s


• [SLOW TEST:73.514 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:16.655: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  2 21:55:27.019: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec pod-sharedvolume-782ab890-c16d-4ac5-886f-2ab2d01ac49c -c busybox-main-container --namespace=emptydir-3694 -- cat /usr/share/volumeshare/shareddata.txt'
Mar  2 21:55:27.355: INFO: stderr: ""
Mar  2 21:55:27.355: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:27.355: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-3694" for this suite.
Mar  2 21:55:33.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:36.841: INFO: namespace emptydir-3694 deletion completed in 9.352706305s


• [SLOW TEST:20.185 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:54:44.309: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:54:44.422: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:54:54.595: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3160" for this suite.
Mar  2 21:55:40.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:44.092: INFO: namespace pods-3160 deletion completed in 49.38580281s


• [SLOW TEST:59.783 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:33.897: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 21:55:34.125: INFO: Waiting up to 5m0s for pod "pod-58072a8e-5760-40c1-a251-2efc802819a5" in namespace "emptydir-1056" to be "success or failure"
Mar  2 21:55:34.139: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.508298ms
Mar  2 21:55:36.151: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026152306s
Mar  2 21:55:38.165: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039648744s
Mar  2 21:55:40.179: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053243939s
Mar  2 21:55:42.191: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065680623s
Mar  2 21:55:44.213: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.087438946s
STEP: Saw pod success
Mar  2 21:55:44.213: INFO: Pod "pod-58072a8e-5760-40c1-a251-2efc802819a5" satisfied condition "success or failure"
Mar  2 21:55:44.241: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-58072a8e-5760-40c1-a251-2efc802819a5 container test-container: <nil>
STEP: delete the pod
Mar  2 21:55:44.296: INFO: Waiting for pod pod-58072a8e-5760-40c1-a251-2efc802819a5 to disappear
Mar  2 21:55:44.310: INFO: Pod pod-58072a8e-5760-40c1-a251-2efc802819a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:44.311: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1056" for this suite.
Mar  2 21:55:50.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:53.787: INFO: namespace emptydir-1056 deletion completed in 9.35265513s


• [SLOW TEST:19.891 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:36.850: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-e2b5361e-d732-4aa6-aa51-0f15ca031ea9
STEP: Creating a pod to test consume secrets
Mar  2 21:55:36.988: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24" in namespace "projected-7268" to be "success or failure"
Mar  2 21:55:37.004: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Pending", Reason="", readiness=false. Elapsed: 16.033572ms
Mar  2 21:55:39.018: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029316839s
Mar  2 21:55:41.031: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042715807s
Mar  2 21:55:43.044: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055997613s
Mar  2 21:55:45.069: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080647121s
Mar  2 21:55:47.086: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097414472s
STEP: Saw pod success
Mar  2 21:55:47.086: INFO: Pod "pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24" satisfied condition "success or failure"
Mar  2 21:55:47.099: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 21:55:47.146: INFO: Waiting for pod pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24 to disappear
Mar  2 21:55:47.163: INFO: Pod pod-projected-secrets-e75ba50f-ada6-4baf-bd7b-3344993cac24 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:47.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7268" for this suite.
Mar  2 21:55:53.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:55:56.622: INFO: namespace projected-7268 deletion completed in 9.354155312s


• [SLOW TEST:19.772 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:44.136: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-a3fad4ce-e964-4474-bff2-c9dc4aeeeb6d
STEP: Creating a pod to test consume secrets
Mar  2 21:55:44.335: INFO: Waiting up to 5m0s for pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f" in namespace "secrets-5087" to be "success or failure"
Mar  2 21:55:44.354: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.727211ms
Mar  2 21:55:46.376: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04178449s
Mar  2 21:55:48.398: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062904269s
Mar  2 21:55:50.426: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090981165s
Mar  2 21:55:52.448: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113504035s
Mar  2 21:55:54.469: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.134529128s
STEP: Saw pod success
Mar  2 21:55:54.469: INFO: Pod "pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f" satisfied condition "success or failure"
Mar  2 21:55:54.490: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 21:55:54.546: INFO: Waiting for pod pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f to disappear
Mar  2 21:55:54.566: INFO: Pod pod-secrets-0cd57d63-d437-4e4e-a1cc-e71ca8bcc69f no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:55:54.566: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5087" for this suite.
Mar  2 21:56:00.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:04.155: INFO: namespace secrets-5087 deletion completed in 9.386642137s


• [SLOW TEST:20.019 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:53.805: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 21:55:54.942: INFO: Waiting up to 5m0s for pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320" in namespace "emptydir-8504" to be "success or failure"
Mar  2 21:55:54.955: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210544ms
Mar  2 21:55:56.968: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026341481s
Mar  2 21:55:58.982: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0399283s
Mar  2 21:56:00.995: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053437354s
Mar  2 21:56:03.009: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066808256s
Mar  2 21:56:05.022: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.080237588s
STEP: Saw pod success
Mar  2 21:56:05.022: INFO: Pod "pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320" satisfied condition "success or failure"
Mar  2 21:56:05.035: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320 container test-container: <nil>
STEP: delete the pod
Mar  2 21:56:05.077: INFO: Waiting for pod pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320 to disappear
Mar  2 21:56:05.089: INFO: Pod pod-4d150bb3-b005-4e76-93ea-4bdd2c0f7320 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:05.089: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8504" for this suite.
Mar  2 21:56:11.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:14.516: INFO: namespace emptydir-8504 deletion completed in 9.353250444s


• [SLOW TEST:20.711 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:56.624: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-90eecfa6-2e82-4032-9615-04c48b17b07e
STEP: Creating a pod to test consume configMaps
Mar  2 21:55:56.787: INFO: Waiting up to 5m0s for pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93" in namespace "configmap-6575" to be "success or failure"
Mar  2 21:55:56.800: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 12.704418ms
Mar  2 21:55:58.813: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025799012s
Mar  2 21:56:00.827: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039555591s
Mar  2 21:56:02.840: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052937287s
Mar  2 21:56:04.854: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067309185s
Mar  2 21:56:06.869: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.081618931s
STEP: Saw pod success
Mar  2 21:56:06.869: INFO: Pod "pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93" satisfied condition "success or failure"
Mar  2 21:56:06.881: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 21:56:06.919: INFO: Waiting for pod pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93 to disappear
Mar  2 21:56:06.931: INFO: Pod pod-configmaps-61e5cede-0c12-44a1-9fb8-31f1f322cf93 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:06.931: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6575" for this suite.
Mar  2 21:56:13.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:16.408: INFO: namespace configmap-6575 deletion completed in 9.35185968s


• [SLOW TEST:19.784 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:14.531: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:14.659: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "tables-2299" for this suite.
Mar  2 21:56:20.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:24.066: INFO: namespace tables-2299 deletion completed in 9.353738137s


• [SLOW TEST:9.536 seconds]
[sig-api-machinery] Servers with support for Table transformation
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:04.163: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Mar  2 21:56:04.965: INFO: created pod pod-service-account-defaultsa
Mar  2 21:56:04.966: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 21:56:04.992: INFO: created pod pod-service-account-mountsa
Mar  2 21:56:04.992: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 21:56:05.019: INFO: created pod pod-service-account-nomountsa
Mar  2 21:56:05.019: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 21:56:05.048: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 21:56:05.048: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 21:56:05.074: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 21:56:05.074: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 21:56:05.119: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 21:56:05.119: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 21:56:05.146: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 21:56:05.146: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 21:56:05.172: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 21:56:05.172: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 21:56:05.200: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 21:56:05.200: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:05.200: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-5350" for this suite.
Mar  2 21:56:23.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:26.748: INFO: namespace svcaccounts-5350 deletion completed in 21.383838651s


• [SLOW TEST:22.585 seconds]
[sig-auth] ServiceAccounts
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:55:36.536: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3623
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 21:55:36.679: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 21:56:23.641: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.130.4.190&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:23.641: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:23.877: INFO: Waiting for endpoints: map[]
Mar  2 21:56:23.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.128.4.102&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:23.897: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:24.083: INFO: Waiting for endpoints: map[]
Mar  2 21:56:24.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.129.2.222&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:24.102: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:24.305: INFO: Waiting for endpoints: map[]
Mar  2 21:56:24.325: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.131.2.104&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:24.325: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:24.524: INFO: Waiting for endpoints: map[]
Mar  2 21:56:24.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.128.2.168&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:24.543: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:24.698: INFO: Waiting for endpoints: map[]
Mar  2 21:56:24.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.130.2.207&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:24.717: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:24.935: INFO: Waiting for endpoints: map[]
Mar  2 21:56:24.954: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.181:8080/dial?request=hostName&protocol=udp&host=10.129.4.138&port=8081&tries=1'] Namespace:pod-network-test-3623 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:56:24.954: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:56:25.107: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:25.108: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-3623" for this suite.
Mar  2 21:56:31.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:34.614: INFO: namespace pod-network-test-3623 deletion completed in 9.382892878s


• [SLOW TEST:58.078 seconds]
[sig-network] Networking
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:16.414: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:56:17.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:56:19.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:56:21.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:56:23.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:56:25.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718782976, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:56:28.457: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:56:28.470: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:29.273: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-2655" for this suite.
Mar  2 21:56:35.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:38.801: INFO: namespace crd-webhook-2655 deletion completed in 9.377026597s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


• [SLOW TEST:22.442 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:24.094: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  2 21:56:24.229: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 21:56:29.256: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:30.347: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-9973" for this suite.
Mar  2 21:56:36.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:39.753: INFO: namespace replication-controller-9973 deletion completed in 9.352197636s


• [SLOW TEST:15.659 seconds]
[sig-apps] ReplicationController
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:26.757: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-3196
STEP: creating replication controller nodeport-test in namespace services-3196
I0302 21:56:26.945366   56392 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-3196, replica count: 2
I0302 21:56:29.996120   56392 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:56:32.996623   56392 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:56:35.997341   56392 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 21:56:35.997: INFO: Creating new exec pod
Mar  2 21:56:47.202: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-3196 execpodtptdq -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  2 21:56:47.497: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 21:56:47.497: INFO: stdout: ""
Mar  2 21:56:47.498: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-3196 execpodtptdq -- /bin/sh -x -c nc -zv -t -w 2 172.30.6.48 80'
Mar  2 21:56:47.821: INFO: stderr: "+ nc -zv -t -w 2 172.30.6.48 80\nConnection to 172.30.6.48 80 port [tcp/http] succeeded!\n"
Mar  2 21:56:47.821: INFO: stdout: ""
Mar  2 21:56:47.822: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-3196 execpodtptdq -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.224 32003'
Mar  2 21:56:48.129: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.224 32003\nConnection to 10.0.130.224 32003 port [tcp/32003] succeeded!\n"
Mar  2 21:56:48.129: INFO: stdout: ""
Mar  2 21:56:48.129: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-3196 execpodtptdq -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.27 32003'
Mar  2 21:56:48.430: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.27 32003\nConnection to 10.0.130.27 32003 port [tcp/32003] succeeded!\n"
Mar  2 21:56:48.430: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:48.431: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3196" for this suite.
Mar  2 21:56:54.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:58.026: INFO: namespace services-3196 deletion completed in 9.448362416s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:31.271 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:39.793: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  2 21:56:40.057: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429367 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 21:56:40.057: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429369 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  2 21:56:40.057: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429371 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 21:56:50.153: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429480 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 21:56:50.153: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429481 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  2 21:56:50.159: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8256 /api/v1/namespaces/watch-8256/configmaps/e2e-watch-test-label-changed 75b86a22-7574-4f64-84e6-bf549e1472cb 15429482 0 2020-03-02 21:56:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:50.160: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-8256" for this suite.
Mar  2 21:56:56.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:56:59.635: INFO: namespace watch-8256 deletion completed in 9.353087527s


• [SLOW TEST:19.843 seconds]
[sig-api-machinery] Watchers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:59.645: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:56:59.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2" in namespace "downward-api-7874" to be "success or failure"
Mar  2 21:56:59.797: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.149256ms
Mar  2 21:57:01.811: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026110018s
Mar  2 21:57:03.824: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039365506s
Mar  2 21:57:05.837: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052333685s
Mar  2 21:57:07.850: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065910393s
Mar  2 21:57:09.864: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.079195923s
STEP: Saw pod success
Mar  2 21:57:09.864: INFO: Pod "downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2" satisfied condition "success or failure"
Mar  2 21:57:09.876: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2 container client-container: <nil>
STEP: delete the pod
Mar  2 21:57:09.912: INFO: Waiting for pod downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2 to disappear
Mar  2 21:57:09.925: INFO: Pod downwardapi-volume-0c15c887-bed1-49b7-9109-847938d8ebb2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:09.925: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7874" for this suite.
Mar  2 21:57:16.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:19.424: INFO: namespace downward-api-7874 deletion completed in 9.376285412s


• [SLOW TEST:19.779 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:38.858: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1361
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 21:56:38.972: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 21:57:15.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.4.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:15.634: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:16.837: INFO: Found all expected endpoints: [netserver-0]
Mar  2 21:57:16.850: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:16.850: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:18.044: INFO: Found all expected endpoints: [netserver-1]
Mar  2 21:57:18.057: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.2.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:18.057: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:19.210: INFO: Found all expected endpoints: [netserver-2]
Mar  2 21:57:19.223: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.130.4.191 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:19.224: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:20.412: INFO: Found all expected endpoints: [netserver-3]
Mar  2 21:57:20.425: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.223 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:20.425: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:21.609: INFO: Found all expected endpoints: [netserver-4]
Mar  2 21:57:21.621: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.4.139 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:21.621: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:22.767: INFO: Found all expected endpoints: [netserver-5]
Mar  2 21:57:22.779: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.130.2.208 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 21:57:22.780: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 21:57:23.931: INFO: Found all expected endpoints: [netserver-6]
[AfterEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:23.932: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-1361" for this suite.
Mar  2 21:57:30.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:33.361: INFO: namespace pod-network-test-1361 deletion completed in 9.35539196s


• [SLOW TEST:54.503 seconds]
[sig-network] Networking
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:34.630: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:56:44.896: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-2959" for this suite.
Mar  2 21:57:31.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:34.637: INFO: namespace kubelet-test-2959 deletion completed in 49.615485176s


• [SLOW TEST:60.007 seconds]
[k8s.io] Kubelet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:19.449: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 21:57:30.146283   56390 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 21:57:30.146: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:30.146: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1061" for this suite.
Mar  2 21:57:38.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:41.601: INFO: namespace gc-1061 deletion completed in 11.377817123s


• [SLOW TEST:22.152 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:56:58.034: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:39.720: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-7303" for this suite.
Mar  2 21:57:46.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:49.344: INFO: namespace container-runtime-7303 deletion completed in 9.385157064s


• [SLOW TEST:51.309 seconds]
[k8s.io] Container Runtime
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:33.377: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:57:33.547: INFO: Waiting up to 5m0s for pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9" in namespace "downward-api-2754" to be "success or failure"
Mar  2 21:57:33.570: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.258664ms
Mar  2 21:57:35.607: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059926598s
Mar  2 21:57:37.637: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089286768s
Mar  2 21:57:39.660: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.112305951s
Mar  2 21:57:41.682: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.134928124s
STEP: Saw pod success
Mar  2 21:57:41.682: INFO: Pod "downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9" satisfied condition "success or failure"
Mar  2 21:57:41.704: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9 container client-container: <nil>
STEP: delete the pod
Mar  2 21:57:41.757: INFO: Waiting for pod downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9 to disappear
Mar  2 21:57:41.771: INFO: Pod downwardapi-volume-301ba29f-a69a-4b74-aa26-96dd22ffd9d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:41.771: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2754" for this suite.
Mar  2 21:57:47.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:51.252: INFO: namespace downward-api-2754 deletion completed in 9.353333333s


• [SLOW TEST:17.875 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:34.659: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:57:34.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6" in namespace "projected-7105" to be "success or failure"
Mar  2 21:57:34.822: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.750287ms
Mar  2 21:57:36.843: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042521514s
Mar  2 21:57:38.863: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062746441s
Mar  2 21:57:40.883: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082837289s
Mar  2 21:57:42.903: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103010667s
Mar  2 21:57:44.924: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.123830945s
STEP: Saw pod success
Mar  2 21:57:44.924: INFO: Pod "downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6" satisfied condition "success or failure"
Mar  2 21:57:44.945: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6 container client-container: <nil>
STEP: delete the pod
Mar  2 21:57:44.993: INFO: Waiting for pod downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6 to disappear
Mar  2 21:57:45.012: INFO: Pod downwardapi-volume-47319d82-5046-440e-9dab-5b6b787a80f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:45.012: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7105" for this suite.
Mar  2 21:57:51.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:57:54.586: INFO: namespace projected-7105 deletion completed in 9.381411201s


• [SLOW TEST:19.927 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:54.593: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 21:57:54.737: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-841'
Mar  2 21:57:54.934: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  2 21:57:54.934: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Mar  2 21:57:54.954: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete jobs e2e-test-httpd-job --namespace=kubectl-841'
Mar  2 21:57:55.091: INFO: stderr: ""
Mar  2 21:57:55.091: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:57:55.091: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-841" for this suite.
Mar  2 21:58:07.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:12.386: INFO: namespace kubectl-841 deletion completed in 17.136886055s


• [SLOW TEST:17.793 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:51.261: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  2 21:57:51.385: INFO: Waiting up to 5m0s for pod "pod-8da268db-921a-43c2-bc81-8820161fc26e" in namespace "emptydir-5385" to be "success or failure"
Mar  2 21:57:51.398: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.434908ms
Mar  2 21:57:53.411: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026207952s
Mar  2 21:57:55.425: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040010557s
Mar  2 21:57:57.439: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054061458s
Mar  2 21:57:59.452: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067093135s
Mar  2 21:58:01.466: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.08055247s
STEP: Saw pod success
Mar  2 21:58:01.466: INFO: Pod "pod-8da268db-921a-43c2-bc81-8820161fc26e" satisfied condition "success or failure"
Mar  2 21:58:01.479: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-8da268db-921a-43c2-bc81-8820161fc26e container test-container: <nil>
STEP: delete the pod
Mar  2 21:58:01.515: INFO: Waiting for pod pod-8da268db-921a-43c2-bc81-8820161fc26e to disappear
Mar  2 21:58:01.527: INFO: Pod pod-8da268db-921a-43c2-bc81-8820161fc26e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:01.527: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5385" for this suite.
Mar  2 21:58:09.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:12.981: INFO: namespace emptydir-5385 deletion completed in 11.357492729s


• [SLOW TEST:21.720 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:49.362: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1643.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1643.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1643.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1643.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 189.167.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.167.189_udp@PTR;check="$$(dig +tcp +noall +answer +search 189.167.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.167.189_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1643.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1643.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1643.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1643.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1643.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1643.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1643.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 189.167.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.167.189_udp@PTR;check="$$(dig +tcp +noall +answer +search 189.167.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.167.189_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 21:57:59.878: INFO: Unable to read jessie_udp@dns-test-service.dns-1643.svc.cluster.local from pod dns-1643/dns-test-380c0def-b723-4ce4-b22d-53dce2401efe: the server could not find the requested resource (get pods dns-test-380c0def-b723-4ce4-b22d-53dce2401efe)
Mar  2 21:57:59.900: INFO: Unable to read jessie_tcp@dns-test-service.dns-1643.svc.cluster.local from pod dns-1643/dns-test-380c0def-b723-4ce4-b22d-53dce2401efe: the server could not find the requested resource (get pods dns-test-380c0def-b723-4ce4-b22d-53dce2401efe)
Mar  2 21:58:00.075: INFO: Lookups using dns-1643/dns-test-380c0def-b723-4ce4-b22d-53dce2401efe failed for: [jessie_udp@dns-test-service.dns-1643.svc.cluster.local jessie_tcp@dns-test-service.dns-1643.svc.cluster.local]

Mar  2 21:58:06.184: INFO: DNS probes using dns-1643/dns-test-380c0def-b723-4ce4-b22d-53dce2401efe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:06.345: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-1643" for this suite.
Mar  2 21:58:12.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:16.558: INFO: namespace dns-1643 deletion completed in 9.955432645s


• [SLOW TEST:27.196 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:16.615: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:23.859: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2290" for this suite.
Mar  2 21:58:31.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:34.764: INFO: namespace resourcequota-2290 deletion completed in 9.695682967s


• [SLOW TEST:18.149 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:57:41.652: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 21:58:09.821: INFO: Container started at 2020-03-02 21:57:48 +0000 UTC, pod became ready at 2020-03-02 21:58:07 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:09.821: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2438" for this suite.
Mar  2 21:58:38.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:41.322: INFO: namespace container-probe-2438 deletion completed in 31.35526178s


• [SLOW TEST:59.670 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:12.462: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 21:58:13.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:58:15.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:58:17.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:58:19.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 21:58:21.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783092, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 21:58:24.687: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:25.050: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9855" for this suite.
Mar  2 21:58:31.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:35.289: INFO: namespace webhook-9855 deletion completed in 9.381096279s
STEP: Destroying namespace "webhook-9855-markers" for this suite.
Mar  2 21:58:41.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:58:44.670: INFO: namespace webhook-9855-markers deletion completed in 9.380756015s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:32.354 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:41.354: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 21:58:52.075: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3c05d506-fe33-440a-ac2c-a64d1b0b54c1"
Mar  2 21:58:52.076: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3c05d506-fe33-440a-ac2c-a64d1b0b54c1" in namespace "pods-7542" to be "terminated due to deadline exceeded"
Mar  2 21:58:52.088: INFO: Pod "pod-update-activedeadlineseconds-3c05d506-fe33-440a-ac2c-a64d1b0b54c1": Phase="Running", Reason="", readiness=true. Elapsed: 12.901789ms
Mar  2 21:58:54.102: INFO: Pod "pod-update-activedeadlineseconds-3c05d506-fe33-440a-ac2c-a64d1b0b54c1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.026558816s
Mar  2 21:58:54.102: INFO: Pod "pod-update-activedeadlineseconds-3c05d506-fe33-440a-ac2c-a64d1b0b54c1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:54.102: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-7542" for this suite.
Mar  2 21:59:00.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:03.562: INFO: namespace pods-7542 deletion completed in 9.355207701s


• [SLOW TEST:22.209 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:34.837: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  2 21:58:35.005: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:58:55.811: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-9129" for this suite.
Mar  2 21:59:02.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:05.411: INFO: namespace pods-9129 deletion completed in 9.451299511s


• [SLOW TEST:30.574 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:05.453: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:15.838: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7053" for this suite.
Mar  2 21:59:22.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:26.072: INFO: namespace emptydir-wrapper-7053 deletion completed in 9.960749109s


• [SLOW TEST:20.619 seconds]
[sig-storage] EmptyDir wrapper volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:44.844: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar  2 21:58:44.990: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-841'
Mar  2 21:58:45.840: INFO: stderr: ""
Mar  2 21:58:45.840: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 21:58:45.840: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:58:45.988: INFO: stderr: ""
Mar  2 21:58:45.988: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-knqz9 "
Mar  2 21:58:45.988: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:46.138: INFO: stderr: ""
Mar  2 21:58:46.138: INFO: stdout: ""
Mar  2 21:58:46.138: INFO: update-demo-nautilus-9dn5j is created but not running
Mar  2 21:58:51.139: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:58:51.291: INFO: stderr: ""
Mar  2 21:58:51.291: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-knqz9 "
Mar  2 21:58:51.291: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:51.436: INFO: stderr: ""
Mar  2 21:58:51.436: INFO: stdout: ""
Mar  2 21:58:51.436: INFO: update-demo-nautilus-9dn5j is created but not running
Mar  2 21:58:56.437: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:58:56.617: INFO: stderr: ""
Mar  2 21:58:56.617: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-knqz9 "
Mar  2 21:58:56.617: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:56.739: INFO: stderr: ""
Mar  2 21:58:56.739: INFO: stdout: "true"
Mar  2 21:58:56.739: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:56.859: INFO: stderr: ""
Mar  2 21:58:56.859: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:58:56.859: INFO: validating pod update-demo-nautilus-9dn5j
Mar  2 21:58:56.881: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:58:56.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:58:56.881: INFO: update-demo-nautilus-9dn5j is verified up and running
Mar  2 21:58:56.882: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-knqz9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:57.002: INFO: stderr: ""
Mar  2 21:58:57.002: INFO: stdout: "true"
Mar  2 21:58:57.002: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-knqz9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:58:57.122: INFO: stderr: ""
Mar  2 21:58:57.122: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:58:57.122: INFO: validating pod update-demo-nautilus-knqz9
Mar  2 21:58:57.144: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:58:57.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:58:57.144: INFO: update-demo-nautilus-knqz9 is verified up and running
STEP: scaling down the replication controller
Mar  2 21:58:57.157: INFO: scanned /root for discovery docs: <nil>
Mar  2 21:58:57.157: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-841'
Mar  2 21:58:57.355: INFO: stderr: ""
Mar  2 21:58:57.355: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 21:58:57.356: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:58:57.505: INFO: stderr: ""
Mar  2 21:58:57.505: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-knqz9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 21:59:02.505: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:59:02.632: INFO: stderr: ""
Mar  2 21:59:02.632: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-knqz9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 21:59:07.634: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:59:07.760: INFO: stderr: ""
Mar  2 21:59:07.760: INFO: stdout: "update-demo-nautilus-9dn5j "
Mar  2 21:59:07.760: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:07.910: INFO: stderr: ""
Mar  2 21:59:07.910: INFO: stdout: "true"
Mar  2 21:59:07.910: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:08.029: INFO: stderr: ""
Mar  2 21:59:08.029: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:59:08.029: INFO: validating pod update-demo-nautilus-9dn5j
Mar  2 21:59:08.050: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:59:08.051: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:59:08.051: INFO: update-demo-nautilus-9dn5j is verified up and running
STEP: scaling up the replication controller
Mar  2 21:59:08.060: INFO: scanned /root for discovery docs: <nil>
Mar  2 21:59:08.060: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-841'
Mar  2 21:59:09.234: INFO: stderr: ""
Mar  2 21:59:09.234: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 21:59:09.234: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:59:09.357: INFO: stderr: ""
Mar  2 21:59:09.357: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-d8pwc "
Mar  2 21:59:09.357: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:09.481: INFO: stderr: ""
Mar  2 21:59:09.481: INFO: stdout: "true"
Mar  2 21:59:09.481: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:09.606: INFO: stderr: ""
Mar  2 21:59:09.606: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:59:09.606: INFO: validating pod update-demo-nautilus-9dn5j
Mar  2 21:59:09.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:59:09.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:59:09.627: INFO: update-demo-nautilus-9dn5j is verified up and running
Mar  2 21:59:09.627: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-d8pwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:09.770: INFO: stderr: ""
Mar  2 21:59:09.770: INFO: stdout: ""
Mar  2 21:59:09.770: INFO: update-demo-nautilus-d8pwc is created but not running
Mar  2 21:59:14.771: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:59:14.927: INFO: stderr: ""
Mar  2 21:59:14.927: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-d8pwc "
Mar  2 21:59:14.927: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:15.073: INFO: stderr: ""
Mar  2 21:59:15.073: INFO: stdout: "true"
Mar  2 21:59:15.074: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:15.216: INFO: stderr: ""
Mar  2 21:59:15.216: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:59:15.216: INFO: validating pod update-demo-nautilus-9dn5j
Mar  2 21:59:15.237: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:59:15.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:59:15.237: INFO: update-demo-nautilus-9dn5j is verified up and running
Mar  2 21:59:15.237: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-d8pwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:15.381: INFO: stderr: ""
Mar  2 21:59:15.381: INFO: stdout: ""
Mar  2 21:59:15.381: INFO: update-demo-nautilus-d8pwc is created but not running
Mar  2 21:59:20.381: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-841'
Mar  2 21:59:20.527: INFO: stderr: ""
Mar  2 21:59:20.527: INFO: stdout: "update-demo-nautilus-9dn5j update-demo-nautilus-d8pwc "
Mar  2 21:59:20.527: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:20.650: INFO: stderr: ""
Mar  2 21:59:20.650: INFO: stdout: "true"
Mar  2 21:59:20.650: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-9dn5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:20.772: INFO: stderr: ""
Mar  2 21:59:20.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:59:20.772: INFO: validating pod update-demo-nautilus-9dn5j
Mar  2 21:59:20.792: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:59:20.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:59:20.793: INFO: update-demo-nautilus-9dn5j is verified up and running
Mar  2 21:59:20.793: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-d8pwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:20.910: INFO: stderr: ""
Mar  2 21:59:20.910: INFO: stdout: "true"
Mar  2 21:59:20.910: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-d8pwc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-841'
Mar  2 21:59:21.028: INFO: stderr: ""
Mar  2 21:59:21.028: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 21:59:21.028: INFO: validating pod update-demo-nautilus-d8pwc
Mar  2 21:59:21.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 21:59:21.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 21:59:21.052: INFO: update-demo-nautilus-d8pwc is verified up and running
STEP: using delete to clean up resources
Mar  2 21:59:21.052: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-841'
Mar  2 21:59:21.217: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 21:59:21.217: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 21:59:21.217: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-841'
Mar  2 21:59:21.591: INFO: stderr: "No resources found in kubectl-841 namespace.\n"
Mar  2 21:59:21.592: INFO: stdout: ""
Mar  2 21:59:21.592: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -l name=update-demo --namespace=kubectl-841 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 21:59:21.738: INFO: stderr: ""
Mar  2 21:59:21.738: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:21.738: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-841" for this suite.
Mar  2 21:59:33.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:37.314: INFO: namespace kubectl-841 deletion completed in 15.384527515s


• [SLOW TEST:52.471 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:58:12.993: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:13.228: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-5505" for this suite.
Mar  2 21:59:41.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:44.717: INFO: namespace container-probe-5505 deletion completed in 31.354646756s


• [SLOW TEST:91.725 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:03.565: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar  2 21:59:03.698: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:37.708: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8523" for this suite.
Mar  2 21:59:43.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 21:59:47.202: INFO: namespace crd-publish-openapi-8523 deletion completed in 9.378847208s


• [SLOW TEST:43.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:44.773: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:59:44.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117" in namespace "projected-6436" to be "success or failure"
Mar  2 21:59:44.966: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Pending", Reason="", readiness=false. Elapsed: 15.670809ms
Mar  2 21:59:46.981: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030680333s
Mar  2 21:59:48.995: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044624043s
Mar  2 21:59:51.009: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058082573s
Mar  2 21:59:53.028: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077181462s
Mar  2 21:59:55.043: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.092216364s
STEP: Saw pod success
Mar  2 21:59:55.043: INFO: Pod "downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117" satisfied condition "success or failure"
Mar  2 21:59:55.072: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117 container client-container: <nil>
STEP: delete the pod
Mar  2 21:59:55.150: INFO: Waiting for pod downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117 to disappear
Mar  2 21:59:55.163: INFO: Pod downwardapi-volume-57cc4077-a3a7-4a74-a921-e88e3d79a117 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:55.164: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6436" for this suite.
Mar  2 22:00:01.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:05.021: INFO: namespace projected-6436 deletion completed in 9.614423505s


• [SLOW TEST:20.248 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:26.083: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2207
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2207 to expose endpoints map[]
Mar  2 21:59:26.256: INFO: successfully validated that service endpoint-test2 in namespace services-2207 exposes endpoints map[] (20.111362ms elapsed)
STEP: Creating pod pod1 in namespace services-2207
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2207 to expose endpoints map[pod1:[80]]
Mar  2 21:59:30.500: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.208494148s elapsed, will retry)
Mar  2 21:59:34.713: INFO: successfully validated that service endpoint-test2 in namespace services-2207 exposes endpoints map[pod1:[80]] (8.421190372s elapsed)
STEP: Creating pod pod2 in namespace services-2207
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2207 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 21:59:39.064: INFO: Unexpected endpoints: found map[b77ed0e4-1ef2-4ab1-af46-4a6b84464c65:[80]], expected map[pod1:[80] pod2:[80]] (4.319872146s elapsed, will retry)
Mar  2 21:59:44.617: INFO: successfully validated that service endpoint-test2 in namespace services-2207 exposes endpoints map[pod1:[80] pod2:[80]] (9.873477693s elapsed)
STEP: Deleting pod pod1 in namespace services-2207
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2207 to expose endpoints map[pod2:[80]]
Mar  2 21:59:44.681: INFO: successfully validated that service endpoint-test2 in namespace services-2207 exposes endpoints map[pod2:[80]] (40.589113ms elapsed)
STEP: Deleting pod pod2 in namespace services-2207
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2207 to expose endpoints map[]
Mar  2 21:59:44.752: INFO: successfully validated that service endpoint-test2 in namespace services-2207 exposes endpoints map[] (21.44992ms elapsed)
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:44.803: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2207" for this suite.
Mar  2 22:00:03.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:06.754: INFO: namespace services-2207 deletion completed in 21.693677464s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:40.671 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:47.217: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6aa5fbd9-ed95-4c3d-9668-9a7d2d690993
STEP: Creating a pod to test consume secrets
Mar  2 21:59:47.402: INFO: Waiting up to 5m0s for pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe" in namespace "secrets-9177" to be "success or failure"
Mar  2 21:59:47.414: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.20048ms
Mar  2 21:59:49.428: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025606918s
Mar  2 21:59:51.442: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039621435s
Mar  2 21:59:53.455: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053155495s
Mar  2 21:59:55.469: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066523152s
Mar  2 21:59:57.482: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.07986692s
STEP: Saw pod success
Mar  2 21:59:57.482: INFO: Pod "pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe" satisfied condition "success or failure"
Mar  2 21:59:57.495: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 21:59:57.537: INFO: Waiting for pod pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe to disappear
Mar  2 21:59:57.550: INFO: Pod pod-secrets-4b1216a4-9ce2-4359-8e30-0776d86723fe no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 21:59:57.550: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9177" for this suite.
Mar  2 22:00:03.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:07.131: INFO: namespace secrets-9177 deletion completed in 9.44390262s


• [SLOW TEST:19.914 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 21:59:37.363: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1403
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1403
STEP: creating replication controller externalsvc in namespace services-1403
I0302 21:59:37.563987   56389 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1403, replica count: 2
I0302 21:59:40.614991   56389 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:59:43.615674   56389 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:59:46.616080   56389 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  2 21:59:46.763: INFO: Creating new exec pod
Mar  2 21:59:56.850: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-1403 execpodbchll -- /bin/sh -x -c nslookup clusterip-service'
Mar  2 21:59:57.180: INFO: stderr: "+ nslookup clusterip-service\n"
Mar  2 21:59:57.180: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-1403.svc.cluster.local\tcanonical name = externalsvc.services-1403.svc.cluster.local.\nName:\texternalsvc.services-1403.svc.cluster.local\nAddress: 172.30.143.104\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1403, will wait for the garbage collector to delete the pods
Mar  2 21:59:57.332: INFO: Deleting ReplicationController externalsvc took: 40.805515ms
Mar  2 21:59:57.432: INFO: Terminating ReplicationController externalsvc pods took: 100.597701ms
Mar  2 22:00:11.269: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:11.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-1403" for this suite.
Mar  2 22:00:17.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:21.044: INFO: namespace services-1403 deletion completed in 9.381571397s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:43.681 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:07.135: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7820.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7820.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7820.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7820.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7820.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7820.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:00:17.448: INFO: DNS probes using dns-7820/dns-test-ad703654-359c-49f8-814d-4c2e23030fd9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:17.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-7820" for this suite.
Mar  2 22:00:23.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:26.951: INFO: namespace dns-7820 deletion completed in 9.352318076s


• [SLOW TEST:19.816 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:06.785: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-118/configmap-test-626241d4-5350-4994-8dd2-94f0d9f74c73
STEP: Creating a pod to test consume configMaps
Mar  2 22:00:07.052: INFO: Waiting up to 5m0s for pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031" in namespace "configmap-118" to be "success or failure"
Mar  2 22:00:07.073: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Pending", Reason="", readiness=false. Elapsed: 21.251014ms
Mar  2 22:00:09.095: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043295178s
Mar  2 22:00:11.117: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064876249s
Mar  2 22:00:13.139: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086579615s
Mar  2 22:00:15.160: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108217224s
Mar  2 22:00:17.182: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.129602903s
STEP: Saw pod success
Mar  2 22:00:17.182: INFO: Pod "pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031" satisfied condition "success or failure"
Mar  2 22:00:17.203: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031 container env-test: <nil>
STEP: delete the pod
Mar  2 22:00:17.331: INFO: Waiting for pod pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031 to disappear
Mar  2 22:00:17.352: INFO: Pod pod-configmaps-a648aba6-a4ff-4ddf-bf27-97da6617e031 no longer exists
[AfterEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:17.352: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-118" for this suite.
Mar  2 22:00:23.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:27.232: INFO: namespace configmap-118 deletion completed in 9.660306009s


• [SLOW TEST:20.447 seconds]
[sig-node] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:27.244: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:27.401: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6611" for this suite.
Mar  2 22:00:33.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:37.023: INFO: namespace services-6611 deletion completed in 9.379634414s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:9.779 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:05.024: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6233
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6233 to expose endpoints map[]
Mar  2 22:00:05.193: INFO: successfully validated that service multi-endpoint-test in namespace services-6233 exposes endpoints map[] (12.310668ms elapsed)
STEP: Creating pod pod1 in namespace services-6233
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6233 to expose endpoints map[pod1:[100]]
Mar  2 22:00:09.417: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.191573923s elapsed, will retry)
Mar  2 22:00:14.570: INFO: successfully validated that service multi-endpoint-test in namespace services-6233 exposes endpoints map[pod1:[100]] (9.344939462s elapsed)
STEP: Creating pod pod2 in namespace services-6233
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6233 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 22:00:18.816: INFO: Unexpected endpoints: found map[1c1c92f7-1eef-4d41-b155-19612780ba50:[100]], expected map[pod1:[100] pod2:[101]] (4.225320779s elapsed, will retry)
Mar  2 22:00:24.021: INFO: successfully validated that service multi-endpoint-test in namespace services-6233 exposes endpoints map[pod1:[100] pod2:[101]] (9.43054673s elapsed)
STEP: Deleting pod pod1 in namespace services-6233
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6233 to expose endpoints map[pod2:[101]]
Mar  2 22:00:24.086: INFO: successfully validated that service multi-endpoint-test in namespace services-6233 exposes endpoints map[pod2:[101]] (49.091816ms elapsed)
STEP: Deleting pod pod2 in namespace services-6233
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6233 to expose endpoints map[]
Mar  2 22:00:24.129: INFO: successfully validated that service multi-endpoint-test in namespace services-6233 exposes endpoints map[] (12.943842ms elapsed)
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:24.159: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6233" for this suite.
Mar  2 22:00:36.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:39.836: INFO: namespace services-6233 deletion completed in 15.354718402s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:34.813 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:21.057: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-a19cf489-6791-4190-a041-c291ac8fb68c
STEP: Creating a pod to test consume secrets
Mar  2 22:00:21.217: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665" in namespace "projected-2315" to be "success or failure"
Mar  2 22:00:21.237: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Pending", Reason="", readiness=false. Elapsed: 19.340375ms
Mar  2 22:00:23.257: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039510922s
Mar  2 22:00:25.276: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058695002s
Mar  2 22:00:27.296: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078814294s
Mar  2 22:00:29.315: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09822052s
Mar  2 22:00:31.335: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.118086644s
STEP: Saw pod success
Mar  2 22:00:31.335: INFO: Pod "pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665" satisfied condition "success or failure"
Mar  2 22:00:31.357: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:00:31.417: INFO: Waiting for pod pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665 to disappear
Mar  2 22:00:31.437: INFO: Pod pod-projected-secrets-7618ef50-1f4b-4e65-a601-9145a3c7d665 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:31.437: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2315" for this suite.
Mar  2 22:00:37.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:40.976: INFO: namespace projected-2315 deletion completed in 9.3891434s


• [SLOW TEST:19.919 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:26.965: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:00:27.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e" in namespace "projected-3943" to be "success or failure"
Mar  2 22:00:27.112: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.707912ms
Mar  2 22:00:29.126: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029813955s
Mar  2 22:00:31.140: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04351289s
Mar  2 22:00:33.157: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060670333s
Mar  2 22:00:35.171: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075072491s
Mar  2 22:00:37.185: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.088536565s
STEP: Saw pod success
Mar  2 22:00:37.185: INFO: Pod "downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e" satisfied condition "success or failure"
Mar  2 22:00:37.198: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e container client-container: <nil>
STEP: delete the pod
Mar  2 22:00:37.237: INFO: Waiting for pod downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e to disappear
Mar  2 22:00:37.250: INFO: Pod downwardapi-volume-301e8499-9391-4461-9de3-e9e58171f92e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:37.250: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3943" for this suite.
Mar  2 22:00:43.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:46.740: INFO: namespace projected-3943 deletion completed in 9.355544069s


• [SLOW TEST:19.774 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:39.862: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 22:00:39.986: INFO: Waiting up to 5m0s for pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501" in namespace "emptydir-746" to be "success or failure"
Mar  2 22:00:39.999: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Pending", Reason="", readiness=false. Elapsed: 12.712535ms
Mar  2 22:00:42.016: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029806145s
Mar  2 22:00:44.030: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04339177s
Mar  2 22:00:46.043: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056304708s
Mar  2 22:00:48.056: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069521138s
Mar  2 22:00:50.069: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.082302857s
STEP: Saw pod success
Mar  2 22:00:50.069: INFO: Pod "pod-50b5c786-fcb2-4c86-9851-f71b8046b501" satisfied condition "success or failure"
Mar  2 22:00:50.081: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-50b5c786-fcb2-4c86-9851-f71b8046b501 container test-container: <nil>
STEP: delete the pod
Mar  2 22:00:50.116: INFO: Waiting for pod pod-50b5c786-fcb2-4c86-9851-f71b8046b501 to disappear
Mar  2 22:00:50.128: INFO: Pod pod-50b5c786-fcb2-4c86-9851-f71b8046b501 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:50.128: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-746" for this suite.
Mar  2 22:00:56.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:00:59.596: INFO: namespace emptydir-746 deletion completed in 9.353875375s


• [SLOW TEST:19.733 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:46.748: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:00:46.865: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-2936'
Mar  2 22:00:46.998: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  2 22:00:46.998: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Mar  2 22:00:51.042: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete deployment e2e-test-httpd-deployment --namespace=kubectl-2936'
Mar  2 22:00:51.178: INFO: stderr: ""
Mar  2 22:00:51.178: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:00:51.178: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2936" for this suite.
Mar  2 22:00:57.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:00.707: INFO: namespace kubectl-2936 deletion completed in 9.355222808s


• [SLOW TEST:13.958 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:37.034: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3827
[It] should have a working scale subresource [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-3827
Mar  2 22:00:37.233: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:00:47.252: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 22:00:47.354: INFO: Deleting all statefulset in ns statefulset-3827
Mar  2 22:00:47.374: INFO: Scaling statefulset ss to 0
Mar  2 22:01:07.455: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:01:07.476: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:07.540: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-3827" for this suite.
Mar  2 22:01:13.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:17.043: INFO: namespace statefulset-3827 deletion completed in 9.381571309s


• [SLOW TEST:40.009 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:59.632: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 22:00:59.757: INFO: Waiting up to 5m0s for pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032" in namespace "emptydir-1459" to be "success or failure"
Mar  2 22:00:59.771: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079947ms
Mar  2 22:01:01.785: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028159338s
Mar  2 22:01:03.798: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04130371s
Mar  2 22:01:05.812: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055019417s
Mar  2 22:01:07.826: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068572373s
STEP: Saw pod success
Mar  2 22:01:07.826: INFO: Pod "pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032" satisfied condition "success or failure"
Mar  2 22:01:07.839: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032 container test-container: <nil>
STEP: delete the pod
Mar  2 22:01:07.874: INFO: Waiting for pod pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032 to disappear
Mar  2 22:01:07.888: INFO: Pod pod-33e9ba3a-2ff1-4731-acc9-ea3da4122032 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:07.888: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1459" for this suite.
Mar  2 22:01:14.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:17.379: INFO: namespace emptydir-1459 deletion completed in 9.36754606s


• [SLOW TEST:17.747 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:00.717: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 22:01:00.870: INFO: Waiting up to 5m0s for pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e" in namespace "emptydir-2583" to be "success or failure"
Mar  2 22:01:00.882: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.455374ms
Mar  2 22:01:02.896: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025848458s
Mar  2 22:01:04.909: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039361076s
Mar  2 22:01:06.923: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05329854s
Mar  2 22:01:08.936: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06672319s
Mar  2 22:01:10.950: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.079905439s
STEP: Saw pod success
Mar  2 22:01:10.950: INFO: Pod "pod-da3e2680-1287-43d4-b087-2042aa064f7e" satisfied condition "success or failure"
Mar  2 22:01:10.963: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-da3e2680-1287-43d4-b087-2042aa064f7e container test-container: <nil>
STEP: delete the pod
Mar  2 22:01:10.999: INFO: Waiting for pod pod-da3e2680-1287-43d4-b087-2042aa064f7e to disappear
Mar  2 22:01:11.011: INFO: Pod pod-da3e2680-1287-43d4-b087-2042aa064f7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:11.011: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2583" for this suite.
Mar  2 22:01:17.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:20.507: INFO: namespace emptydir-2583 deletion completed in 9.352537549s


• [SLOW TEST:19.791 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:17.047: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:01:17.431: INFO: (0) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 26.448554ms)
Mar  2 22:01:17.464: INFO: (1) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 33.32ms)
Mar  2 22:01:17.485: INFO: (2) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.4756ms)
Mar  2 22:01:17.504: INFO: (3) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.626174ms)
Mar  2 22:01:17.526: INFO: (4) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.393034ms)
Mar  2 22:01:17.547: INFO: (5) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.461104ms)
Mar  2 22:01:17.567: INFO: (6) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.430556ms)
Mar  2 22:01:17.588: INFO: (7) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.515666ms)
Mar  2 22:01:17.608: INFO: (8) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.591277ms)
Mar  2 22:01:17.629: INFO: (9) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.910666ms)
Mar  2 22:01:17.649: INFO: (10) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.808615ms)
Mar  2 22:01:17.670: INFO: (11) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.568656ms)
Mar  2 22:01:17.690: INFO: (12) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.081951ms)
Mar  2 22:01:17.711: INFO: (13) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.163815ms)
Mar  2 22:01:17.731: INFO: (14) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.557872ms)
Mar  2 22:01:17.751: INFO: (15) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.926251ms)
Mar  2 22:01:17.772: INFO: (16) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.565277ms)
Mar  2 22:01:17.793: INFO: (17) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.967246ms)
Mar  2 22:01:17.814: INFO: (18) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.089245ms)
Mar  2 22:01:17.835: INFO: (19) /api/v1/nodes/ip-10-0-130-224.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.173107ms)
[AfterEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:17.835: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-7316" for this suite.
Mar  2 22:01:24.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:26.050: INFO: namespace proxy-7316 deletion completed in 8.005985244s


• [SLOW TEST:9.003 seconds]
[sig-network] Proxy
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:00:40.990: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7840, will wait for the garbage collector to delete the pods
Mar  2 22:00:51.230: INFO: Deleting Job.batch foo took: 24.21688ms
Mar  2 22:00:51.330: INFO: Terminating Job.batch foo pods took: 100.381171ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:25.849: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-7840" for this suite.
Mar  2 22:01:32.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:35.360: INFO: namespace job-7840 deletion completed in 9.377568507s


• [SLOW TEST:54.370 seconds]
[sig-apps] Job
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:17.409: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  2 22:01:17.536: INFO: Waiting up to 5m0s for pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c" in namespace "downward-api-8169" to be "success or failure"
Mar  2 22:01:17.559: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.628032ms
Mar  2 22:01:19.582: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045702889s
Mar  2 22:01:21.596: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05957351s
Mar  2 22:01:23.612: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075098092s
Mar  2 22:01:25.624: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0879039s
Mar  2 22:01:27.638: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101547827s
STEP: Saw pod success
Mar  2 22:01:27.638: INFO: Pod "downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c" satisfied condition "success or failure"
Mar  2 22:01:27.651: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:01:27.688: INFO: Waiting for pod downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c to disappear
Mar  2 22:01:27.701: INFO: Pod downward-api-8a6a228e-0c3a-4851-8610-ac2daf63fa1c no longer exists
[AfterEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:27.701: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8169" for this suite.
Mar  2 22:01:33.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:37.181: INFO: namespace downward-api-8169 deletion completed in 9.353606975s


• [SLOW TEST:19.772 seconds]
[sig-node] Downward API
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:20.538: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:31.774: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5147" for this suite.
Mar  2 22:01:37.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:41.252: INFO: namespace resourcequota-5147 deletion completed in 9.352989595s


• [SLOW TEST:20.714 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:37.233: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:01:37.347: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1479'
Mar  2 22:01:41.209: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  2 22:01:41.209: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar  2 22:01:41.238: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-9jx58]
Mar  2 22:01:41.238: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-9jx58" in namespace "kubectl-1479" to be "running and ready"
Mar  2 22:01:41.251: INFO: Pod "e2e-test-httpd-rc-9jx58": Phase="Pending", Reason="", readiness=false. Elapsed: 12.150772ms
Mar  2 22:01:43.265: INFO: Pod "e2e-test-httpd-rc-9jx58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026082449s
Mar  2 22:01:45.278: INFO: Pod "e2e-test-httpd-rc-9jx58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039389603s
Mar  2 22:01:47.291: INFO: Pod "e2e-test-httpd-rc-9jx58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052996135s
Mar  2 22:01:49.305: INFO: Pod "e2e-test-httpd-rc-9jx58": Phase="Running", Reason="", readiness=true. Elapsed: 8.066676753s
Mar  2 22:01:49.305: INFO: Pod "e2e-test-httpd-rc-9jx58" satisfied condition "running and ready"
Mar  2 22:01:49.305: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-9jx58]
Mar  2 22:01:49.305: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs rc/e2e-test-httpd-rc --namespace=kubectl-1479'
Mar  2 22:01:50.329: INFO: stderr: ""
Mar  2 22:01:50.329: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.2.234. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.2.234. Set the 'ServerName' directive globally to suppress this message\n[Mon Mar 02 22:01:48.081638 2020] [mpm_event:notice] [pid 1:tid 140044884523880] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Mar 02 22:01:48.081688 2020] [core:notice] [pid 1:tid 140044884523880] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Mar  2 22:01:50.329: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete rc e2e-test-httpd-rc --namespace=kubectl-1479'
Mar  2 22:01:50.502: INFO: stderr: ""
Mar  2 22:01:50.502: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:50.502: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1479" for this suite.
Mar  2 22:01:56.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:01:59.979: INFO: namespace kubectl-1479 deletion completed in 9.353014567s


• [SLOW TEST:22.747 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:41.261: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Mar  2 22:01:41.414: INFO: Waiting up to 5m0s for pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4" in namespace "containers-5385" to be "success or failure"
Mar  2 22:01:41.426: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.320496ms
Mar  2 22:01:43.439: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025219832s
Mar  2 22:01:45.452: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03830694s
Mar  2 22:01:47.465: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051087361s
Mar  2 22:01:49.478: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064217135s
Mar  2 22:01:51.626: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.211978723s
STEP: Saw pod success
Mar  2 22:01:51.626: INFO: Pod "client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4" satisfied condition "success or failure"
Mar  2 22:01:51.639: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4 container test-container: <nil>
STEP: delete the pod
Mar  2 22:01:51.673: INFO: Waiting for pod client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4 to disappear
Mar  2 22:01:51.685: INFO: Pod client-containers-3fe7ddcd-5903-4f31-ac93-5db7f87446f4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:51.685: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-5385" for this suite.
Mar  2 22:01:57.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:01.112: INFO: namespace containers-5385 deletion completed in 9.35274384s


• [SLOW TEST:19.851 seconds]
[k8s.io] Docker Containers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:26.086: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 22:01:44.444: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:01:44.463: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 22:01:46.464: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:01:46.484: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 22:01:48.464: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 22:01:48.485: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:48.485: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-547" for this suite.
Mar  2 22:02:00.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:04.075: INFO: namespace container-lifecycle-hook-547 deletion completed in 15.382412441s


• [SLOW TEST:37.989 seconds]
[k8s.io] Container Lifecycle Hook
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:00.028: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Mar  2 22:02:00.133: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig cluster-info'
Mar  2 22:02:00.253: INFO: stderr: ""
Mar  2 22:02:00.253: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.jeder-43-lts.x0w4.p1.openshiftapps.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:00.253: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-649" for this suite.
Mar  2 22:02:06.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:09.700: INFO: namespace kubectl-649 deletion completed in 9.352823555s


• [SLOW TEST:9.672 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:01:35.369: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:01:35.613: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-4afda351-db2d-43a6-b79e-3b694dd97c57
STEP: Creating secret with name s-test-opt-upd-b9dafaa3-afc3-40b1-9ca1-935679910f9e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4afda351-db2d-43a6-b79e-3b694dd97c57
STEP: Updating secret s-test-opt-upd-b9dafaa3-afc3-40b1-9ca1-935679910f9e
STEP: Creating secret with name s-test-opt-create-1cec3d52-cc55-481f-9ad8-1cdcbe6c38ee
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:01:50.004: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1281" for this suite.
Mar  2 22:02:08.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:11.492: INFO: namespace secrets-1281 deletion completed in 21.376541876s


• [SLOW TEST:36.123 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:01.174: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:18.448: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-8527" for this suite.
Mar  2 22:02:24.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:27.846: INFO: namespace resourcequota-8527 deletion completed in 9.353812222s


• [SLOW TEST:26.673 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:09.711: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Mar  2 22:02:09.852: INFO: Waiting up to 5m0s for pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a" in namespace "var-expansion-9433" to be "success or failure"
Mar  2 22:02:09.866: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.213819ms
Mar  2 22:02:11.880: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028757118s
Mar  2 22:02:13.895: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043490899s
Mar  2 22:02:15.909: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057508642s
Mar  2 22:02:17.923: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070794165s
Mar  2 22:02:19.936: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.084103936s
STEP: Saw pod success
Mar  2 22:02:19.936: INFO: Pod "var-expansion-e134a501-3153-495c-87fa-90d058ce036a" satisfied condition "success or failure"
Mar  2 22:02:19.949: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod var-expansion-e134a501-3153-495c-87fa-90d058ce036a container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:02:19.983: INFO: Waiting for pod var-expansion-e134a501-3153-495c-87fa-90d058ce036a to disappear
Mar  2 22:02:19.996: INFO: Pod var-expansion-e134a501-3153-495c-87fa-90d058ce036a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:19.996: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9433" for this suite.
Mar  2 22:02:26.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:29.425: INFO: namespace var-expansion-9433 deletion completed in 9.355130557s


• [SLOW TEST:19.715 seconds]
[k8s.io] Variable Expansion
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:04.078: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:26.246: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-7683" for this suite.
Mar  2 22:02:34.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:37.894: INFO: namespace job-7683 deletion completed in 11.441141324s


• [SLOW TEST:33.816 seconds]
[sig-apps] Job
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:11.566: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  2 22:02:22.395: INFO: Successfully updated pod "annotationupdate9b04f843-b166-48cd-a434-63f9dd51ea29"
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:24.438: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3423" for this suite.
Mar  2 22:02:38.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:41.900: INFO: namespace projected-3423 deletion completed in 17.37586614s


• [SLOW TEST:30.334 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:29.435: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:02:29.565: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:32.312: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5284" for this suite.
Mar  2 22:02:40.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:43.780: INFO: namespace custom-resource-definition-5284 deletion completed in 11.3557229s


• [SLOW TEST:14.346 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:41.942: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:02:42.241: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"09da7b94-d675-406a-93a3-e476623f7bf0", Controller:(*bool)(0xc003162b22), BlockOwnerDeletion:(*bool)(0xc003162b23)}}
Mar  2 22:02:42.271: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9ea6a3ec-97c9-4c22-89be-45770210b521", Controller:(*bool)(0xc0038fdbda), BlockOwnerDeletion:(*bool)(0xc0038fdbdb)}}
Mar  2 22:02:42.296: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6e9e16c6-f328-4b5a-b3ec-b601e8b847a2", Controller:(*bool)(0xc0018db9ea), BlockOwnerDeletion:(*bool)(0xc0018db9eb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:47.341: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-6814" for this suite.
Mar  2 22:02:53.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:02:56.820: INFO: namespace gc-6814 deletion completed in 9.376929193s


• [SLOW TEST:14.878 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:43.800: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-85427e59-dc8b-466c-804c-71d6ee4cc155
STEP: Creating a pod to test consume configMaps
Mar  2 22:02:43.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db" in namespace "projected-4904" to be "success or failure"
Mar  2 22:02:43.961: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.592232ms
Mar  2 22:02:45.975: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025799246s
Mar  2 22:02:47.988: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039602835s
Mar  2 22:02:50.002: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053312833s
Mar  2 22:02:52.016: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067376177s
Mar  2 22:02:54.031: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.081651635s
STEP: Saw pod success
Mar  2 22:02:54.031: INFO: Pod "pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db" satisfied condition "success or failure"
Mar  2 22:02:54.043: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:02:54.091: INFO: Waiting for pod pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db to disappear
Mar  2 22:02:54.104: INFO: Pod pod-projected-configmaps-85303bea-7daf-4e45-91ca-85bb47eab7db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:54.104: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4904" for this suite.
Mar  2 22:03:00.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:03.583: INFO: namespace projected-4904 deletion completed in 9.354787918s


• [SLOW TEST:19.784 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:27.851: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1103
[It] Should recreate evicted statefulset [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1103
STEP: Creating statefulset with conflicting port in namespace statefulset-1103
STEP: Waiting until pod test-pod will start running in namespace statefulset-1103
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1103
Mar  2 22:02:38.090: INFO: Observed stateful pod in namespace: statefulset-1103, name: ss-0, uid: f0e5f6f3-2d67-4879-8b9c-bf6931fed5e1, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 22:02:38.092: INFO: Observed stateful pod in namespace: statefulset-1103, name: ss-0, uid: f0e5f6f3-2d67-4879-8b9c-bf6931fed5e1, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 22:02:38.106: INFO: Observed stateful pod in namespace: statefulset-1103, name: ss-0, uid: f0e5f6f3-2d67-4879-8b9c-bf6931fed5e1, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 22:02:38.110: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1103
STEP: Removing pod with conflicting port in namespace statefulset-1103
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1103 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 22:02:48.215: INFO: Deleting all statefulset in ns statefulset-1103
Mar  2 22:02:48.228: INFO: Scaling statefulset ss to 0
Mar  2 22:02:58.283: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:02:58.295: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:02:58.335: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-1103" for this suite.
Mar  2 22:03:06.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:09.846: INFO: namespace statefulset-1103 deletion completed in 11.467839162s


• [SLOW TEST:41.995 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:37.905: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-d5mj
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:02:38.135: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d5mj" in namespace "subpath-3866" to be "success or failure"
Mar  2 22:02:38.154: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Pending", Reason="", readiness=false. Elapsed: 19.216564ms
Mar  2 22:02:40.175: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040249664s
Mar  2 22:02:42.218: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083247272s
Mar  2 22:02:44.239: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104575745s
Mar  2 22:02:46.259: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12406712s
Mar  2 22:02:48.279: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 10.144624264s
Mar  2 22:02:50.299: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 12.164506209s
Mar  2 22:02:52.320: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 14.185030978s
Mar  2 22:02:54.340: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 16.205289747s
Mar  2 22:02:56.360: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 18.225502588s
Mar  2 22:02:58.380: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 20.245530211s
Mar  2 22:03:00.419: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 22.284135213s
Mar  2 22:03:02.438: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 24.303874381s
Mar  2 22:03:04.469: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Running", Reason="", readiness=true. Elapsed: 26.334551274s
Mar  2 22:03:06.489: INFO: Pod "pod-subpath-test-configmap-d5mj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.354402116s
STEP: Saw pod success
Mar  2 22:03:06.489: INFO: Pod "pod-subpath-test-configmap-d5mj" satisfied condition "success or failure"
Mar  2 22:03:06.509: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-subpath-test-configmap-d5mj container test-container-subpath-configmap-d5mj: <nil>
STEP: delete the pod
Mar  2 22:03:06.564: INFO: Waiting for pod pod-subpath-test-configmap-d5mj to disappear
Mar  2 22:03:06.584: INFO: Pod pod-subpath-test-configmap-d5mj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d5mj
Mar  2 22:03:06.584: INFO: Deleting pod "pod-subpath-test-configmap-d5mj" in namespace "subpath-3866"
[AfterEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:06.603: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-3866" for this suite.
Mar  2 22:03:12.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:16.276: INFO: namespace subpath-3866 deletion completed in 9.482306685s


• [SLOW TEST:38.372 seconds]
[sig-storage] Subpath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:02:56.829: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:02:56.987: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3" in namespace "security-context-test-7669" to be "success or failure"
Mar  2 22:02:57.006: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.6993ms
Mar  2 22:02:59.026: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038687324s
Mar  2 22:03:01.045: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057412723s
Mar  2 22:03:03.065: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07739013s
Mar  2 22:03:05.084: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096854397s
Mar  2 22:03:07.104: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.116693194s
Mar  2 22:03:07.104: INFO: Pod "alpine-nnp-false-a2f6d7ad-0013-44e6-b65d-cde9c36e5fd3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:07.125: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-7669" for this suite.
Mar  2 22:03:13.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:16.690: INFO: namespace security-context-test-7669 deletion completed in 9.479946159s


• [SLOW TEST:19.862 seconds]
[k8s.io] Security Context
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:03.600: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:03:03.691: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 22:03:09.577: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-7824 create -f -'
Mar  2 22:03:15.146: INFO: stderr: ""
Mar  2 22:03:15.146: INFO: stdout: "e2e-test-crd-publish-openapi-316-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 22:03:15.146: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-7824 delete e2e-test-crd-publish-openapi-316-crds test-cr'
Mar  2 22:03:15.288: INFO: stderr: ""
Mar  2 22:03:15.288: INFO: stdout: "e2e-test-crd-publish-openapi-316-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 22:03:15.288: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-7824 apply -f -'
Mar  2 22:03:16.010: INFO: stderr: ""
Mar  2 22:03:16.010: INFO: stdout: "e2e-test-crd-publish-openapi-316-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 22:03:16.010: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-7824 delete e2e-test-crd-publish-openapi-316-crds test-cr'
Mar  2 22:03:16.175: INFO: stderr: ""
Mar  2 22:03:16.175: INFO: stdout: "e2e-test-crd-publish-openapi-316-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  2 22:03:16.175: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-316-crds'
Mar  2 22:03:17.035: INFO: stderr: ""
Mar  2 22:03:17.035: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-316-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:23.008: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7824" for this suite.
Mar  2 22:03:29.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:32.461: INFO: namespace crd-publish-openapi-7824 deletion completed in 9.354705877s


• [SLOW TEST:28.861 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:16.699: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:03:16.914: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132" in namespace "downward-api-8589" to be "success or failure"
Mar  2 22:03:16.979: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Pending", Reason="", readiness=false. Elapsed: 18.410942ms
Mar  2 22:03:18.999: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038059866s
Mar  2 22:03:21.019: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057778598s
Mar  2 22:03:23.039: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077823474s
Mar  2 22:03:25.059: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097664931s
Mar  2 22:03:27.078: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.11739774s
STEP: Saw pod success
Mar  2 22:03:27.079: INFO: Pod "downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132" satisfied condition "success or failure"
Mar  2 22:03:27.097: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132 container client-container: <nil>
STEP: delete the pod
Mar  2 22:03:27.145: INFO: Waiting for pod downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132 to disappear
Mar  2 22:03:27.163: INFO: Pod downwardapi-volume-47806b4c-c229-4539-8669-0686ca4d8132 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:27.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8589" for this suite.
Mar  2 22:03:33.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:36.626: INFO: namespace downward-api-8589 deletion completed in 9.378261718s


• [SLOW TEST:19.927 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:32.487: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:32.672: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5367" for this suite.
Mar  2 22:03:38.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:42.098: INFO: namespace resourcequota-5367 deletion completed in 9.352640302s


• [SLOW TEST:9.611 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:16.331: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 22:03:27.113: INFO: Successfully updated pod "pod-update-61355084-d539-4250-acdc-63cf5d8119a2"
STEP: verifying the updated pod is in kubernetes
Mar  2 22:03:27.152: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:27.152: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1482" for this suite.
Mar  2 22:03:39.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:42.639: INFO: namespace pods-1482 deletion completed in 15.381693783s


• [SLOW TEST:26.308 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:09.851: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3383
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 22:03:09.954: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 22:03:44.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.128.2.245&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:44.475: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:44.624: INFO: Waiting for endpoints: map[]
Mar  2 22:03:44.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.129.2.227&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:44.636: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:44.793: INFO: Waiting for endpoints: map[]
Mar  2 22:03:44.806: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.130.4.193&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:44.806: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:44.965: INFO: Waiting for endpoints: map[]
Mar  2 22:03:44.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.129.4.140&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:44.978: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:45.173: INFO: Waiting for endpoints: map[]
Mar  2 22:03:45.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.128.4.104&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:45.186: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:45.372: INFO: Waiting for endpoints: map[]
Mar  2 22:03:45.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.130.2.212&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:45.385: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:45.587: INFO: Waiting for endpoints: map[]
Mar  2 22:03:45.599: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.248:8080/dial?request=hostName&protocol=http&host=10.131.2.106&port=8080&tries=1'] Namespace:pod-network-test-3383 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:03:45.600: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:03:45.748: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:45.749: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-3383" for this suite.
Mar  2 22:03:51.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:55.146: INFO: namespace pod-network-test-3383 deletion completed in 9.353165545s


• [SLOW TEST:45.296 seconds]
[sig-network] Networking
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:36.638: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-64281495-1570-4fb2-ab86-7c0fa8e5ba4a
STEP: Creating a pod to test consume secrets
Mar  2 22:03:36.848: INFO: Waiting up to 5m0s for pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356" in namespace "secrets-3926" to be "success or failure"
Mar  2 22:03:36.867: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Pending", Reason="", readiness=false. Elapsed: 18.736098ms
Mar  2 22:03:38.886: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037912951s
Mar  2 22:03:40.906: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057623431s
Mar  2 22:03:42.928: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080154225s
Mar  2 22:03:44.947: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099348285s
Mar  2 22:03:46.967: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119314653s
STEP: Saw pod success
Mar  2 22:03:46.968: INFO: Pod "pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356" satisfied condition "success or failure"
Mar  2 22:03:46.987: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:03:47.035: INFO: Waiting for pod pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356 to disappear
Mar  2 22:03:47.054: INFO: Pod pod-secrets-27af8c36-8a3a-4845-b73e-0ca0fba6e356 no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:47.054: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3926" for this suite.
Mar  2 22:03:53.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:03:56.518: INFO: namespace secrets-3926 deletion completed in 9.378590736s


• [SLOW TEST:19.880 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:42.132: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:03:42.263: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705" in namespace "security-context-test-1065" to be "success or failure"
Mar  2 22:03:42.276: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Pending", Reason="", readiness=false. Elapsed: 12.143644ms
Mar  2 22:03:44.341: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076969123s
Mar  2 22:03:46.354: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090253551s
Mar  2 22:03:48.367: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10326665s
Mar  2 22:03:50.380: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116368309s
Mar  2 22:03:52.395: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.131646089s
Mar  2 22:03:52.395: INFO: Pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705" satisfied condition "success or failure"
Mar  2 22:03:52.410: INFO: Got logs for pod "busybox-privileged-false-72513537-9df8-4c06-9914-90559d05e705": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:03:52.410: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-1065" for this suite.
Mar  2 22:03:58.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:01.857: INFO: namespace security-context-test-1065 deletion completed in 9.352747126s


• [SLOW TEST:19.725 seconds]
[k8s.io] Security Context
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:55.193: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6479.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:04:05.394: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.408: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.421: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.434: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.473: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.487: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local from pod dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b: the server could not find the requested resource (get pods dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b)
Mar  2 22:04:05.538: INFO: Lookups using dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6479.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6479.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6479.svc.cluster.local]

Mar  2 22:04:10.696: INFO: DNS probes using dns-6479/dns-test-12ac537b-038e-44c4-b7e0-74088ea9ed5b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:10.740: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-6479" for this suite.
Mar  2 22:04:16.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:20.138: INFO: namespace dns-6479 deletion completed in 9.351467762s


• [SLOW TEST:24.946 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:01.885: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-1435e7ee-dd77-4fcf-96d1-76fbf1271936
STEP: Creating a pod to test consume secrets
Mar  2 22:04:02.034: INFO: Waiting up to 5m0s for pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252" in namespace "secrets-7479" to be "success or failure"
Mar  2 22:04:02.051: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Pending", Reason="", readiness=false. Elapsed: 16.897692ms
Mar  2 22:04:04.065: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030765021s
Mar  2 22:04:06.079: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044735074s
Mar  2 22:04:08.093: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058291614s
Mar  2 22:04:10.106: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07145352s
Mar  2 22:04:12.119: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.084476471s
STEP: Saw pod success
Mar  2 22:04:12.119: INFO: Pod "pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252" satisfied condition "success or failure"
Mar  2 22:04:12.131: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:04:12.167: INFO: Waiting for pod pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252 to disappear
Mar  2 22:04:12.179: INFO: Pod pod-secrets-7e7197cb-039b-43dd-a4da-76e8bb559252 no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:12.180: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-7479" for this suite.
Mar  2 22:04:18.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:21.628: INFO: namespace secrets-7479 deletion completed in 9.35329382s


• [SLOW TEST:19.743 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:56.524: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:06.800: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-1928" for this suite.
Mar  2 22:04:28.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:32.491: INFO: namespace containers-1928 deletion completed in 25.621676026s


• [SLOW TEST:35.995 seconds]
[k8s.io] Docker Containers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:21.660: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:04:21.759: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 22:04:28.037: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-2508 create -f -'
Mar  2 22:04:33.503: INFO: stderr: ""
Mar  2 22:04:33.503: INFO: stdout: "e2e-test-crd-publish-openapi-1651-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 22:04:33.503: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-2508 delete e2e-test-crd-publish-openapi-1651-crds test-cr'
Mar  2 22:04:33.640: INFO: stderr: ""
Mar  2 22:04:33.640: INFO: stdout: "e2e-test-crd-publish-openapi-1651-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 22:04:33.640: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-2508 apply -f -'
Mar  2 22:04:34.139: INFO: stderr: ""
Mar  2 22:04:34.139: INFO: stdout: "e2e-test-crd-publish-openapi-1651-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 22:04:34.139: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-2508 delete e2e-test-crd-publish-openapi-1651-crds test-cr'
Mar  2 22:04:34.307: INFO: stderr: ""
Mar  2 22:04:34.307: INFO: stdout: "e2e-test-crd-publish-openapi-1651-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 22:04:34.307: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-1651-crds'
Mar  2 22:04:35.105: INFO: stderr: ""
Mar  2 22:04:35.105: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1651-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:40.990: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2508" for this suite.
Mar  2 22:04:47.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:50.439: INFO: namespace crd-publish-openapi-2508 deletion completed in 9.378459544s


• [SLOW TEST:28.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:33.498: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:46.885: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2292" for this suite.
Mar  2 22:04:53.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:56.451: INFO: namespace resourcequota-2292 deletion completed in 9.378512227s


• [SLOW TEST:22.953 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:20.156: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:04:20.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:04:22.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:04:24.851: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:04:26.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:04:28.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:04:30.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783460, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:04:33.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:34.088: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7377" for this suite.
Mar  2 22:04:46.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:49.530: INFO: namespace webhook-7377 deletion completed in 15.386597404s
STEP: Destroying namespace "webhook-7377-markers" for this suite.
Mar  2 22:04:55.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:04:58.886: INFO: namespace webhook-7377-markers deletion completed in 9.355270511s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:38.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:50.462: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-33460e65-12c1-44d3-8673-fe2ed75d8aa1
STEP: Creating a pod to test consume secrets
Mar  2 22:04:50.638: INFO: Waiting up to 5m0s for pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931" in namespace "secrets-9978" to be "success or failure"
Mar  2 22:04:50.658: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931": Phase="Pending", Reason="", readiness=false. Elapsed: 19.983308ms
Mar  2 22:04:52.678: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039718581s
Mar  2 22:04:54.699: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060746267s
Mar  2 22:04:56.719: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080704472s
Mar  2 22:04:58.738: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.099876637s
STEP: Saw pod success
Mar  2 22:04:58.738: INFO: Pod "pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931" satisfied condition "success or failure"
Mar  2 22:04:58.760: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931 container secret-env-test: <nil>
STEP: delete the pod
Mar  2 22:04:58.815: INFO: Waiting for pod pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931 to disappear
Mar  2 22:04:58.837: INFO: Pod pod-secrets-b28482b7-1758-4375-925d-1c8cb9fc4931 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:04:58.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9978" for this suite.
Mar  2 22:05:05.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:05:08.486: INFO: namespace secrets-9978 deletion completed in 9.528163922s


• [SLOW TEST:18.025 seconds]
[sig-api-machinery] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:58.990: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:04:59.129: INFO: Creating deployment "webserver-deployment"
Mar  2 22:04:59.146: INFO: Waiting for observed generation 1
Mar  2 22:05:01.448: INFO: Waiting for all required pods to come up
Mar  2 22:05:01.501: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  2 22:05:09.555: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 22:05:09.581: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 22:05:09.620: INFO: Updating deployment webserver-deployment
Mar  2 22:05:09.620: INFO: Waiting for observed generation 2
Mar  2 22:05:11.645: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 22:05:11.658: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 22:05:11.670: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:05:11.707: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 22:05:11.707: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 22:05:11.720: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:05:11.744: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 22:05:11.744: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 22:05:11.771: INFO: Updating deployment webserver-deployment
Mar  2 22:05:11.771: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 22:05:11.797: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 22:05:13.839: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  2 22:05:13.870: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6274 /apis/apps/v1/namespaces/deployment-6274/deployments/webserver-deployment 98f38854-bdd7-42ff-8e04-510920a02ece 15440587 3 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002edcdd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-02 22:05:11 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-02 22:05:11 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 22:05:13.884: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6274 /apis/apps/v1/namespaces/deployment-6274/replicasets/webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 15440586 3 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 98f38854-bdd7-42ff-8e04-510920a02ece 0xc002edd2e7 0xc002edd2e8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002edd358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:05:13.884: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 22:05:13.884: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6274 /apis/apps/v1/namespaces/deployment-6274/replicasets/webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 15440544 3 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 98f38854-bdd7-42ff-8e04-510920a02ece 0xc002edd217 0xc002edd218}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002edd288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:05:13.929: INFO: Pod "webserver-deployment-595b5b9587-2g8q9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2g8q9 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-2g8q9 e11d2f5c-3a8a-4fe1-b120-9b7104c35797 15440406 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.4"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c65c7 0xc0039c65c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.4,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4fd14f03b3cdd3d2154aa838d659fb63311c985cd1a28e44a9ea019755de0179,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.929: INFO: Pod "webserver-deployment-595b5b9587-2mlxp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2mlxp webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-2mlxp d872db8b-7be8-4747-925c-e01f2e05fc39 15440547 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c6757 0xc0039c6758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.929: INFO: Pod "webserver-deployment-595b5b9587-5d578" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5d578 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-5d578 7ed13bb2-6d4f-4185-a3fc-ac7a6c805698 15440338 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.213"
    ],
    "dns": {},
    "default-route": [
        "10.130.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c68c7 0xc0039c68c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:10.130.2.213,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a048555ef39b45973bae39778339ecb5c18cd548780e4b9f6ac652b0cb70a396,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.130.2.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.929: INFO: Pod "webserver-deployment-595b5b9587-627x6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-627x6 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-627x6 3d96dc1d-c9f5-47db-ac62-f3929e87e7a8 15440540 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c6a57 0xc0039c6a58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.929: INFO: Pod "webserver-deployment-595b5b9587-697bt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-697bt webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-697bt 91df4196-342e-4116-be27-eb20f67c6029 15440546 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c6bc7 0xc0039c6bc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.930: INFO: Pod "webserver-deployment-595b5b9587-6jz52" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6jz52 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-6jz52 e26b6fc0-70eb-44b8-b964-e03b2dee0107 15440319 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.4.194"
    ],
    "dns": {},
    "default-route": [
        "10.130.4.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c6d57 0xc0039c6d58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:10.130.4.194,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://907e1d44f76681936a4628be471179284aaecf01e58e8482874811f1913a8ec4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.130.4.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.930: INFO: Pod "webserver-deployment-595b5b9587-cxhh9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cxhh9 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-cxhh9 82b28c58-bbf7-492c-adfc-20afdd9d67d5 15440550 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7047 0xc0039c7048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.930: INFO: Pod "webserver-deployment-595b5b9587-gbchn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gbchn webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-gbchn d3220994-0c80-408b-9c29-8e215cb5658b 15440316 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.228"
    ],
    "dns": {},
    "default-route": [
        "10.129.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7237 0xc0039c7238}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:10.129.2.228,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://16563d6188a272f21bc87ca2ee5e8d7671b6d07a600cc0312e9311ed00fdc8a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.930: INFO: Pod "webserver-deployment-595b5b9587-hzwf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hzwf8 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-hzwf8 2925b7c1-645f-442e-a0fd-1e35ab8e3d6d 15440515 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c73e7 0xc0039c73e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.930: INFO: Pod "webserver-deployment-595b5b9587-knklm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-knklm webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-knklm 14580a99-6dba-4724-9e13-28bf707d23c2 15440514 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7557 0xc0039c7558}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-l6c6z" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l6c6z webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-l6c6z 3c988a7f-6a75-477f-8635-8d0e8a10b7fb 15440329 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.3"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c76c7 0xc0039c76c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.3,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d801a1f188d454ace0e644e987ac48dd36047613f670eae2bdbb19c5333d8238,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-lwwm9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lwwm9 webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-lwwm9 f7de98d7-8012-43d4-921a-0400b7b2d15a 15440400 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.6"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7857 0xc0039c7858}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.6,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://548bb7bc0f1ec75b7cb48890419f9102b1a296577d8d757a620b38900d3d5296,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-ndptz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ndptz webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-ndptz 342af6dc-035a-413b-96bd-d27eb13dfb4f 15440392 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.5"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c79e7 0xc0039c79e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.5,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://cb024a846f7b4a717178cc1f5a5cef9f4aea3ee42061064bcb1c8b182f626768,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-qmctf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qmctf webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-qmctf 0703b052-fd0a-4374-a32e-c8f0965fb63a 15440398 0 2020-03-02 22:04:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.8"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7b77 0xc0039c7b78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:04:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.8,StartTime:2020-03-02 22:04:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:05:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e704be9e3b2d0083635030d4b0214eea952daaf1af24cd6812f5bdacdae328c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-r24jf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r24jf webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-r24jf 891dbc7d-4ccc-48c8-ba37-b1ad4884c59a 15440512 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7d07 0xc0039c7d08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.931: INFO: Pod "webserver-deployment-595b5b9587-rdr6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rdr6k webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-rdr6k 86ab9ecb-6101-4ae2-a683-c85f1dd46912 15440555 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7e77 0xc0039c7e78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-595b5b9587-smqzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-smqzk webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-smqzk c1ec0d55-a518-4ab6-853f-aa86d37917d3 15440560 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc0039c7fe7 0xc0039c7fe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-595b5b9587-v9gdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v9gdb webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-v9gdb 04f04f5c-df77-4ac6-8796-30a79cde94a8 15440509 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc002fc6157 0xc002fc6158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-595b5b9587-whfqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-whfqz webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-whfqz d4864337-b431-48c9-9e3c-09e527c46c9f 15440552 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc002fc62c7 0xc002fc62c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-595b5b9587-zwx5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zwx5x webserver-deployment-595b5b9587- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-595b5b9587-zwx5x 38889871-3806-4fa1-a879-b750dbf5de54 15440498 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e4d6a89-0b3d-4ffb-baeb-12bf7cdf47f6 0xc002fc6637 0xc002fc6638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-c7997dcc8-7zqbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7zqbb webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-7zqbb 70d02597-f34e-4124-b2c7-303f6f67d230 15440575 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc67a7 0xc002fc67a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.932: INFO: Pod "webserver-deployment-c7997dcc8-8kr6g" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8kr6g webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-8kr6g 96e6e6f4-bb04-42ea-9e99-caa4fff9f2eb 15440439 0 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc6937 0xc002fc6938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-dfc6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dfc6q webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-dfc6q d5cee8b1-b878-4b26-98ee-a8a798d66e9c 15440584 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc6ac7 0xc002fc6ac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-gtjvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gtjvs webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-gtjvs 13d445ec-5747-47b2-878d-6912caf737bb 15440465 0 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc6c57 0xc002fc6c58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-gwcwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gwcwx webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-gwcwx 2bdaef1a-9880-4f29-99d0-9510ed9c47b1 15440576 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc6de7 0xc002fc6de8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-jf8gg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jf8gg webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-jf8gg 92dde088-03d6-4008-9b0d-070283e6ac4a 15440456 0 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc6f77 0xc002fc6f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-l8j2q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l8j2q webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-l8j2q 4910a52a-b809-4763-8aee-8c640a0121c3 15440523 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc7107 0xc002fc7108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-lpxdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lpxdb webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-lpxdb cbd6f599-3630-45d3-a2b9-251805cbf90c 15440574 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc7297 0xc002fc7298}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.933: INFO: Pod "webserver-deployment-c7997dcc8-mcb89" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mcb89 webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-mcb89 3ff14396-cdd6-437f-a7ad-848cf15d28b0 15440533 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc7427 0xc002fc7428}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-137-145.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.137.145,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.934: INFO: Pod "webserver-deployment-c7997dcc8-q77zb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-q77zb webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-q77zb 4ab64fa1-ff09-4a2f-b89d-d1a5582c881e 15440449 0 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc75b7 0xc002fc75b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-141-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.141.17,PodIP:,StartTime:2020-03-02 22:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.934: INFO: Pod "webserver-deployment-c7997dcc8-tbdb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tbdb8 webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-tbdb8 3b9e9acd-b8f6-4046-8cb8-76545e8252fe 15440569 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc7747 0xc002fc7748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.934: INFO: Pod "webserver-deployment-c7997dcc8-vs8lf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vs8lf webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-vs8lf 1a3e105e-f0d4-497a-ad96-6f17dd82dcbe 15440553 0 2020-03-02 22:05:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc78d7 0xc002fc78d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 22:05:13.934: INFO: Pod "webserver-deployment-c7997dcc8-wrtgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wrtgp webserver-deployment-c7997dcc8- deployment-6274 /api/v1/namespaces/deployment-6274/pods/webserver-deployment-c7997dcc8-wrtgp 40469b4d-f1f3-4252-841c-83b12c174e2a 15440450 0 2020-03-02 22:05:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 961e655c-6a0e-4ec5-a378-8e88f0032ccd 0xc002fc7a67 0xc002fc7a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nxt6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nxt6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nxt6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-224.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lklhp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:05:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.224,PodIP:,StartTime:2020-03-02 22:05:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:05:13.934: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-6274" for this suite.
Mar  2 22:05:22.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:05:25.361: INFO: namespace deployment-6274 deletion completed in 11.352639222s


• [SLOW TEST:26.371 seconds]
[sig-apps] Deployment
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:04:56.480: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar  2 22:04:56.610: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:05:28.368: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5457" for this suite.
Mar  2 22:05:34.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:05:37.800: INFO: namespace crd-publish-openapi-5457 deletion completed in 9.354522348s


• [SLOW TEST:41.320 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:05:37.823: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  2 22:05:37.960: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:05:48.886: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-2556" for this suite.
Mar  2 22:05:55.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:05:58.331: INFO: namespace init-container-2556 deletion completed in 9.356732963s


• [SLOW TEST:20.508 seconds]
[k8s.io] InitContainer [NodeConformance]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:05:25.405: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9510
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 22:05:25.516: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Mar  2 22:06:02.050: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.3.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.051: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:02.201: INFO: Found all expected endpoints: [netserver-0]
Mar  2 22:06:02.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.2.107:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.214: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:02.363: INFO: Found all expected endpoints: [netserver-1]
Mar  2 22:06:02.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.4.141:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.376: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:02.531: INFO: Found all expected endpoints: [netserver-2]
Mar  2 22:06:02.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.130.2.220:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.543: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:02.689: INFO: Found all expected endpoints: [netserver-3]
Mar  2 22:06:02.702: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.4.105:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.702: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:02.903: INFO: Found all expected endpoints: [netserver-4]
Mar  2 22:06:02.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.237:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:02.918: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:03.066: INFO: Found all expected endpoints: [netserver-5]
Mar  2 22:06:03.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.130.4.201:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9510 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:06:03.079: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:06:03.283: INFO: Found all expected endpoints: [netserver-6]
[AfterEach] [sig-network] Networking
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:03.283: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-9510" for this suite.
Mar  2 22:06:09.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:12.692: INFO: namespace pod-network-test-9510 deletion completed in 9.353792689s


• [SLOW TEST:47.287 seconds]
[sig-network] Networking
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:05:08.519: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 22:05:30.925: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:30.944: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:32.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:32.964: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:34.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:34.964: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:36.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:36.964: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:38.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:38.964: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:40.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:40.965: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 22:05:42.944: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 22:05:42.964: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:05:42.964: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9717" for this suite.
Mar  2 22:06:11.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:14.497: INFO: namespace container-lifecycle-hook-9717 deletion completed in 31.379383981s


• [SLOW TEST:65.978 seconds]
[k8s.io] Container Lifecycle Hook
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:05:58.353: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8
Mar  2 22:05:58.502: INFO: Pod name my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8: Found 0 pods out of 1
Mar  2 22:06:03.516: INFO: Pod name my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8: Found 1 pods out of 1
Mar  2 22:06:03.517: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8" are running
Mar  2 22:06:07.544: INFO: Pod "my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8-klmh7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-02 22:05:57 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-02 22:05:57 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-02 22:05:57 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-02 22:05:57 +0000 UTC Reason: Message:}])
Mar  2 22:06:07.544: INFO: Trying to dial the pod
Mar  2 22:06:12.588: INFO: Controller my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8: Got expected result from replica 1 [my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8-klmh7]: "my-hostname-basic-e2dea371-bcc7-4ffb-bc32-f0883c7941a8-klmh7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:12.588: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-7904" for this suite.
Mar  2 22:06:18.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:22.011: INFO: namespace replication-controller-7904 deletion completed in 9.355881326s


• [SLOW TEST:23.658 seconds]
[sig-apps] ReplicationController
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:14.501: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:06:14.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375" in namespace "downward-api-2691" to be "success or failure"
Mar  2 22:06:14.690: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Pending", Reason="", readiness=false. Elapsed: 21.243291ms
Mar  2 22:06:16.711: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041742805s
Mar  2 22:06:18.731: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061556253s
Mar  2 22:06:20.752: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082815636s
Mar  2 22:06:22.772: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103103716s
Mar  2 22:06:24.792: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.123279874s
STEP: Saw pod success
Mar  2 22:06:24.793: INFO: Pod "downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375" satisfied condition "success or failure"
Mar  2 22:06:24.812: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375 container client-container: <nil>
STEP: delete the pod
Mar  2 22:06:24.864: INFO: Waiting for pod downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375 to disappear
Mar  2 22:06:24.883: INFO: Pod downwardapi-volume-2dfabfc4-e16a-4f14-9b8f-109dd4aeb375 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:24.883: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2691" for this suite.
Mar  2 22:06:31.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:34.449: INFO: namespace downward-api-2691 deletion completed in 9.378643004s


• [SLOW TEST:19.949 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:22.023: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ccf2d7c9-a607-4247-a99a-8293a7955754
STEP: Creating a pod to test consume configMaps
Mar  2 22:06:22.235: INFO: Waiting up to 5m0s for pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68" in namespace "configmap-3237" to be "success or failure"
Mar  2 22:06:22.250: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 15.821508ms
Mar  2 22:06:24.265: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030604498s
Mar  2 22:06:26.279: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044617438s
Mar  2 22:06:28.293: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058675152s
Mar  2 22:06:30.324: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089637542s
Mar  2 22:06:32.338: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103351636s
STEP: Saw pod success
Mar  2 22:06:32.338: INFO: Pod "pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68" satisfied condition "success or failure"
Mar  2 22:06:32.351: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:06:32.388: INFO: Waiting for pod pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68 to disappear
Mar  2 22:06:32.401: INFO: Pod pod-configmaps-181cefe2-5e5c-43a5-8cb4-be97e2bdfc68 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:32.401: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3237" for this suite.
Mar  2 22:06:38.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:41.855: INFO: namespace configmap-3237 deletion completed in 9.355060771s


• [SLOW TEST:19.833 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:12.712: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7999
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7999
I0302 22:06:12.917106   56390 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7999, replica count: 2
I0302 22:06:15.968583   56390 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:06:18.969204   56390 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:06:21.969770   56390 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:06:24.970653   56390 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:06:24.970: INFO: Creating new exec pod
Mar  2 22:06:36.020: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-7999 execpod7jtwx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 22:06:36.425: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 22:06:36.425: INFO: stdout: ""
Mar  2 22:06:36.426: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-7999 execpod7jtwx -- /bin/sh -x -c nc -zv -t -w 2 172.30.84.27 80'
Mar  2 22:06:36.764: INFO: stderr: "+ nc -zv -t -w 2 172.30.84.27 80\nConnection to 172.30.84.27 80 port [tcp/http] succeeded!\n"
Mar  2 22:06:36.764: INFO: stdout: ""
Mar  2 22:06:36.764: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:36.793: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7999" for this suite.
Mar  2 22:06:42.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:06:46.219: INFO: namespace services-7999 deletion completed in 9.355077777s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:33.507 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:41.889: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 22:06:42.048: INFO: Waiting up to 5m0s for pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf" in namespace "emptydir-6572" to be "success or failure"
Mar  2 22:06:42.061: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.09249ms
Mar  2 22:06:44.077: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029328358s
Mar  2 22:06:46.092: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043709075s
Mar  2 22:06:48.106: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058018263s
Mar  2 22:06:50.121: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.073129457s
Mar  2 22:06:52.135: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.087071302s
STEP: Saw pod success
Mar  2 22:06:52.135: INFO: Pod "pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf" satisfied condition "success or failure"
Mar  2 22:06:52.149: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf container test-container: <nil>
STEP: delete the pod
Mar  2 22:06:52.186: INFO: Waiting for pod pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf to disappear
Mar  2 22:06:52.199: INFO: Pod pod-b9072e61-983f-4ff9-8f5c-6c419d34cfaf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:52.200: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6572" for this suite.
Mar  2 22:06:58.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:01.686: INFO: namespace emptydir-6572 deletion completed in 9.355554704s


• [SLOW TEST:19.797 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:46.269: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2308.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2308.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:06:56.752: INFO: DNS probes using dns-2308/dns-test-aa387ac2-984e-4248-b5d2-3379f2d00501 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:06:56.783: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2308" for this suite.
Mar  2 22:07:02.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:06.352: INFO: namespace dns-2308 deletion completed in 9.46432862s


• [SLOW TEST:20.083 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:01.691: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  2 22:07:01.850: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7888 /api/v1/namespaces/watch-7888/configmaps/e2e-watch-test-watch-closed 1c690bcb-2e2f-4a83-82c9-244548ae975d 15442544 0 2020-03-02 22:07:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 22:07:01.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7888 /api/v1/namespaces/watch-7888/configmaps/e2e-watch-test-watch-closed 1c690bcb-2e2f-4a83-82c9-244548ae975d 15442546 0 2020-03-02 22:07:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 22:07:01.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7888 /api/v1/namespaces/watch-7888/configmaps/e2e-watch-test-watch-closed 1c690bcb-2e2f-4a83-82c9-244548ae975d 15442547 0 2020-03-02 22:07:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 22:07:01.908: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7888 /api/v1/namespaces/watch-7888/configmaps/e2e-watch-test-watch-closed 1c690bcb-2e2f-4a83-82c9-244548ae975d 15442549 0 2020-03-02 22:07:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:01.908: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-7888" for this suite.
Mar  2 22:07:08.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:11.329: INFO: namespace watch-7888 deletion completed in 9.354269154s


• [SLOW TEST:9.638 seconds]
[sig-api-machinery] Watchers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:06.360: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:07:06.505: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2531'
Mar  2 22:07:06.682: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  2 22:07:06.682: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Mar  2 22:07:08.718: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete deployment e2e-test-httpd-deployment --namespace=kubectl-2531'
Mar  2 22:07:08.856: INFO: stderr: ""
Mar  2 22:07:08.856: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:08.856: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2531" for this suite.
Mar  2 22:07:21.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:24.342: INFO: namespace kubectl-2531 deletion completed in 15.352544487s


• [SLOW TEST:17.983 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:11.427: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-f1223fc9-945e-4aec-ba13-a097458cb063
STEP: Creating a pod to test consume secrets
Mar  2 22:07:11.570: INFO: Waiting up to 5m0s for pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346" in namespace "secrets-1617" to be "success or failure"
Mar  2 22:07:11.582: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Pending", Reason="", readiness=false. Elapsed: 12.424414ms
Mar  2 22:07:13.597: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026664354s
Mar  2 22:07:15.623: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05315085s
Mar  2 22:07:17.637: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067428598s
Mar  2 22:07:19.654: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083566181s
Mar  2 22:07:21.668: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097757536s
STEP: Saw pod success
Mar  2 22:07:21.668: INFO: Pod "pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346" satisfied condition "success or failure"
Mar  2 22:07:21.682: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:07:21.717: INFO: Waiting for pod pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346 to disappear
Mar  2 22:07:21.730: INFO: Pod pod-secrets-819ce455-64cf-4c36-9c1e-34066b3ed346 no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:21.730: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1617" for this suite.
Mar  2 22:07:27.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:31.145: INFO: namespace secrets-1617 deletion completed in 9.35703589s


• [SLOW TEST:19.718 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:24.353: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Mar  2 22:07:24.591: INFO: Waiting up to 5m0s for pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435" in namespace "var-expansion-4224" to be "success or failure"
Mar  2 22:07:24.603: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Pending", Reason="", readiness=false. Elapsed: 12.065054ms
Mar  2 22:07:26.617: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025820495s
Mar  2 22:07:28.630: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038900092s
Mar  2 22:07:30.643: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051990212s
Mar  2 22:07:32.657: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065654233s
Mar  2 22:07:34.670: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.078659442s
STEP: Saw pod success
Mar  2 22:07:34.670: INFO: Pod "var-expansion-c7499179-5ee1-4621-a223-b598fc51c435" satisfied condition "success or failure"
Mar  2 22:07:34.682: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod var-expansion-c7499179-5ee1-4621-a223-b598fc51c435 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:07:34.718: INFO: Waiting for pod var-expansion-c7499179-5ee1-4621-a223-b598fc51c435 to disappear
Mar  2 22:07:34.730: INFO: Pod var-expansion-c7499179-5ee1-4621-a223-b598fc51c435 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:34.730: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-4224" for this suite.
Mar  2 22:07:40.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:44.179: INFO: namespace var-expansion-4224 deletion completed in 9.353649114s


• [SLOW TEST:19.826 seconds]
[k8s.io] Variable Expansion
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:03:42.684: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-74bc2d5a-3d21-4c7f-bc20-2859160dbb55 in namespace container-probe-996
Mar  2 22:03:52.993: INFO: Started pod test-webserver-74bc2d5a-3d21-4c7f-bc20-2859160dbb55 in namespace container-probe-996
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:03:53.013: INFO: Initial restart count of pod test-webserver-74bc2d5a-3d21-4c7f-bc20-2859160dbb55 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:53.481: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-996" for this suite.
Mar  2 22:07:59.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:02.967: INFO: namespace container-probe-996 deletion completed in 9.380458697s


• [SLOW TEST:260.284 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:31.156: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:07:31.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:07:33.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:07:35.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:07:37.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:07:39.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:07:41.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783651, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:07:44.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:07:44.863: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7675-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:45.593: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2426" for this suite.
Mar  2 22:07:51.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:07:55.087: INFO: namespace webhook-2426 deletion completed in 9.3610029s
STEP: Destroying namespace "webhook-2426-markers" for this suite.
Mar  2 22:08:01.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:04.441: INFO: namespace webhook-2426-markers deletion completed in 9.354132134s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] HostPath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:04.504: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Mar  2 22:08:04.616: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8197" to be "success or failure"
Mar  2 22:08:04.630: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 13.531622ms
Mar  2 22:08:06.643: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026512757s
Mar  2 22:08:08.657: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040691145s
Mar  2 22:08:10.671: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05482817s
Mar  2 22:08:12.685: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068330361s
Mar  2 22:08:14.699: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.082428292s
STEP: Saw pod success
Mar  2 22:08:14.699: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  2 22:08:14.713: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  2 22:08:14.749: INFO: Waiting for pod pod-host-path-test to disappear
Mar  2 22:08:14.761: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:08:14.762: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "hostpath-8197" for this suite.
Mar  2 22:08:20.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:24.189: INFO: namespace hostpath-8197 deletion completed in 9.36004954s


• [SLOW TEST:19.685 seconds]
[sig-storage] HostPath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:24.241: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-89cd4d3b-f927-4cae-8579-7e9ef5346bd3
STEP: Creating a pod to test consume secrets
Mar  2 22:08:24.497: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273" in namespace "projected-4646" to be "success or failure"
Mar  2 22:08:24.510: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Pending", Reason="", readiness=false. Elapsed: 12.410847ms
Mar  2 22:08:26.523: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025891956s
Mar  2 22:08:28.542: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044346316s
Mar  2 22:08:30.555: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058144792s
Mar  2 22:08:32.569: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Pending", Reason="", readiness=false. Elapsed: 8.071477544s
Mar  2 22:08:34.582: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.085027179s
STEP: Saw pod success
Mar  2 22:08:34.582: INFO: Pod "pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273" satisfied condition "success or failure"
Mar  2 22:08:34.596: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:08:34.633: INFO: Waiting for pod pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273 to disappear
Mar  2 22:08:34.646: INFO: Pod pod-projected-secrets-815b6b6e-5e3d-446a-a2da-6676e2d13273 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:08:34.646: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4646" for this suite.
Mar  2 22:08:40.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:44.058: INFO: namespace projected-4646 deletion completed in 9.354631819s


• [SLOW TEST:19.817 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:07:44.197: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 22:07:56.551: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:56.551: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:56.788: INFO: Exec stderr: ""
Mar  2 22:07:56.788: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:56.788: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:56.975: INFO: Exec stderr: ""
Mar  2 22:07:56.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:56.975: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.126: INFO: Exec stderr: ""
Mar  2 22:07:57.126: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.127: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.311: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 22:07:57.311: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.311: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.456: INFO: Exec stderr: ""
Mar  2 22:07:57.456: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.456: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.639: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 22:07:57.640: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.640: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.786: INFO: Exec stderr: ""
Mar  2 22:07:57.786: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.786: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:57.982: INFO: Exec stderr: ""
Mar  2 22:07:57.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:57.982: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:58.169: INFO: Exec stderr: ""
Mar  2 22:07:58.169: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  2 22:07:58.169: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:07:58.358: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:07:58.358: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2692" for this suite.
Mar  2 22:08:44.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:47.811: INFO: namespace e2e-kubelet-etc-hosts-2692 deletion completed in 49.357185364s


• [SLOW TEST:63.615 seconds]
[k8s.io] KubeletManagedEtcHosts
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:02.991: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:08:11.284: INFO: Waiting up to 5m0s for pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2" in namespace "pods-8278" to be "success or failure"
Mar  2 22:08:11.303: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.300222ms
Mar  2 22:08:13.324: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040338704s
Mar  2 22:08:15.345: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061205433s
Mar  2 22:08:17.366: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081583636s
Mar  2 22:08:19.386: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.102242902s
Mar  2 22:08:21.408: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.124323212s
STEP: Saw pod success
Mar  2 22:08:21.408: INFO: Pod "client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2" satisfied condition "success or failure"
Mar  2 22:08:21.428: INFO: Trying to get logs from node ip-10-0-141-17.ec2.internal pod client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2 container env3cont: <nil>
STEP: delete the pod
Mar  2 22:08:21.495: INFO: Waiting for pod client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2 to disappear
Mar  2 22:08:21.515: INFO: Pod client-envvars-e81a87cd-540a-4de3-bd32-1f76aad792c2 no longer exists
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:08:21.515: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8278" for this suite.
Mar  2 22:08:49.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:08:52.986: INFO: namespace pods-8278 deletion completed in 31.381053773s


• [SLOW TEST:49.995 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:44.107: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-79138aae-92e7-4164-a947-188cbf668ff5
STEP: Creating a pod to test consume secrets
Mar  2 22:08:44.383: INFO: Waiting up to 5m0s for pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c" in namespace "secrets-7205" to be "success or failure"
Mar  2 22:08:44.397: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.358541ms
Mar  2 22:08:46.410: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027047172s
Mar  2 22:08:48.425: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041458538s
Mar  2 22:08:50.439: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055617589s
Mar  2 22:08:52.452: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068956468s
STEP: Saw pod success
Mar  2 22:08:52.452: INFO: Pod "pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c" satisfied condition "success or failure"
Mar  2 22:08:52.466: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:08:52.502: INFO: Waiting for pod pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c to disappear
Mar  2 22:08:52.514: INFO: Pod pod-secrets-adf50c28-b123-4b6a-9e14-d9503179410c no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:08:52.515: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-7205" for this suite.
Mar  2 22:08:58.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:01.926: INFO: namespace secrets-7205 deletion completed in 9.354090761s
STEP: Destroying namespace "secret-namespace-2178" for this suite.
Mar  2 22:09:07.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:11.281: INFO: namespace secret-namespace-2178 deletion completed in 9.354937528s


• [SLOW TEST:27.174 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:47.821: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar  2 22:08:58.031: INFO: Asynchronously running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  2 22:09:03.255: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:03.269: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1983" for this suite.
Mar  2 22:09:09.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:12.675: INFO: namespace pods-1983 deletion completed in 9.351808793s


• [SLOW TEST:24.854 seconds]
[k8s.io] [sig-node] Pods Extended
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:08:53.006: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:04.328: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-1178" for this suite.
Mar  2 22:09:10.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:13.798: INFO: namespace resourcequota-1178 deletion completed in 9.380499946s


• [SLOW TEST:20.792 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:06:34.508: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-0d8132da-3783-435a-8e2c-58be61956177 in namespace container-probe-8910
Mar  2 22:06:44.759: INFO: Started pod liveness-0d8132da-3783-435a-8e2c-58be61956177 in namespace container-probe-8910
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:06:44.778: INFO: Initial restart count of pod liveness-0d8132da-3783-435a-8e2c-58be61956177 is 0
Mar  2 22:06:58.934: INFO: Restart count of pod container-probe-8910/liveness-0d8132da-3783-435a-8e2c-58be61956177 is now 1 (14.15626262s elapsed)
Mar  2 22:07:17.112: INFO: Restart count of pod container-probe-8910/liveness-0d8132da-3783-435a-8e2c-58be61956177 is now 2 (32.334690863s elapsed)
Mar  2 22:07:37.315: INFO: Restart count of pod container-probe-8910/liveness-0d8132da-3783-435a-8e2c-58be61956177 is now 3 (52.537579751s elapsed)
Mar  2 22:07:57.514: INFO: Restart count of pod container-probe-8910/liveness-0d8132da-3783-435a-8e2c-58be61956177 is now 4 (1m12.73597742s elapsed)
Mar  2 22:09:08.218: INFO: Restart count of pod container-probe-8910/liveness-0d8132da-3783-435a-8e2c-58be61956177 is now 5 (2m23.440809017s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:08.246: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-8910" for this suite.
Mar  2 22:09:14.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:17.728: INFO: namespace container-probe-8910 deletion completed in 9.378279027s


• [SLOW TEST:163.220 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:13.803: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:09:14.016: INFO: (0) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 27.358474ms)
Mar  2 22:09:14.039: INFO: (1) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.022332ms)
Mar  2 22:09:14.062: INFO: (2) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 23.354727ms)
Mar  2 22:09:14.084: INFO: (3) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.851587ms)
Mar  2 22:09:14.105: INFO: (4) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.460061ms)
Mar  2 22:09:14.126: INFO: (5) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.821773ms)
Mar  2 22:09:14.148: INFO: (6) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.308452ms)
Mar  2 22:09:14.169: INFO: (7) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.881318ms)
Mar  2 22:09:14.196: INFO: (8) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 27.094878ms)
Mar  2 22:09:14.218: INFO: (9) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.50036ms)
Mar  2 22:09:14.238: INFO: (10) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.332747ms)
Mar  2 22:09:14.258: INFO: (11) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.113468ms)
Mar  2 22:09:14.279: INFO: (12) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.615893ms)
Mar  2 22:09:14.300: INFO: (13) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.597012ms)
Mar  2 22:09:14.320: INFO: (14) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.207784ms)
Mar  2 22:09:14.341: INFO: (15) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.924305ms)
Mar  2 22:09:14.362: INFO: (16) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.51547ms)
Mar  2 22:09:14.382: INFO: (17) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.082437ms)
Mar  2 22:09:14.403: INFO: (18) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.529475ms)
Mar  2 22:09:14.423: INFO: (19) /api/v1/nodes/ip-10-0-130-224.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.375205ms)
[AfterEach] version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:14.423: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-7879" for this suite.
Mar  2 22:09:20.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:22.424: INFO: namespace proxy-7879 deletion completed in 7.9461416s


• [SLOW TEST:8.621 seconds]
[sig-network] Proxy
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:12.686: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Mar  2 22:09:12.830: INFO: Waiting up to 5m0s for pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b" in namespace "containers-8862" to be "success or failure"
Mar  2 22:09:12.842: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.943311ms
Mar  2 22:09:14.856: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025136311s
Mar  2 22:09:16.869: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038109178s
Mar  2 22:09:18.882: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051536266s
Mar  2 22:09:20.896: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065570059s
Mar  2 22:09:22.909: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.078792616s
STEP: Saw pod success
Mar  2 22:09:22.909: INFO: Pod "client-containers-f98650b1-7d25-405b-addc-77068049e69b" satisfied condition "success or failure"
Mar  2 22:09:22.922: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod client-containers-f98650b1-7d25-405b-addc-77068049e69b container test-container: <nil>
STEP: delete the pod
Mar  2 22:09:22.962: INFO: Waiting for pod client-containers-f98650b1-7d25-405b-addc-77068049e69b to disappear
Mar  2 22:09:22.974: INFO: Pod client-containers-f98650b1-7d25-405b-addc-77068049e69b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:22.974: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-8862" for this suite.
Mar  2 22:09:29.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:32.381: INFO: namespace containers-8862 deletion completed in 9.35236837s


• [SLOW TEST:19.695 seconds]
[k8s.io] Docker Containers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:17.753: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:34.236: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-6560" for this suite.
Mar  2 22:09:40.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:43.721: INFO: namespace resourcequota-6560 deletion completed in 9.379759319s


• [SLOW TEST:25.969 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:32.391: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Mar  2 22:09:32.501: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-2447'
Mar  2 22:09:33.360: INFO: stderr: ""
Mar  2 22:09:33.360: INFO: stdout: "pod/pause created\n"
Mar  2 22:09:33.360: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 22:09:33.360: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2447" to be "running and ready"
Mar  2 22:09:33.373: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.334714ms
Mar  2 22:09:35.386: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025423034s
Mar  2 22:09:37.399: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038554455s
Mar  2 22:09:39.412: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0518082s
Mar  2 22:09:41.426: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065295024s
Mar  2 22:09:43.439: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.078248568s
Mar  2 22:09:43.439: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 22:09:43.439: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  2 22:09:43.439: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig label pods pause testing-label=testing-label-value --namespace=kubectl-2447'
Mar  2 22:09:43.580: INFO: stderr: ""
Mar  2 22:09:43.580: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 22:09:43.581: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pod pause -L testing-label --namespace=kubectl-2447'
Mar  2 22:09:43.702: INFO: stderr: ""
Mar  2 22:09:43.702: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  2 22:09:43.702: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig label pods pause testing-label- --namespace=kubectl-2447'
Mar  2 22:09:43.865: INFO: stderr: ""
Mar  2 22:09:43.865: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  2 22:09:43.865: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pod pause -L testing-label --namespace=kubectl-2447'
Mar  2 22:09:44.007: INFO: stderr: ""
Mar  2 22:09:44.007: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] Kubectl label
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Mar  2 22:09:44.007: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-2447'
Mar  2 22:09:44.145: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:09:44.145: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 22:09:44.145: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get rc,svc -l name=pause --no-headers --namespace=kubectl-2447'
Mar  2 22:09:44.316: INFO: stderr: "No resources found in kubectl-2447 namespace.\n"
Mar  2 22:09:44.316: INFO: stdout: ""
Mar  2 22:09:44.316: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -l name=pause --namespace=kubectl-2447 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 22:09:44.435: INFO: stderr: ""
Mar  2 22:09:44.435: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:44.435: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2447" for this suite.
Mar  2 22:09:50.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:53.858: INFO: namespace kubectl-2447 deletion completed in 9.358744086s


• [SLOW TEST:21.467 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:11.301: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar  2 22:09:11.403: INFO: namespace kubectl-5930
Mar  2 22:09:11.403: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-5930'
Mar  2 22:09:12.229: INFO: stderr: ""
Mar  2 22:09:12.229: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  2 22:09:13.242: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:13.243: INFO: Found 0 / 1
Mar  2 22:09:14.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:14.243: INFO: Found 0 / 1
Mar  2 22:09:15.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:15.243: INFO: Found 0 / 1
Mar  2 22:09:16.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:16.244: INFO: Found 0 / 1
Mar  2 22:09:17.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:17.243: INFO: Found 0 / 1
Mar  2 22:09:18.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:18.243: INFO: Found 0 / 1
Mar  2 22:09:19.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:19.243: INFO: Found 0 / 1
Mar  2 22:09:20.243: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:20.243: INFO: Found 0 / 1
Mar  2 22:09:21.242: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:21.243: INFO: Found 1 / 1
Mar  2 22:09:21.243: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 22:09:21.257: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:09:21.257: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:09:21.257: INFO: wait on redis-master startup in kubectl-5930 
Mar  2 22:09:21.257: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs redis-master-hmq24 redis-master --namespace=kubectl-5930'
Mar  2 22:09:21.398: INFO: stderr: ""
Mar  2 22:09:21.398: INFO: stdout: "1:C 02 Mar 2020 22:09:19.170 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 02 Mar 2020 22:09:19.170 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 02 Mar 2020 22:09:19.170 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 02 Mar 2020 22:09:19.172 * Running mode=standalone, port=6379.\n1:M 02 Mar 2020 22:09:19.172 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Mar 2020 22:09:19.172 # Server initialized\n1:M 02 Mar 2020 22:09:19.172 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Mar 2020 22:09:19.172 * Ready to accept connections\n"
STEP: exposing RC
Mar  2 22:09:21.398: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5930'
Mar  2 22:09:21.575: INFO: stderr: ""
Mar  2 22:09:21.575: INFO: stdout: "service/rm2 exposed\n"
Mar  2 22:09:21.588: INFO: Service rm2 in namespace kubectl-5930 found.
STEP: exposing service
Mar  2 22:09:23.615: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5930'
Mar  2 22:09:23.797: INFO: stderr: ""
Mar  2 22:09:23.797: INFO: stdout: "service/rm3 exposed\n"
Mar  2 22:09:23.810: INFO: Service rm3 in namespace kubectl-5930 found.
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:25.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5930" for this suite.
Mar  2 22:09:55.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:09:59.261: INFO: namespace kubectl-5930 deletion completed in 33.356512337s


• [SLOW TEST:47.961 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:43.742: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-3cf9015d-b409-42d3-a961-ef55e9440aa0
STEP: Creating a pod to test consume secrets
Mar  2 22:09:43.901: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775" in namespace "projected-2326" to be "success or failure"
Mar  2 22:09:43.921: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Pending", Reason="", readiness=false. Elapsed: 19.799991ms
Mar  2 22:09:45.942: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040821654s
Mar  2 22:09:47.962: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061194018s
Mar  2 22:09:49.983: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082561398s
Mar  2 22:09:52.004: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103183279s
Mar  2 22:09:54.025: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.124135423s
STEP: Saw pod success
Mar  2 22:09:54.025: INFO: Pod "pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775" satisfied condition "success or failure"
Mar  2 22:09:54.045: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:09:54.102: INFO: Waiting for pod pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775 to disappear
Mar  2 22:09:54.123: INFO: Pod pod-projected-secrets-f1f765bb-fdb5-4110-a0c8-2a42e2e72775 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:54.123: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2326" for this suite.
Mar  2 22:10:00.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:03.631: INFO: namespace projected-2326 deletion completed in 9.404548806s


• [SLOW TEST:19.890 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:22.440: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:09:22.579: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:09:30.778: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8063" for this suite.
Mar  2 22:10:16.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:20.252: INFO: namespace pods-8063 deletion completed in 49.384288524s


• [SLOW TEST:57.812 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:03.657: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:10:03.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30" in namespace "downward-api-1767" to be "success or failure"
Mar  2 22:10:03.877: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30": Phase="Pending", Reason="", readiness=false. Elapsed: 22.489921ms
Mar  2 22:10:05.897: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04220724s
Mar  2 22:10:07.918: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063332552s
Mar  2 22:10:09.938: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083359156s
Mar  2 22:10:11.959: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.104355183s
STEP: Saw pod success
Mar  2 22:10:11.960: INFO: Pod "downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30" satisfied condition "success or failure"
Mar  2 22:10:11.978: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30 container client-container: <nil>
STEP: delete the pod
Mar  2 22:10:12.031: INFO: Waiting for pod downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30 to disappear
Mar  2 22:10:12.050: INFO: Pod downwardapi-volume-01ec13bb-bda8-4caa-9e75-909e1d431c30 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:12.050: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1767" for this suite.
Mar  2 22:10:18.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:21.533: INFO: namespace downward-api-1767 deletion completed in 9.378823572s


• [SLOW TEST:17.877 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] Aggregator
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:59.297: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar  2 22:09:59.414: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Mar  2 22:10:00.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:02.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:04.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:06.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:08.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783799, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:10.628: INFO: Waited 146.478119ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:13.164: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-3165" for this suite.
Mar  2 22:10:19.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:22.657: INFO: namespace aggregator-3165 deletion completed in 9.362542038s


• [SLOW TEST:23.360 seconds]
[sig-api-machinery] Aggregator
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:21.535: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:29.847: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-8880" for this suite.
Mar  2 22:10:36.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:40.112: INFO: namespace kubelet-test-8880 deletion completed in 10.057805137s


• [SLOW TEST:18.577 seconds]
[k8s.io] Kubelet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:20.281: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Mar  2 22:10:20.422: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-1491 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 22:10:20.556: INFO: stderr: ""
Mar  2 22:10:20.556: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Mar  2 22:10:20.556: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 22:10:20.556: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1491" to be "running and ready, or succeeded"
Mar  2 22:10:20.575: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.841781ms
Mar  2 22:10:22.595: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038618562s
Mar  2 22:10:24.657: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100539247s
Mar  2 22:10:26.677: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.120604658s
Mar  2 22:10:28.698: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.141952333s
Mar  2 22:10:30.719: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 10.162372512s
Mar  2 22:10:30.719: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 22:10:30.719: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  2 22:10:30.719: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491'
Mar  2 22:10:31.193: INFO: stderr: ""
Mar  2 22:10:31.193: INFO: stdout: "I0302 22:10:27.706094       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/lwl 440\nI0302 22:10:27.906190       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/lgz4 564\nI0302 22:10:28.106216       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mx7 251\nI0302 22:10:28.306269       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/l4gk 578\nI0302 22:10:28.506216       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/ldg 376\nI0302 22:10:28.706205       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/s2vt 484\nI0302 22:10:28.906202       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/t7cn 563\nI0302 22:10:29.106245       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/qzb6 330\nI0302 22:10:29.306217       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/mtfj 277\nI0302 22:10:29.506232       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/j4l 248\nI0302 22:10:29.706213       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/btq 450\nI0302 22:10:29.906201       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/mcl 527\nI0302 22:10:30.106233       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/6xp4 518\nI0302 22:10:30.306224       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/2kn 260\n"
STEP: limiting log lines
Mar  2 22:10:31.193: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491 --tail=1'
Mar  2 22:10:31.365: INFO: stderr: ""
Mar  2 22:10:31.365: INFO: stdout: "I0302 22:10:30.706209       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/42l 458\n"
STEP: limiting log bytes
Mar  2 22:10:31.365: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491 --limit-bytes=1'
Mar  2 22:10:31.504: INFO: stderr: ""
Mar  2 22:10:31.504: INFO: stdout: "I"
STEP: exposing timestamps
Mar  2 22:10:31.504: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491 --tail=1 --timestamps'
Mar  2 22:10:31.678: INFO: stderr: ""
Mar  2 22:10:31.678: INFO: stdout: "2020-03-02T22:10:31.106258312Z I0302 22:10:31.106211       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/hqb 243\n"
STEP: restricting to a time range
Mar  2 22:10:34.179: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491 --since=1s'
Mar  2 22:10:34.354: INFO: stderr: ""
Mar  2 22:10:34.354: INFO: stdout: "I0302 22:10:32.906207       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/kzp 474\nI0302 22:10:33.106226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/v4x 510\nI0302 22:10:33.306226       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/g6c 440\nI0302 22:10:33.506228       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/gcm 378\nI0302 22:10:33.706219       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/4rnt 527\n"
Mar  2 22:10:34.354: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig logs logs-generator logs-generator --namespace=kubectl-1491 --since=24h'
Mar  2 22:10:34.523: INFO: stderr: ""
Mar  2 22:10:34.523: INFO: stdout: "I0302 22:10:27.706094       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/lwl 440\nI0302 22:10:27.906190       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/lgz4 564\nI0302 22:10:28.106216       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/mx7 251\nI0302 22:10:28.306269       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/l4gk 578\nI0302 22:10:28.506216       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/ldg 376\nI0302 22:10:28.706205       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/s2vt 484\nI0302 22:10:28.906202       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/t7cn 563\nI0302 22:10:29.106245       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/qzb6 330\nI0302 22:10:29.306217       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/mtfj 277\nI0302 22:10:29.506232       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/j4l 248\nI0302 22:10:29.706213       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/btq 450\nI0302 22:10:29.906201       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/mcl 527\nI0302 22:10:30.106233       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/6xp4 518\nI0302 22:10:30.306224       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/2kn 260\nI0302 22:10:30.506226       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/tqdb 471\nI0302 22:10:30.706209       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/42l 458\nI0302 22:10:30.906207       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/sc7b 277\nI0302 22:10:31.106211       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/hqb 243\nI0302 22:10:31.308412       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/hf7m 318\nI0302 22:10:31.506226       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2hd 570\nI0302 22:10:31.706215       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/tb5 539\nI0302 22:10:31.906209       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/qdzj 311\nI0302 22:10:32.106221       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/s85 503\nI0302 22:10:32.306219       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/cmdk 528\nI0302 22:10:32.506220       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/5ln 359\nI0302 22:10:32.706211       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/6f2z 373\nI0302 22:10:32.906207       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/kzp 474\nI0302 22:10:33.106226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/v4x 510\nI0302 22:10:33.306226       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/g6c 440\nI0302 22:10:33.506228       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/gcm 378\nI0302 22:10:33.706219       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/4rnt 527\nI0302 22:10:33.906208       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/s8nv 493\n"
[AfterEach] Kubectl logs
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Mar  2 22:10:34.524: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete pod logs-generator --namespace=kubectl-1491'
Mar  2 22:10:45.796: INFO: stderr: ""
Mar  2 22:10:45.796: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:45.796: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1491" for this suite.
Mar  2 22:10:52.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:55.386: INFO: namespace kubectl-1491 deletion completed in 9.420479673s


• [SLOW TEST:35.105 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:09:53.901: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:09:54.577: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:09:56.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783793, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:09:58.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783793, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:00.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783793, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:10:02.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783794, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783793, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:10:05.873: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  2 22:10:15.982: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig attach --namespace=webhook-326 to-be-attached-pod -i -c=container1'
Mar  2 22:10:16.223: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:16.239: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-326" for this suite.
Mar  2 22:10:44.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:47.717: INFO: namespace webhook-326 deletion completed in 31.352950803s
STEP: Destroying namespace "webhook-326-markers" for this suite.
Mar  2 22:10:53.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:57.070: INFO: namespace webhook-326-markers deletion completed in 9.352500497s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:63.222 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:40.115: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 22:10:49.580: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:49.627: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-1005" for this suite.
Mar  2 22:10:55.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:10:59.209: INFO: namespace container-runtime-1005 deletion completed in 9.379179134s


• [SLOW TEST:19.094 seconds]
[k8s.io] Container Runtime
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:55.391: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 22:10:55.672: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-755 /api/v1/namespaces/watch-755/configmaps/e2e-watch-test-resource-version 6c014a6d-73c9-48e3-8e71-c12bb2438ac9 15446618 0 2020-03-02 22:10:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 22:10:55.672: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-755 /api/v1/namespaces/watch-755/configmaps/e2e-watch-test-resource-version 6c014a6d-73c9-48e3-8e71-c12bb2438ac9 15446619 0 2020-03-02 22:10:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:10:55.672: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-755" for this suite.
Mar  2 22:11:01.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:05.191: INFO: namespace watch-755 deletion completed in 9.38148469s


• [SLOW TEST:9.800 seconds]
[sig-api-machinery] Watchers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:59.216: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-5d7e45ef-f562-404b-ada5-726da9873a7f
STEP: Creating a pod to test consume configMaps
Mar  2 22:10:59.372: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c" in namespace "projected-5490" to be "success or failure"
Mar  2 22:10:59.390: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.527873ms
Mar  2 22:11:01.410: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037953709s
Mar  2 22:11:03.430: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058169814s
Mar  2 22:11:05.453: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080802923s
Mar  2 22:11:07.473: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101033552s
Mar  2 22:11:09.493: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121481069s
STEP: Saw pod success
Mar  2 22:11:09.494: INFO: Pod "pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c" satisfied condition "success or failure"
Mar  2 22:11:09.513: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:11:09.563: INFO: Waiting for pod pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c to disappear
Mar  2 22:11:09.583: INFO: Pod pod-projected-configmaps-1d277251-a5fa-443e-bc10-18b21913dd2c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:09.583: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5490" for this suite.
Mar  2 22:11:17.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:21.084: INFO: namespace projected-5490 deletion completed in 11.380705337s


• [SLOW TEST:21.868 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:57.148: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:10:57.272: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 22:11:07.297: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  2 22:11:17.403: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4850 /apis/apps/v1/namespaces/deployment-4850/deployments/test-cleanup-deployment 5819a07b-0557-46aa-b512-a8aa92bcfeb0 15447072 1 2020-03-02 22:11:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002c50198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-02 22:11:06 +0000 UTC,LastTransitionTime:2020-03-02 22:11:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-03-02 22:11:15 +0000 UTC,LastTransitionTime:2020-03-02 22:11:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 22:11:17.417: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-4850 /apis/apps/v1/namespaces/deployment-4850/replicasets/test-cleanup-deployment-65db99849b fb6f5c55-a4b1-4ce2-9a1e-123e7ca84fd5 15447057 1 2020-03-02 22:11:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5819a07b-0557-46aa-b512-a8aa92bcfeb0 0xc002c505a7 0xc002c505a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002c50608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:11:17.430: INFO: Pod "test-cleanup-deployment-65db99849b-jc26x" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-jc26x test-cleanup-deployment-65db99849b- deployment-4850 /api/v1/namespaces/deployment-4850/pods/test-cleanup-deployment-65db99849b-jc26x e140d681-01b7-428a-818a-415d20729f8f 15447056 0 2020-03-02 22:11:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.52"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b fb6f5c55-a4b1-4ce2-9a1e-123e7ca84fd5 0xc002c509b7 0xc002c509b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mvkjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mvkjw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mvkjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-j6zwr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:11:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:11:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.52,StartTime:2020-03-02 22:11:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:11:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://b387147d1df6ae3a47c5e65610a75ab69b4b7a6efa2a8c724aa71b8ad04f0f36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:17.430: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-4850" for this suite.
Mar  2 22:11:25.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:28.899: INFO: namespace deployment-4850 deletion completed in 11.353990198s


• [SLOW TEST:31.751 seconds]
[sig-apps] Deployment
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:05.247: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:11:05.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:11:07.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:11:09.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:11:11.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:11:13.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718783865, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:11:16.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:11:16.953: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4600-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:17.859: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8883" for this suite.
Mar  2 22:11:26.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:29.344: INFO: namespace webhook-8883 deletion completed in 11.38068735s
STEP: Destroying namespace "webhook-8883-markers" for this suite.
Mar  2 22:11:35.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:38.726: INFO: namespace webhook-8883-markers deletion completed in 9.381363694s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.559 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:21.111: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Mar  2 22:11:21.277: INFO: Waiting up to 5m0s for pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344" in namespace "containers-7002" to be "success or failure"
Mar  2 22:11:21.299: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Pending", Reason="", readiness=false. Elapsed: 22.052126ms
Mar  2 22:11:23.321: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043780237s
Mar  2 22:11:25.341: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063723038s
Mar  2 22:11:27.360: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083337616s
Mar  2 22:11:29.380: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103495922s
Mar  2 22:11:31.401: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.123741801s
STEP: Saw pod success
Mar  2 22:11:31.401: INFO: Pod "client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344" satisfied condition "success or failure"
Mar  2 22:11:31.420: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344 container test-container: <nil>
STEP: delete the pod
Mar  2 22:11:31.468: INFO: Waiting for pod client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344 to disappear
Mar  2 22:11:31.487: INFO: Pod client-containers-69b0fbfe-34e8-4cec-b19a-88c0a51dc344 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:31.487: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-7002" for this suite.
Mar  2 22:11:37.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:40.955: INFO: namespace containers-7002 deletion completed in 9.380184213s


• [SLOW TEST:19.844 seconds]
[k8s.io] Docker Containers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:28.910: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:11:29.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406" in namespace "projected-4582" to be "success or failure"
Mar  2 22:11:29.059: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13846ms
Mar  2 22:11:31.073: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025760489s
Mar  2 22:11:33.086: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039160079s
Mar  2 22:11:35.101: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053456022s
Mar  2 22:11:37.113: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066179521s
Mar  2 22:11:39.127: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.07969189s
STEP: Saw pod success
Mar  2 22:11:39.127: INFO: Pod "downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406" satisfied condition "success or failure"
Mar  2 22:11:39.140: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406 container client-container: <nil>
STEP: delete the pod
Mar  2 22:11:39.181: INFO: Waiting for pod downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406 to disappear
Mar  2 22:11:39.193: INFO: Pod downwardapi-volume-9f3e5dd0-f3f0-4f57-a9b1-36d449240406 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:39.193: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4582" for this suite.
Mar  2 22:11:45.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:48.640: INFO: namespace projected-4582 deletion completed in 9.353957268s


• [SLOW TEST:19.731 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:48.645: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:11:48.753: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:11:48.935: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2481" for this suite.
Mar  2 22:11:55.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:11:58.453: INFO: namespace custom-resource-definition-2481 deletion completed in 9.377597565s


• [SLOW TEST:9.808 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:58.478: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b1d82bfe-a5bd-44d3-b800-84634c08954b
STEP: Creating a pod to test consume configMaps
Mar  2 22:11:58.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40" in namespace "configmap-7675" to be "success or failure"
Mar  2 22:11:58.664: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Pending", Reason="", readiness=false. Elapsed: 12.968966ms
Mar  2 22:12:00.678: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026805641s
Mar  2 22:12:02.692: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040738988s
Mar  2 22:12:04.705: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054162947s
Mar  2 22:12:06.721: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069978933s
Mar  2 22:12:08.735: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.083920386s
STEP: Saw pod success
Mar  2 22:12:08.735: INFO: Pod "pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40" satisfied condition "success or failure"
Mar  2 22:12:08.748: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:12:08.784: INFO: Waiting for pod pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40 to disappear
Mar  2 22:12:08.796: INFO: Pod pod-configmaps-2be6af78-51d5-43f6-b2df-5e1843851d40 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:08.796: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7675" for this suite.
Mar  2 22:12:14.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:12:18.277: INFO: namespace configmap-7675 deletion completed in 9.356755883s


• [SLOW TEST:19.798 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:40.971: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0302 22:12:11.335853   56391 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 22:12:11.336: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:11.336: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-9552" for this suite.
Mar  2 22:12:19.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:12:22.872: INFO: namespace gc-9552 deletion completed in 11.465380056s


• [SLOW TEST:41.901 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:11:38.830: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-cmhn
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:11:39.042: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cmhn" in namespace "subpath-3674" to be "success or failure"
Mar  2 22:11:39.061: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 19.194106ms
Mar  2 22:11:41.082: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040317136s
Mar  2 22:11:43.102: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060127585s
Mar  2 22:11:45.123: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080982408s
Mar  2 22:11:47.143: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101201238s
Mar  2 22:11:49.179: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.137313336s
Mar  2 22:11:51.199: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 12.157492818s
Mar  2 22:11:53.220: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 14.178035376s
Mar  2 22:11:55.239: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 16.197655493s
Mar  2 22:11:57.259: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 18.217031867s
Mar  2 22:11:59.280: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 20.237863137s
Mar  2 22:12:01.299: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 22.257528851s
Mar  2 22:12:03.320: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 24.278446978s
Mar  2 22:12:05.340: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 26.298420863s
Mar  2 22:12:07.360: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 28.318098915s
Mar  2 22:12:09.380: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Running", Reason="", readiness=true. Elapsed: 30.338450203s
Mar  2 22:12:11.400: INFO: Pod "pod-subpath-test-projected-cmhn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.358741772s
STEP: Saw pod success
Mar  2 22:12:11.401: INFO: Pod "pod-subpath-test-projected-cmhn" satisfied condition "success or failure"
Mar  2 22:12:11.420: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-subpath-test-projected-cmhn container test-container-subpath-projected-cmhn: <nil>
STEP: delete the pod
Mar  2 22:12:11.476: INFO: Waiting for pod pod-subpath-test-projected-cmhn to disappear
Mar  2 22:12:11.494: INFO: Pod pod-subpath-test-projected-cmhn no longer exists
STEP: Deleting pod pod-subpath-test-projected-cmhn
Mar  2 22:12:11.495: INFO: Deleting pod "pod-subpath-test-projected-cmhn" in namespace "subpath-3674"
[AfterEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:11.514: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-3674" for this suite.
Mar  2 22:12:19.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:12:23.056: INFO: namespace subpath-3674 deletion completed in 11.436665892s


• [SLOW TEST:44.227 seconds]
[sig-storage] Subpath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:12:22.892: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6a4a2b11-be5a-4d9c-82b8-94e3c8de6278
STEP: Creating a pod to test consume secrets
Mar  2 22:12:23.079: INFO: Waiting up to 5m0s for pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58" in namespace "secrets-3222" to be "success or failure"
Mar  2 22:12:23.100: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Pending", Reason="", readiness=false. Elapsed: 20.67395ms
Mar  2 22:12:25.121: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041134331s
Mar  2 22:12:27.141: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061666213s
Mar  2 22:12:29.161: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081723219s
Mar  2 22:12:31.182: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.102423309s
Mar  2 22:12:33.202: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.122716823s
STEP: Saw pod success
Mar  2 22:12:33.202: INFO: Pod "pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58" satisfied condition "success or failure"
Mar  2 22:12:33.223: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:12:33.272: INFO: Waiting for pod pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58 to disappear
Mar  2 22:12:33.291: INFO: Pod pod-secrets-e6a95c87-bc06-4af1-bcf3-b625b6867f58 no longer exists
[AfterEach] [sig-storage] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:33.292: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3222" for this suite.
Mar  2 22:12:39.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:12:42.777: INFO: namespace secrets-3222 deletion completed in 9.380250558s


• [SLOW TEST:19.885 seconds]
[sig-storage] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:12:23.072: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar  2 22:12:23.227: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-1764'
Mar  2 22:12:24.093: INFO: stderr: ""
Mar  2 22:12:24.093: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 22:12:24.093: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1764'
Mar  2 22:12:24.243: INFO: stderr: ""
Mar  2 22:12:24.243: INFO: stdout: "update-demo-nautilus-r2pvx update-demo-nautilus-vp2kp "
Mar  2 22:12:24.243: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-r2pvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:24.364: INFO: stderr: ""
Mar  2 22:12:24.364: INFO: stdout: ""
Mar  2 22:12:24.364: INFO: update-demo-nautilus-r2pvx is created but not running
Mar  2 22:12:29.365: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1764'
Mar  2 22:12:29.491: INFO: stderr: ""
Mar  2 22:12:29.491: INFO: stdout: "update-demo-nautilus-r2pvx update-demo-nautilus-vp2kp "
Mar  2 22:12:29.491: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-r2pvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:30.703: INFO: stderr: ""
Mar  2 22:12:30.703: INFO: stdout: ""
Mar  2 22:12:30.703: INFO: update-demo-nautilus-r2pvx is created but not running
Mar  2 22:12:35.703: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1764'
Mar  2 22:12:35.968: INFO: stderr: ""
Mar  2 22:12:35.968: INFO: stdout: "update-demo-nautilus-r2pvx update-demo-nautilus-vp2kp "
Mar  2 22:12:35.968: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-r2pvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:36.086: INFO: stderr: ""
Mar  2 22:12:36.086: INFO: stdout: "true"
Mar  2 22:12:36.086: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-r2pvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:36.205: INFO: stderr: ""
Mar  2 22:12:36.205: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:12:36.205: INFO: validating pod update-demo-nautilus-r2pvx
Mar  2 22:12:36.228: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:12:36.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:12:36.228: INFO: update-demo-nautilus-r2pvx is verified up and running
Mar  2 22:12:36.228: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-vp2kp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:36.373: INFO: stderr: ""
Mar  2 22:12:36.373: INFO: stdout: "true"
Mar  2 22:12:36.373: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods update-demo-nautilus-vp2kp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1764'
Mar  2 22:12:36.493: INFO: stderr: ""
Mar  2 22:12:36.493: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 22:12:36.493: INFO: validating pod update-demo-nautilus-vp2kp
Mar  2 22:12:36.515: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 22:12:36.515: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 22:12:36.515: INFO: update-demo-nautilus-vp2kp is verified up and running
STEP: using delete to clean up resources
Mar  2 22:12:36.516: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-1764'
Mar  2 22:12:36.704: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 22:12:36.704: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 22:12:36.705: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1764'
Mar  2 22:12:36.873: INFO: stderr: "No resources found in kubectl-1764 namespace.\n"
Mar  2 22:12:36.873: INFO: stdout: ""
Mar  2 22:12:36.873: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pods -l name=update-demo --namespace=kubectl-1764 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 22:12:36.999: INFO: stderr: ""
Mar  2 22:12:36.999: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:37.000: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1764" for this suite.
Mar  2 22:12:43.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:12:46.492: INFO: namespace kubectl-1764 deletion completed in 9.388582039s


• [SLOW TEST:23.420 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:12:42.789: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:12:42.958: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0" in namespace "projected-5928" to be "success or failure"
Mar  2 22:12:42.978: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.499921ms
Mar  2 22:12:44.998: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039190904s
Mar  2 22:12:47.017: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05899446s
Mar  2 22:12:49.038: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079662219s
Mar  2 22:12:51.058: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099983316s
Mar  2 22:12:53.078: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119982915s
STEP: Saw pod success
Mar  2 22:12:53.079: INFO: Pod "downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0" satisfied condition "success or failure"
Mar  2 22:12:53.097: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0 container client-container: <nil>
STEP: delete the pod
Mar  2 22:12:53.145: INFO: Waiting for pod downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0 to disappear
Mar  2 22:12:53.165: INFO: Pod downwardapi-volume-63db34f5-3637-490e-b559-02573454d1f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:53.165: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5928" for this suite.
Mar  2 22:12:59.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:02.722: INFO: namespace projected-5928 deletion completed in 9.402481678s


• [SLOW TEST:19.934 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:12:46.530: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-3987/configmap-test-b4e40dce-bde5-4d03-8f2c-bd12a54a4aa6
STEP: Creating a pod to test consume configMaps
Mar  2 22:12:46.735: INFO: Waiting up to 5m0s for pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa" in namespace "configmap-3987" to be "success or failure"
Mar  2 22:12:46.756: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Pending", Reason="", readiness=false. Elapsed: 21.401342ms
Mar  2 22:12:48.776: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041556158s
Mar  2 22:12:50.796: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061595628s
Mar  2 22:12:52.816: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081680776s
Mar  2 22:12:54.836: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10146433s
Mar  2 22:12:56.856: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121368458s
STEP: Saw pod success
Mar  2 22:12:56.856: INFO: Pod "pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa" satisfied condition "success or failure"
Mar  2 22:12:56.875: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa container env-test: <nil>
STEP: delete the pod
Mar  2 22:12:56.925: INFO: Waiting for pod pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa to disappear
Mar  2 22:12:56.944: INFO: Pod pod-configmaps-a36e79b4-caaf-47ee-a9b2-f9e507394baa no longer exists
[AfterEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:56.944: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3987" for this suite.
Mar  2 22:13:03.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:06.428: INFO: namespace configmap-3987 deletion completed in 9.380337137s


• [SLOW TEST:19.898 seconds]
[sig-node] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:12:18.283: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0302 22:12:58.608551   56390 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 22:12:58.608: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:12:58.608: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-743" for this suite.
Mar  2 22:13:04.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:08.016: INFO: namespace gc-743 deletion completed in 9.353016099s


• [SLOW TEST:49.733 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:02.736: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 22:13:02.868: INFO: Waiting up to 5m0s for pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1" in namespace "emptydir-2940" to be "success or failure"
Mar  2 22:13:02.886: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.522885ms
Mar  2 22:13:04.906: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03813786s
Mar  2 22:13:06.925: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057394675s
Mar  2 22:13:08.945: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077220085s
Mar  2 22:13:10.988: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.120135285s
STEP: Saw pod success
Mar  2 22:13:10.988: INFO: Pod "pod-964169e7-745b-4b98-929f-547d83e9e9c1" satisfied condition "success or failure"
Mar  2 22:13:11.007: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-964169e7-745b-4b98-929f-547d83e9e9c1 container test-container: <nil>
STEP: delete the pod
Mar  2 22:13:11.055: INFO: Waiting for pod pod-964169e7-745b-4b98-929f-547d83e9e9c1 to disappear
Mar  2 22:13:11.074: INFO: Pod pod-964169e7-745b-4b98-929f-547d83e9e9c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:11.075: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2940" for this suite.
Mar  2 22:13:17.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:20.660: INFO: namespace emptydir-2940 deletion completed in 9.381551674s


• [SLOW TEST:17.925 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:06.433: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:13:06.658: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-195a3a5c-0a52-49ef-9444-fb894edb0503
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-195a3a5c-0a52-49ef-9444-fb894edb0503
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:16.854: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2185" for this suite.
Mar  2 22:13:29.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:32.323: INFO: namespace configmap-2185 deletion completed in 15.380176092s


• [SLOW TEST:25.891 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:20.665: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-b062b706-7080-4da4-b218-29990f8c1c69
STEP: Creating a pod to test consume secrets
Mar  2 22:13:20.836: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4" in namespace "projected-5783" to be "success or failure"
Mar  2 22:13:20.855: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.799837ms
Mar  2 22:13:22.876: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039438268s
Mar  2 22:13:24.895: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059208196s
Mar  2 22:13:26.916: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07945838s
Mar  2 22:13:28.937: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100936563s
Mar  2 22:13:30.958: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.122034363s
STEP: Saw pod success
Mar  2 22:13:30.958: INFO: Pod "pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4" satisfied condition "success or failure"
Mar  2 22:13:30.978: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:13:31.030: INFO: Waiting for pod pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4 to disappear
Mar  2 22:13:31.049: INFO: Pod pod-projected-secrets-daf3a933-4f14-4dad-865b-bd1d9d4517b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:31.049: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5783" for this suite.
Mar  2 22:13:37.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:13:40.536: INFO: namespace projected-5783 deletion completed in 9.382412499s


• [SLOW TEST:19.871 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] PreStop
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:08.053: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-7088
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7088
STEP: Deleting pre-stop pod
Mar  2 22:13:33.322: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:33.417: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-7088" for this suite.
Mar  2 22:14:07.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:10.833: INFO: namespace prestop-7088 deletion completed in 37.35246166s


• [SLOW TEST:62.780 seconds]
[k8s.io] [sig-node] PreStop
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:40.549: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  2 22:13:51.372: INFO: Successfully updated pod "labelsupdated2fd781e-d7b0-4193-9a0a-378fc05c987e"
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:53.416: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6962" for this suite.
Mar  2 22:14:07.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:10.924: INFO: namespace downward-api-6962 deletion completed in 17.38628095s


• [SLOW TEST:30.375 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:13:32.354: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:13:32.492: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-5453'
Mar  2 22:13:33.494: INFO: stderr: ""
Mar  2 22:13:33.494: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  2 22:13:33.494: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-5453'
Mar  2 22:13:34.194: INFO: stderr: ""
Mar  2 22:13:34.194: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  2 22:13:35.216: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:35.216: INFO: Found 0 / 1
Mar  2 22:13:36.214: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:36.215: INFO: Found 0 / 1
Mar  2 22:13:37.214: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:37.215: INFO: Found 0 / 1
Mar  2 22:13:38.214: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:38.214: INFO: Found 0 / 1
Mar  2 22:13:39.215: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:39.215: INFO: Found 0 / 1
Mar  2 22:13:40.214: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:40.214: INFO: Found 0 / 1
Mar  2 22:13:41.215: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:41.215: INFO: Found 0 / 1
Mar  2 22:13:42.215: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:42.215: INFO: Found 1 / 1
Mar  2 22:13:42.215: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 22:13:42.234: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:13:42.235: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:13:42.235: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig describe pod redis-master-2hn8h --namespace=kubectl-5453'
Mar  2 22:13:42.675: INFO: stderr: ""
Mar  2 22:13:42.675: INFO: stdout: "Name:         redis-master-2hn8h\nNamespace:    kubectl-5453\nPriority:     0\nNode:         ip-10-0-130-27.ec2.internal/10.0.130.27\nStart Time:   Mon, 02 Mar 2020 22:13:32 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.3.76\"\n                    ],\n                    \"dns\": {},\n                    \"default-route\": [\n                        \"10.128.2.1\"\n                    ]\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           10.128.3.76\nIPs:\n  IP:           10.128.3.76\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://ef40f72ff9bfc7a71d0d0bb58b9c7eafa2d8ee0e8ee01d2389be52469d2e7868\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 02 Mar 2020 22:13:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pjsr4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-pjsr4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-pjsr4\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                  Message\n  ----    ------     ----       ----                                  -------\n  Normal  Scheduled  <unknown>  default-scheduler                     Successfully assigned kubectl-5453/redis-master-2hn8h to ip-10-0-130-27.ec2.internal\n  Normal  Pulled     2s         kubelet, ip-10-0-130-27.ec2.internal  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, ip-10-0-130-27.ec2.internal  Created container redis-master\n  Normal  Started    1s         kubelet, ip-10-0-130-27.ec2.internal  Started container redis-master\n"
Mar  2 22:13:42.675: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig describe rc redis-master --namespace=kubectl-5453'
Mar  2 22:13:42.845: INFO: stderr: ""
Mar  2 22:13:42.845: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5453\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  10s   replication-controller  Created pod: redis-master-2hn8h\n"
Mar  2 22:13:42.845: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig describe service redis-master --namespace=kubectl-5453'
Mar  2 22:13:43.008: INFO: stderr: ""
Mar  2 22:13:43.008: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5453\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.77.251\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.128.3.76:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 22:13:43.112: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig describe node ip-10-0-130-224.ec2.internal'
Mar  2 22:13:43.332: INFO: stderr: ""
Mar  2 22:13:43.332: INFO: stdout: "Name:               ip-10-0-130-224.ec2.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-130-224\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.openshift.io/os_id=rhcos\nAnnotations:        machine.openshift.io/machine: openshift-machine-api/jeder-43-lts-fll8r-worker-us-east-1a-rpq9s\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-d85046e718b0b8dc1ee88ce5ea4cd4f5\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-d85046e718b0b8dc1ee88ce5ea4cd4f5\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 05 Feb 2020 15:35:53 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 02 Mar 2020 22:12:53 +0000   Mon, 10 Feb 2020 20:08:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 02 Mar 2020 22:12:53 +0000   Mon, 10 Feb 2020 20:08:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 02 Mar 2020 22:12:53 +0000   Mon, 10 Feb 2020 20:08:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 02 Mar 2020 22:12:53 +0000   Mon, 10 Feb 2020 20:08:57 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.130.224\n  Hostname:     ip-10-0-130-224.ec2.internal\n  InternalDNS:  ip-10-0-130-224.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         4\n ephemeral-storage:           313511916Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15946308Ki\n pods:                        250\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         3\n ephemeral-storage:           288932581308\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      14795332Ki\n pods:                        250\nSystem Info:\n Machine ID:                              ec2db7a30b674b824730ebef66791ffe\n System UUID:                             ec2db7a3-0b67-4b82-4730-ebef66791ffe\n Boot ID:                                 d06b1a7a-bf35-415a-bff2-c3e0f24a0d7e\n Kernel Version:                          4.18.0-147.3.1.el8_1.x86_64\n OS Image:                                Red Hat Enterprise Linux CoreOS 43.81.202001142154.0 (Ootpa)\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.2-6.dev.rhaos4.3.git9e3db66.el8\n Kubelet Version:                         v1.16.2\n Kube-Proxy Version:                      v1.16.2\nProviderID:                               aws:///us-east-1a/i-09b5a6571322eed7a\nNon-terminated Pods:                      (15 in total)\n  Namespace                               Name                                            CPU Requests  CPU Limits  Memory Requests   Memory Limits     AGE\n  ---------                               ----                                            ------------  ----------  ---------------   -------------     ---\n  openshift-cluster-node-tuning-operator  tuned-cvjkf                                     10m (0%)      0 (0%)      50Mi (0%)         0 (0%)            21d\n  openshift-dns                           dns-default-78jzw                               110m (3%)     0 (0%)      70Mi (0%)         512Mi (3%)        21d\n  openshift-image-registry                node-ca-5lj4v                                   10m (0%)      0 (0%)      10Mi (0%)         0 (0%)            21d\n  openshift-logging                       elasticsearch-cdm-85dgpaaj-2-584fd5c5f-lfzq2    200m (6%)     0 (0%)      8067108864 (53%)  8067108864 (53%)  7d7h\n  openshift-logging                       fluentd-vgqp4                                   100m (3%)     0 (0%)      736Mi (5%)        736Mi (5%)        7d7h\n  openshift-logging                       kibana-85fc498c95-z56md                         200m (6%)     0 (0%)      992Mi (6%)        992Mi (6%)        7d7h\n  openshift-machine-config-operator       machine-config-daemon-xfc2n                     20m (0%)      0 (0%)      50Mi (0%)         0 (0%)            21d\n  openshift-monitoring                    node-exporter-s8c2n                             10m (0%)      0 (0%)      20Mi (0%)         0 (0%)            21d\n  openshift-monitoring                    sre-dns-latency-exporter-sds5m                  0 (0%)        0 (0%)      0 (0%)            0 (0%)            26d\n  openshift-multus                        multus-c92b7                                    10m (0%)      0 (0%)      150Mi (1%)        0 (0%)            21d\n  openshift-rbac-permissions              rbac-permissions-operator-7ddb7b95c-tf5qr       0 (0%)        0 (0%)      0 (0%)            0 (0%)            21d\n  openshift-sdn                           ovs-m9222                                       200m (6%)     0 (0%)      400Mi (2%)        0 (0%)            21d\n  openshift-sdn                           sdn-qrnvj                                       100m (3%)     0 (0%)      200Mi (1%)        0 (0%)            21d\n  openshift-security                      splunkforwarder-ds-2j9mv                        0 (0%)        0 (0%)      0 (0%)            0 (0%)            7d3h\n  openshift-validation-webhook            validation-webhook-6f8dcdd9fb-94rfx             0 (0%)        0 (0%)      0 (0%)            0 (0%)            21d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests          Limits\n  --------                    --------          ------\n  cpu                         970m (32%)        0 (0%)\n  memory                      10620308Ki (71%)  10171796Ki (68%)\n  ephemeral-storage           0 (0%)            0 (0%)\n  attachable-volumes-aws-ebs  0                 0\nEvents:                       <none>\n"
Mar  2 22:13:43.332: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig describe namespace kubectl-5453'
Mar  2 22:13:43.495: INFO: stderr: ""
Mar  2 22:13:43.495: INFO: stdout: "Name:         kubectl-5453\nLabels:       e2e-framework=kubectl\n              e2e-run=e5d109aa-c7b6-4f02-8f82-ef739b61deef\nAnnotations:  openshift.io/sa.scc.mcs: s0:c83,c12\n              openshift.io/sa.scc.supplemental-groups: 1006830000/10000\n              openshift.io/sa.scc.uid-range: 1006830000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:13:43.495: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5453" for this suite.
Mar  2 22:14:11.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:15.173: INFO: namespace kubectl-5453 deletion completed in 31.573219541s


• [SLOW TEST:42.819 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:10.929: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:14:11.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb" in namespace "projected-7441" to be "success or failure"
Mar  2 22:14:11.248: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.654426ms
Mar  2 22:14:13.261: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026227689s
Mar  2 22:14:15.274: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039074637s
Mar  2 22:14:17.304: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068486207s
Mar  2 22:14:19.327: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092051658s
Mar  2 22:14:21.342: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.106541328s
STEP: Saw pod success
Mar  2 22:14:21.342: INFO: Pod "downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb" satisfied condition "success or failure"
Mar  2 22:14:21.355: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb container client-container: <nil>
STEP: delete the pod
Mar  2 22:14:21.401: INFO: Waiting for pod downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb to disappear
Mar  2 22:14:21.413: INFO: Pod downwardapi-volume-36d345de-6fd6-448b-be81-14cfe068facb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:14:21.414: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7441" for this suite.
Mar  2 22:14:27.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:30.890: INFO: namespace projected-7441 deletion completed in 9.351866837s


• [SLOW TEST:19.961 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:15.275: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0302 22:14:25.795320   56392 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  2 22:14:25.795: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:14:25.795: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-8883" for this suite.
Mar  2 22:14:31.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:35.313: INFO: namespace gc-8883 deletion completed in 9.380862735s


• [SLOW TEST:20.038 seconds]
[sig-api-machinery] Garbage collector
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:10:22.684: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-0431a5be-7be2-4c1f-993d-504ea71798f5 in namespace container-probe-3178
Mar  2 22:10:32.836: INFO: Started pod busybox-0431a5be-7be2-4c1f-993d-504ea71798f5 in namespace container-probe-3178
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 22:10:32.849: INFO: Initial restart count of pod busybox-0431a5be-7be2-4c1f-993d-504ea71798f5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:14:34.576: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-3178" for this suite.
Mar  2 22:14:40.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:44.110: INFO: namespace container-probe-3178 deletion completed in 9.455142715s


• [SLOW TEST:261.489 seconds]
[k8s.io] Probing container
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:44.230: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:14:44.478: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:14:45.066: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5279" for this suite.
Mar  2 22:14:53.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:14:56.529: INFO: namespace custom-resource-definition-5279 deletion completed in 11.355793476s


• [SLOW TEST:12.316 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:35.321: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:14:35.440: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 22:14:41.359: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-8590 create -f -'
Mar  2 22:14:46.759: INFO: stderr: ""
Mar  2 22:14:46.759: INFO: stdout: "e2e-test-crd-publish-openapi-9837-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 22:14:46.759: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-8590 delete e2e-test-crd-publish-openapi-9837-crds test-cr'
Mar  2 22:14:46.966: INFO: stderr: ""
Mar  2 22:14:46.966: INFO: stdout: "e2e-test-crd-publish-openapi-9837-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 22:14:46.966: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-8590 apply -f -'
Mar  2 22:14:48.403: INFO: stderr: ""
Mar  2 22:14:48.403: INFO: stdout: "e2e-test-crd-publish-openapi-9837-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 22:14:48.403: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=crd-publish-openapi-8590 delete e2e-test-crd-publish-openapi-9837-crds test-cr'
Mar  2 22:14:48.537: INFO: stderr: ""
Mar  2 22:14:48.537: INFO: stdout: "e2e-test-crd-publish-openapi-9837-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 22:14:48.537: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig explain e2e-test-crd-publish-openapi-9837-crds'
Mar  2 22:14:49.821: INFO: stderr: ""
Mar  2 22:14:49.821: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9837-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:14:55.566: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8590" for this suite.
Mar  2 22:15:01.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:05.089: INFO: namespace crd-publish-openapi-8590 deletion completed in 9.433484184s


• [SLOW TEST:29.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:56.567: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 22:14:56.723: INFO: Waiting up to 5m0s for pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299" in namespace "emptydir-235" to be "success or failure"
Mar  2 22:14:56.736: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299": Phase="Pending", Reason="", readiness=false. Elapsed: 12.648552ms
Mar  2 22:14:58.751: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027986615s
Mar  2 22:15:00.768: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045067315s
Mar  2 22:15:02.783: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059266111s
Mar  2 22:15:04.796: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.073158263s
STEP: Saw pod success
Mar  2 22:15:04.797: INFO: Pod "pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299" satisfied condition "success or failure"
Mar  2 22:15:04.810: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299 container test-container: <nil>
STEP: delete the pod
Mar  2 22:15:04.847: INFO: Waiting for pod pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299 to disappear
Mar  2 22:15:04.861: INFO: Pod pod-1477f6d5-a7f6-4e3f-a81d-eb50a97e0299 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:04.861: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-235" for this suite.
Mar  2 22:15:10.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:14.305: INFO: namespace emptydir-235 deletion completed in 9.355410696s


• [SLOW TEST:17.739 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:30.925: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:14:41.145: INFO: DNS probes using dns-test-82cb99de-d6dc-4f04-842b-8c5e558e3bea succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:14:51.284: INFO: DNS probes using dns-test-d1decb5f-07a1-47b1-8dc4-bdb7e2ede6a5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9612.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9612.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:15:05.448: INFO: DNS probes using dns-test-ac59c99e-ed83-458b-ad0d-3a2500e809f6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:05.491: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-9612" for this suite.
Mar  2 22:15:11.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:14.921: INFO: namespace dns-9612 deletion completed in 9.354266529s


• [SLOW TEST:43.996 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:14:10.978: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 22:14:11.226: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450137 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 22:14:11.226: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450137 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 22:14:21.281: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450276 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  2 22:14:21.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450276 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 22:14:31.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450413 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 22:14:31.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450413 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 22:14:41.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450601 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  2 22:14:41.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-a 3dbadff5-446c-414e-98c2-9e234c9e0fe4 15450601 0 2020-03-02 22:14:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 22:14:51.396: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-b f5ad4ef1-fa7f-473c-b95e-8747792fdeaa 15450750 0 2020-03-02 22:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 22:14:51.396: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-b f5ad4ef1-fa7f-473c-b95e-8747792fdeaa 15450750 0 2020-03-02 22:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 22:15:01.420: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-b f5ad4ef1-fa7f-473c-b95e-8747792fdeaa 15450889 0 2020-03-02 22:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  2 22:15:01.421: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5680 /api/v1/namespaces/watch-5680/configmaps/e2e-watch-test-configmap-b f5ad4ef1-fa7f-473c-b95e-8747792fdeaa 15450889 0 2020-03-02 22:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:11.422: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-5680" for this suite.
Mar  2 22:15:17.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:21.059: INFO: namespace watch-5680 deletion completed in 9.448483424s


• [SLOW TEST:70.081 seconds]
[sig-api-machinery] Watchers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:14.937: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:15.109: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9788" for this suite.
Mar  2 22:15:21.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:24.660: INFO: namespace custom-resource-definition-9788 deletion completed in 9.3723107s


• [SLOW TEST:9.723 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:05.118: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:15:05.232: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6973'
Mar  2 22:15:05.370: INFO: stderr: ""
Mar  2 22:15:05.370: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Mar  2 22:15:05.389: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-6973'
Mar  2 22:15:15.799: INFO: stderr: ""
Mar  2 22:15:15.799: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:15.799: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6973" for this suite.
Mar  2 22:15:22.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:25.941: INFO: namespace kubectl-6973 deletion completed in 9.792128016s


• [SLOW TEST:20.823 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Service endpoints latency
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:14.336: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4082
I0302 22:15:14.451698   56389 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4082, replica count: 1
I0302 22:15:15.502748   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:16.503254   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:17.503788   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:18.504292   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:19.504823   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:20.505319   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:21.505874   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:22.506359   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:15:23.506979   56389 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:15:23.627: INFO: Created: latency-svc-qzg29
Mar  2 22:15:23.632: INFO: Got endpoints: latency-svc-qzg29 [25.179883ms]
Mar  2 22:15:23.658: INFO: Created: latency-svc-fptvc
Mar  2 22:15:23.665: INFO: Created: latency-svc-nq9p5
Mar  2 22:15:23.668: INFO: Got endpoints: latency-svc-fptvc [35.44699ms]
Mar  2 22:15:23.671: INFO: Got endpoints: latency-svc-nq9p5 [38.14448ms]
Mar  2 22:15:23.676: INFO: Created: latency-svc-d7mk4
Mar  2 22:15:23.680: INFO: Got endpoints: latency-svc-d7mk4 [47.067502ms]
Mar  2 22:15:23.690: INFO: Created: latency-svc-zfk4t
Mar  2 22:15:23.690: INFO: Got endpoints: latency-svc-zfk4t [57.281111ms]
Mar  2 22:15:23.695: INFO: Created: latency-svc-wmzfm
Mar  2 22:15:23.701: INFO: Created: latency-svc-6hbvq
Mar  2 22:15:23.705: INFO: Got endpoints: latency-svc-wmzfm [71.849411ms]
Mar  2 22:15:23.706: INFO: Got endpoints: latency-svc-6hbvq [72.50327ms]
Mar  2 22:15:23.709: INFO: Created: latency-svc-z8598
Mar  2 22:15:23.715: INFO: Got endpoints: latency-svc-z8598 [82.128485ms]
Mar  2 22:15:23.715: INFO: Created: latency-svc-wmdk2
Mar  2 22:15:23.722: INFO: Got endpoints: latency-svc-wmdk2 [88.670832ms]
Mar  2 22:15:23.722: INFO: Created: latency-svc-czxr8
Mar  2 22:15:23.723: INFO: Got endpoints: latency-svc-czxr8 [89.910487ms]
Mar  2 22:15:23.726: INFO: Created: latency-svc-tvfcb
Mar  2 22:15:23.731: INFO: Got endpoints: latency-svc-tvfcb [98.164468ms]
Mar  2 22:15:23.737: INFO: Created: latency-svc-sjl7d
Mar  2 22:15:23.741: INFO: Created: latency-svc-r4p6t
Mar  2 22:15:23.742: INFO: Got endpoints: latency-svc-sjl7d [108.906533ms]
Mar  2 22:15:23.745: INFO: Got endpoints: latency-svc-r4p6t [111.920095ms]
Mar  2 22:15:23.751: INFO: Created: latency-svc-f6ww6
Mar  2 22:15:23.759: INFO: Got endpoints: latency-svc-f6ww6 [125.590437ms]
Mar  2 22:15:23.759: INFO: Created: latency-svc-k99rr
Mar  2 22:15:23.762: INFO: Created: latency-svc-dzt97
Mar  2 22:15:23.767: INFO: Got endpoints: latency-svc-dzt97 [133.86804ms]
Mar  2 22:15:23.768: INFO: Got endpoints: latency-svc-k99rr [134.794887ms]
Mar  2 22:15:23.771: INFO: Created: latency-svc-ldb9z
Mar  2 22:15:23.775: INFO: Got endpoints: latency-svc-ldb9z [106.706134ms]
Mar  2 22:15:23.778: INFO: Created: latency-svc-sskxk
Mar  2 22:15:23.781: INFO: Created: latency-svc-dx9v8
Mar  2 22:15:23.783: INFO: Got endpoints: latency-svc-sskxk [111.965049ms]
Mar  2 22:15:23.786: INFO: Got endpoints: latency-svc-dx9v8 [106.170225ms]
Mar  2 22:15:23.789: INFO: Created: latency-svc-w9qgs
Mar  2 22:15:23.793: INFO: Got endpoints: latency-svc-w9qgs [102.260571ms]
Mar  2 22:15:23.797: INFO: Created: latency-svc-2k9ls
Mar  2 22:15:23.803: INFO: Got endpoints: latency-svc-2k9ls [97.54624ms]
Mar  2 22:15:23.804: INFO: Created: latency-svc-m94x2
Mar  2 22:15:23.809: INFO: Got endpoints: latency-svc-m94x2 [102.8015ms]
Mar  2 22:15:23.809: INFO: Created: latency-svc-jzh8x
Mar  2 22:15:23.815: INFO: Created: latency-svc-bq5vp
Mar  2 22:15:23.817: INFO: Got endpoints: latency-svc-jzh8x [101.325765ms]
Mar  2 22:15:23.819: INFO: Created: latency-svc-g4pkb
Mar  2 22:15:23.823: INFO: Got endpoints: latency-svc-bq5vp [100.79368ms]
Mar  2 22:15:23.826: INFO: Created: latency-svc-nsljb
Mar  2 22:15:23.828: INFO: Got endpoints: latency-svc-g4pkb [104.410988ms]
Mar  2 22:15:23.831: INFO: Got endpoints: latency-svc-nsljb [99.293433ms]
Mar  2 22:15:23.835: INFO: Created: latency-svc-hzmpv
Mar  2 22:15:23.839: INFO: Created: latency-svc-zwd44
Mar  2 22:15:23.841: INFO: Got endpoints: latency-svc-hzmpv [98.579084ms]
Mar  2 22:15:23.842: INFO: Got endpoints: latency-svc-zwd44 [97.047879ms]
Mar  2 22:15:23.845: INFO: Created: latency-svc-kmmxb
Mar  2 22:15:23.850: INFO: Got endpoints: latency-svc-kmmxb [27.433873ms]
Mar  2 22:15:23.851: INFO: Created: latency-svc-228dv
Mar  2 22:15:23.856: INFO: Created: latency-svc-xhqkv
Mar  2 22:15:23.858: INFO: Got endpoints: latency-svc-228dv [99.215743ms]
Mar  2 22:15:23.861: INFO: Got endpoints: latency-svc-xhqkv [93.879933ms]
Mar  2 22:15:23.863: INFO: Created: latency-svc-9bvcn
Mar  2 22:15:23.869: INFO: Got endpoints: latency-svc-9bvcn [100.4916ms]
Mar  2 22:15:23.870: INFO: Created: latency-svc-d44zx
Mar  2 22:15:23.874: INFO: Got endpoints: latency-svc-d44zx [98.995378ms]
Mar  2 22:15:23.876: INFO: Created: latency-svc-hmcdr
Mar  2 22:15:23.882: INFO: Created: latency-svc-2h6k5
Mar  2 22:15:23.882: INFO: Got endpoints: latency-svc-hmcdr [98.86058ms]
Mar  2 22:15:23.891: INFO: Created: latency-svc-msgzb
Mar  2 22:15:23.894: INFO: Got endpoints: latency-svc-2h6k5 [107.428345ms]
Mar  2 22:15:23.897: INFO: Created: latency-svc-tz6jf
Mar  2 22:15:23.900: INFO: Got endpoints: latency-svc-msgzb [107.260953ms]
Mar  2 22:15:23.902: INFO: Got endpoints: latency-svc-tz6jf [99.750064ms]
Mar  2 22:15:23.904: INFO: Created: latency-svc-4598z
Mar  2 22:15:23.913: INFO: Got endpoints: latency-svc-4598z [104.127441ms]
Mar  2 22:15:23.913: INFO: Created: latency-svc-hp2ch
Mar  2 22:15:23.917: INFO: Created: latency-svc-6hdtq
Mar  2 22:15:23.920: INFO: Got endpoints: latency-svc-hp2ch [103.05674ms]
Mar  2 22:15:23.923: INFO: Got endpoints: latency-svc-6hdtq [94.806667ms]
Mar  2 22:15:23.924: INFO: Created: latency-svc-dbsgz
Mar  2 22:15:23.929: INFO: Got endpoints: latency-svc-dbsgz [98.386501ms]
Mar  2 22:15:23.931: INFO: Created: latency-svc-6765r
Mar  2 22:15:23.936: INFO: Got endpoints: latency-svc-6765r [94.678519ms]
Mar  2 22:15:23.938: INFO: Created: latency-svc-kthc5
Mar  2 22:15:23.943: INFO: Got endpoints: latency-svc-kthc5 [100.464542ms]
Mar  2 22:15:23.943: INFO: Created: latency-svc-4w842
Mar  2 22:15:23.949: INFO: Created: latency-svc-cqq29
Mar  2 22:15:23.950: INFO: Got endpoints: latency-svc-4w842 [99.151478ms]
Mar  2 22:15:23.955: INFO: Created: latency-svc-p4xcl
Mar  2 22:15:23.955: INFO: Got endpoints: latency-svc-cqq29 [97.07529ms]
Mar  2 22:15:23.963: INFO: Created: latency-svc-8b4pl
Mar  2 22:15:23.970: INFO: Got endpoints: latency-svc-p4xcl [108.853727ms]
Mar  2 22:15:23.972: INFO: Got endpoints: latency-svc-8b4pl [103.028123ms]
Mar  2 22:15:23.972: INFO: Created: latency-svc-m8s84
Mar  2 22:15:23.979: INFO: Got endpoints: latency-svc-m8s84 [105.274505ms]
Mar  2 22:15:23.980: INFO: Created: latency-svc-qrkp7
Mar  2 22:15:23.985: INFO: Got endpoints: latency-svc-qrkp7 [102.579299ms]
Mar  2 22:15:23.986: INFO: Created: latency-svc-54dd9
Mar  2 22:15:23.993: INFO: Created: latency-svc-p6xc4
Mar  2 22:15:24.001: INFO: Got endpoints: latency-svc-54dd9 [106.82064ms]
Mar  2 22:15:24.003: INFO: Got endpoints: latency-svc-p6xc4 [103.001528ms]
Mar  2 22:15:24.006: INFO: Created: latency-svc-wqzz6
Mar  2 22:15:24.011: INFO: Got endpoints: latency-svc-wqzz6 [108.672874ms]
Mar  2 22:15:24.013: INFO: Created: latency-svc-jc7lb
Mar  2 22:15:24.019: INFO: Created: latency-svc-nbzk8
Mar  2 22:15:24.019: INFO: Got endpoints: latency-svc-jc7lb [106.507841ms]
Mar  2 22:15:24.025: INFO: Created: latency-svc-g4qcz
Mar  2 22:15:24.025: INFO: Got endpoints: latency-svc-nbzk8 [104.85514ms]
Mar  2 22:15:24.028: INFO: Got endpoints: latency-svc-g4qcz [105.537453ms]
Mar  2 22:15:24.033: INFO: Created: latency-svc-d58c2
Mar  2 22:15:24.037: INFO: Created: latency-svc-48msm
Mar  2 22:15:24.039: INFO: Got endpoints: latency-svc-d58c2 [109.572778ms]
Mar  2 22:15:24.042: INFO: Got endpoints: latency-svc-48msm [106.337929ms]
Mar  2 22:15:24.048: INFO: Created: latency-svc-crj2n
Mar  2 22:15:24.051: INFO: Got endpoints: latency-svc-crj2n [108.074156ms]
Mar  2 22:15:24.055: INFO: Created: latency-svc-rgr8p
Mar  2 22:15:24.061: INFO: Got endpoints: latency-svc-rgr8p [111.79126ms]
Mar  2 22:15:24.064: INFO: Created: latency-svc-k4kcf
Mar  2 22:15:24.070: INFO: Got endpoints: latency-svc-k4kcf [114.869947ms]
Mar  2 22:15:24.072: INFO: Created: latency-svc-xbgkd
Mar  2 22:15:24.077: INFO: Got endpoints: latency-svc-xbgkd [106.831028ms]
Mar  2 22:15:24.079: INFO: Created: latency-svc-fxtvs
Mar  2 22:15:24.086: INFO: Got endpoints: latency-svc-fxtvs [114.188405ms]
Mar  2 22:15:24.086: INFO: Created: latency-svc-zsxhj
Mar  2 22:15:24.091: INFO: Got endpoints: latency-svc-zsxhj [111.700926ms]
Mar  2 22:15:24.092: INFO: Created: latency-svc-sz4nr
Mar  2 22:15:24.099: INFO: Got endpoints: latency-svc-sz4nr [113.866718ms]
Mar  2 22:15:24.100: INFO: Created: latency-svc-7lwf2
Mar  2 22:15:24.105: INFO: Got endpoints: latency-svc-7lwf2 [104.451705ms]
Mar  2 22:15:24.106: INFO: Created: latency-svc-sbdlp
Mar  2 22:15:24.112: INFO: Created: latency-svc-bsbq2
Mar  2 22:15:24.112: INFO: Got endpoints: latency-svc-sbdlp [108.874076ms]
Mar  2 22:15:24.118: INFO: Got endpoints: latency-svc-bsbq2 [106.446018ms]
Mar  2 22:15:24.121: INFO: Created: latency-svc-6xgz6
Mar  2 22:15:24.128: INFO: Created: latency-svc-vd4wv
Mar  2 22:15:24.129: INFO: Got endpoints: latency-svc-6xgz6 [109.19055ms]
Mar  2 22:15:24.133: INFO: Created: latency-svc-kxvl2
Mar  2 22:15:24.133: INFO: Got endpoints: latency-svc-vd4wv [104.358454ms]
Mar  2 22:15:24.141: INFO: Got endpoints: latency-svc-kxvl2 [116.323042ms]
Mar  2 22:15:24.151: INFO: Created: latency-svc-jqhpq
Mar  2 22:15:24.152: INFO: Got endpoints: latency-svc-jqhpq [112.922816ms]
Mar  2 22:15:24.155: INFO: Created: latency-svc-2jvh4
Mar  2 22:15:24.160: INFO: Created: latency-svc-md86h
Mar  2 22:15:24.160: INFO: Got endpoints: latency-svc-2jvh4 [118.113692ms]
Mar  2 22:15:24.166: INFO: Got endpoints: latency-svc-md86h [114.766454ms]
Mar  2 22:15:24.179: INFO: Created: latency-svc-f9jzh
Mar  2 22:15:24.185: INFO: Got endpoints: latency-svc-f9jzh [123.883097ms]
Mar  2 22:15:24.186: INFO: Created: latency-svc-gvpcm
Mar  2 22:15:24.187: INFO: Created: latency-svc-nsj82
Mar  2 22:15:24.194: INFO: Got endpoints: latency-svc-gvpcm [124.026464ms]
Mar  2 22:15:24.201: INFO: Got endpoints: latency-svc-nsj82 [123.996871ms]
Mar  2 22:15:24.224: INFO: Created: latency-svc-qggwp
Mar  2 22:15:24.227: INFO: Created: latency-svc-hjk28
Mar  2 22:15:24.234: INFO: Got endpoints: latency-svc-qggwp [148.317028ms]
Mar  2 22:15:24.236: INFO: Created: latency-svc-tzpd9
Mar  2 22:15:24.242: INFO: Got endpoints: latency-svc-hjk28 [150.26055ms]
Mar  2 22:15:24.243: INFO: Got endpoints: latency-svc-tzpd9 [144.012807ms]
Mar  2 22:15:24.256: INFO: Created: latency-svc-xjmv9
Mar  2 22:15:24.261: INFO: Got endpoints: latency-svc-xjmv9 [155.20618ms]
Mar  2 22:15:24.263: INFO: Created: latency-svc-rbrph
Mar  2 22:15:24.269: INFO: Got endpoints: latency-svc-rbrph [157.065552ms]
Mar  2 22:15:24.270: INFO: Created: latency-svc-b52ck
Mar  2 22:15:24.280: INFO: Got endpoints: latency-svc-b52ck [162.107313ms]
Mar  2 22:15:24.283: INFO: Created: latency-svc-spppn
Mar  2 22:15:24.285: INFO: Got endpoints: latency-svc-spppn [156.464516ms]
Mar  2 22:15:24.291: INFO: Created: latency-svc-hxmsm
Mar  2 22:15:24.296: INFO: Got endpoints: latency-svc-hxmsm [163.080629ms]
Mar  2 22:15:24.298: INFO: Created: latency-svc-8cgrf
Mar  2 22:15:24.307: INFO: Got endpoints: latency-svc-8cgrf [165.161301ms]
Mar  2 22:15:24.310: INFO: Created: latency-svc-578fb
Mar  2 22:15:24.320: INFO: Got endpoints: latency-svc-578fb [168.024358ms]
Mar  2 22:15:24.322: INFO: Created: latency-svc-xwcg6
Mar  2 22:15:24.328: INFO: Got endpoints: latency-svc-xwcg6 [168.103433ms]
Mar  2 22:15:24.347: INFO: Created: latency-svc-5mvkj
Mar  2 22:15:24.347: INFO: Got endpoints: latency-svc-5mvkj [181.20803ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-wsfgf
Mar  2 22:15:24.574: INFO: Created: latency-svc-d6z9n
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-d6z9n [313.078023ms]
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-wsfgf [226.478942ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-xt64r
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-xt64r [245.508142ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-dk9z6
Mar  2 22:15:24.574: INFO: Created: latency-svc-4bxwn
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-4bxwn [294.058109ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-2dc6n
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-2dc6n [267.52627ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-dpwpz
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-dpwpz [305.010777ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-f4mvf
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-f4mvf [332.70028ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-s4z78
Mar  2 22:15:24.574: INFO: Created: latency-svc-nh2v9
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-nh2v9 [331.598185ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-252z8
Mar  2 22:15:24.575: INFO: Got endpoints: latency-svc-252z8 [373.474738ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-mnxhw
Mar  2 22:15:24.575: INFO: Got endpoints: latency-svc-mnxhw [254.465867ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-dlrzg
Mar  2 22:15:24.575: INFO: Got endpoints: latency-svc-dlrzg [289.520895ms]
Mar  2 22:15:24.574: INFO: Created: latency-svc-flrrl
Mar  2 22:15:24.574: INFO: Created: latency-svc-fjmb5
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-dk9z6 [388.563101ms]
Mar  2 22:15:24.574: INFO: Got endpoints: latency-svc-s4z78 [278.543431ms]
Mar  2 22:15:24.575: INFO: Got endpoints: latency-svc-flrrl [340.410878ms]
Mar  2 22:15:24.575: INFO: Got endpoints: latency-svc-fjmb5 [380.394742ms]
Mar  2 22:15:24.605: INFO: Created: latency-svc-4rmfb
Mar  2 22:15:24.611: INFO: Got endpoints: latency-svc-4rmfb [37.085169ms]
Mar  2 22:15:24.616: INFO: Created: latency-svc-696vq
Mar  2 22:15:24.621: INFO: Created: latency-svc-2lvh4
Mar  2 22:15:24.621: INFO: Got endpoints: latency-svc-696vq [46.48724ms]
Mar  2 22:15:24.628: INFO: Got endpoints: latency-svc-2lvh4 [53.566146ms]
Mar  2 22:15:24.639: INFO: Created: latency-svc-7w5bl
Mar  2 22:15:24.642: INFO: Got endpoints: latency-svc-7w5bl [68.332247ms]
Mar  2 22:15:24.643: INFO: Created: latency-svc-4plnd
Mar  2 22:15:24.650: INFO: Got endpoints: latency-svc-4plnd [76.527797ms]
Mar  2 22:15:24.651: INFO: Created: latency-svc-hgmsb
Mar  2 22:15:24.659: INFO: Got endpoints: latency-svc-hgmsb [84.997569ms]
Mar  2 22:15:24.669: INFO: Created: latency-svc-96ppv
Mar  2 22:15:24.677: INFO: Created: latency-svc-54hwc
Mar  2 22:15:24.681: INFO: Got endpoints: latency-svc-96ppv [106.310866ms]
Mar  2 22:15:24.683: INFO: Created: latency-svc-h5vlx
Mar  2 22:15:24.685: INFO: Got endpoints: latency-svc-54hwc [110.262633ms]
Mar  2 22:15:24.697: INFO: Got endpoints: latency-svc-h5vlx [123.126798ms]
Mar  2 22:15:24.701: INFO: Created: latency-svc-tbxcw
Mar  2 22:15:24.707: INFO: Got endpoints: latency-svc-tbxcw [132.456296ms]
Mar  2 22:15:24.717: INFO: Created: latency-svc-lbv6j
Mar  2 22:15:24.724: INFO: Got endpoints: latency-svc-lbv6j [149.107396ms]
Mar  2 22:15:24.739: INFO: Created: latency-svc-kqzl2
Mar  2 22:15:24.739: INFO: Got endpoints: latency-svc-kqzl2 [164.713371ms]
Mar  2 22:15:24.774: INFO: Created: latency-svc-xwmwt
Mar  2 22:15:24.784: INFO: Got endpoints: latency-svc-xwmwt [208.800874ms]
Mar  2 22:15:24.800: INFO: Created: latency-svc-gtd2t
Mar  2 22:15:24.818: INFO: Got endpoints: latency-svc-gtd2t [242.823367ms]
Mar  2 22:15:24.819: INFO: Created: latency-svc-94j7p
Mar  2 22:15:24.840: INFO: Got endpoints: latency-svc-94j7p [265.382316ms]
Mar  2 22:15:24.881: INFO: Created: latency-svc-jlgj5
Mar  2 22:15:24.896: INFO: Created: latency-svc-npfnh
Mar  2 22:15:24.905: INFO: Created: latency-svc-n8zqm
Mar  2 22:15:24.925: INFO: Got endpoints: latency-svc-npfnh [304.4702ms]
Mar  2 22:15:24.925: INFO: Got endpoints: latency-svc-jlgj5 [314.248528ms]
Mar  2 22:15:24.934: INFO: Created: latency-svc-p6zpq
Mar  2 22:15:24.944: INFO: Got endpoints: latency-svc-n8zqm [316.541644ms]
Mar  2 22:15:24.946: INFO: Created: latency-svc-qq8sx
Mar  2 22:15:24.959: INFO: Got endpoints: latency-svc-p6zpq [316.637069ms]
Mar  2 22:15:24.964: INFO: Created: latency-svc-gxzp9
Mar  2 22:15:25.000: INFO: Got endpoints: latency-svc-gxzp9 [341.066118ms]
Mar  2 22:15:25.000: INFO: Got endpoints: latency-svc-qq8sx [349.980372ms]
Mar  2 22:15:25.024: INFO: Created: latency-svc-cbxsl
Mar  2 22:15:25.032: INFO: Created: latency-svc-v5jh6
Mar  2 22:15:25.043: INFO: Got endpoints: latency-svc-cbxsl [362.443255ms]
Mar  2 22:15:25.060: INFO: Created: latency-svc-q6pt4
Mar  2 22:15:25.080: INFO: Got endpoints: latency-svc-v5jh6 [395.243617ms]
Mar  2 22:15:25.080: INFO: Got endpoints: latency-svc-q6pt4 [382.787625ms]
Mar  2 22:15:25.103: INFO: Created: latency-svc-hlh5x
Mar  2 22:15:25.111: INFO: Created: latency-svc-w6grl
Mar  2 22:15:25.111: INFO: Got endpoints: latency-svc-hlh5x [403.810991ms]
Mar  2 22:15:25.121: INFO: Created: latency-svc-25fn2
Mar  2 22:15:25.135: INFO: Got endpoints: latency-svc-w6grl [411.139695ms]
Mar  2 22:15:25.135: INFO: Got endpoints: latency-svc-25fn2 [396.122245ms]
Mar  2 22:15:25.144: INFO: Created: latency-svc-2cmtb
Mar  2 22:15:25.154: INFO: Got endpoints: latency-svc-2cmtb [370.361824ms]
Mar  2 22:15:25.179: INFO: Created: latency-svc-bmfwf
Mar  2 22:15:25.188: INFO: Got endpoints: latency-svc-bmfwf [370.085222ms]
Mar  2 22:15:25.196: INFO: Created: latency-svc-79cxp
Mar  2 22:15:25.208: INFO: Got endpoints: latency-svc-79cxp [367.691444ms]
Mar  2 22:15:25.222: INFO: Created: latency-svc-l6qvg
Mar  2 22:15:25.233: INFO: Got endpoints: latency-svc-l6qvg [307.997471ms]
Mar  2 22:15:25.258: INFO: Created: latency-svc-zszxr
Mar  2 22:15:25.262: INFO: Got endpoints: latency-svc-zszxr [336.590115ms]
Mar  2 22:15:25.265: INFO: Created: latency-svc-cnmtw
Mar  2 22:15:25.271: INFO: Got endpoints: latency-svc-cnmtw [326.721517ms]
Mar  2 22:15:25.278: INFO: Created: latency-svc-lm2wv
Mar  2 22:15:25.285: INFO: Got endpoints: latency-svc-lm2wv [325.59097ms]
Mar  2 22:15:25.287: INFO: Created: latency-svc-8j5bt
Mar  2 22:15:25.295: INFO: Got endpoints: latency-svc-8j5bt [294.782146ms]
Mar  2 22:15:25.299: INFO: Created: latency-svc-z6hbd
Mar  2 22:15:25.305: INFO: Got endpoints: latency-svc-z6hbd [304.402632ms]
Mar  2 22:15:25.310: INFO: Created: latency-svc-nglgx
Mar  2 22:15:25.314: INFO: Got endpoints: latency-svc-nglgx [29.786834ms]
Mar  2 22:15:25.320: INFO: Created: latency-svc-m7kcl
Mar  2 22:15:25.331: INFO: Got endpoints: latency-svc-m7kcl [287.615453ms]
Mar  2 22:15:25.331: INFO: Created: latency-svc-cfx2d
Mar  2 22:15:25.338: INFO: Got endpoints: latency-svc-cfx2d [258.023401ms]
Mar  2 22:15:25.351: INFO: Created: latency-svc-wpd5c
Mar  2 22:15:25.360: INFO: Got endpoints: latency-svc-wpd5c [279.154832ms]
Mar  2 22:15:25.360: INFO: Created: latency-svc-jthb5
Mar  2 22:15:25.365: INFO: Got endpoints: latency-svc-jthb5 [254.265723ms]
Mar  2 22:15:25.365: INFO: Created: latency-svc-xhvxb
Mar  2 22:15:25.380: INFO: Got endpoints: latency-svc-xhvxb [244.298567ms]
Mar  2 22:15:25.380: INFO: Created: latency-svc-65hpd
Mar  2 22:15:25.383: INFO: Created: latency-svc-f984d
Mar  2 22:15:25.394: INFO: Got endpoints: latency-svc-f984d [239.860597ms]
Mar  2 22:15:25.394: INFO: Got endpoints: latency-svc-65hpd [258.576608ms]
Mar  2 22:15:25.394: INFO: Created: latency-svc-lwdd4
Mar  2 22:15:25.404: INFO: Got endpoints: latency-svc-lwdd4 [216.305713ms]
Mar  2 22:15:25.420: INFO: Created: latency-svc-46ptn
Mar  2 22:15:25.421: INFO: Got endpoints: latency-svc-46ptn [212.731284ms]
Mar  2 22:15:25.431: INFO: Created: latency-svc-28f7k
Mar  2 22:15:25.439: INFO: Created: latency-svc-cc7rx
Mar  2 22:15:25.442: INFO: Got endpoints: latency-svc-28f7k [208.733973ms]
Mar  2 22:15:25.458: INFO: Got endpoints: latency-svc-cc7rx [196.235871ms]
Mar  2 22:15:25.460: INFO: Created: latency-svc-29pfv
Mar  2 22:15:25.465: INFO: Got endpoints: latency-svc-29pfv [194.221511ms]
Mar  2 22:15:25.480: INFO: Created: latency-svc-lksr4
Mar  2 22:15:25.486: INFO: Got endpoints: latency-svc-lksr4 [190.335438ms]
Mar  2 22:15:25.488: INFO: Created: latency-svc-m64ng
Mar  2 22:15:25.500: INFO: Got endpoints: latency-svc-m64ng [195.085743ms]
Mar  2 22:15:25.505: INFO: Created: latency-svc-55kx4
Mar  2 22:15:25.512: INFO: Got endpoints: latency-svc-55kx4 [197.12736ms]
Mar  2 22:15:25.518: INFO: Created: latency-svc-fpzcv
Mar  2 22:15:25.522: INFO: Got endpoints: latency-svc-fpzcv [190.958999ms]
Mar  2 22:15:25.523: INFO: Created: latency-svc-lskcb
Mar  2 22:15:25.531: INFO: Got endpoints: latency-svc-lskcb [192.515029ms]
Mar  2 22:15:25.534: INFO: Created: latency-svc-sxl5c
Mar  2 22:15:25.541: INFO: Got endpoints: latency-svc-sxl5c [181.337263ms]
Mar  2 22:15:25.547: INFO: Created: latency-svc-pjtz4
Mar  2 22:15:25.556: INFO: Created: latency-svc-wqmmc
Mar  2 22:15:25.556: INFO: Got endpoints: latency-svc-pjtz4 [191.109265ms]
Mar  2 22:15:25.566: INFO: Created: latency-svc-h6q5k
Mar  2 22:15:25.567: INFO: Got endpoints: latency-svc-wqmmc [187.357516ms]
Mar  2 22:15:25.572: INFO: Got endpoints: latency-svc-h6q5k [177.462477ms]
Mar  2 22:15:25.573: INFO: Created: latency-svc-fxqhv
Mar  2 22:15:25.581: INFO: Got endpoints: latency-svc-fxqhv [186.882644ms]
Mar  2 22:15:25.583: INFO: Created: latency-svc-h6scn
Mar  2 22:15:25.588: INFO: Got endpoints: latency-svc-h6scn [183.547569ms]
Mar  2 22:15:25.596: INFO: Created: latency-svc-8c24v
Mar  2 22:15:25.605: INFO: Got endpoints: latency-svc-8c24v [183.819426ms]
Mar  2 22:15:25.609: INFO: Created: latency-svc-w98b2
Mar  2 22:15:25.617: INFO: Got endpoints: latency-svc-w98b2 [174.529828ms]
Mar  2 22:15:25.617: INFO: Created: latency-svc-lcrds
Mar  2 22:15:25.629: INFO: Got endpoints: latency-svc-lcrds [170.635163ms]
Mar  2 22:15:25.638: INFO: Created: latency-svc-8dhnr
Mar  2 22:15:25.638: INFO: Got endpoints: latency-svc-8dhnr [172.773036ms]
Mar  2 22:15:25.644: INFO: Created: latency-svc-kss5j
Mar  2 22:15:25.659: INFO: Got endpoints: latency-svc-kss5j [173.0875ms]
Mar  2 22:15:25.695: INFO: Created: latency-svc-8qcfq
Mar  2 22:15:25.706: INFO: Got endpoints: latency-svc-8qcfq [205.643809ms]
Mar  2 22:15:25.725: INFO: Created: latency-svc-44trr
Mar  2 22:15:25.725: INFO: Got endpoints: latency-svc-44trr [212.813268ms]
Mar  2 22:15:25.728: INFO: Created: latency-svc-d6v9k
Mar  2 22:15:25.740: INFO: Got endpoints: latency-svc-d6v9k [217.823634ms]
Mar  2 22:15:25.741: INFO: Created: latency-svc-w5tgb
Mar  2 22:15:25.759: INFO: Got endpoints: latency-svc-w5tgb [227.561732ms]
Mar  2 22:15:25.766: INFO: Created: latency-svc-tb9b5
Mar  2 22:15:25.777: INFO: Got endpoints: latency-svc-tb9b5 [236.104412ms]
Mar  2 22:15:25.795: INFO: Created: latency-svc-2ss7g
Mar  2 22:15:25.803: INFO: Got endpoints: latency-svc-2ss7g [246.513791ms]
Mar  2 22:15:25.810: INFO: Created: latency-svc-pxnd9
Mar  2 22:15:25.815: INFO: Got endpoints: latency-svc-pxnd9 [248.279758ms]
Mar  2 22:15:25.829: INFO: Created: latency-svc-vsjbc
Mar  2 22:15:25.836: INFO: Got endpoints: latency-svc-vsjbc [264.536226ms]
Mar  2 22:15:25.836: INFO: Created: latency-svc-dlqbq
Mar  2 22:15:25.845: INFO: Got endpoints: latency-svc-dlqbq [264.332643ms]
Mar  2 22:15:25.846: INFO: Created: latency-svc-dnnbx
Mar  2 22:15:25.854: INFO: Got endpoints: latency-svc-dnnbx [265.979731ms]
Mar  2 22:15:25.859: INFO: Created: latency-svc-fd6tb
Mar  2 22:15:25.866: INFO: Got endpoints: latency-svc-fd6tb [261.139932ms]
Mar  2 22:15:25.872: INFO: Created: latency-svc-7hxqs
Mar  2 22:15:25.880: INFO: Got endpoints: latency-svc-7hxqs [262.850628ms]
Mar  2 22:15:25.887: INFO: Created: latency-svc-6nfct
Mar  2 22:15:25.896: INFO: Got endpoints: latency-svc-6nfct [266.784547ms]
Mar  2 22:15:25.911: INFO: Created: latency-svc-lrjm7
Mar  2 22:15:25.917: INFO: Got endpoints: latency-svc-lrjm7 [279.215328ms]
Mar  2 22:15:25.927: INFO: Created: latency-svc-7584c
Mar  2 22:15:25.940: INFO: Created: latency-svc-nxflh
Mar  2 22:15:25.952: INFO: Got endpoints: latency-svc-7584c [293.280817ms]
Mar  2 22:15:25.954: INFO: Got endpoints: latency-svc-nxflh [248.382946ms]
Mar  2 22:15:25.957: INFO: Created: latency-svc-qml7x
Mar  2 22:15:25.970: INFO: Got endpoints: latency-svc-qml7x [245.70761ms]
Mar  2 22:15:25.980: INFO: Created: latency-svc-xffjq
Mar  2 22:15:25.992: INFO: Got endpoints: latency-svc-xffjq [251.789322ms]
Mar  2 22:15:25.995: INFO: Created: latency-svc-pb86h
Mar  2 22:15:26.009: INFO: Created: latency-svc-z5td9
Mar  2 22:15:26.013: INFO: Got endpoints: latency-svc-pb86h [254.645392ms]
Mar  2 22:15:26.014: INFO: Created: latency-svc-x4t8h
Mar  2 22:15:26.014: INFO: Got endpoints: latency-svc-z5td9 [237.118618ms]
Mar  2 22:15:26.036: INFO: Created: latency-svc-rw99z
Mar  2 22:15:26.039: INFO: Created: latency-svc-krqq8
Mar  2 22:15:26.057: INFO: Got endpoints: latency-svc-rw99z [241.633328ms]
Mar  2 22:15:26.059: INFO: Created: latency-svc-vrh2m
Mar  2 22:15:26.067: INFO: Got endpoints: latency-svc-krqq8 [231.220197ms]
Mar  2 22:15:26.069: INFO: Got endpoints: latency-svc-x4t8h [265.601462ms]
Mar  2 22:15:26.083: INFO: Created: latency-svc-j9cl8
Mar  2 22:15:26.097: INFO: Created: latency-svc-l49d7
Mar  2 22:15:26.100: INFO: Got endpoints: latency-svc-vrh2m [254.857775ms]
Mar  2 22:15:26.105: INFO: Got endpoints: latency-svc-j9cl8 [250.92674ms]
Mar  2 22:15:26.115: INFO: Created: latency-svc-tgz9d
Mar  2 22:15:26.131: INFO: Got endpoints: latency-svc-l49d7 [265.158222ms]
Mar  2 22:15:26.148: INFO: Created: latency-svc-2x2ps
Mar  2 22:15:26.165: INFO: Created: latency-svc-dr6f2
Mar  2 22:15:26.172: INFO: Created: latency-svc-lgw27
Mar  2 22:15:26.189: INFO: Got endpoints: latency-svc-tgz9d [309.001441ms]
Mar  2 22:15:26.189: INFO: Created: latency-svc-sfspc
Mar  2 22:15:26.190: INFO: Got endpoints: latency-svc-2x2ps [294.224796ms]
Mar  2 22:15:26.190: INFO: Got endpoints: latency-svc-dr6f2 [272.748599ms]
Mar  2 22:15:26.196: INFO: Created: latency-svc-6cslg
Mar  2 22:15:26.198: INFO: Got endpoints: latency-svc-lgw27 [244.973793ms]
Mar  2 22:15:26.198: INFO: Got endpoints: latency-svc-sfspc [243.03252ms]
Mar  2 22:15:26.205: INFO: Created: latency-svc-4fczc
Mar  2 22:15:26.217: INFO: Got endpoints: latency-svc-6cslg [246.501675ms]
Mar  2 22:15:26.233: INFO: Got endpoints: latency-svc-4fczc [241.156415ms]
Mar  2 22:15:26.233: INFO: Latencies: [27.433873ms 29.786834ms 35.44699ms 37.085169ms 38.14448ms 46.48724ms 47.067502ms 53.566146ms 57.281111ms 68.332247ms 71.849411ms 72.50327ms 76.527797ms 82.128485ms 84.997569ms 88.670832ms 89.910487ms 93.879933ms 94.678519ms 94.806667ms 97.047879ms 97.07529ms 97.54624ms 98.164468ms 98.386501ms 98.579084ms 98.86058ms 98.995378ms 99.151478ms 99.215743ms 99.293433ms 99.750064ms 100.464542ms 100.4916ms 100.79368ms 101.325765ms 102.260571ms 102.579299ms 102.8015ms 103.001528ms 103.028123ms 103.05674ms 104.127441ms 104.358454ms 104.410988ms 104.451705ms 104.85514ms 105.274505ms 105.537453ms 106.170225ms 106.310866ms 106.337929ms 106.446018ms 106.507841ms 106.706134ms 106.82064ms 106.831028ms 107.260953ms 107.428345ms 108.074156ms 108.672874ms 108.853727ms 108.874076ms 108.906533ms 109.19055ms 109.572778ms 110.262633ms 111.700926ms 111.79126ms 111.920095ms 111.965049ms 112.922816ms 113.866718ms 114.188405ms 114.766454ms 114.869947ms 116.323042ms 118.113692ms 123.126798ms 123.883097ms 123.996871ms 124.026464ms 125.590437ms 132.456296ms 133.86804ms 134.794887ms 144.012807ms 148.317028ms 149.107396ms 150.26055ms 155.20618ms 156.464516ms 157.065552ms 162.107313ms 163.080629ms 164.713371ms 165.161301ms 168.024358ms 168.103433ms 170.635163ms 172.773036ms 173.0875ms 174.529828ms 177.462477ms 181.20803ms 181.337263ms 183.547569ms 183.819426ms 186.882644ms 187.357516ms 190.335438ms 190.958999ms 191.109265ms 192.515029ms 194.221511ms 195.085743ms 196.235871ms 197.12736ms 205.643809ms 208.733973ms 208.800874ms 212.731284ms 212.813268ms 216.305713ms 217.823634ms 226.478942ms 227.561732ms 231.220197ms 236.104412ms 237.118618ms 239.860597ms 241.156415ms 241.633328ms 242.823367ms 243.03252ms 244.298567ms 244.973793ms 245.508142ms 245.70761ms 246.501675ms 246.513791ms 248.279758ms 248.382946ms 250.92674ms 251.789322ms 254.265723ms 254.465867ms 254.645392ms 254.857775ms 258.023401ms 258.576608ms 261.139932ms 262.850628ms 264.332643ms 264.536226ms 265.158222ms 265.382316ms 265.601462ms 265.979731ms 266.784547ms 267.52627ms 272.748599ms 278.543431ms 279.154832ms 279.215328ms 287.615453ms 289.520895ms 293.280817ms 294.058109ms 294.224796ms 294.782146ms 304.402632ms 304.4702ms 305.010777ms 307.997471ms 309.001441ms 313.078023ms 314.248528ms 316.541644ms 316.637069ms 325.59097ms 326.721517ms 331.598185ms 332.70028ms 336.590115ms 340.410878ms 341.066118ms 349.980372ms 362.443255ms 367.691444ms 370.085222ms 370.361824ms 373.474738ms 380.394742ms 382.787625ms 388.563101ms 395.243617ms 396.122245ms 403.810991ms 411.139695ms]
Mar  2 22:15:26.234: INFO: 50 %ile: 172.773036ms
Mar  2 22:15:26.234: INFO: 90 %ile: 325.59097ms
Mar  2 22:15:26.234: INFO: 99 %ile: 403.810991ms
Mar  2 22:15:26.234: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:26.234: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-4082" for this suite.
Mar  2 22:15:40.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:43.689: INFO: namespace svc-latency-4082 deletion completed in 17.373186127s


• [SLOW TEST:29.353 seconds]
[sig-network] Service endpoints latency
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:24.695: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:15:25.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:15:27.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:29.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:31.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:33.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784125, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:15:36.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:36.858: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-417" for this suite.
Mar  2 22:15:43.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:46.577: INFO: namespace webhook-417 deletion completed in 9.372549821s
STEP: Destroying namespace "webhook-417-markers" for this suite.
Mar  2 22:15:52.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:15:55.932: INFO: namespace webhook-417-markers deletion completed in 9.354429205s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:31.289 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:21.064: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  2 22:15:21.184: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:32.737: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-2864" for this suite.
Mar  2 22:16:01.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:04.449: INFO: namespace init-container-2864 deletion completed in 31.382060443s


• [SLOW TEST:43.386 seconds]
[k8s.io] InitContainer [NodeConformance]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:25.965: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Mar  2 22:15:36.405: INFO: Pod pod-hostip-abcb384b-c9b7-4236-bcee-4bbec362285a has hostIP: 10.0.130.27
[AfterEach] [k8s.io] Pods
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:15:36.406: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1556" for this suite.
Mar  2 22:16:04.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:08.166: INFO: namespace pods-1556 deletion completed in 31.397739114s


• [SLOW TEST:42.201 seconds]
[k8s.io] Pods
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:56.003: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:07.202: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-3324" for this suite.
Mar  2 22:16:15.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:18.680: INFO: namespace resourcequota-3324 deletion completed in 11.354224087s


• [SLOW TEST:22.677 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:15:43.707: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Mar  2 22:15:44.588: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:15:44.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:46.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:48.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:50.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:15:52.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784144, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:15:55.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:06.249: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4959" for this suite.
Mar  2 22:16:12.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:15.743: INFO: namespace webhook-4959 deletion completed in 9.354087385s
STEP: Destroying namespace "webhook-4959-markers" for this suite.
Mar  2 22:16:21.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:25.101: INFO: namespace webhook-4959-markers deletion completed in 9.357188002s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:41.447 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:18.716: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-113b9bcb-5da1-4ddd-81e6-aa2159c24d78
[AfterEach] [sig-node] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:18.840: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-5071" for this suite.
Mar  2 22:16:25.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:28.317: INFO: namespace configmap-5071 deletion completed in 9.352783945s


• [SLOW TEST:9.600 seconds]
[sig-node] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:04.464: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2689
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2689
I0302 22:16:04.675677   56391 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2689, replica count: 2
I0302 22:16:07.726401   56391 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:16:10.726774   56391 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 22:16:13.727147   56391 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 22:16:13.727: INFO: Creating new exec pod
Mar  2 22:16:24.967: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-2689 execpodh9nnr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 22:16:27.896: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 22:16:27.896: INFO: stdout: ""
Mar  2 22:16:27.897: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-2689 execpodh9nnr -- /bin/sh -x -c nc -zv -t -w 2 172.30.120.32 80'
Mar  2 22:16:28.160: INFO: stderr: "+ nc -zv -t -w 2 172.30.120.32 80\nConnection to 172.30.120.32 80 port [tcp/http] succeeded!\n"
Mar  2 22:16:28.160: INFO: stdout: ""
Mar  2 22:16:28.160: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-2689 execpodh9nnr -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.224 30167'
Mar  2 22:16:28.472: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.224 30167\nConnection to 10.0.130.224 30167 port [tcp/30167] succeeded!\n"
Mar  2 22:16:28.472: INFO: stdout: ""
Mar  2 22:16:28.472: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=services-2689 execpodh9nnr -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.27 30167'
Mar  2 22:16:28.815: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.27 30167\nConnection to 10.0.130.27 30167 port [tcp/30167] succeeded!\n"
Mar  2 22:16:28.815: INFO: stdout: ""
Mar  2 22:16:28.815: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:28.857: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2689" for this suite.
Mar  2 22:16:35.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:38.377: INFO: namespace services-2689 deletion completed in 9.381571794s
[AfterEach] [sig-network] Services
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


• [SLOW TEST:33.913 seconds]
[sig-network] Services
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:25.171: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  2 22:16:25.305: INFO: Waiting up to 5m0s for pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c" in namespace "downward-api-7533" to be "success or failure"
Mar  2 22:16:25.321: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.992465ms
Mar  2 22:16:27.334: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028321995s
Mar  2 22:16:29.348: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042395472s
Mar  2 22:16:31.362: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056322176s
Mar  2 22:16:33.376: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070045999s
Mar  2 22:16:35.390: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.084350725s
STEP: Saw pod success
Mar  2 22:16:35.390: INFO: Pod "downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c" satisfied condition "success or failure"
Mar  2 22:16:35.403: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:16:35.446: INFO: Waiting for pod downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c to disappear
Mar  2 22:16:35.460: INFO: Pod downward-api-ef48ce0c-6fd3-485f-a6d6-5dda5986171c no longer exists
[AfterEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:35.460: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7533" for this suite.
Mar  2 22:16:41.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:44.948: INFO: namespace downward-api-7533 deletion completed in 9.355602042s


• [SLOW TEST:19.777 seconds]
[sig-node] Downward API
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:08.172: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-c2hq
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:16:08.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-c2hq" in namespace "subpath-1587" to be "success or failure"
Mar  2 22:16:08.412: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Pending", Reason="", readiness=false. Elapsed: 20.082512ms
Mar  2 22:16:10.433: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041085585s
Mar  2 22:16:12.455: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062789076s
Mar  2 22:16:14.476: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084259781s
Mar  2 22:16:16.498: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105613086s
Mar  2 22:16:18.519: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 10.12686772s
Mar  2 22:16:20.540: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 12.148126537s
Mar  2 22:16:22.562: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 14.169846138s
Mar  2 22:16:24.583: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 16.190770646s
Mar  2 22:16:26.604: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 18.211651246s
Mar  2 22:16:28.632: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 20.239522331s
Mar  2 22:16:30.653: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 22.261309956s
Mar  2 22:16:32.675: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 24.2829464s
Mar  2 22:16:34.697: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Running", Reason="", readiness=true. Elapsed: 26.304509012s
Mar  2 22:16:36.718: INFO: Pod "pod-subpath-test-downwardapi-c2hq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.326282343s
STEP: Saw pod success
Mar  2 22:16:36.719: INFO: Pod "pod-subpath-test-downwardapi-c2hq" satisfied condition "success or failure"
Mar  2 22:16:36.739: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-subpath-test-downwardapi-c2hq container test-container-subpath-downwardapi-c2hq: <nil>
STEP: delete the pod
Mar  2 22:16:36.795: INFO: Waiting for pod pod-subpath-test-downwardapi-c2hq to disappear
Mar  2 22:16:36.815: INFO: Pod pod-subpath-test-downwardapi-c2hq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-c2hq
Mar  2 22:16:36.815: INFO: Deleting pod "pod-subpath-test-downwardapi-c2hq" in namespace "subpath-1587"
[AfterEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:36.836: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-1587" for this suite.
Mar  2 22:16:43.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:46.387: INFO: namespace subpath-1587 deletion completed in 9.385462039s


• [SLOW TEST:38.215 seconds]
[sig-storage] Subpath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:28.325: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-83b2a841-dcbe-455a-966f-bf9d59cbccba
STEP: Creating a pod to test consume configMaps
Mar  2 22:16:28.455: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada" in namespace "projected-5076" to be "success or failure"
Mar  2 22:16:28.467: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Pending", Reason="", readiness=false. Elapsed: 11.962794ms
Mar  2 22:16:30.480: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025472297s
Mar  2 22:16:32.494: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038901596s
Mar  2 22:16:34.507: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051997174s
Mar  2 22:16:36.520: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065399556s
Mar  2 22:16:38.536: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.080734937s
STEP: Saw pod success
Mar  2 22:16:38.536: INFO: Pod "pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada" satisfied condition "success or failure"
Mar  2 22:16:38.549: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:16:38.586: INFO: Waiting for pod pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada to disappear
Mar  2 22:16:38.600: INFO: Pod pod-projected-configmaps-44af9bc0-701f-4c9d-85e1-3c86c8139ada no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:38.600: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5076" for this suite.
Mar  2 22:16:44.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:48.027: INFO: namespace projected-5076 deletion completed in 9.35336693s


• [SLOW TEST:19.702 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:38.443: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:16:38.600: INFO: Waiting up to 5m0s for pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615" in namespace "security-context-test-1713" to be "success or failure"
Mar  2 22:16:38.619: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615": Phase="Pending", Reason="", readiness=false. Elapsed: 18.581761ms
Mar  2 22:16:40.639: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038260981s
Mar  2 22:16:42.660: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059773818s
Mar  2 22:16:44.680: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079957035s
Mar  2 22:16:46.700: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.099612794s
Mar  2 22:16:46.700: INFO: Pod "busybox-user-65534-eb2c53da-ffb8-4275-b415-b9dc59b08615" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:16:46.700: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-1713" for this suite.
Mar  2 22:16:52.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:16:56.196: INFO: namespace security-context-test-1713 deletion completed in 9.391383996s


• [SLOW TEST:17.753 seconds]
[k8s.io] Security Context
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:46.446: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 22:16:46.593: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-527'
Mar  2 22:16:46.728: INFO: stderr: ""
Mar  2 22:16:46.728: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  2 22:16:56.780: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig get pod e2e-test-httpd-pod --namespace=kubectl-527 -o json'
Mar  2 22:16:56.900: INFO: stderr: ""
Mar  2 22:16:56.900: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.3.96\\\"\\n    ],\\n    \\\"dns\\\": {},\\n    \\\"default-route\\\": [\\n        \\\"10.128.2.1\\\"\\n    ]\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-03-02T22:16:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-527\",\n        \"resourceVersion\": \"15454789\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-527/pods/e2e-test-httpd-pod\",\n        \"uid\": \"241daeb2-3f27-4dae-b3ec-1382457d330d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5ngsv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-jx8kn\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-130-27.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c84,c79\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5ngsv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5ngsv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-02T22:16:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-02T22:16:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-02T22:16:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-02T22:16:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://96c6989efb4fa2f440b8dee2a16c7fa8efb34da63c234b9963a479651d1275a5\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-02T22:16:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.130.27\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.3.96\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.3.96\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-02T22:16:46Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  2 22:16:56.900: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig replace -f - --namespace=kubectl-527'
Mar  2 22:16:57.793: INFO: stderr: ""
Mar  2 22:16:57.793: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Mar  2 22:16:57.814: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-527'
Mar  2 22:17:05.808: INFO: stderr: ""
Mar  2 22:17:05.808: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:05.808: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-527" for this suite.
Mar  2 22:17:12.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:15.360: INFO: namespace kubectl-527 deletion completed in 9.38693978s


• [SLOW TEST:28.915 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:56.204: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-01529f8f-3047-4c0d-a4a7-607e1aa76cf7
STEP: Creating a pod to test consume configMaps
Mar  2 22:16:56.382: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3" in namespace "projected-3828" to be "success or failure"
Mar  2 22:16:56.403: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.857225ms
Mar  2 22:16:58.422: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040458492s
Mar  2 22:17:00.442: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059990361s
Mar  2 22:17:02.463: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080659611s
Mar  2 22:17:04.482: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100453807s
Mar  2 22:17:06.503: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.120901112s
STEP: Saw pod success
Mar  2 22:17:06.503: INFO: Pod "pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3" satisfied condition "success or failure"
Mar  2 22:17:06.522: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:17:06.571: INFO: Waiting for pod pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3 to disappear
Mar  2 22:17:06.590: INFO: Pod pod-projected-configmaps-48c6ef46-347d-4e36-858b-1ce08ddc1ba3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:06.590: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3828" for this suite.
Mar  2 22:17:12.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:16.107: INFO: namespace projected-3828 deletion completed in 9.37913828s


• [SLOW TEST:19.903 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:48.045: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:16:48.146: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  2 22:16:48.176: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 22:16:58.201: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 22:16:58.216: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 22:16:58.247: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar  2 22:17:00.273: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 22:17:00.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:02.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:04.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:06.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784217, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:08.299: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  2 22:17:08.336: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8383 /apis/apps/v1/namespaces/deployment-8383/deployments/test-rolling-update-deployment d1be1897-5b6d-4bdd-9492-0a781ea4c55c 15454980 1 2020-03-02 22:16:57 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00374a8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-02 22:16:57 +0000 UTC,LastTransitionTime:2020-03-02 22:16:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-03-02 22:17:06 +0000 UTC,LastTransitionTime:2020-03-02 22:16:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 22:17:08.350: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8383 /apis/apps/v1/namespaces/deployment-8383/replicasets/test-rolling-update-deployment-55d946486 4023683e-0edc-4415-93ff-1ed36155741b 15454969 1 2020-03-02 22:16:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d1be1897-5b6d-4bdd-9492-0a781ea4c55c 0xc00374ad90 0xc00374ad91}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00374adf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:17:08.350: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 22:17:08.350: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8383 /apis/apps/v1/namespaces/deployment-8383/replicasets/test-rolling-update-controller d47d058a-e43b-4b8d-8165-3419e2838b4f 15454979 2 2020-03-02 22:16:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d1be1897-5b6d-4bdd-9492-0a781ea4c55c 0xc00374acb7 0xc00374acb8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00374ad18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:17:08.363: INFO: Pod "test-rolling-update-deployment-55d946486-z4d9w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-z4d9w test-rolling-update-deployment-55d946486- deployment-8383 /api/v1/namespaces/deployment-8383/pods/test-rolling-update-deployment-55d946486-z4d9w 009b54a0-dad8-42d2-92c7-9bce01cb10f0 15454968 0 2020-03-02 22:16:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.99"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 4023683e-0edc-4415-93ff-1ed36155741b 0xc00374b290 0xc00374b291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8g8jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8g8jd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8g8jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rxz24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:16:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:16:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.99,StartTime:2020-03-02 22:16:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:17:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://60261748597c96d79ea1e26fb613f81a865884238eb504b6405c33d657b11190,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:08.363: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-8383" for this suite.
Mar  2 22:17:14.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:17.778: INFO: namespace deployment-8383 deletion completed in 9.351617055s


• [SLOW TEST:29.733 seconds]
[sig-apps] Deployment
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:16:44.956: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:16:46.284: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:16:48.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:50.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:52.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:16:54.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784205, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:16:57.361: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:09.683: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7939" for this suite.
Mar  2 22:17:15.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:19.118: INFO: namespace webhook-7939 deletion completed in 9.356775536s
STEP: Destroying namespace "webhook-7939-markers" for this suite.
Mar  2 22:17:25.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:28.473: INFO: namespace webhook-7939-markers deletion completed in 9.354081779s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:43.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:15.376: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1698.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1698.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1698.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1698.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 22:17:25.771: INFO: DNS probes using dns-1698/dns-test-87f7a9a7-3312-4ab7-9b22-c520310bcb8c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:25.831: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-1698" for this suite.
Mar  2 22:17:32.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:35.328: INFO: namespace dns-1698 deletion completed in 9.386256584s


• [SLOW TEST:19.952 seconds]
[sig-network] DNS
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:17.801: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Mar  2 22:17:17.943: INFO: Waiting up to 5m0s for pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966" in namespace "var-expansion-1370" to be "success or failure"
Mar  2 22:17:17.956: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Pending", Reason="", readiness=false. Elapsed: 12.674382ms
Mar  2 22:17:19.969: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026589282s
Mar  2 22:17:21.983: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039830891s
Mar  2 22:17:23.996: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052903816s
Mar  2 22:17:26.009: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066186066s
Mar  2 22:17:28.022: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.079038614s
STEP: Saw pod success
Mar  2 22:17:28.022: INFO: Pod "var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966" satisfied condition "success or failure"
Mar  2 22:17:28.035: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:17:28.070: INFO: Waiting for pod var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966 to disappear
Mar  2 22:17:28.082: INFO: Pod var-expansion-ea9d20e1-f478-4189-820a-b7301aec4966 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:28.082: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-1370" for this suite.
Mar  2 22:17:34.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:37.520: INFO: namespace var-expansion-1370 deletion completed in 9.353465532s


• [SLOW TEST:19.720 seconds]
[k8s.io] Variable Expansion
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:35.339: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-b1f663f6-a165-48ad-92d1-ba6186c67e2d
STEP: Creating a pod to test consume secrets
Mar  2 22:17:35.529: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356" in namespace "projected-5302" to be "success or failure"
Mar  2 22:17:35.549: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Pending", Reason="", readiness=false. Elapsed: 20.191269ms
Mar  2 22:17:37.572: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042869891s
Mar  2 22:17:39.593: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064183654s
Mar  2 22:17:41.614: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.085167503s
Mar  2 22:17:43.636: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106943826s
Mar  2 22:17:45.658: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.128668746s
STEP: Saw pod success
Mar  2 22:17:45.658: INFO: Pod "pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356" satisfied condition "success or failure"
Mar  2 22:17:45.678: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 22:17:45.730: INFO: Waiting for pod pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356 to disappear
Mar  2 22:17:45.750: INFO: Pod pod-projected-secrets-3676b727-06cb-4f7b-a9b4-165826d38356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:45.751: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5302" for this suite.
Mar  2 22:17:51.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:17:55.239: INFO: namespace projected-5302 deletion completed in 9.39406396s


• [SLOW TEST:19.900 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:37.523: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:17:38.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:40.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:42.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:44.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:46.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784257, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:17:49.311: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:17:49.326: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:50.132: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2992" for this suite.
Mar  2 22:17:58.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:01.550: INFO: namespace webhook-2992 deletion completed in 11.353867344s
STEP: Destroying namespace "webhook-2992-markers" for this suite.
Mar  2 22:18:07.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:10.904: INFO: namespace webhook-2992-markers deletion completed in 9.354068495s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:33.436 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:28.544: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:17:28.670: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 22:17:33.683: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 22:17:39.711: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 22:17:41.724: INFO: Creating deployment "test-rollover-deployment"
Mar  2 22:17:41.752: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 22:17:43.781: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 22:17:43.808: INFO: Ensure that both replica sets have 1 created replica
Mar  2 22:17:43.834: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 22:17:43.862: INFO: Updating deployment test-rollover-deployment
Mar  2 22:17:43.862: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 22:17:45.889: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 22:17:45.916: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 22:17:45.942: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:45.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784263, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:47.969: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:47.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784263, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:49.970: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:49.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784263, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:51.971: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:51.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784263, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:53.969: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:53.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784272, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:55.969: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:55.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784272, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:57.969: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:57.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784272, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:17:59.968: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:17:59.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784272, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:01.969: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 22:18:01.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784272, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784261, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:03.970: INFO: 
Mar  2 22:18:03.970: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  2 22:18:04.010: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3471 /apis/apps/v1/namespaces/deployment-3471/deployments/test-rollover-deployment f43e2d0d-ae26-4a80-bbf0-b5042996a0a2 15456173 2 2020-03-02 22:17:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003163c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-02 22:17:41 +0000 UTC,LastTransitionTime:2020-03-02 22:17:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-03-02 22:18:02 +0000 UTC,LastTransitionTime:2020-03-02 22:17:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 22:18:04.024: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-3471 /apis/apps/v1/namespaces/deployment-3471/replicasets/test-rollover-deployment-7d7dc6548c 1eacbc0f-38cf-426e-bad0-245678f2d1df 15456161 2 2020-03-02 22:17:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f43e2d0d-ae26-4a80-bbf0-b5042996a0a2 0xc00605eba7 0xc00605eba8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00605ec08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:18:04.024: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 22:18:04.025: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3471 /apis/apps/v1/namespaces/deployment-3471/replicasets/test-rollover-controller 12f8fbab-59ed-405e-853e-38cdf83d6762 15456172 2 2020-03-02 22:17:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f43e2d0d-ae26-4a80-bbf0-b5042996a0a2 0xc00605ead7 0xc00605ead8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00605eb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:18:04.025: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3471 /apis/apps/v1/namespaces/deployment-3471/replicasets/test-rollover-deployment-f6c94f66c 1631d6bf-bd12-419a-bb26-21e84851daa0 15455881 2 2020-03-02 22:17:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f43e2d0d-ae26-4a80-bbf0-b5042996a0a2 0xc00605ec70 0xc00605ec71}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00605ece8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 22:18:04.039: INFO: Pod "test-rollover-deployment-7d7dc6548c-q94d5" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-q94d5 test-rollover-deployment-7d7dc6548c- deployment-3471 /api/v1/namespaces/deployment-3471/pods/test-rollover-deployment-7d7dc6548c-q94d5 5433b733-a1ec-49ea-b286-6ad12379b8e8 15456012 0 2020-03-02 22:17:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.106"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 1eacbc0f-38cf-426e-bad0-245678f2d1df 0xc00605f257 0xc00605f258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fpqm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fpqm6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fpqm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k4zdw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:17:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.106,StartTime:2020-03-02 22:17:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:17:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://9f57c30d00442aa2ab6512dba6c0815da72903932f64a5e4acf26b2bda062e20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:04.039: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-3471" for this suite.
Mar  2 22:18:10.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:13.493: INFO: namespace deployment-3471 deletion completed in 9.353851245s


• [SLOW TEST:44.949 seconds]
[sig-apps] Deployment
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:16.147: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:17:26.368: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-2335" for this suite.
Mar  2 22:18:12.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:15.858: INFO: namespace kubelet-test-2335 deletion completed in 49.381347712s


• [SLOW TEST:59.711 seconds]
[k8s.io] Kubelet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:15.888: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:18:16.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b" in namespace "projected-5332" to be "success or failure"
Mar  2 22:18:16.148: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.331171ms
Mar  2 22:18:18.168: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044417131s
Mar  2 22:18:20.188: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064278822s
Mar  2 22:18:22.208: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0837005s
Mar  2 22:18:24.227: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.10320768s
STEP: Saw pod success
Mar  2 22:18:24.227: INFO: Pod "downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b" satisfied condition "success or failure"
Mar  2 22:18:24.247: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b container client-container: <nil>
STEP: delete the pod
Mar  2 22:18:24.307: INFO: Waiting for pod downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b to disappear
Mar  2 22:18:24.326: INFO: Pod downwardapi-volume-cf5d8ef1-12c8-4c25-b428-1a60d52e935b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:24.326: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5332" for this suite.
Mar  2 22:18:30.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:33.861: INFO: namespace projected-5332 deletion completed in 9.380008569s


• [SLOW TEST:17.973 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:17:55.301: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  2 22:18:15.731: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:18:15.752: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:18:17.753: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:18:17.774: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 22:18:19.753: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 22:18:19.774: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:19.799: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-148" for this suite.
Mar  2 22:18:47.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:51.582: INFO: namespace container-lifecycle-hook-148 deletion completed in 31.670727466s


• [SLOW TEST:56.281 seconds]
[k8s.io] Container Lifecycle Hook
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:13.534: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-gqsb
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:18:13.706: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gqsb" in namespace "subpath-6202" to be "success or failure"
Mar  2 22:18:13.719: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.254161ms
Mar  2 22:18:15.733: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026570509s
Mar  2 22:18:17.747: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040987089s
Mar  2 22:18:19.761: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054948802s
Mar  2 22:18:21.775: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069147908s
Mar  2 22:18:23.790: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 10.084115196s
Mar  2 22:18:25.804: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 12.098076535s
Mar  2 22:18:27.818: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 14.111589653s
Mar  2 22:18:29.834: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 16.128161338s
Mar  2 22:18:31.848: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 18.141747707s
Mar  2 22:18:33.862: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 20.155529211s
Mar  2 22:18:35.876: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 22.169721709s
Mar  2 22:18:37.889: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 24.183189836s
Mar  2 22:18:39.903: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 26.197114149s
Mar  2 22:18:41.919: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Running", Reason="", readiness=true. Elapsed: 28.212551672s
Mar  2 22:18:43.932: INFO: Pod "pod-subpath-test-secret-gqsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.226208109s
STEP: Saw pod success
Mar  2 22:18:43.933: INFO: Pod "pod-subpath-test-secret-gqsb" satisfied condition "success or failure"
Mar  2 22:18:43.946: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-subpath-test-secret-gqsb container test-container-subpath-secret-gqsb: <nil>
STEP: delete the pod
Mar  2 22:18:43.982: INFO: Waiting for pod pod-subpath-test-secret-gqsb to disappear
Mar  2 22:18:43.994: INFO: Pod pod-subpath-test-secret-gqsb no longer exists
STEP: Deleting pod pod-subpath-test-secret-gqsb
Mar  2 22:18:43.995: INFO: Deleting pod "pod-subpath-test-secret-gqsb" in namespace "subpath-6202"
[AfterEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:44.007: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-6202" for this suite.
Mar  2 22:18:50.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:53.483: INFO: namespace subpath-6202 deletion completed in 9.375356551s


• [SLOW TEST:39.949 seconds]
[sig-storage] Subpath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected combined
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:33.887: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-d3d86ddc-6430-4010-923c-ea9e8e7f4b98
STEP: Creating secret with name secret-projected-all-test-volume-f17ca504-9086-4ce0-9706-775a3cd30cec
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  2 22:18:34.107: INFO: Waiting up to 5m0s for pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436" in namespace "projected-9546" to be "success or failure"
Mar  2 22:18:34.128: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Pending", Reason="", readiness=false. Elapsed: 20.280559ms
Mar  2 22:18:36.147: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039993083s
Mar  2 22:18:38.167: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059926151s
Mar  2 22:18:40.188: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080703868s
Mar  2 22:18:42.208: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100611018s
Mar  2 22:18:44.228: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121017405s
STEP: Saw pod success
Mar  2 22:18:44.229: INFO: Pod "projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436" satisfied condition "success or failure"
Mar  2 22:18:44.249: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  2 22:18:44.299: INFO: Waiting for pod projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436 to disappear
Mar  2 22:18:44.318: INFO: Pod projected-volume-09993c5e-d029-49d2-90c1-76c74af8a436 no longer exists
[AfterEach] [sig-storage] Projected combined
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:44.319: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9546" for this suite.
Mar  2 22:18:50.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:18:54.089: INFO: namespace projected-9546 deletion completed in 9.683490495s


• [SLOW TEST:20.202 seconds]
[sig-storage] Projected combined
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:10.968: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  2 22:18:21.670: INFO: Successfully updated pod "adopt-release-dlj6b"
STEP: Checking that the Job readopts the Pod
Mar  2 22:18:21.671: INFO: Waiting up to 15m0s for pod "adopt-release-dlj6b" in namespace "job-5559" to be "adopted"
Mar  2 22:18:21.684: INFO: Pod "adopt-release-dlj6b": Phase="Running", Reason="", readiness=true. Elapsed: 12.958674ms
Mar  2 22:18:23.697: INFO: Pod "adopt-release-dlj6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.02620892s
Mar  2 22:18:23.697: INFO: Pod "adopt-release-dlj6b" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  2 22:18:24.230: INFO: Successfully updated pod "adopt-release-dlj6b"
STEP: Checking that the Job releases the Pod
Mar  2 22:18:24.230: INFO: Waiting up to 15m0s for pod "adopt-release-dlj6b" in namespace "job-5559" to be "released"
Mar  2 22:18:24.243: INFO: Pod "adopt-release-dlj6b": Phase="Running", Reason="", readiness=true. Elapsed: 12.927683ms
Mar  2 22:18:24.243: INFO: Pod "adopt-release-dlj6b" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:18:24.243: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-5559" for this suite.
Mar  2 22:19:04.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:07.804: INFO: namespace job-5559 deletion completed in 43.486259268s


• [SLOW TEST:56.836 seconds]
[sig-apps] Job
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:51.605: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:18:52.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:54.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:56.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:58.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:19:00.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784331, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:19:03.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:03.460: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2960" for this suite.
Mar  2 22:19:09.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:13.174: INFO: namespace webhook-2960 deletion completed in 9.398966128s
STEP: Destroying namespace "webhook-2960-markers" for this suite.
Mar  2 22:19:19.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:22.615: INFO: namespace webhook-2960-markers deletion completed in 9.440543482s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:31.097 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:53.554: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Mar  2 22:18:54.319: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:18:54.592: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 22:18:56.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:58.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:19:00.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:19:02.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:19:05.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:05.813: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4527" for this suite.
Mar  2 22:19:11.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:15.285: INFO: namespace webhook-4527 deletion completed in 9.362305377s
STEP: Destroying namespace "webhook-4527-markers" for this suite.
Mar  2 22:19:21.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:24.641: INFO: namespace webhook-4527-markers deletion completed in 9.355747266s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:31.144 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:18:54.095: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Mar  2 22:18:54.964: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 22:18:55.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:57.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:18:59.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:19:01.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 22:19:03.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784334, loc:(*time.Location)(0x84b02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 22:19:06.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:06.781: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-744" for this suite.
Mar  2 22:19:12.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:16.280: INFO: namespace webhook-744 deletion completed in 9.378422747s
STEP: Destroying namespace "webhook-744-markers" for this suite.
Mar  2 22:19:22.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:25.683: INFO: namespace webhook-744-markers deletion completed in 9.403180102s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


• [SLOW TEST:31.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:24.709: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:19:24.837: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  2 22:19:25.979: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:27.008: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-140" for this suite.
Mar  2 22:19:33.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:36.691: INFO: namespace replication-controller-140 deletion completed in 9.553717202s


• [SLOW TEST:11.982 seconds]
[sig-apps] ReplicationController
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:25.794: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  2 22:19:27.087: INFO: Waiting up to 5m0s for pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd" in namespace "downward-api-3165" to be "success or failure"
Mar  2 22:19:27.106: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800531ms
Mar  2 22:19:29.127: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039421913s
Mar  2 22:19:31.149: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061133464s
Mar  2 22:19:33.169: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081712283s
Mar  2 22:19:35.190: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.102116575s
Mar  2 22:19:37.210: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.122030793s
STEP: Saw pod success
Mar  2 22:19:37.210: INFO: Pod "downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd" satisfied condition "success or failure"
Mar  2 22:19:37.302: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:19:37.351: INFO: Waiting for pod downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd to disappear
Mar  2 22:19:37.370: INFO: Pod downward-api-c70375d8-b1f3-4cc6-a6e6-3ae1545a14bd no longer exists
[AfterEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:37.371: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3165" for this suite.
Mar  2 22:19:43.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:47.006: INFO: namespace downward-api-3165 deletion completed in 9.482555193s


• [SLOW TEST:21.212 seconds]
[sig-node] Downward API
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:36.720: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:41.912: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-1670" for this suite.
Mar  2 22:19:48.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:19:51.384: INFO: namespace watch-1670 deletion completed in 9.38268577s


• [SLOW TEST:14.664 seconds]
[sig-api-machinery] Watchers
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:22.720: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  2 22:19:22.912: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
Mar  2 22:19:28.889: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:19:52.172: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5984" for this suite.
Mar  2 22:19:58.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:20:01.670: INFO: namespace crd-publish-openapi-5984 deletion completed in 9.386058024s


• [SLOW TEST:38.951 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:51.402: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:20:07.738: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-3388" for this suite.
Mar  2 22:20:13.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:20:17.212: INFO: namespace resourcequota-3388 deletion completed in 9.354041896s


• [SLOW TEST:25.811 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:20:01.705: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:20:01.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c" in namespace "downward-api-4783" to be "success or failure"
Mar  2 22:20:01.885: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.482738ms
Mar  2 22:20:03.907: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043223481s
Mar  2 22:20:05.928: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064254749s
Mar  2 22:20:07.950: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086692422s
Mar  2 22:20:09.972: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108172665s
Mar  2 22:20:11.993: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.129330821s
Mar  2 22:20:14.014: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.150337659s
STEP: Saw pod success
Mar  2 22:20:14.014: INFO: Pod "downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c" satisfied condition "success or failure"
Mar  2 22:20:14.035: INFO: Trying to get logs from node ip-10-0-137-145.ec2.internal pod downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c container client-container: <nil>
STEP: delete the pod
Mar  2 22:20:14.102: INFO: Waiting for pod downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c to disappear
Mar  2 22:20:14.122: INFO: Pod downwardapi-volume-6d75ded2-36d3-46a8-a4cc-9e77b0a8113c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:20:14.123: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4783" for this suite.
Mar  2 22:20:20.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:20:23.602: INFO: namespace downward-api-4783 deletion completed in 9.38536375s


• [SLOW TEST:21.897 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:20:23.612: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  2 22:20:23.806: INFO: Waiting up to 5m0s for pod "pod-872c2950-087c-4103-b7e9-d22911b581e6" in namespace "emptydir-6196" to be "success or failure"
Mar  2 22:20:23.826: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.890235ms
Mar  2 22:20:25.847: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041007832s
Mar  2 22:20:27.869: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062186187s
Mar  2 22:20:29.890: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083725078s
Mar  2 22:20:31.913: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.10620288s
STEP: Saw pod success
Mar  2 22:20:31.913: INFO: Pod "pod-872c2950-087c-4103-b7e9-d22911b581e6" satisfied condition "success or failure"
Mar  2 22:20:31.933: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-872c2950-087c-4103-b7e9-d22911b581e6 container test-container: <nil>
STEP: delete the pod
Mar  2 22:20:31.991: INFO: Waiting for pod pod-872c2950-087c-4103-b7e9-d22911b581e6 to disappear
Mar  2 22:20:32.011: INFO: Pod pod-872c2950-087c-4103-b7e9-d22911b581e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:20:32.012: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6196" for this suite.
Mar  2 22:20:38.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:20:41.582: INFO: namespace emptydir-6196 deletion completed in 9.386364222s


• [SLOW TEST:17.971 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:20:41.587: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-tr4r
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 22:20:41.815: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tr4r" in namespace "subpath-3637" to be "success or failure"
Mar  2 22:20:41.834: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Pending", Reason="", readiness=false. Elapsed: 19.504927ms
Mar  2 22:20:43.856: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041460714s
Mar  2 22:20:45.878: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062817333s
Mar  2 22:20:47.899: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084316229s
Mar  2 22:20:49.922: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106794886s
Mar  2 22:20:51.943: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 10.128147648s
Mar  2 22:20:53.965: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 12.149798405s
Mar  2 22:20:55.986: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 14.171487757s
Mar  2 22:20:58.008: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 16.192758588s
Mar  2 22:21:00.029: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 18.213880168s
Mar  2 22:21:02.051: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 20.235834961s
Mar  2 22:21:04.072: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 22.257028545s
Mar  2 22:21:06.093: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 24.277928754s
Mar  2 22:21:08.119: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Running", Reason="", readiness=true. Elapsed: 26.30445358s
Mar  2 22:21:10.141: INFO: Pod "pod-subpath-test-configmap-tr4r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.326397084s
STEP: Saw pod success
Mar  2 22:21:10.141: INFO: Pod "pod-subpath-test-configmap-tr4r" satisfied condition "success or failure"
Mar  2 22:21:10.162: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-subpath-test-configmap-tr4r container test-container-subpath-configmap-tr4r: <nil>
STEP: delete the pod
Mar  2 22:21:10.214: INFO: Waiting for pod pod-subpath-test-configmap-tr4r to disappear
Mar  2 22:21:10.234: INFO: Pod pod-subpath-test-configmap-tr4r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tr4r
Mar  2 22:21:10.234: INFO: Deleting pod "pod-subpath-test-configmap-tr4r" in namespace "subpath-3637"
[AfterEach] [sig-storage] Subpath
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:10.255: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-3637" for this suite.
Mar  2 22:21:16.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:19.814: INFO: namespace subpath-3637 deletion completed in 9.391843328s


• [SLOW TEST:38.227 seconds]
[sig-storage] Subpath
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Events
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:20:17.224: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  2 22:20:27.416: INFO: &Pod{ObjectMeta:{send-events-602844a9-2f59-444b-a4f5-d226896e3468  events-493 /api/v1/namespaces/events-493/pods/send-events-602844a9-2f59-444b-a4f5-d226896e3468 b65c6b6c-50a4-4895-9c46-a232e483170a 15459083 0 2020-03-02 22:20:16 +0000 UTC <nil> <nil> map[name:foo time:339592369] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.125"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sds7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sds7z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sds7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-27.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c86,c65,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ktpmd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:20:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:20:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-02 22:20:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.27,PodIP:10.128.3.125,StartTime:2020-03-02 22:20:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-02 22:20:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://c56fae744bc1db0524de421499f8a3b2e3f5c5668b844475b4f5c6dff8079ed0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  2 22:20:29.430: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  2 22:20:31.444: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:20:31.462: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-493" for this suite.
Mar  2 22:21:17.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:20.897: INFO: namespace events-493 deletion completed in 49.356857018s


• [SLOW TEST:63.674 seconds]
[k8s.io] [sig-node] Events
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:07.815: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9032
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar  2 22:19:07.966: INFO: Found 0 stateful pods, waiting for 3
Mar  2 22:19:17.981: INFO: Found 2 stateful pods, waiting for 3
Mar  2 22:19:27.980: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:27.980: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:27.980: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:19:37.979: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:37.979: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:37.979: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 22:19:38.075: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  2 22:19:38.140: INFO: Updating stateful set ss2
Mar  2 22:19:38.165: INFO: Waiting for Pod statefulset-9032/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  2 22:19:48.252: INFO: Found 2 stateful pods, waiting for 3
Mar  2 22:19:58.266: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:58.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:19:58.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:20:08.266: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:20:08.266: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:20:08.266: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:20:18.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:20:18.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:20:18.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  2 22:20:18.332: INFO: Updating stateful set ss2
Mar  2 22:20:18.374: INFO: Waiting for Pod statefulset-9032/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:20:28.447: INFO: Updating stateful set ss2
Mar  2 22:20:28.481: INFO: Waiting for StatefulSet statefulset-9032/ss2 to complete update
Mar  2 22:20:28.482: INFO: Waiting for Pod statefulset-9032/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:20:38.518: INFO: Waiting for StatefulSet statefulset-9032/ss2 to complete update
Mar  2 22:20:38.518: INFO: Waiting for Pod statefulset-9032/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:20:48.517: INFO: Waiting for StatefulSet statefulset-9032/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 22:20:58.509: INFO: Deleting all statefulset in ns statefulset-9032
Mar  2 22:20:58.522: INFO: Scaling statefulset ss2 to 0
Mar  2 22:21:18.577: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:21:18.590: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:18.642: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-9032" for this suite.
Mar  2 22:21:24.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:28.121: INFO: namespace statefulset-9032 deletion completed in 9.351582848s


• [SLOW TEST:140.307 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:19:47.032: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:19:47.260: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-30177e74-594e-46c4-b8dc-3368632298df
STEP: Creating secret with name s-test-opt-upd-d9296976-29f5-4210-b532-605656530806
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-30177e74-594e-46c4-b8dc-3368632298df
STEP: Updating secret s-test-opt-upd-d9296976-29f5-4210-b532-605656530806
STEP: Creating secret with name s-test-opt-create-ab19d405-b10d-40ba-b032-02a12e3943d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:14.477: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1761" for this suite.
Mar  2 22:21:26.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:29.961: INFO: namespace projected-1761 deletion completed in 15.37783412s


• [SLOW TEST:102.929 seconds]
[sig-storage] Projected secret
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:20.958: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:21:21.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6" in namespace "downward-api-7692" to be "success or failure"
Mar  2 22:21:21.197: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.849518ms
Mar  2 22:21:23.212: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039415205s
Mar  2 22:21:25.226: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05390441s
Mar  2 22:21:27.240: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067613467s
Mar  2 22:21:29.253: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080822751s
Mar  2 22:21:31.267: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.094540546s
STEP: Saw pod success
Mar  2 22:21:31.267: INFO: Pod "downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6" satisfied condition "success or failure"
Mar  2 22:21:31.280: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6 container client-container: <nil>
STEP: delete the pod
Mar  2 22:21:31.317: INFO: Waiting for pod downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6 to disappear
Mar  2 22:21:31.332: INFO: Pod downwardapi-volume-d96206c9-414b-460c-980f-c4b08c1bccf6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:31.333: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7692" for this suite.
Mar  2 22:21:37.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:40.819: INFO: namespace downward-api-7692 deletion completed in 9.35586385s


• [SLOW TEST:19.861 seconds]
[sig-storage] Downward API volume
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:28.143: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-583c7a70-97af-4345-be0f-1c37438b0c63
STEP: Creating a pod to test consume configMaps
Mar  2 22:21:28.302: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de" in namespace "configmap-8647" to be "success or failure"
Mar  2 22:21:28.326: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Pending", Reason="", readiness=false. Elapsed: 23.947491ms
Mar  2 22:21:30.340: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037711513s
Mar  2 22:21:32.353: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05102079s
Mar  2 22:21:34.367: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064365369s
Mar  2 22:21:36.380: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077861031s
Mar  2 22:21:38.393: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.090978557s
STEP: Saw pod success
Mar  2 22:21:38.394: INFO: Pod "pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de" satisfied condition "success or failure"
Mar  2 22:21:38.406: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:21:38.492: INFO: Waiting for pod pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de to disappear
Mar  2 22:21:38.545: INFO: Pod pod-configmaps-fa2dced6-cba5-4aeb-9817-673c3d6242de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:38.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8647" for this suite.
Mar  2 22:21:44.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:48.041: INFO: namespace configmap-8647 deletion completed in 9.351311162s


• [SLOW TEST:19.899 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:40.843: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:21:40.947: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig version'
Mar  2 22:21:41.216: INFO: stderr: ""
Mar  2 22:21:41.216: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.3-beta.0.38+c3aac8e00076fd\", GitCommit:\"c3aac8e00076fd2b7c1af2dc221f1c452e868de4\", GitTreeState:\"clean\", BuildDate:\"2020-03-02T21:26:18Z\", GoVersion:\"go1.12.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2\", GitCommit:\"4320e48\", GitTreeState:\"clean\", BuildDate:\"2020-01-21T19:50:59Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:41.216: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2876" for this suite.
Mar  2 22:21:47.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:21:50.722: INFO: namespace kubectl-2876 deletion completed in 9.354910839s


• [SLOW TEST:9.880 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:48.064: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  2 22:21:48.206: INFO: Waiting up to 5m0s for pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6" in namespace "downward-api-3057" to be "success or failure"
Mar  2 22:21:48.219: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.565258ms
Mar  2 22:21:50.232: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025828185s
Mar  2 22:21:52.246: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039018427s
Mar  2 22:21:54.259: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052551197s
Mar  2 22:21:56.273: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065937577s
Mar  2 22:21:58.286: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.079549483s
STEP: Saw pod success
Mar  2 22:21:58.286: INFO: Pod "downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6" satisfied condition "success or failure"
Mar  2 22:21:58.299: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:21:58.335: INFO: Waiting for pod downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6 to disappear
Mar  2 22:21:58.347: INFO: Pod downward-api-f30d4957-f8fd-4a61-b352-601d3af529a6 no longer exists
[AfterEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:58.347: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3057" for this suite.
Mar  2 22:22:04.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:07.823: INFO: namespace downward-api-3057 deletion completed in 9.35313611s


• [SLOW TEST:19.759 seconds]
[sig-node] Downward API
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:50.750: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:21:50.858: INFO: Creating ReplicaSet my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67
Mar  2 22:21:50.903: INFO: Pod name my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67: Found 1 pods out of 1
Mar  2 22:21:50.903: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67" is running
Mar  2 22:22:01.083: INFO: Pod "my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67-bwksx" is running (conditions: [])
Mar  2 22:22:01.083: INFO: Trying to dial the pod
Mar  2 22:22:06.166: INFO: Controller my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67: Got expected result from replica 1 [my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67-bwksx]: "my-hostname-basic-99046eff-c94b-4a30-99a3-660104334a67-bwksx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:22:06.166: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-3139" for this suite.
Mar  2 22:22:12.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:15.618: INFO: namespace replicaset-3139 deletion completed in 9.353500267s


• [SLOW TEST:24.869 seconds]
[sig-apps] ReplicaSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:19.849: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:21:30.109: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-8663" for this suite.
Mar  2 22:22:18.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:21.733: INFO: namespace kubelet-test-8663 deletion completed in 51.385840703s


• [SLOW TEST:61.884 seconds]
[k8s.io] Kubelet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:21.747: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Mar  2 22:22:21.880: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig --namespace=kubectl-6078 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  2 22:22:30.908: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  2 22:22:30.908: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:22:32.949: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6078" for this suite.
Mar  2 22:22:39.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:42.482: INFO: namespace kubectl-6078 deletion completed in 9.38594703s


• [SLOW TEST:20.736 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:07.846: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:22:08.095: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-442aebb0-97d5-418c-9ff9-a0acf265b889
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:22:18.200: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2246" for this suite.
Mar  2 22:22:40.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:43.624: INFO: namespace configmap-2246 deletion completed in 25.351284825s


• [SLOW TEST:35.778 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:15.654: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar  2 22:22:15.787: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig create -f - --namespace=kubectl-7736'
Mar  2 22:22:16.903: INFO: stderr: ""
Mar  2 22:22:16.903: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  2 22:22:17.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:17.917: INFO: Found 0 / 1
Mar  2 22:22:18.922: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:18.922: INFO: Found 0 / 1
Mar  2 22:22:19.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:19.917: INFO: Found 0 / 1
Mar  2 22:22:20.927: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:20.927: INFO: Found 0 / 1
Mar  2 22:22:21.921: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:21.921: INFO: Found 0 / 1
Mar  2 22:22:22.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:22.917: INFO: Found 0 / 1
Mar  2 22:22:23.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:23.917: INFO: Found 0 / 1
Mar  2 22:22:24.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:24.917: INFO: Found 0 / 1
Mar  2 22:22:25.917: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:25.917: INFO: Found 1 / 1
Mar  2 22:22:25.917: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  2 22:22:25.930: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:25.931: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 22:22:25.931: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig patch pod redis-master-gf8rp --namespace=kubectl-7736 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 22:22:26.100: INFO: stderr: ""
Mar  2 22:22:26.100: INFO: stdout: "pod/redis-master-gf8rp patched\n"
STEP: checking annotations
Mar  2 22:22:26.114: INFO: Selector matched 1 pods for map[app:redis]
Mar  2 22:22:26.114: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:22:26.114: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-7736" for this suite.
Mar  2 22:22:54.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:22:57.567: INFO: namespace kubectl-7736 deletion completed in 31.354364123s


• [SLOW TEST:41.913 seconds]
[sig-cli] Kubectl client
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:43.649: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-7a8be23f-1a3b-4f50-97c6-02d9feb98aae
STEP: Creating a pod to test consume configMaps
Mar  2 22:22:43.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa" in namespace "configmap-258" to be "success or failure"
Mar  2 22:22:43.993: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026268ms
Mar  2 22:22:46.007: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026222293s
Mar  2 22:22:48.021: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040561714s
Mar  2 22:22:50.034: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053692445s
Mar  2 22:22:52.050: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068832788s
Mar  2 22:22:54.063: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.082345518s
STEP: Saw pod success
Mar  2 22:22:54.063: INFO: Pod "pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa" satisfied condition "success or failure"
Mar  2 22:22:54.076: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:22:54.112: INFO: Waiting for pod pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa to disappear
Mar  2 22:22:54.125: INFO: Pod pod-configmaps-6d04385a-8701-4b07-b58f-a8f5134a37fa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:22:54.125: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-258" for this suite.
Mar  2 22:23:00.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:03.570: INFO: namespace configmap-258 deletion completed in 9.352747857s


• [SLOW TEST:19.921 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:03.614: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:19.849: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5748" for this suite.
Mar  2 22:23:25.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:29.288: INFO: namespace resourcequota-5748 deletion completed in 9.353850001s


• [SLOW TEST:25.675 seconds]
[sig-api-machinery] ResourceQuota
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:57.576: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  2 22:23:17.832: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:23:17.845: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:23:19.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:23:19.907: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:23:21.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:23:21.860: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:23:23.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:23:23.861: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 22:23:25.846: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 22:23:25.860: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:25.876: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4260" for this suite.
Mar  2 22:23:38.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:41.351: INFO: namespace container-lifecycle-hook-4260 deletion completed in 15.355367298s


• [SLOW TEST:43.776 seconds]
[k8s.io] Container Lifecycle Hook
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:29.302: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 22:23:37.591: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:37.624: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-5792" for this suite.
Mar  2 22:23:43.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:47.083: INFO: namespace container-runtime-5792 deletion completed in 9.354025901s


• [SLOW TEST:17.781 seconds]
[k8s.io] Container Runtime
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:47.117: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-a2852e74-fc36-422c-bc19-c33f75a7f533
[AfterEach] [sig-api-machinery] Secrets
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:47.231: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3392" for this suite.
Mar  2 22:23:53.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:57.505: INFO: namespace secrets-3392 deletion completed in 10.18074167s


• [SLOW TEST:10.388 seconds]
[sig-api-machinery] Secrets
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:41.359: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 22:23:41.487: INFO: Waiting up to 5m0s for pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa" in namespace "emptydir-5940" to be "success or failure"
Mar  2 22:23:41.501: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.51685ms
Mar  2 22:23:43.514: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027182203s
Mar  2 22:23:45.528: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040467602s
Mar  2 22:23:47.542: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054922461s
Mar  2 22:23:49.557: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069239068s
STEP: Saw pod success
Mar  2 22:23:49.557: INFO: Pod "pod-a357cd03-870f-48b7-afca-3b4881c267aa" satisfied condition "success or failure"
Mar  2 22:23:49.570: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-a357cd03-870f-48b7-afca-3b4881c267aa container test-container: <nil>
STEP: delete the pod
Mar  2 22:23:49.607: INFO: Waiting for pod pod-a357cd03-870f-48b7-afca-3b4881c267aa to disappear
Mar  2 22:23:49.620: INFO: Pod pod-a357cd03-870f-48b7-afca-3b4881c267aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:49.620: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5940" for this suite.
Mar  2 22:23:55.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:23:59.303: INFO: namespace emptydir-5940 deletion completed in 9.605583447s


• [SLOW TEST:17.945 seconds]
[sig-storage] EmptyDir volumes
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:59.316: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:23:59.939: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-4401" for this suite.
Mar  2 22:24:07.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:10.641: INFO: namespace kubelet-test-4401 deletion completed in 9.441791306s


• [SLOW TEST:11.325 seconds]
[k8s.io] Kubelet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:23:57.518: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-eb61e0c9-1e2d-4a60-8896-0075314709fc
STEP: Creating a pod to test consume configMaps
Mar  2 22:23:57.655: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251" in namespace "configmap-737" to be "success or failure"
Mar  2 22:23:57.668: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Pending", Reason="", readiness=false. Elapsed: 12.952351ms
Mar  2 22:23:59.681: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026435824s
Mar  2 22:24:01.695: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039844171s
Mar  2 22:24:03.709: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054264134s
Mar  2 22:24:05.722: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06752671s
Mar  2 22:24:07.736: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.081117601s
STEP: Saw pod success
Mar  2 22:24:07.736: INFO: Pod "pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251" satisfied condition "success or failure"
Mar  2 22:24:07.749: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:24:07.784: INFO: Waiting for pod pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251 to disappear
Mar  2 22:24:07.797: INFO: Pod pod-configmaps-c1bb70e2-c403-4cd6-a840-2cd0e0a23251 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:07.797: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-737" for this suite.
Mar  2 22:24:14.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:17.761: INFO: namespace configmap-737 deletion completed in 9.353245395s


• [SLOW TEST:20.243 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:22:42.490: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  2 22:22:42.762: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-e0cc16ae-bb2c-480b-97d1-3f29e84262ae
STEP: Creating configMap with name cm-test-opt-upd-926099c8-4a7c-45a9-bd06-3e09047619b4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e0cc16ae-bb2c-480b-97d1-3f29e84262ae
STEP: Updating configmap cm-test-opt-upd-926099c8-4a7c-45a9-bd06-3e09047619b4
STEP: Creating configMap with name cm-test-opt-create-74b450a8-9145-469f-a18c-bc3df0bc5505
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:12.899: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3411" for this suite.
Mar  2 22:24:25.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:28.703: INFO: namespace configmap-3411 deletion completed in 15.386294732s


• [SLOW TEST:106.214 seconds]
[sig-storage] ConfigMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:10.649: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-84d79257-8bba-41ea-8ae4-fa63ea735f0d
STEP: Creating a pod to test consume configMaps
Mar  2 22:24:10.811: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522" in namespace "projected-9564" to be "success or failure"
Mar  2 22:24:10.825: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723173ms
Mar  2 22:24:12.843: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031683273s
Mar  2 22:24:14.857: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045498751s
Mar  2 22:24:16.870: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059084619s
Mar  2 22:24:18.884: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Pending", Reason="", readiness=false. Elapsed: 8.073126072s
Mar  2 22:24:20.898: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.087367491s
STEP: Saw pod success
Mar  2 22:24:20.899: INFO: Pod "pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522" satisfied condition "success or failure"
Mar  2 22:24:20.911: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 22:24:21.077: INFO: Waiting for pod pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522 to disappear
Mar  2 22:24:21.092: INFO: Pod pod-projected-configmaps-c1b21e33-e14a-45b2-9d22-042f2e34a522 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:21.092: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9564" for this suite.
Mar  2 22:24:27.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:30.580: INFO: namespace projected-9564 deletion completed in 9.356011257s


• [SLOW TEST:19.931 seconds]
[sig-storage] Projected configMap
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:28.789: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  2 22:24:28.938: INFO: Waiting up to 5m0s for pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745" in namespace "downward-api-3718" to be "success or failure"
Mar  2 22:24:28.959: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Pending", Reason="", readiness=false. Elapsed: 20.547603ms
Mar  2 22:24:30.980: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041882493s
Mar  2 22:24:33.000: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062412721s
Mar  2 22:24:35.022: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083941475s
Mar  2 22:24:37.043: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105211995s
Mar  2 22:24:39.064: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.12611387s
STEP: Saw pod success
Mar  2 22:24:39.064: INFO: Pod "downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745" satisfied condition "success or failure"
Mar  2 22:24:39.085: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745 container dapi-container: <nil>
STEP: delete the pod
Mar  2 22:24:39.136: INFO: Waiting for pod downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745 to disappear
Mar  2 22:24:39.158: INFO: Pod downward-api-c5d4e598-a536-4749-9e82-e3ca94c6e745 no longer exists
[AfterEach] [sig-node] Downward API
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:39.158: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3718" for this suite.
Mar  2 22:24:45.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:49.068: INFO: namespace downward-api-3718 deletion completed in 9.779937897s


• [SLOW TEST:20.347 seconds]
[sig-node] Downward API
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:17.795: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:28.014: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-249" for this suite.
Mar  2 22:24:56.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:24:59.511: INFO: namespace replication-controller-249 deletion completed in 31.352428775s


• [SLOW TEST:41.716 seconds]
[sig-apps] ReplicationController
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:49.427: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  2 22:24:49.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7" in namespace "projected-8669" to be "success or failure"
Mar  2 22:24:49.718: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.652619ms
Mar  2 22:24:51.739: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041857885s
Mar  2 22:24:53.762: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064046962s
Mar  2 22:24:55.782: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084776198s
Mar  2 22:24:57.803: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105408865s
Mar  2 22:24:59.824: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.126695455s
STEP: Saw pod success
Mar  2 22:24:59.824: INFO: Pod "downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7" satisfied condition "success or failure"
Mar  2 22:24:59.845: INFO: Trying to get logs from node ip-10-0-130-27.ec2.internal pod downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7 container client-container: <nil>
STEP: delete the pod
Mar  2 22:24:59.927: INFO: Waiting for pod downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7 to disappear
Mar  2 22:24:59.948: INFO: Pod downwardapi-volume-d5497448-6f07-4e1f-8b49-0a64f3b11fc7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:24:59.948: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8669" for this suite.
Mar  2 22:25:06.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:25:09.589: INFO: namespace projected-8669 deletion completed in 9.385831409s


• [SLOW TEST:20.210 seconds]
[sig-storage] Projected downwardAPI
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:59.547: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  2 22:24:59.661: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 22:25:51.312: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-003d5aaa-1b16-445b-b79c-16e80f059d00", GenerateName:"", Namespace:"init-container-4831", SelfLink:"/api/v1/namespaces/init-container-4831/pods/pod-init-003d5aaa-1b16-445b-b79c-16e80f059d00", UID:"822e2285-8585-4949-b330-ea0c363904e2", ResourceVersion:"15463760", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718784699, loc:(*time.Location)(0x84b02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"661237748"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.3.153\"\n    ],\n    \"dns\": {},\n    \"default-route\": [\n        \"10.128.2.1\"\n    ]\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7lw9n", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002b23880), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7lw9n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038772c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7lw9n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003877360), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7lw9n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003877220), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003a160c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-130-27.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a30420), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-tnbdd"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003a16170)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003a16190)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003a161ac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003a161b0), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784699, loc:(*time.Location)(0x84b02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784699, loc:(*time.Location)(0x84b02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784699, loc:(*time.Location)(0x84b02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718784699, loc:(*time.Location)(0x84b02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.130.27", PodIP:"10.128.3.153", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.128.3.153"}}, StartTime:(*v1.Time)(0xc00321cde0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002241b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002241b90)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://d7d868a0a966ee4b196bb0306676570b4297efaecc3cf8a63221724581a913a5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00321ce20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00321ce00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc003a1622f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:25:51.314: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-4831" for this suite.
Mar  2 22:26:19.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:26:22.767: INFO: namespace init-container-4831 deletion completed in 31.357934315s


• [SLOW TEST:83.220 seconds]
[k8s.io] InitContainer [NodeConformance]
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Mar  2 22:26:22.770: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:24:30.606: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3104
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar  2 22:24:30.758: INFO: Found 0 stateful pods, waiting for 3
Mar  2 22:24:40.772: INFO: Found 2 stateful pods, waiting for 3
Mar  2 22:24:50.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:24:50.773: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:24:50.773: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:25:00.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:25:00.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:25:00.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:25:00.813: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-3104 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:25:01.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:25:01.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:25:01.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 22:25:11.235: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  2 22:25:11.276: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-3104 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:11.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:25:11.555: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:25:11.555: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:25:21.635: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:25:21.636: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:25:21.636: INFO: Waiting for Pod statefulset-3104/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:25:31.672: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:25:31.672: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:25:31.672: INFO: Waiting for Pod statefulset-3104/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:25:41.663: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:25:41.663: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 22:25:51.663: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  2 22:26:01.664: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-3104 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:26:01.981: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:26:01.981: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:26:01.981: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:26:12.080: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  2 22:26:22.150: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-3104 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:22.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:26:22.495: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:26:22.495: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:26:32.574: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:26:32.574: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:26:32.574: INFO: Waiting for Pod statefulset-3104/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:26:42.601: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:26:42.601: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:26:42.601: INFO: Waiting for Pod statefulset-3104/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:26:52.601: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
Mar  2 22:26:52.601: INFO: Waiting for Pod statefulset-3104/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 22:27:02.601: INFO: Waiting for StatefulSet statefulset-3104/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 22:27:12.602: INFO: Deleting all statefulset in ns statefulset-3104
Mar  2 22:27:12.615: INFO: Scaling statefulset ss2 to 0
Mar  2 22:27:22.671: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:27:22.684: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:27:22.728: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-3104" for this suite.
Mar  2 22:27:28.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:27:32.161: INFO: namespace statefulset-3104 deletion completed in 9.354931936s


• [SLOW TEST:181.556 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Mar  2 22:27:32.164: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  2 22:21:30.010: INFO: >>> kubeConfig: /1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-516
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-516
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-516
Mar  2 22:21:30.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:21:40.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 22:21:40.251: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:21:40.590: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:21:40.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:21:40.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:21:40.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 22:21:50.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:21:50.631: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:21:50.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999766s
Mar  2 22:21:51.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980748184s
Mar  2 22:21:52.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.960395927s
Mar  2 22:21:53.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.939318336s
Mar  2 22:21:54.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9191305s
Mar  2 22:21:55.812: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.898476075s
Mar  2 22:21:56.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.878111084s
Mar  2 22:21:57.853: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.858144722s
Mar  2 22:21:58.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.83756749s
Mar  2 22:21:59.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 816.250637ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-516
Mar  2 22:22:00.923: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:22:01.200: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:22:01.200: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:22:01.200: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:22:01.220: INFO: Found 1 stateful pods, waiting for 3
Mar  2 22:22:11.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:22:11.241: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:22:11.241: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 22:22:21.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:22:21.241: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 22:22:21.241: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  2 22:22:21.279: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:22:21.586: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:22:21.586: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:22:21.586: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:22:21.586: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:22:21.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:22:21.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:22:21.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:22:21.896: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 22:22:22.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 22:22:22.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 22:22:22.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 22:22:22.230: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:22:22.249: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 22:22:32.289: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:22:32.289: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:22:32.289: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 22:22:32.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999766s
Mar  2 22:22:33.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980493991s
Mar  2 22:22:34.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.959850068s
Mar  2 22:22:35.410: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.939356153s
Mar  2 22:22:36.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.919344776s
Mar  2 22:22:37.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.899095357s
Mar  2 22:22:38.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.878902297s
Mar  2 22:22:39.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.858593854s
Mar  2 22:22:40.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.838222229s
Mar  2 22:22:41.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 818.094387ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-516
Mar  2 22:22:42.552: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:22:42.843: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:22:42.843: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:22:42.843: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:22:42.843: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:22:43.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 22:22:43.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 22:22:43.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 22:22:43.184: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:22:43.458: INFO: rc: 137
Mar  2 22:22:43.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil> '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'
 + mv -v /tmp/index.html /usr/local/apache2/htdocs/
command terminated with exit code 137
 [] <nil> 0xc00288c900 exit status 137 <nil> <nil> true [0xc0032e5638 0xc0032e5650 0xc0032e5668] [0xc0032e5638 0xc0032e5650 0xc0032e5668] [0xc0032e5648 0xc0032e5660] [0x10efd60 0x10efd60] 0xc001f5ca80 <nil>}:
Command stdout:
'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

stderr:
+ mv -v /tmp/index.html /usr/local/apache2/htdocs/
command terminated with exit code 137

error:
exit status 137
Mar  2 22:22:53.458: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:22:53.698: INFO: rc: 1
Mar  2 22:22:53.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc00288ca50 exit status 1 <nil> <nil> true [0xc0032e5670 0xc0032e5688 0xc0032e56a0] [0xc0032e5670 0xc0032e5688 0xc0032e56a0] [0xc0032e5680 0xc0032e5698] [0x10efd60 0x10efd60] 0xc0020c6000 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 22:23:03.698: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:03.866: INFO: rc: 1
Mar  2 22:23:03.866: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00288cc90 exit status 1 <nil> <nil> true [0xc0032e56a8 0xc0032e56c0 0xc0032e56d8] [0xc0032e56a8 0xc0032e56c0 0xc0032e56d8] [0xc0032e56b8 0xc0032e56d0] [0x10efd60 0x10efd60] 0xc0020c6fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:23:13.867: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:14.002: INFO: rc: 1
Mar  2 22:23:14.002: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000ad5dd0 exit status 1 <nil> <nil> true [0xc0080bc138 0xc0080bc150 0xc0080bc168] [0xc0080bc138 0xc0080bc150 0xc0080bc168] [0xc0080bc148 0xc0080bc160] [0x10efd60 0x10efd60] 0xc002dcfce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:23:24.003: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:24.167: INFO: rc: 1
Mar  2 22:23:24.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003338180 exit status 1 <nil> <nil> true [0xc000408730 0xc000408800 0xc000408930] [0xc000408730 0xc000408800 0xc000408930] [0xc0004087d0 0xc0004088b0] [0x10efd60 0x10efd60] 0xc001f5cfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:23:34.168: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:34.308: INFO: rc: 1
Mar  2 22:23:34.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926120 exit status 1 <nil> <nil> true [0xc0000103d0 0xc0000109e0 0xc000010cc0] [0xc0000103d0 0xc0000109e0 0xc000010cc0] [0xc000010940 0xc000010b98] [0x10efd60 0x10efd60] 0xc002cd8cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:23:44.309: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:44.501: INFO: rc: 1
Mar  2 22:23:44.501: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a0c0 exit status 1 <nil> <nil> true [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d5040 0xc0007d59e0] [0x10efd60 0x10efd60] 0xc001977e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:23:54.502: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:23:54.666: INFO: rc: 1
Mar  2 22:23:54.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038ae180 exit status 1 <nil> <nil> true [0xc00082e018 0xc00082e0b0 0xc00082e158] [0xc00082e018 0xc00082e0b0 0xc00082e158] [0xc00082e090 0xc00082e100] [0x10efd60 0x10efd60] 0xc002087bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:04.667: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:04.814: INFO: rc: 1
Mar  2 22:24:04.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a210 exit status 1 <nil> <nil> true [0xc0007d5fb0 0xc0080bc010 0xc0080bc028] [0xc0007d5fb0 0xc0080bc010 0xc0080bc028] [0xc0080bc008 0xc0080bc020] [0x10efd60 0x10efd60] 0xc001fb1920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:14.815: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:14.972: INFO: rc: 1
Mar  2 22:24:14.972: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926300 exit status 1 <nil> <nil> true [0xc000010e58 0xc0000112d8 0xc000011428] [0xc000010e58 0xc0000112d8 0xc000011428] [0xc0000110f0 0xc0000113d8] [0x10efd60 0x10efd60] 0xc00227e360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:24.972: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:25.102: INFO: rc: 1
Mar  2 22:24:25.103: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926480 exit status 1 <nil> <nil> true [0xc0000114b8 0xc0000116f0 0xc0000118f0] [0xc0000114b8 0xc0000116f0 0xc0000118f0] [0xc0000116c8 0xc000011820] [0x10efd60 0x10efd60] 0xc002445920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:35.104: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:35.271: INFO: rc: 1
Mar  2 22:24:35.271: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926600 exit status 1 <nil> <nil> true [0xc000011908 0xc000011c08 0xc000011d20] [0xc000011908 0xc000011c08 0xc000011d20] [0xc000011b28 0xc000011cc0] [0x10efd60 0x10efd60] 0xc0022fe2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:45.272: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:49.705: INFO: rc: 1
Mar  2 22:24:49.705: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926780 exit status 1 <nil> <nil> true [0xc000011da0 0xc000011fc0 0xc0032e4010] [0xc000011da0 0xc000011fc0 0xc0032e4010] [0xc000011f18 0xc0032e4008] [0x10efd60 0x10efd60] 0xc002308120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:24:59.705: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:24:59.845: INFO: rc: 1
Mar  2 22:24:59.846: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0069268d0 exit status 1 <nil> <nil> true [0xc0032e4020 0xc0032e4058 0xc0032e4090] [0xc0032e4020 0xc0032e4058 0xc0032e4090] [0xc0032e4048 0xc0032e4088] [0x10efd60 0x10efd60] 0xc002309f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:25:09.846: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:09.983: INFO: rc: 1
Mar  2 22:25:09.983: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038ae330 exit status 1 <nil> <nil> true [0xc00082e1f8 0xc00082e2d8 0xc00082e3b0] [0xc00082e1f8 0xc00082e2d8 0xc00082e3b0] [0xc00082e228 0xc00082e380] [0x10efd60 0x10efd60] 0xc0028b7920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:25:19.984: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:20.147: INFO: rc: 1
Mar  2 22:25:20.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a0f0 exit status 1 <nil> <nil> true [0xc0080bc008 0xc0080bc020 0xc0080bc038] [0xc0080bc008 0xc0080bc020 0xc0080bc038] [0xc0080bc018 0xc0080bc030] [0x10efd60 0x10efd60] 0xc0023084e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:25:30.147: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:30.277: INFO: rc: 1
Mar  2 22:25:30.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a2a0 exit status 1 <nil> <nil> true [0xc0080bc040 0xc0080bc058 0xc0080bc070] [0xc0080bc040 0xc0080bc058 0xc0080bc070] [0xc0080bc050 0xc0080bc068] [0x10efd60 0x10efd60] 0xc0022fe2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:25:40.278: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:40.442: INFO: rc: 1
Mar  2 22:25:40.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033381e0 exit status 1 <nil> <nil> true [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d5040 0xc0007d59e0] [0x10efd60 0x10efd60] 0xc00185bf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:25:50.442: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:25:50.600: INFO: rc: 1
Mar  2 22:25:50.600: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a450 exit status 1 <nil> <nil> true [0xc0080bc078 0xc0080bc090 0xc0080bc0a8] [0xc0080bc078 0xc0080bc090 0xc0080bc0a8] [0xc0080bc088 0xc0080bc0a0] [0x10efd60 0x10efd60] 0xc002502060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:00.600: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:00.733: INFO: rc: 1
Mar  2 22:26:00.734: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00062a5a0 exit status 1 <nil> <nil> true [0xc0080bc0b0 0xc0080bc0c8 0xc0080bc0e0] [0xc0080bc0b0 0xc0080bc0c8 0xc0080bc0e0] [0xc0080bc0c0 0xc0080bc0d8] [0x10efd60 0x10efd60] 0xc000b805a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:10.734: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:10.898: INFO: rc: 1
Mar  2 22:26:10.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003338450 exit status 1 <nil> <nil> true [0xc0007d5fb0 0xc000010940 0xc000010b98] [0xc0007d5fb0 0xc000010940 0xc000010b98] [0xc000010768 0xc000010ae8] [0x10efd60 0x10efd60] 0xc002086f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:20.898: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:21.076: INFO: rc: 1
Mar  2 22:26:21.077: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003338630 exit status 1 <nil> <nil> true [0xc000010cc0 0xc0000110f0 0xc0000113d8] [0xc000010cc0 0xc0000110f0 0xc0000113d8] [0xc000010e68 0xc000011390] [0x10efd60 0x10efd60] 0xc001c65260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:31.077: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:31.244: INFO: rc: 1
Mar  2 22:26:31.244: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926180 exit status 1 <nil> <nil> true [0xc00082e018 0xc00082e0b0 0xc00082e158] [0xc00082e018 0xc00082e0b0 0xc00082e158] [0xc00082e090 0xc00082e100] [0x10efd60 0x10efd60] 0xc0020025a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:41.245: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:41.402: INFO: rc: 1
Mar  2 22:26:41.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0033387e0 exit status 1 <nil> <nil> true [0xc000011428 0xc0000116c8 0xc000011820] [0xc000011428 0xc0000116c8 0xc000011820] [0xc0000115f8 0xc000011770] [0x10efd60 0x10efd60] 0xc002cd8cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:26:51.403: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:26:51.565: INFO: rc: 1
Mar  2 22:26:51.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038ae240 exit status 1 <nil> <nil> true [0xc000408600 0xc0004087d0 0xc0004088b0] [0xc000408600 0xc0004087d0 0xc0004088b0] [0xc0004087a0 0xc000408870] [0x10efd60 0x10efd60] 0xc0029cf860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:01.565: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:01.700: INFO: rc: 1
Mar  2 22:27:01.700: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003338960 exit status 1 <nil> <nil> true [0xc0000118f0 0xc000011b28 0xc000011cc0] [0xc0000118f0 0xc000011b28 0xc000011cc0] [0xc000011ae0 0xc000011c30] [0x10efd60 0x10efd60] 0xc001f5c180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:11.701: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:11.866: INFO: rc: 1
Mar  2 22:27:11.866: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003338ae0 exit status 1 <nil> <nil> true [0xc000011d20 0xc000011f18 0xc0032e4008] [0xc000011d20 0xc000011f18 0xc0032e4008] [0xc000011e98 0xc0032e4000] [0x10efd60 0x10efd60] 0xc001f5d4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:21.867: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:22.041: INFO: rc: 1
Mar  2 22:27:22.041: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036940f0 exit status 1 <nil> <nil> true [0xc00082e1f8 0xc0023a0010 0xc0023a0028] [0xc00082e1f8 0xc0023a0010 0xc0023a0028] [0xc0023a0008 0xc0023a0020] [0x10efd60 0x10efd60] 0xc001fc1a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:32.042: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:32.202: INFO: rc: 1
Mar  2 22:27:32.202: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926090 exit status 1 <nil> <nil> true [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d4488 0xc0007d53b8 0xc0007d5d98] [0xc0007d5040 0xc0007d59e0] [0x10efd60 0x10efd60] 0xc002cd8cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:42.203: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:42.368: INFO: rc: 1
Mar  2 22:27:42.368: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc006926210 exit status 1 <nil> <nil> true [0xc0007d5fb0 0xc00082e090 0xc00082e100] [0xc0007d5fb0 0xc00082e090 0xc00082e100] [0xc00082e048 0xc00082e0d0] [0x10efd60 0x10efd60] 0xc0020025a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  2 22:27:52.368: INFO: Running '/osd_43_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/1b48r1uu3uen4d0hc53ql13r3spejpg3.kubeconfig exec --namespace=statefulset-516 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 22:27:52.529: INFO: rc: 1
Mar  2 22:27:52.529: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar  2 22:27:52.529: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  2 22:27:52.594: INFO: Deleting all statefulset in ns statefulset-516
Mar  2 22:27:52.614: INFO: Scaling statefulset ss to 0
Mar  2 22:27:52.676: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 22:27:52.696: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  2 22:27:52.779: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-516" for this suite.
Mar  2 22:27:59.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  2 22:28:02.335: INFO: namespace statefulset-516 deletion completed in 9.390251217s


• [SLOW TEST:392.326 seconds]
[sig-apps] StatefulSet
/osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /osd_43_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Mar  2 22:28:02.338: INFO: Running AfterSuite actions on all nodes


Mar  2 22:25:09.609: INFO: Running AfterSuite actions on all nodes
Mar  2 22:28:02.404: INFO: Running AfterSuite actions on node 1
Mar  2 22:28:02.404: INFO: Dumping logs locally to: /osd_43_conformance/origin/_output/scripts/conformance-k8s/artifacts
Mar  2 22:28:02.404: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory


Ran 261 of 4897 Specs in 2318.264 seconds
SUCCESS! -- 261 Passed | 0 Failed | 0 Pending | 4636 Skipped


Ginkgo ran 1 suite in 38m39.485243311s
Test Suite Passed
