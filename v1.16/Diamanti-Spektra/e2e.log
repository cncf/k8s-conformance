I0601 16:25:31.570182      26 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-621748879
I0601 16:25:31.570393      26 e2e.go:92] Starting e2e run "edc48979-7dfd-4125-a002-6fec5feb9fc3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1591028729 - Will randomize all specs
Will run 274 of 4731 specs

Jun  1 16:25:31.666: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 16:25:31.670: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun  1 16:25:31.693: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  1 16:25:31.728: INFO: 6 / 6 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  1 16:25:31.728: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jun  1 16:25:31.728: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  1 16:25:31.737: INFO: e2e test version: v1.16.8
Jun  1 16:25:31.738: INFO: kube-apiserver version: v1.16.8
Jun  1 16:25:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 16:25:31.745: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:25:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename subpath
Jun  1 16:25:31.780: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun  1 16:25:31.795: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-7fgf
STEP: Creating a pod to test atomic-volume-subpath
Jun  1 16:25:31.920: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7fgf" in namespace "subpath-7564" to be "success or failure"
Jun  1 16:25:31.923: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.447355ms
Jun  1 16:25:33.929: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008951179s
Jun  1 16:25:35.933: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012571479s
Jun  1 16:25:37.937: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 6.016752727s
Jun  1 16:25:39.940: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 8.019986085s
Jun  1 16:25:41.945: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 10.024653365s
Jun  1 16:25:43.949: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 12.028100169s
Jun  1 16:25:45.952: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 14.031619528s
Jun  1 16:25:47.955: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 16.034820868s
Jun  1 16:25:49.959: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 18.038304307s
Jun  1 16:25:51.962: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Running", Reason="", readiness=true. Elapsed: 20.042008513s
Jun  1 16:25:53.966: INFO: Pod "pod-subpath-test-configmap-7fgf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045639395s
STEP: Saw pod success
Jun  1 16:25:53.966: INFO: Pod "pod-subpath-test-configmap-7fgf" satisfied condition "success or failure"
Jun  1 16:25:53.969: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-configmap-7fgf container test-container-subpath-configmap-7fgf: <nil>
STEP: delete the pod
Jun  1 16:25:54.000: INFO: Waiting for pod pod-subpath-test-configmap-7fgf to disappear
Jun  1 16:25:54.003: INFO: Pod pod-subpath-test-configmap-7fgf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7fgf
Jun  1 16:25:54.003: INFO: Deleting pod "pod-subpath-test-configmap-7fgf" in namespace "subpath-7564"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:25:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7564" for this suite.
Jun  1 16:26:00.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:26:00.110: INFO: namespace subpath-7564 deletion completed in 6.100366973s

• [SLOW TEST:28.365 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:26:00.110: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun  1 16:26:00.243: INFO: PodSpec: initContainers in spec.initContainers
Jun  1 16:26:45.641: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b671e82a-45bb-4266-a4cb-121d79e3e0f5", GenerateName:"", Namespace:"init-container-6422", SelfLink:"/api/v1/namespaces/init-container-6422/pods/pod-init-b671e82a-45bb-4266-a4cb-121d79e3e0f5", UID:"33501034-d33d-4c16-ad25-839e7e70f57c", ResourceVersion:"1949", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63726625560, loc:(*time.Location)(0x789e8e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"243382289"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lkzd4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002cda680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lkzd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lkzd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lkzd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002b7b888), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"appserv9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00251faa0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b7b910)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b7b930)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002b7b938), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002b7b93c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625560, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625560, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625560, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625560, loc:(*time.Location)(0x789e8e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.6.109", PodIP:"172.16.141.11", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.141.11"}}, StartTime:(*v1.Time)(0xc002e02200), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f9eb60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f9ec40)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f541ef0f2dfc673938a227127248127223eea7aeb269d7630176026e3c437119", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002e02240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002e02220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002b7b9bf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:26:45.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6422" for this suite.
Jun  1 16:27:13.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:27:13.742: INFO: namespace init-container-6422 deletion completed in 28.095319031s

• [SLOW TEST:73.632 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:27:13.743: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:27:13.882: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  1 16:27:18.886: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  1 16:27:18.886: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  1 16:27:20.889: INFO: Creating deployment "test-rollover-deployment"
Jun  1 16:27:20.896: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  1 16:27:22.901: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  1 16:27:22.907: INFO: Ensure that both replica sets have 1 created replica
Jun  1 16:27:22.913: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  1 16:27:22.920: INFO: Updating deployment test-rollover-deployment
Jun  1 16:27:22.920: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  1 16:27:24.925: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  1 16:27:24.931: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  1 16:27:24.937: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:24.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625643, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:26.944: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:26.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625645, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:28.943: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:28.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625645, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:30.945: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:30.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625645, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:32.945: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:32.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625645, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:34.944: INFO: all replica sets need to contain the pod-template-hash label
Jun  1 16:27:34.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625645, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625641, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 16:27:36.944: INFO: 
Jun  1 16:27:36.944: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun  1 16:27:36.952: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2690 /apis/apps/v1/namespaces/deployment-2690/deployments/test-rollover-deployment b5eba8bf-8705-496f-b94e-83635e55f882 2163 2 2020-06-01 16:27:21 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031f5138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-01 16:27:21 +0000 UTC,LastTransitionTime:2020-06-01 16:27:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-06-01 16:27:35 +0000 UTC,LastTransitionTime:2020-06-01 16:27:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  1 16:27:36.956: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-2690 /apis/apps/v1/namespaces/deployment-2690/replicasets/test-rollover-deployment-7d7dc6548c 38ef4403-e842-423d-838f-243c8a315fad 2151 2 2020-06-01 16:27:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b5eba8bf-8705-496f-b94e-83635e55f882 0xc0031f59a7 0xc0031f59a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031f5a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:27:36.956: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  1 16:27:36.956: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2690 /apis/apps/v1/namespaces/deployment-2690/replicasets/test-rollover-controller 90f56bd9-8cf4-485a-a50a-5c1daf2b76c7 2160 2 2020-06-01 16:27:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b5eba8bf-8705-496f-b94e-83635e55f882 0xc0031f583f 0xc0031f5850}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031f58d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:27:36.956: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2690 /apis/apps/v1/namespaces/deployment-2690/replicasets/test-rollover-deployment-f6c94f66c 280c24d5-7008-4b24-9407-86f094b46113 2106 2 2020-06-01 16:27:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b5eba8bf-8705-496f-b94e-83635e55f882 0xc0031f5af0 0xc0031f5af1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031f5b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:27:36.960: INFO: Pod "test-rollover-deployment-7d7dc6548c-kjxlz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-kjxlz test-rollover-deployment-7d7dc6548c- deployment-2690 /api/v1/namespaces/deployment-2690/pods/test-rollover-deployment-7d7dc6548c-kjxlz 9ba12f7c-a2bf-4b2d-b76f-4ffa0bc3829d 2128 0 2020-06-01 16:27:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 38ef4403-e842-423d-838f-243c8a315fad 0xc00312be87 0xc00312be88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvjcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvjcn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvjcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:27:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:27:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.13,StartTime:2020-06-01 16:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:27:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/redis:5.0.5-alpine,ImageID:docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://c7e45a75094cf4129716c84f5af83c21c322d9950a83e46d85cb0104fab6212d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:27:36.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2690" for this suite.
Jun  1 16:27:42.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:27:43.066: INFO: namespace deployment-2690 deletion completed in 6.101916331s

• [SLOW TEST:29.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:27:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun  1 16:27:43.226: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2174 /api/v1/namespaces/watch-2174/configmaps/e2e-watch-test-resource-version 5163810d-ee9c-45cf-8833-942c7c638c54 2226 0 2020-06-01 16:27:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  1 16:27:43.226: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2174 /api/v1/namespaces/watch-2174/configmaps/e2e-watch-test-resource-version 5163810d-ee9c-45cf-8833-942c7c638c54 2227 0 2020-06-01 16:27:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:27:43.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2174" for this suite.
Jun  1 16:27:49.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:27:49.318: INFO: namespace watch-2174 deletion completed in 6.088051285s

• [SLOW TEST:6.251 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:27:49.318: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Jun  1 16:27:49.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 api-versions'
Jun  1 16:27:49.598: INFO: stderr: ""
Jun  1 16:27:49.598: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nspektra.diamanti.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:27:49.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4559" for this suite.
Jun  1 16:27:55.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:27:55.692: INFO: namespace kubectl-4559 deletion completed in 6.090110666s

• [SLOW TEST:6.374 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:27:55.692: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0601 16:28:35.857161      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun  1 16:28:35.857: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:28:35.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6264" for this suite.
Jun  1 16:28:41.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:28:42.142: INFO: namespace gc-6264 deletion completed in 6.281741296s

• [SLOW TEST:46.450 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:28:42.142: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-993
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-324f0990-2b16-4855-a7cc-83e800f10cfb
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-324f0990-2b16-4855-a7cc-83e800f10cfb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:30:02.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-993" for this suite.
Jun  1 16:30:30.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:30:31.214: INFO: namespace configmap-993 deletion completed in 28.461774162s

• [SLOW TEST:109.072 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:30:31.214: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-1783
STEP: creating replication controller nodeport-test in namespace services-1783
I0601 16:30:31.362569      26 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-1783, replica count: 2
Jun  1 16:30:34.413: INFO: Creating new exec pod
I0601 16:30:34.413047      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  1 16:30:39.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1783 execpodzt46k -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun  1 16:30:39.740: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  1 16:30:39.740: INFO: stdout: ""
Jun  1 16:30:39.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1783 execpodzt46k -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.81 80'
Jun  1 16:30:39.974: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.81 80\nConnection to 10.0.0.81 80 port [tcp/http] succeeded!\n"
Jun  1 16:30:39.974: INFO: stdout: ""
Jun  1 16:30:39.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1783 execpodzt46k -- /bin/sh -x -c nc -zv -t -w 2 172.16.6.110 30127'
Jun  1 16:30:40.212: INFO: stderr: "+ nc -zv -t -w 2 172.16.6.110 30127\nConnection to 172.16.6.110 30127 port [tcp/30127] succeeded!\n"
Jun  1 16:30:40.212: INFO: stdout: ""
Jun  1 16:30:40.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1783 execpodzt46k -- /bin/sh -x -c nc -zv -t -w 2 172.16.6.111 30127'
Jun  1 16:30:40.449: INFO: stderr: "+ nc -zv -t -w 2 172.16.6.111 30127\nConnection to 172.16.6.111 30127 port [tcp/30127] succeeded!\n"
Jun  1 16:30:40.449: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:30:40.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1783" for this suite.
Jun  1 16:30:46.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:30:46.550: INFO: namespace services-1783 deletion completed in 6.096667786s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.336 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:30:46.551: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun  1 16:30:46.692: INFO: Waiting up to 5m0s for pod "downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b" in namespace "downward-api-6573" to be "success or failure"
Jun  1 16:30:46.694: INFO: Pod "downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.47286ms
Jun  1 16:30:48.697: INFO: Pod "downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005637854s
Jun  1 16:30:50.701: INFO: Pod "downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009555261s
STEP: Saw pod success
Jun  1 16:30:50.701: INFO: Pod "downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b" satisfied condition "success or failure"
Jun  1 16:30:50.704: INFO: Trying to get logs from node appserv9 pod downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b container dapi-container: <nil>
STEP: delete the pod
Jun  1 16:30:50.728: INFO: Waiting for pod downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b to disappear
Jun  1 16:30:50.730: INFO: Pod downward-api-f94c7f1b-1172-48d5-929f-a24b8c743d1b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:30:50.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6573" for this suite.
Jun  1 16:30:56.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:30:56.822: INFO: namespace downward-api-6573 deletion completed in 6.088754396s

• [SLOW TEST:10.271 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:30:56.822: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 16:30:56.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9248'
Jun  1 16:30:57.090: INFO: stderr: ""
Jun  1 16:30:57.090: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun  1 16:31:02.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pod e2e-test-httpd-pod --namespace=kubectl-9248 -o json'
Jun  1 16:31:02.271: INFO: stderr: ""
Jun  1 16:31:02.271: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-06-01T16:30:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9248\",\n        \"resourceVersion\": \"3102\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9248/pods/e2e-test-httpd-pod\",\n        \"uid\": \"0d7c9b38-111e-45fb-8fd2-a5787a2620b6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qsh98\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"appserv11\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qsh98\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qsh98\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-01T16:30:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-01T16:30:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-01T16:30:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-01T16:30:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://1d9a480fe3aa6d1b8f024e642503db4268beeb3f5482de611cf2845f2974d73f\",\n                \"image\": \"docker.io/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-01T16:30:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.6.111\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.141.11\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.141.11\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-01T16:30:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun  1 16:31:02.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 replace -f - --namespace=kubectl-9248'
Jun  1 16:31:02.588: INFO: stderr: ""
Jun  1 16:31:02.588: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Jun  1 16:31:02.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete pods e2e-test-httpd-pod --namespace=kubectl-9248'
Jun  1 16:31:06.712: INFO: stderr: ""
Jun  1 16:31:06.712: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:31:06.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9248" for this suite.
Jun  1 16:31:12.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:31:12.808: INFO: namespace kubectl-9248 deletion completed in 6.091933159s

• [SLOW TEST:15.986 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:31:12.809: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-9c2cf3f2-6129-4e4e-b7cb-b2d37e267af3
STEP: Creating a pod to test consume configMaps
Jun  1 16:31:12.963: INFO: Waiting up to 5m0s for pod "pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78" in namespace "configmap-961" to be "success or failure"
Jun  1 16:31:12.965: INFO: Pod "pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359701ms
Jun  1 16:31:14.969: INFO: Pod "pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00592326s
STEP: Saw pod success
Jun  1 16:31:14.969: INFO: Pod "pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78" satisfied condition "success or failure"
Jun  1 16:31:14.979: INFO: Trying to get logs from node appserv9 pod pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 16:31:14.996: INFO: Waiting for pod pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78 to disappear
Jun  1 16:31:14.998: INFO: Pod pod-configmaps-652ec0d9-8869-41d5-b2c5-7c147b4e3b78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:31:14.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-961" for this suite.
Jun  1 16:31:21.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:31:21.096: INFO: namespace configmap-961 deletion completed in 6.094220687s

• [SLOW TEST:8.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:31:21.097: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3696
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3696
STEP: creating replication controller externalsvc in namespace services-3696
I0601 16:31:21.251798      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3696, replica count: 2
I0601 16:31:24.302289      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun  1 16:31:24.315: INFO: Creating new exec pod
Jun  1 16:31:26.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-3696 execpodcbm7w -- /bin/sh -x -c nslookup clusterip-service'
Jun  1 16:31:26.599: INFO: stderr: "+ nslookup clusterip-service\n"
Jun  1 16:31:26.599: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-3696.svc.cluster.local\tcanonical name = externalsvc.services-3696.svc.cluster.local.\nName:\texternalsvc.services-3696.svc.cluster.local\nAddress: 10.0.0.6\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3696, will wait for the garbage collector to delete the pods
Jun  1 16:31:26.658: INFO: Deleting ReplicationController externalsvc took: 6.15196ms
Jun  1 16:31:27.258: INFO: Terminating ReplicationController externalsvc pods took: 600.231705ms
Jun  1 16:31:44.671: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:31:44.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3696" for this suite.
Jun  1 16:31:50.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:31:50.855: INFO: namespace services-3696 deletion completed in 6.172180022s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.759 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:31:50.856: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 16:31:51.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618" in namespace "projected-1822" to be "success or failure"
Jun  1 16:31:51.005: INFO: Pod "downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.650352ms
Jun  1 16:31:53.008: INFO: Pod "downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005937504s
STEP: Saw pod success
Jun  1 16:31:53.008: INFO: Pod "downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618" satisfied condition "success or failure"
Jun  1 16:31:53.011: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618 container client-container: <nil>
STEP: delete the pod
Jun  1 16:31:53.026: INFO: Waiting for pod downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618 to disappear
Jun  1 16:31:53.028: INFO: Pod downwardapi-volume-bde08988-c0c7-47c4-8131-ed3bbcdf3618 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:31:53.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1822" for this suite.
Jun  1 16:31:59.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:31:59.120: INFO: namespace projected-1822 deletion completed in 6.089231178s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:31:59.121: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:31:59.257: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46" in namespace "security-context-test-8037" to be "success or failure"
Jun  1 16:31:59.260: INFO: Pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397286ms
Jun  1 16:32:01.263: INFO: Pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005877944s
Jun  1 16:32:03.266: INFO: Pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009187694s
Jun  1 16:32:03.267: INFO: Pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46" satisfied condition "success or failure"
Jun  1 16:32:03.276: INFO: Got logs for pod "busybox-privileged-false-1abdcb9b-b126-4fc1-a589-2169eefd1f46": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:32:03.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8037" for this suite.
Jun  1 16:32:09.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:32:09.365: INFO: namespace security-context-test-8037 deletion completed in 6.084664743s

• [SLOW TEST:10.244 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:32:09.365: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:32:26.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4318" for this suite.
Jun  1 16:32:32.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:32:32.665: INFO: namespace resourcequota-4318 deletion completed in 6.127939659s

• [SLOW TEST:23.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:32:32.665: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 16:32:33.270: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 16:32:35.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625953, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625953, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625953, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726625953, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 16:32:38.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:32:38.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8800" for this suite.
Jun  1 16:32:44.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:32:44.443: INFO: namespace webhook-8800 deletion completed in 6.090552582s
STEP: Destroying namespace "webhook-8800-markers" for this suite.
Jun  1 16:32:50.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:32:50.609: INFO: namespace webhook-8800-markers deletion completed in 6.166249135s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.956 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:32:50.622: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2845
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun  1 16:32:50.755: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun  1 16:33:04.207: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 16:33:07.889: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:33:22.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2845" for this suite.
Jun  1 16:33:28.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:33:28.410: INFO: namespace crd-publish-openapi-2845 deletion completed in 6.081389013s

• [SLOW TEST:37.789 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:33:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-72aefa4a-4831-456b-87a6-ec8e96ab2691 in namespace container-probe-3446
Jun  1 16:33:32.553: INFO: Started pod liveness-72aefa4a-4831-456b-87a6-ec8e96ab2691 in namespace container-probe-3446
STEP: checking the pod's current state and verifying that restartCount is present
Jun  1 16:33:32.556: INFO: Initial restart count of pod liveness-72aefa4a-4831-456b-87a6-ec8e96ab2691 is 0
Jun  1 16:33:48.586: INFO: Restart count of pod container-probe-3446/liveness-72aefa4a-4831-456b-87a6-ec8e96ab2691 is now 1 (16.030072982s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:33:48.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3446" for this suite.
Jun  1 16:33:54.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:33:54.758: INFO: namespace container-probe-3446 deletion completed in 6.160575158s

• [SLOW TEST:26.347 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:33:54.758: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:34:05.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5584" for this suite.
Jun  1 16:34:11.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:34:12.039: INFO: namespace resourcequota-5584 deletion completed in 6.105786719s

• [SLOW TEST:17.281 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:34:12.040: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:34:12.173: INFO: Creating deployment "webserver-deployment"
Jun  1 16:34:12.177: INFO: Waiting for observed generation 1
Jun  1 16:34:14.182: INFO: Waiting for all required pods to come up
Jun  1 16:34:14.186: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun  1 16:34:16.193: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  1 16:34:16.199: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  1 16:34:16.206: INFO: Updating deployment webserver-deployment
Jun  1 16:34:16.206: INFO: Waiting for observed generation 2
Jun  1 16:34:18.212: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  1 16:34:18.215: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  1 16:34:18.217: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  1 16:34:18.225: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  1 16:34:18.225: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  1 16:34:18.227: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  1 16:34:18.232: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  1 16:34:18.232: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  1 16:34:18.237: INFO: Updating deployment webserver-deployment
Jun  1 16:34:18.237: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  1 16:34:18.241: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  1 16:34:18.244: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun  1 16:34:18.250: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3764 /apis/apps/v1/namespaces/deployment-3764/deployments/webserver-deployment 8a44dbb4-3a99-40db-9e46-17d7f69635a3 4192 3 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f767c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-01 16:34:16 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-01 16:34:18 +0000 UTC,LastTransitionTime:2020-06-01 16:34:18 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  1 16:34:18.255: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-3764 /apis/apps/v1/namespaces/deployment-3764/replicasets/webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 4188 3 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8a44dbb4-3a99-40db-9e46-17d7f69635a3 0xc0040713c7 0xc0040713c8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0040714a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:34:18.255: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  1 16:34:18.255: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-3764 /apis/apps/v1/namespaces/deployment-3764/replicasets/webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 4187 3 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8a44dbb4-3a99-40db-9e46-17d7f69635a3 0xc0040712f7 0xc0040712f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004071368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:34:18.261: INFO: Pod "webserver-deployment-595b5b9587-2mvzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2mvzr webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-2mvzr 303084f1-bd99-4f2a-a314-c91d8af3b215 4209 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc004071b00 0xc004071b01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.261: INFO: Pod "webserver-deployment-595b5b9587-4blnf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4blnf webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-4blnf c94ad4fd-313d-4973-8de5-4736c8cbbc7c 4212 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc004071c67 0xc004071c68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.261: INFO: Pod "webserver-deployment-595b5b9587-9z8nj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9z8nj webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-9z8nj 5d703a60-140f-4229-bd5f-a37676d3f69b 4194 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc004071de7 0xc004071de8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.261: INFO: Pod "webserver-deployment-595b5b9587-bvmjs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bvmjs webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-bvmjs b0e40b3d-95c6-48f1-89e9-1cfdc857bc6d 4083 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc004071f27 0xc004071f28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.12,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b16891a2458ce758b998c12e7101de419effe35eb237815c399e72d8220a5551,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.261: INFO: Pod "webserver-deployment-595b5b9587-bxhtz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bxhtz webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-bxhtz fc1a6664-61fc-45cb-bc1b-29fcf97b577d 4200 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3a0e7 0xc003f3a0e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-fqlxq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fqlxq webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-fqlxq d95c7b19-76b8-46cf-9fa4-d3868b1afb10 4097 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3a267 0xc003f3a268}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.17,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://146e89c5f0f62919479e68eb16fd7d51a8c6de1ba8767a7555e6542248285204,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-hkn5c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hkn5c webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-hkn5c 21ab60ec-5432-47ad-a0d2-a9b002950675 4091 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3a517 0xc003f3a518}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.15,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cf3793b286bcc461573cb27159be0b624a4bfa13e97adb1a7c6f83531694b251,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-jhv52" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jhv52 webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-jhv52 4c85e1a1-fa03-49ee-9b91-67f463ef9356 4190 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3a7f7 0xc003f3a7f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-jpwxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jpwxt webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-jpwxt a4709a82-5f64-4d0b-974f-b583656de9f7 4202 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3a9b7 0xc003f3a9b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-n6xcr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n6xcr webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-n6xcr 119f19b2-09cc-448e-9cdb-7cd88e23d406 4085 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3ab27 0xc003f3ab28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.18,StartTime:2020-06-01 16:34:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2d7af4de07ec9d1516d43fbfe4a39c392da4a10056daa2639751b9831033a557,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.262: INFO: Pod "webserver-deployment-595b5b9587-ps67d" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ps67d webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-ps67d e91a3473-e16b-4c6e-a565-910a4c263678 4195 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3ad67 0xc003f3ad68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-px8p6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-px8p6 webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-px8p6 90b825fa-40fd-4512-bbfb-31ad3106c373 4095 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3ae97 0xc003f3ae98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.16,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ddee2e055da73e2e992da235b61bc871d04bd0d674a35b1e7bd60ca0a3f8e811,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-pzb4g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pzb4g webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-pzb4g 1bc32e29-6113-4862-a31f-e044bc702dee 4210 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b0b7 0xc003f3b0b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-rkttf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rkttf webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-rkttf 017cea58-2d99-4ebd-833f-511e9b59b140 4199 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b1e7 0xc003f3b1e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-rpfjp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rpfjp webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-rpfjp eee39bc0-7c49-4326-a805-32e7cc4b2856 4089 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b317 0xc003f3b318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.13,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6a1516159440ed7041f3606bd1c125f2318f0da3e07a69274b815c590f6f407d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-rqjmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rqjmh webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-rqjmh c11db7ac-45e8-4443-b37e-80fdd12d4377 4211 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b557 0xc003f3b558}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-smg7d" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-smg7d webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-smg7d 46c477df-4d44-492b-94fc-62b773bc645d 4201 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b677 0xc003f3b678}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.263: INFO: Pod "webserver-deployment-595b5b9587-w7phv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w7phv webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-w7phv 9116519a-320c-4512-ba4a-97d0cc6a8ee0 4077 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3b7f7 0xc003f3b7f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.14,StartTime:2020-06-01 16:34:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ffc560be0c4cf15192bbc6d68281c76631f299eb9404444b925d2f873e299606,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.264: INFO: Pod "webserver-deployment-595b5b9587-wfxg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wfxg6 webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-wfxg6 27f489f1-494c-456a-8a3e-b2ce754b6c37 4213 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3ba07 0xc003f3ba08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.264: INFO: Pod "webserver-deployment-595b5b9587-z98nn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z98nn webserver-deployment-595b5b9587- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-595b5b9587-z98nn e5c22ca0-9fa0-4d11-9f06-cb8b8235df92 4101 0 2020-06-01 16:34:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba1a9ad-a3b5-4d81-9539-7b00e83abd9f 0xc003f3bb37 0xc003f3bb38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.19,StartTime:2020-06-01 16:34:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2c9b3ce767162a7af12c89ef3eae7849f732a0f6e045a46ab686c5477bd6723c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.264: INFO: Pod "webserver-deployment-c7997dcc8-22frf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-22frf webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-22frf f78587d6-e6c4-469d-b1df-6b1ee1f67ed1 4174 0 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f3bd60 0xc003f3bd61}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:,StartTime:2020-06-01 16:34:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.264: INFO: Pod "webserver-deployment-c7997dcc8-5qnp4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5qnp4 webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-5qnp4 978da5aa-5824-456d-bd36-5bc217e1b1fb 4207 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f3bf20 0xc003f3bf21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-7qgmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7qgmj webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-7qgmj dc8003bd-944e-4206-823b-8cf372573fcb 4162 0 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0e057 0xc003f0e058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.110,PodIP:,StartTime:2020-06-01 16:34:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-7wbpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7wbpf webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-7wbpf 059c8f40-8e10-4012-a6fc-c6e80286eb39 4152 0 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0e300 0xc003f0e301}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:,StartTime:2020-06-01 16:34:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-dbpl4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dbpl4 webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-dbpl4 3c630b03-4458-4ec1-82fb-c901c578cf79 4205 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0e5c0 0xc003f0e5c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-gd7vz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gd7vz webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-gd7vz 3ae07ac0-e980-41d4-98ef-28b21f2b3bc6 4214 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0e727 0xc003f0e728}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-ht86j" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ht86j webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-ht86j 972ce51e-c2db-4fe2-93c4-bbe83e534f45 4204 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0e907 0xc003f0e908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-lqsbw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lqsbw webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-lqsbw ec75bc77-3bdc-4da9-8104-940251d3820f 4198 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0ead7 0xc003f0ead8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.265: INFO: Pod "webserver-deployment-c7997dcc8-mp6ng" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mp6ng webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-mp6ng 44e41ce4-13ab-418b-8f29-b4f842cd72be 4197 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0eca7 0xc003f0eca8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.266: INFO: Pod "webserver-deployment-c7997dcc8-n4rxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n4rxd webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-n4rxd 719deed6-399d-4d9b-baed-62c1b100b871 4157 0 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0ee37 0xc003f0ee38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:,StartTime:2020-06-01 16:34:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.266: INFO: Pod "webserver-deployment-c7997dcc8-pqrpl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pqrpl webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-pqrpl d80db2c4-d9e3-493c-880b-b0e1b29a39d6 4169 0 2020-06-01 16:34:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0f030 0xc003f0f031}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:34:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:,StartTime:2020-06-01 16:34:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.266: INFO: Pod "webserver-deployment-c7997dcc8-qsbsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qsbsv webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-qsbsv 75b7b3c3-8e1a-494a-aa0a-8068de6c0ed4 4191 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0f290 0xc003f0f291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:34:18.266: INFO: Pod "webserver-deployment-c7997dcc8-vq2hd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vq2hd webserver-deployment-c7997dcc8- deployment-3764 /api/v1/namespaces/deployment-3764/pods/webserver-deployment-c7997dcc8-vq2hd 2ca4682e-7c21-461b-8dde-2413adf86732 4206 0 2020-06-01 16:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 dc121b34-2217-4c26-9dbb-4335fcbe0c9e 0xc003f0f3e7 0xc003f0f3e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t7s8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t7s8q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t7s8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:34:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3764" for this suite.
Jun  1 16:34:24.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:34:24.410: INFO: namespace deployment-3764 deletion completed in 6.141175939s

• [SLOW TEST:12.370 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:34:24.411: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3128
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3128
STEP: Creating statefulset with conflicting port in namespace statefulset-3128
STEP: Waiting until pod test-pod will start running in namespace statefulset-3128
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3128
Jun  1 16:34:30.572: INFO: Observed stateful pod in namespace: statefulset-3128, name: ss-0, uid: c11ab529-9c94-4df8-a848-e39a76f26f6d, status phase: Pending. Waiting for statefulset controller to delete.
Jun  1 16:34:30.768: INFO: Observed stateful pod in namespace: statefulset-3128, name: ss-0, uid: c11ab529-9c94-4df8-a848-e39a76f26f6d, status phase: Failed. Waiting for statefulset controller to delete.
Jun  1 16:34:30.774: INFO: Observed stateful pod in namespace: statefulset-3128, name: ss-0, uid: c11ab529-9c94-4df8-a848-e39a76f26f6d, status phase: Failed. Waiting for statefulset controller to delete.
Jun  1 16:34:30.777: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3128
STEP: Removing pod with conflicting port in namespace statefulset-3128
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3128 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 16:34:34.795: INFO: Deleting all statefulset in ns statefulset-3128
Jun  1 16:34:34.798: INFO: Scaling statefulset ss to 0
Jun  1 16:34:44.811: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 16:34:44.814: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:34:44.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3128" for this suite.
Jun  1 16:34:50.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:34:50.927: INFO: namespace statefulset-3128 deletion completed in 6.095324329s

• [SLOW TEST:26.516 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:34:50.927: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:34:51.060: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:34:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8855" for this suite.
Jun  1 16:35:37.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:35:37.310: INFO: namespace pods-8855 deletion completed in 44.108498417s

• [SLOW TEST:46.384 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:35:37.311: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Jun  1 16:35:39.452: INFO: Pod pod-hostip-e4946de5-8014-4995-99be-6c4a6f949c2e has hostIP: 172.16.6.111
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:35:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3911" for this suite.
Jun  1 16:36:07.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:36:07.698: INFO: namespace pods-3911 deletion completed in 28.241609718s

• [SLOW TEST:30.387 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:36:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  1 16:36:07.838: INFO: Waiting up to 5m0s for pod "pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82" in namespace "emptydir-5028" to be "success or failure"
Jun  1 16:36:07.841: INFO: Pod "pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318337ms
Jun  1 16:36:09.844: INFO: Pod "pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82": Phase="Running", Reason="", readiness=true. Elapsed: 2.005634945s
Jun  1 16:36:11.848: INFO: Pod "pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518608s
STEP: Saw pod success
Jun  1 16:36:11.848: INFO: Pod "pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82" satisfied condition "success or failure"
Jun  1 16:36:11.851: INFO: Trying to get logs from node appserv11 pod pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82 container test-container: <nil>
STEP: delete the pod
Jun  1 16:36:11.917: INFO: Waiting for pod pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82 to disappear
Jun  1 16:36:11.920: INFO: Pod pod-4257b33b-92e6-4d0b-bdbf-965d20c28e82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:36:11.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5028" for this suite.
Jun  1 16:36:17.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:36:18.003: INFO: namespace emptydir-5028 deletion completed in 6.079240592s

• [SLOW TEST:10.304 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:36:18.003: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun  1 16:36:18.137: INFO: Waiting up to 5m0s for pod "downward-api-494800d7-3f43-4810-a6ca-8c6d736112af" in namespace "downward-api-1325" to be "success or failure"
Jun  1 16:36:18.139: INFO: Pod "downward-api-494800d7-3f43-4810-a6ca-8c6d736112af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186328ms
Jun  1 16:36:20.143: INFO: Pod "downward-api-494800d7-3f43-4810-a6ca-8c6d736112af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00573864s
STEP: Saw pod success
Jun  1 16:36:20.143: INFO: Pod "downward-api-494800d7-3f43-4810-a6ca-8c6d736112af" satisfied condition "success or failure"
Jun  1 16:36:20.146: INFO: Trying to get logs from node appserv11 pod downward-api-494800d7-3f43-4810-a6ca-8c6d736112af container dapi-container: <nil>
STEP: delete the pod
Jun  1 16:36:20.164: INFO: Waiting for pod downward-api-494800d7-3f43-4810-a6ca-8c6d736112af to disappear
Jun  1 16:36:20.167: INFO: Pod downward-api-494800d7-3f43-4810-a6ca-8c6d736112af no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:36:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1325" for this suite.
Jun  1 16:36:26.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:36:26.261: INFO: namespace downward-api-1325 deletion completed in 6.090086601s

• [SLOW TEST:8.258 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:36:26.261: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Jun  1 16:36:26.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-9415'
Jun  1 16:36:26.729: INFO: stderr: ""
Jun  1 16:36:26.729: INFO: stdout: "pod/pause created\n"
Jun  1 16:36:26.729: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  1 16:36:26.729: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9415" to be "running and ready"
Jun  1 16:36:26.731: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675667ms
Jun  1 16:36:28.734: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005656384s
Jun  1 16:36:28.734: INFO: Pod "pause" satisfied condition "running and ready"
Jun  1 16:36:28.734: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Jun  1 16:36:28.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 label pods pause testing-label=testing-label-value --namespace=kubectl-9415'
Jun  1 16:36:28.855: INFO: stderr: ""
Jun  1 16:36:28.855: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun  1 16:36:28.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pod pause -L testing-label --namespace=kubectl-9415'
Jun  1 16:36:28.966: INFO: stderr: ""
Jun  1 16:36:28.966: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun  1 16:36:28.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 label pods pause testing-label- --namespace=kubectl-9415'
Jun  1 16:36:29.083: INFO: stderr: ""
Jun  1 16:36:29.083: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun  1 16:36:29.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pod pause -L testing-label --namespace=kubectl-9415'
Jun  1 16:36:29.194: INFO: stderr: ""
Jun  1 16:36:29.194: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Jun  1 16:36:29.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-9415'
Jun  1 16:36:29.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 16:36:29.306: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  1 16:36:29.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get rc,svc -l name=pause --no-headers --namespace=kubectl-9415'
Jun  1 16:36:29.414: INFO: stderr: "No resources found in kubectl-9415 namespace.\n"
Jun  1 16:36:29.414: INFO: stdout: ""
Jun  1 16:36:29.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -l name=pause --namespace=kubectl-9415 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  1 16:36:29.513: INFO: stderr: ""
Jun  1 16:36:29.513: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:36:29.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9415" for this suite.
Jun  1 16:36:35.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:36:35.603: INFO: namespace kubectl-9415 deletion completed in 6.087483558s

• [SLOW TEST:9.342 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:36:35.604: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 16:36:35.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5973'
Jun  1 16:36:35.860: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  1 16:36:35.861: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Jun  1 16:36:35.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5973'
Jun  1 16:36:35.976: INFO: stderr: ""
Jun  1 16:36:35.976: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:36:35.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5973" for this suite.
Jun  1 16:36:41.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:36:42.071: INFO: namespace kubectl-5973 deletion completed in 6.091909671s

• [SLOW TEST:6.467 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:36:42.071: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:36:42.223: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun  1 16:36:42.232: INFO: Number of nodes with available pods: 0
Jun  1 16:36:42.232: INFO: Node appserv10 is running more than one daemon pod
Jun  1 16:36:43.241: INFO: Number of nodes with available pods: 0
Jun  1 16:36:43.241: INFO: Node appserv10 is running more than one daemon pod
Jun  1 16:36:44.240: INFO: Number of nodes with available pods: 2
Jun  1 16:36:44.240: INFO: Node appserv11 is running more than one daemon pod
Jun  1 16:36:45.240: INFO: Number of nodes with available pods: 3
Jun  1 16:36:45.240: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun  1 16:36:45.270: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:45.270: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:45.270: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:46.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:46.279: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:46.279: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:47.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:47.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:47.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:48.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:48.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:48.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:49.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:49.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:49.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:50.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:50.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:50.278: INFO: Pod daemon-set-98l5w is not available
Jun  1 16:36:50.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:51.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:51.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:51.278: INFO: Pod daemon-set-98l5w is not available
Jun  1 16:36:51.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:52.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:52.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:52.278: INFO: Pod daemon-set-98l5w is not available
Jun  1 16:36:52.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:53.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:53.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:53.278: INFO: Pod daemon-set-98l5w is not available
Jun  1 16:36:53.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:54.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:54.278: INFO: Wrong image for pod: daemon-set-98l5w. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:54.278: INFO: Pod daemon-set-98l5w is not available
Jun  1 16:36:54.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:55.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:55.278: INFO: Pod daemon-set-5lbbf is not available
Jun  1 16:36:55.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:56.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:56.278: INFO: Pod daemon-set-5lbbf is not available
Jun  1 16:36:56.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:57.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:57.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:58.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:58.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:59.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:36:59.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:00.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:00.278: INFO: Pod daemon-set-4xsks is not available
Jun  1 16:37:00.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:01.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:01.278: INFO: Pod daemon-set-4xsks is not available
Jun  1 16:37:01.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:02.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:02.278: INFO: Pod daemon-set-4xsks is not available
Jun  1 16:37:02.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:03.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:03.278: INFO: Pod daemon-set-4xsks is not available
Jun  1 16:37:03.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:04.278: INFO: Wrong image for pod: daemon-set-4xsks. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:04.278: INFO: Pod daemon-set-4xsks is not available
Jun  1 16:37:04.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:05.278: INFO: Pod daemon-set-2z2sl is not available
Jun  1 16:37:05.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:06.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:07.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:08.277: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:09.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:10.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:10.278: INFO: Pod daemon-set-s7j9b is not available
Jun  1 16:37:11.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:11.278: INFO: Pod daemon-set-s7j9b is not available
Jun  1 16:37:12.279: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:12.279: INFO: Pod daemon-set-s7j9b is not available
Jun  1 16:37:13.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:13.278: INFO: Pod daemon-set-s7j9b is not available
Jun  1 16:37:14.278: INFO: Wrong image for pod: daemon-set-s7j9b. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun  1 16:37:14.278: INFO: Pod daemon-set-s7j9b is not available
Jun  1 16:37:15.277: INFO: Pod daemon-set-bcr6h is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun  1 16:37:15.288: INFO: Number of nodes with available pods: 2
Jun  1 16:37:15.288: INFO: Node appserv11 is running more than one daemon pod
Jun  1 16:37:16.296: INFO: Number of nodes with available pods: 2
Jun  1 16:37:16.296: INFO: Node appserv11 is running more than one daemon pod
Jun  1 16:37:17.296: INFO: Number of nodes with available pods: 3
Jun  1 16:37:17.296: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4914, will wait for the garbage collector to delete the pods
Jun  1 16:37:17.369: INFO: Deleting DaemonSet.extensions daemon-set took: 6.055298ms
Jun  1 16:37:17.969: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.241154ms
Jun  1 16:37:24.772: INFO: Number of nodes with available pods: 0
Jun  1 16:37:24.772: INFO: Number of running nodes: 0, number of available pods: 0
Jun  1 16:37:24.775: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4914/daemonsets","resourceVersion":"5471"},"items":null}

Jun  1 16:37:24.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4914/pods","resourceVersion":"5471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:37:24.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4914" for this suite.
Jun  1 16:37:30.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:37:30.884: INFO: namespace daemonsets-4914 deletion completed in 6.088983324s

• [SLOW TEST:48.813 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:37:30.885: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 16:37:31.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 16:37:33.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626252, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626252, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626252, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626252, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 16:37:36.811: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:37:46.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6973" for this suite.
Jun  1 16:37:52.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:37:53.050: INFO: namespace webhook-6973 deletion completed in 6.09057458s
STEP: Destroying namespace "webhook-6973-markers" for this suite.
Jun  1 16:37:59.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:37:59.135: INFO: namespace webhook-6973-markers deletion completed in 6.084864085s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.261 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:37:59.146: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun  1 16:38:01.292: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-621748879 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun  1 16:38:11.418: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:38:11.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4124" for this suite.
Jun  1 16:38:17.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:38:17.521: INFO: namespace pods-4124 deletion completed in 6.095690403s

• [SLOW TEST:18.375 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:38:17.521: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 16:38:17.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 16:38:21.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:38:21.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2240" for this suite.
Jun  1 16:38:27.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:38:27.112: INFO: namespace webhook-2240 deletion completed in 6.090710253s
STEP: Destroying namespace "webhook-2240-markers" for this suite.
Jun  1 16:38:33.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:38:33.207: INFO: namespace webhook-2240-markers deletion completed in 6.095101607s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:38:33.220: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9970
I0601 16:38:33.356495      26 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9970, replica count: 1
I0601 16:38:34.406994      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0601 16:38:35.407249      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  1 16:38:35.514: INFO: Created: latency-svc-5hxm4
Jun  1 16:38:35.518: INFO: Got endpoints: latency-svc-5hxm4 [10.748144ms]
Jun  1 16:38:35.524: INFO: Created: latency-svc-xgjfl
Jun  1 16:38:35.527: INFO: Created: latency-svc-rs7dk
Jun  1 16:38:35.527: INFO: Got endpoints: latency-svc-xgjfl [9.248258ms]
Jun  1 16:38:35.529: INFO: Created: latency-svc-zrbpt
Jun  1 16:38:35.530: INFO: Got endpoints: latency-svc-rs7dk [12.004376ms]
Jun  1 16:38:35.532: INFO: Created: latency-svc-6nzt4
Jun  1 16:38:35.532: INFO: Got endpoints: latency-svc-zrbpt [14.384508ms]
Jun  1 16:38:35.534: INFO: Created: latency-svc-fzggz
Jun  1 16:38:35.535: INFO: Got endpoints: latency-svc-6nzt4 [16.870434ms]
Jun  1 16:38:35.537: INFO: Created: latency-svc-bm7np
Jun  1 16:38:35.538: INFO: Got endpoints: latency-svc-fzggz [19.454546ms]
Jun  1 16:38:35.540: INFO: Got endpoints: latency-svc-bm7np [21.709792ms]
Jun  1 16:38:35.545: INFO: Created: latency-svc-r67vw
Jun  1 16:38:35.545: INFO: Got endpoints: latency-svc-r67vw [27.400075ms]
Jun  1 16:38:35.545: INFO: Created: latency-svc-wlk5q
Jun  1 16:38:35.546: INFO: Created: latency-svc-86tj5
Jun  1 16:38:35.546: INFO: Got endpoints: latency-svc-wlk5q [27.850045ms]
Jun  1 16:38:35.548: INFO: Created: latency-svc-srwkl
Jun  1 16:38:35.549: INFO: Got endpoints: latency-svc-86tj5 [30.919589ms]
Jun  1 16:38:35.551: INFO: Got endpoints: latency-svc-srwkl [32.61694ms]
Jun  1 16:38:35.551: INFO: Created: latency-svc-82bnx
Jun  1 16:38:35.553: INFO: Created: latency-svc-9x7h7
Jun  1 16:38:35.553: INFO: Got endpoints: latency-svc-82bnx [35.424746ms]
Jun  1 16:38:35.556: INFO: Created: latency-svc-q97fr
Jun  1 16:38:35.557: INFO: Got endpoints: latency-svc-9x7h7 [38.404764ms]
Jun  1 16:38:35.558: INFO: Got endpoints: latency-svc-q97fr [40.141073ms]
Jun  1 16:38:35.559: INFO: Created: latency-svc-24jn6
Jun  1 16:38:35.561: INFO: Created: latency-svc-xkkwf
Jun  1 16:38:35.561: INFO: Got endpoints: latency-svc-24jn6 [43.495063ms]
Jun  1 16:38:35.564: INFO: Created: latency-svc-n7v8n
Jun  1 16:38:35.565: INFO: Got endpoints: latency-svc-xkkwf [46.53754ms]
Jun  1 16:38:35.566: INFO: Got endpoints: latency-svc-n7v8n [39.176672ms]
Jun  1 16:38:35.567: INFO: Created: latency-svc-vwg8c
Jun  1 16:38:35.569: INFO: Created: latency-svc-nvq7r
Jun  1 16:38:35.569: INFO: Got endpoints: latency-svc-vwg8c [39.125232ms]
Jun  1 16:38:35.572: INFO: Created: latency-svc-jfg29
Jun  1 16:38:35.572: INFO: Got endpoints: latency-svc-nvq7r [39.52364ms]
Jun  1 16:38:35.574: INFO: Created: latency-svc-vwtm5
Jun  1 16:38:35.574: INFO: Got endpoints: latency-svc-jfg29 [39.201604ms]
Jun  1 16:38:35.576: INFO: Created: latency-svc-mpfsw
Jun  1 16:38:35.576: INFO: Got endpoints: latency-svc-vwtm5 [38.501989ms]
Jun  1 16:38:35.578: INFO: Created: latency-svc-dbwnm
Jun  1 16:38:35.578: INFO: Got endpoints: latency-svc-mpfsw [38.605335ms]
Jun  1 16:38:35.581: INFO: Got endpoints: latency-svc-dbwnm [35.286358ms]
Jun  1 16:38:35.581: INFO: Created: latency-svc-spq4r
Jun  1 16:38:35.583: INFO: Created: latency-svc-zgbn5
Jun  1 16:38:35.583: INFO: Got endpoints: latency-svc-spq4r [37.393271ms]
Jun  1 16:38:35.585: INFO: Created: latency-svc-67m7b
Jun  1 16:38:35.585: INFO: Got endpoints: latency-svc-zgbn5 [35.948845ms]
Jun  1 16:38:35.587: INFO: Got endpoints: latency-svc-67m7b [36.495461ms]
Jun  1 16:38:35.588: INFO: Created: latency-svc-24696
Jun  1 16:38:35.589: INFO: Created: latency-svc-6dw82
Jun  1 16:38:35.590: INFO: Got endpoints: latency-svc-24696 [36.490366ms]
Jun  1 16:38:35.592: INFO: Created: latency-svc-bslmf
Jun  1 16:38:35.592: INFO: Got endpoints: latency-svc-6dw82 [35.106722ms]
Jun  1 16:38:35.594: INFO: Created: latency-svc-5n94p
Jun  1 16:38:35.594: INFO: Got endpoints: latency-svc-bslmf [35.683573ms]
Jun  1 16:38:35.596: INFO: Created: latency-svc-lgvdd
Jun  1 16:38:35.596: INFO: Got endpoints: latency-svc-5n94p [34.586749ms]
Jun  1 16:38:35.598: INFO: Got endpoints: latency-svc-lgvdd [32.994131ms]
Jun  1 16:38:35.598: INFO: Created: latency-svc-dqg47
Jun  1 16:38:35.601: INFO: Created: latency-svc-xnckm
Jun  1 16:38:35.603: INFO: Created: latency-svc-tshdw
Jun  1 16:38:35.605: INFO: Created: latency-svc-dglh4
Jun  1 16:38:35.607: INFO: Created: latency-svc-m765s
Jun  1 16:38:35.609: INFO: Created: latency-svc-6pkcd
Jun  1 16:38:35.611: INFO: Created: latency-svc-zlxgt
Jun  1 16:38:35.613: INFO: Created: latency-svc-knmmd
Jun  1 16:38:35.615: INFO: Created: latency-svc-fjffm
Jun  1 16:38:35.617: INFO: Got endpoints: latency-svc-dqg47 [50.177904ms]
Jun  1 16:38:35.617: INFO: Created: latency-svc-mjdbh
Jun  1 16:38:35.619: INFO: Created: latency-svc-2q57s
Jun  1 16:38:35.621: INFO: Created: latency-svc-p5c7x
Jun  1 16:38:35.623: INFO: Created: latency-svc-pzw4n
Jun  1 16:38:35.624: INFO: Created: latency-svc-wfhzw
Jun  1 16:38:35.627: INFO: Created: latency-svc-jwnb9
Jun  1 16:38:35.629: INFO: Created: latency-svc-n8dr9
Jun  1 16:38:35.667: INFO: Got endpoints: latency-svc-xnckm [97.509804ms]
Jun  1 16:38:35.671: INFO: Created: latency-svc-k46sn
Jun  1 16:38:35.717: INFO: Got endpoints: latency-svc-tshdw [145.493332ms]
Jun  1 16:38:35.723: INFO: Created: latency-svc-hq8lb
Jun  1 16:38:35.768: INFO: Got endpoints: latency-svc-dglh4 [193.41769ms]
Jun  1 16:38:35.773: INFO: Created: latency-svc-7zbnf
Jun  1 16:38:35.817: INFO: Got endpoints: latency-svc-m765s [241.351813ms]
Jun  1 16:38:35.823: INFO: Created: latency-svc-jkxbf
Jun  1 16:38:35.867: INFO: Got endpoints: latency-svc-6pkcd [289.011112ms]
Jun  1 16:38:35.873: INFO: Created: latency-svc-q42jf
Jun  1 16:38:35.917: INFO: Got endpoints: latency-svc-zlxgt [336.472204ms]
Jun  1 16:38:35.923: INFO: Created: latency-svc-ftj22
Jun  1 16:38:35.968: INFO: Got endpoints: latency-svc-knmmd [384.335738ms]
Jun  1 16:38:35.973: INFO: Created: latency-svc-z94sq
Jun  1 16:38:36.017: INFO: Got endpoints: latency-svc-fjffm [432.353206ms]
Jun  1 16:38:36.023: INFO: Created: latency-svc-6gn7q
Jun  1 16:38:36.067: INFO: Got endpoints: latency-svc-mjdbh [480.151198ms]
Jun  1 16:38:36.073: INFO: Created: latency-svc-d9w2j
Jun  1 16:38:36.117: INFO: Got endpoints: latency-svc-2q57s [527.189924ms]
Jun  1 16:38:36.123: INFO: Created: latency-svc-q8hbx
Jun  1 16:38:36.167: INFO: Got endpoints: latency-svc-p5c7x [575.736617ms]
Jun  1 16:38:36.173: INFO: Created: latency-svc-9rjxt
Jun  1 16:38:36.217: INFO: Got endpoints: latency-svc-pzw4n [623.396441ms]
Jun  1 16:38:36.223: INFO: Created: latency-svc-gwvzp
Jun  1 16:38:36.268: INFO: Got endpoints: latency-svc-wfhzw [671.511404ms]
Jun  1 16:38:36.274: INFO: Created: latency-svc-xk4vw
Jun  1 16:38:36.318: INFO: Got endpoints: latency-svc-jwnb9 [719.823384ms]
Jun  1 16:38:36.324: INFO: Created: latency-svc-5562f
Jun  1 16:38:36.368: INFO: Got endpoints: latency-svc-n8dr9 [750.973687ms]
Jun  1 16:38:36.374: INFO: Created: latency-svc-hplqm
Jun  1 16:38:36.417: INFO: Got endpoints: latency-svc-k46sn [750.840908ms]
Jun  1 16:38:36.424: INFO: Created: latency-svc-zlhjz
Jun  1 16:38:36.468: INFO: Got endpoints: latency-svc-hq8lb [750.198732ms]
Jun  1 16:38:36.474: INFO: Created: latency-svc-b78dh
Jun  1 16:38:36.517: INFO: Got endpoints: latency-svc-7zbnf [749.756526ms]
Jun  1 16:38:36.524: INFO: Created: latency-svc-8l8m7
Jun  1 16:38:36.568: INFO: Got endpoints: latency-svc-jkxbf [750.099852ms]
Jun  1 16:38:36.574: INFO: Created: latency-svc-jgdx2
Jun  1 16:38:36.618: INFO: Got endpoints: latency-svc-q42jf [750.261041ms]
Jun  1 16:38:36.624: INFO: Created: latency-svc-6l9g6
Jun  1 16:38:36.667: INFO: Got endpoints: latency-svc-ftj22 [750.120373ms]
Jun  1 16:38:36.673: INFO: Created: latency-svc-dcrrj
Jun  1 16:38:36.718: INFO: Got endpoints: latency-svc-z94sq [749.800036ms]
Jun  1 16:38:36.724: INFO: Created: latency-svc-6z7gc
Jun  1 16:38:36.768: INFO: Got endpoints: latency-svc-6gn7q [750.164828ms]
Jun  1 16:38:36.774: INFO: Created: latency-svc-78fqx
Jun  1 16:38:36.817: INFO: Got endpoints: latency-svc-d9w2j [749.661385ms]
Jun  1 16:38:36.823: INFO: Created: latency-svc-qfcx7
Jun  1 16:38:36.868: INFO: Got endpoints: latency-svc-q8hbx [750.364445ms]
Jun  1 16:38:36.874: INFO: Created: latency-svc-srxq2
Jun  1 16:38:36.918: INFO: Got endpoints: latency-svc-9rjxt [750.172821ms]
Jun  1 16:38:36.924: INFO: Created: latency-svc-8kwsq
Jun  1 16:38:36.968: INFO: Got endpoints: latency-svc-gwvzp [750.842799ms]
Jun  1 16:38:37.009: INFO: Created: latency-svc-rtqld
Jun  1 16:38:37.018: INFO: Got endpoints: latency-svc-xk4vw [750.070915ms]
Jun  1 16:38:37.023: INFO: Created: latency-svc-rts75
Jun  1 16:38:37.067: INFO: Got endpoints: latency-svc-5562f [749.750135ms]
Jun  1 16:38:37.073: INFO: Created: latency-svc-bm9wq
Jun  1 16:38:37.118: INFO: Got endpoints: latency-svc-hplqm [750.297019ms]
Jun  1 16:38:37.123: INFO: Created: latency-svc-skch8
Jun  1 16:38:37.167: INFO: Got endpoints: latency-svc-zlhjz [749.76267ms]
Jun  1 16:38:37.173: INFO: Created: latency-svc-nn688
Jun  1 16:38:37.217: INFO: Got endpoints: latency-svc-b78dh [749.463961ms]
Jun  1 16:38:37.223: INFO: Created: latency-svc-67vsq
Jun  1 16:38:37.267: INFO: Got endpoints: latency-svc-8l8m7 [749.63141ms]
Jun  1 16:38:37.272: INFO: Created: latency-svc-r8g2h
Jun  1 16:38:37.317: INFO: Got endpoints: latency-svc-jgdx2 [749.324564ms]
Jun  1 16:38:37.322: INFO: Created: latency-svc-ksg98
Jun  1 16:38:37.367: INFO: Got endpoints: latency-svc-6l9g6 [749.455815ms]
Jun  1 16:38:37.373: INFO: Created: latency-svc-8lfsg
Jun  1 16:38:37.437: INFO: Got endpoints: latency-svc-dcrrj [769.761299ms]
Jun  1 16:38:37.443: INFO: Created: latency-svc-9xwf5
Jun  1 16:38:37.470: INFO: Got endpoints: latency-svc-6z7gc [752.0891ms]
Jun  1 16:38:37.478: INFO: Created: latency-svc-fqssg
Jun  1 16:38:37.517: INFO: Got endpoints: latency-svc-78fqx [749.658206ms]
Jun  1 16:38:37.522: INFO: Created: latency-svc-jljkt
Jun  1 16:38:37.567: INFO: Got endpoints: latency-svc-qfcx7 [750.21108ms]
Jun  1 16:38:37.573: INFO: Created: latency-svc-tkw2p
Jun  1 16:38:37.617: INFO: Got endpoints: latency-svc-srxq2 [749.550523ms]
Jun  1 16:38:37.622: INFO: Created: latency-svc-kvbp8
Jun  1 16:38:37.667: INFO: Got endpoints: latency-svc-8kwsq [749.396393ms]
Jun  1 16:38:37.672: INFO: Created: latency-svc-nwbrn
Jun  1 16:38:37.718: INFO: Got endpoints: latency-svc-rtqld [749.358806ms]
Jun  1 16:38:37.723: INFO: Created: latency-svc-sfjct
Jun  1 16:38:37.767: INFO: Got endpoints: latency-svc-rts75 [749.417597ms]
Jun  1 16:38:37.772: INFO: Created: latency-svc-pcg22
Jun  1 16:38:37.817: INFO: Got endpoints: latency-svc-bm9wq [749.58677ms]
Jun  1 16:38:37.821: INFO: Created: latency-svc-c4pcs
Jun  1 16:38:37.867: INFO: Got endpoints: latency-svc-skch8 [748.910689ms]
Jun  1 16:38:37.871: INFO: Created: latency-svc-nn6vj
Jun  1 16:38:37.917: INFO: Got endpoints: latency-svc-nn688 [749.786516ms]
Jun  1 16:38:37.928: INFO: Created: latency-svc-bxd22
Jun  1 16:38:37.967: INFO: Got endpoints: latency-svc-67vsq [749.470497ms]
Jun  1 16:38:37.972: INFO: Created: latency-svc-tldlt
Jun  1 16:38:38.017: INFO: Got endpoints: latency-svc-r8g2h [749.933392ms]
Jun  1 16:38:38.022: INFO: Created: latency-svc-7csxr
Jun  1 16:38:38.067: INFO: Got endpoints: latency-svc-ksg98 [750.060351ms]
Jun  1 16:38:38.071: INFO: Created: latency-svc-svcw2
Jun  1 16:38:38.116: INFO: Got endpoints: latency-svc-8lfsg [749.265291ms]
Jun  1 16:38:38.121: INFO: Created: latency-svc-bv8s7
Jun  1 16:38:38.167: INFO: Got endpoints: latency-svc-9xwf5 [729.461856ms]
Jun  1 16:38:38.171: INFO: Created: latency-svc-2jjlv
Jun  1 16:38:38.217: INFO: Got endpoints: latency-svc-fqssg [747.208602ms]
Jun  1 16:38:38.221: INFO: Created: latency-svc-nvbxk
Jun  1 16:38:38.267: INFO: Got endpoints: latency-svc-jljkt [749.489833ms]
Jun  1 16:38:38.271: INFO: Created: latency-svc-hvzxj
Jun  1 16:38:38.317: INFO: Got endpoints: latency-svc-tkw2p [749.702928ms]
Jun  1 16:38:38.323: INFO: Created: latency-svc-jjpzr
Jun  1 16:38:38.367: INFO: Got endpoints: latency-svc-kvbp8 [749.46911ms]
Jun  1 16:38:38.372: INFO: Created: latency-svc-clvlc
Jun  1 16:38:38.417: INFO: Got endpoints: latency-svc-nwbrn [750.009836ms]
Jun  1 16:38:38.423: INFO: Created: latency-svc-l6jcb
Jun  1 16:38:38.467: INFO: Got endpoints: latency-svc-sfjct [749.544643ms]
Jun  1 16:38:38.473: INFO: Created: latency-svc-dlr5t
Jun  1 16:38:38.517: INFO: Got endpoints: latency-svc-pcg22 [750.311129ms]
Jun  1 16:38:38.523: INFO: Created: latency-svc-drcn5
Jun  1 16:38:38.567: INFO: Got endpoints: latency-svc-c4pcs [750.341109ms]
Jun  1 16:38:38.573: INFO: Created: latency-svc-qn7g8
Jun  1 16:38:38.617: INFO: Got endpoints: latency-svc-nn6vj [750.343933ms]
Jun  1 16:38:38.623: INFO: Created: latency-svc-zpb7n
Jun  1 16:38:38.667: INFO: Got endpoints: latency-svc-bxd22 [750.011398ms]
Jun  1 16:38:38.672: INFO: Created: latency-svc-cft58
Jun  1 16:38:38.717: INFO: Got endpoints: latency-svc-tldlt [750.426877ms]
Jun  1 16:38:38.723: INFO: Created: latency-svc-hfhnm
Jun  1 16:38:38.767: INFO: Got endpoints: latency-svc-7csxr [750.118387ms]
Jun  1 16:38:38.773: INFO: Created: latency-svc-h676j
Jun  1 16:38:38.817: INFO: Got endpoints: latency-svc-svcw2 [750.232125ms]
Jun  1 16:38:38.823: INFO: Created: latency-svc-z8tqh
Jun  1 16:38:38.867: INFO: Got endpoints: latency-svc-bv8s7 [750.682529ms]
Jun  1 16:38:38.873: INFO: Created: latency-svc-8pstp
Jun  1 16:38:38.917: INFO: Got endpoints: latency-svc-2jjlv [750.555724ms]
Jun  1 16:38:38.923: INFO: Created: latency-svc-tjqdk
Jun  1 16:38:38.967: INFO: Got endpoints: latency-svc-nvbxk [750.455143ms]
Jun  1 16:38:38.973: INFO: Created: latency-svc-8tcd5
Jun  1 16:38:39.017: INFO: Got endpoints: latency-svc-hvzxj [750.541802ms]
Jun  1 16:38:39.023: INFO: Created: latency-svc-zhtv5
Jun  1 16:38:39.067: INFO: Got endpoints: latency-svc-jjpzr [750.013757ms]
Jun  1 16:38:39.073: INFO: Created: latency-svc-m42zc
Jun  1 16:38:39.117: INFO: Got endpoints: latency-svc-clvlc [750.470177ms]
Jun  1 16:38:39.122: INFO: Created: latency-svc-jn7n6
Jun  1 16:38:39.167: INFO: Got endpoints: latency-svc-l6jcb [749.959169ms]
Jun  1 16:38:39.173: INFO: Created: latency-svc-xc9qm
Jun  1 16:38:39.217: INFO: Got endpoints: latency-svc-dlr5t [749.931422ms]
Jun  1 16:38:39.223: INFO: Created: latency-svc-h2vkr
Jun  1 16:38:39.267: INFO: Got endpoints: latency-svc-drcn5 [749.758898ms]
Jun  1 16:38:39.273: INFO: Created: latency-svc-txpj8
Jun  1 16:38:39.317: INFO: Got endpoints: latency-svc-qn7g8 [749.886508ms]
Jun  1 16:38:39.322: INFO: Created: latency-svc-qbshd
Jun  1 16:38:39.367: INFO: Got endpoints: latency-svc-zpb7n [749.913285ms]
Jun  1 16:38:39.372: INFO: Created: latency-svc-428bk
Jun  1 16:38:39.417: INFO: Got endpoints: latency-svc-cft58 [750.039985ms]
Jun  1 16:38:39.423: INFO: Created: latency-svc-2dxf5
Jun  1 16:38:39.467: INFO: Got endpoints: latency-svc-hfhnm [749.971315ms]
Jun  1 16:38:39.473: INFO: Created: latency-svc-zfqnl
Jun  1 16:38:39.517: INFO: Got endpoints: latency-svc-h676j [750.065857ms]
Jun  1 16:38:39.524: INFO: Created: latency-svc-tnhm7
Jun  1 16:38:39.567: INFO: Got endpoints: latency-svc-z8tqh [749.942861ms]
Jun  1 16:38:39.573: INFO: Created: latency-svc-vkrxz
Jun  1 16:38:39.618: INFO: Got endpoints: latency-svc-8pstp [750.352479ms]
Jun  1 16:38:39.624: INFO: Created: latency-svc-68d44
Jun  1 16:38:39.667: INFO: Got endpoints: latency-svc-tjqdk [750.03558ms]
Jun  1 16:38:39.673: INFO: Created: latency-svc-bf9lz
Jun  1 16:38:39.717: INFO: Got endpoints: latency-svc-8tcd5 [750.061094ms]
Jun  1 16:38:39.723: INFO: Created: latency-svc-nhbrk
Jun  1 16:38:39.768: INFO: Got endpoints: latency-svc-zhtv5 [750.517739ms]
Jun  1 16:38:39.774: INFO: Created: latency-svc-4vbn6
Jun  1 16:38:39.818: INFO: Got endpoints: latency-svc-m42zc [750.284408ms]
Jun  1 16:38:39.823: INFO: Created: latency-svc-qwg59
Jun  1 16:38:39.868: INFO: Got endpoints: latency-svc-jn7n6 [750.435478ms]
Jun  1 16:38:39.873: INFO: Created: latency-svc-76x5f
Jun  1 16:38:39.917: INFO: Got endpoints: latency-svc-xc9qm [750.23141ms]
Jun  1 16:38:39.924: INFO: Created: latency-svc-ngv8w
Jun  1 16:38:39.968: INFO: Got endpoints: latency-svc-h2vkr [750.408092ms]
Jun  1 16:38:39.974: INFO: Created: latency-svc-6hql7
Jun  1 16:38:40.018: INFO: Got endpoints: latency-svc-txpj8 [750.229011ms]
Jun  1 16:38:40.024: INFO: Created: latency-svc-lqrx5
Jun  1 16:38:40.067: INFO: Got endpoints: latency-svc-qbshd [750.167282ms]
Jun  1 16:38:40.074: INFO: Created: latency-svc-hqf2r
Jun  1 16:38:40.117: INFO: Got endpoints: latency-svc-428bk [750.058551ms]
Jun  1 16:38:40.123: INFO: Created: latency-svc-gwzrr
Jun  1 16:38:40.167: INFO: Got endpoints: latency-svc-2dxf5 [750.038938ms]
Jun  1 16:38:40.172: INFO: Created: latency-svc-24w5t
Jun  1 16:38:40.218: INFO: Got endpoints: latency-svc-zfqnl [750.376758ms]
Jun  1 16:38:40.224: INFO: Created: latency-svc-25lvr
Jun  1 16:38:40.267: INFO: Got endpoints: latency-svc-tnhm7 [749.791621ms]
Jun  1 16:38:40.273: INFO: Created: latency-svc-bcvvd
Jun  1 16:38:40.318: INFO: Got endpoints: latency-svc-vkrxz [750.20972ms]
Jun  1 16:38:40.323: INFO: Created: latency-svc-2ccxw
Jun  1 16:38:40.367: INFO: Got endpoints: latency-svc-68d44 [749.75816ms]
Jun  1 16:38:40.373: INFO: Created: latency-svc-gvh95
Jun  1 16:38:40.417: INFO: Got endpoints: latency-svc-bf9lz [750.065387ms]
Jun  1 16:38:40.423: INFO: Created: latency-svc-nxv6c
Jun  1 16:38:40.468: INFO: Got endpoints: latency-svc-nhbrk [750.390581ms]
Jun  1 16:38:40.474: INFO: Created: latency-svc-2nx6j
Jun  1 16:38:40.518: INFO: Got endpoints: latency-svc-4vbn6 [749.729398ms]
Jun  1 16:38:40.524: INFO: Created: latency-svc-dprrw
Jun  1 16:38:40.568: INFO: Got endpoints: latency-svc-qwg59 [749.999302ms]
Jun  1 16:38:40.574: INFO: Created: latency-svc-7m62m
Jun  1 16:38:40.618: INFO: Got endpoints: latency-svc-76x5f [749.793677ms]
Jun  1 16:38:40.624: INFO: Created: latency-svc-sg2wv
Jun  1 16:38:40.668: INFO: Got endpoints: latency-svc-ngv8w [750.558461ms]
Jun  1 16:38:40.673: INFO: Created: latency-svc-hc5wb
Jun  1 16:38:40.717: INFO: Got endpoints: latency-svc-6hql7 [749.634767ms]
Jun  1 16:38:40.723: INFO: Created: latency-svc-hvpdw
Jun  1 16:38:40.767: INFO: Got endpoints: latency-svc-lqrx5 [749.745301ms]
Jun  1 16:38:40.772: INFO: Created: latency-svc-c57pm
Jun  1 16:38:40.818: INFO: Got endpoints: latency-svc-hqf2r [750.206417ms]
Jun  1 16:38:40.823: INFO: Created: latency-svc-6tdhb
Jun  1 16:38:40.867: INFO: Got endpoints: latency-svc-gwzrr [749.839283ms]
Jun  1 16:38:40.872: INFO: Created: latency-svc-z2wp2
Jun  1 16:38:40.917: INFO: Got endpoints: latency-svc-24w5t [749.765114ms]
Jun  1 16:38:40.923: INFO: Created: latency-svc-rnn22
Jun  1 16:38:40.967: INFO: Got endpoints: latency-svc-25lvr [749.441131ms]
Jun  1 16:38:40.973: INFO: Created: latency-svc-4lrdn
Jun  1 16:38:41.017: INFO: Got endpoints: latency-svc-bcvvd [749.926042ms]
Jun  1 16:38:41.023: INFO: Created: latency-svc-dg4bm
Jun  1 16:38:41.067: INFO: Got endpoints: latency-svc-2ccxw [749.685038ms]
Jun  1 16:38:41.073: INFO: Created: latency-svc-sv2dw
Jun  1 16:38:41.118: INFO: Got endpoints: latency-svc-gvh95 [750.228679ms]
Jun  1 16:38:41.124: INFO: Created: latency-svc-99g72
Jun  1 16:38:41.168: INFO: Got endpoints: latency-svc-nxv6c [750.013016ms]
Jun  1 16:38:41.174: INFO: Created: latency-svc-tkl2z
Jun  1 16:38:41.218: INFO: Got endpoints: latency-svc-2nx6j [749.716863ms]
Jun  1 16:38:41.224: INFO: Created: latency-svc-qfwvq
Jun  1 16:38:41.267: INFO: Got endpoints: latency-svc-dprrw [749.77601ms]
Jun  1 16:38:41.273: INFO: Created: latency-svc-wp5m7
Jun  1 16:38:41.317: INFO: Got endpoints: latency-svc-7m62m [749.833447ms]
Jun  1 16:38:41.323: INFO: Created: latency-svc-7lsm6
Jun  1 16:38:41.368: INFO: Got endpoints: latency-svc-sg2wv [749.935017ms]
Jun  1 16:38:41.373: INFO: Created: latency-svc-w5v9h
Jun  1 16:38:41.417: INFO: Got endpoints: latency-svc-hc5wb [749.413334ms]
Jun  1 16:38:41.423: INFO: Created: latency-svc-pq7sw
Jun  1 16:38:41.467: INFO: Got endpoints: latency-svc-hvpdw [749.981076ms]
Jun  1 16:38:41.483: INFO: Created: latency-svc-d4lwm
Jun  1 16:38:41.518: INFO: Got endpoints: latency-svc-c57pm [750.339657ms]
Jun  1 16:38:41.524: INFO: Created: latency-svc-vr8mg
Jun  1 16:38:41.568: INFO: Got endpoints: latency-svc-6tdhb [749.943487ms]
Jun  1 16:38:41.574: INFO: Created: latency-svc-9mvlg
Jun  1 16:38:41.618: INFO: Got endpoints: latency-svc-z2wp2 [750.410482ms]
Jun  1 16:38:41.624: INFO: Created: latency-svc-x7dkm
Jun  1 16:38:41.668: INFO: Got endpoints: latency-svc-rnn22 [750.491395ms]
Jun  1 16:38:41.674: INFO: Created: latency-svc-8fhwh
Jun  1 16:38:41.717: INFO: Got endpoints: latency-svc-4lrdn [750.359258ms]
Jun  1 16:38:41.724: INFO: Created: latency-svc-wpksp
Jun  1 16:38:41.767: INFO: Got endpoints: latency-svc-dg4bm [750.194512ms]
Jun  1 16:38:41.773: INFO: Created: latency-svc-kwhkb
Jun  1 16:38:41.817: INFO: Got endpoints: latency-svc-sv2dw [750.158781ms]
Jun  1 16:38:41.823: INFO: Created: latency-svc-mc7sk
Jun  1 16:38:41.868: INFO: Got endpoints: latency-svc-99g72 [749.961555ms]
Jun  1 16:38:41.873: INFO: Created: latency-svc-6hpvm
Jun  1 16:38:41.918: INFO: Got endpoints: latency-svc-tkl2z [750.13193ms]
Jun  1 16:38:41.924: INFO: Created: latency-svc-j2752
Jun  1 16:38:41.967: INFO: Got endpoints: latency-svc-qfwvq [749.697005ms]
Jun  1 16:38:41.973: INFO: Created: latency-svc-lcw5f
Jun  1 16:38:42.017: INFO: Got endpoints: latency-svc-wp5m7 [749.948474ms]
Jun  1 16:38:42.024: INFO: Created: latency-svc-7gfkt
Jun  1 16:38:42.068: INFO: Got endpoints: latency-svc-7lsm6 [750.190623ms]
Jun  1 16:38:42.074: INFO: Created: latency-svc-zzwfv
Jun  1 16:38:42.118: INFO: Got endpoints: latency-svc-w5v9h [750.140586ms]
Jun  1 16:38:42.123: INFO: Created: latency-svc-d7m8q
Jun  1 16:38:42.167: INFO: Got endpoints: latency-svc-pq7sw [749.65507ms]
Jun  1 16:38:42.173: INFO: Created: latency-svc-n8b78
Jun  1 16:38:42.217: INFO: Got endpoints: latency-svc-d4lwm [750.068845ms]
Jun  1 16:38:42.224: INFO: Created: latency-svc-4h9wv
Jun  1 16:38:42.268: INFO: Got endpoints: latency-svc-vr8mg [749.944674ms]
Jun  1 16:38:42.274: INFO: Created: latency-svc-h97sv
Jun  1 16:38:42.317: INFO: Got endpoints: latency-svc-9mvlg [749.702251ms]
Jun  1 16:38:42.323: INFO: Created: latency-svc-2fmd7
Jun  1 16:38:42.367: INFO: Got endpoints: latency-svc-x7dkm [749.782291ms]
Jun  1 16:38:42.373: INFO: Created: latency-svc-qj42t
Jun  1 16:38:42.417: INFO: Got endpoints: latency-svc-8fhwh [749.763976ms]
Jun  1 16:38:42.423: INFO: Created: latency-svc-nk6xq
Jun  1 16:38:42.468: INFO: Got endpoints: latency-svc-wpksp [750.098136ms]
Jun  1 16:38:42.474: INFO: Created: latency-svc-54hw7
Jun  1 16:38:42.517: INFO: Got endpoints: latency-svc-kwhkb [749.973305ms]
Jun  1 16:38:42.523: INFO: Created: latency-svc-2nk25
Jun  1 16:38:42.567: INFO: Got endpoints: latency-svc-mc7sk [749.788337ms]
Jun  1 16:38:42.573: INFO: Created: latency-svc-cjfth
Jun  1 16:38:42.617: INFO: Got endpoints: latency-svc-6hpvm [749.672857ms]
Jun  1 16:38:42.623: INFO: Created: latency-svc-wbwzk
Jun  1 16:38:42.667: INFO: Got endpoints: latency-svc-j2752 [749.520694ms]
Jun  1 16:38:42.673: INFO: Created: latency-svc-lwqfn
Jun  1 16:38:42.717: INFO: Got endpoints: latency-svc-lcw5f [749.877634ms]
Jun  1 16:38:42.723: INFO: Created: latency-svc-cb4m9
Jun  1 16:38:42.768: INFO: Got endpoints: latency-svc-7gfkt [750.102806ms]
Jun  1 16:38:42.774: INFO: Created: latency-svc-c4knc
Jun  1 16:38:42.818: INFO: Got endpoints: latency-svc-zzwfv [750.053722ms]
Jun  1 16:38:42.824: INFO: Created: latency-svc-5kmfb
Jun  1 16:38:42.867: INFO: Got endpoints: latency-svc-d7m8q [749.623545ms]
Jun  1 16:38:42.873: INFO: Created: latency-svc-xr9bc
Jun  1 16:38:42.917: INFO: Got endpoints: latency-svc-n8b78 [750.185938ms]
Jun  1 16:38:42.923: INFO: Created: latency-svc-nth54
Jun  1 16:38:42.967: INFO: Got endpoints: latency-svc-4h9wv [749.868752ms]
Jun  1 16:38:42.973: INFO: Created: latency-svc-85glp
Jun  1 16:38:43.018: INFO: Got endpoints: latency-svc-h97sv [749.833178ms]
Jun  1 16:38:43.023: INFO: Created: latency-svc-6wfns
Jun  1 16:38:43.067: INFO: Got endpoints: latency-svc-2fmd7 [749.722353ms]
Jun  1 16:38:43.073: INFO: Created: latency-svc-2ltnf
Jun  1 16:38:43.118: INFO: Got endpoints: latency-svc-qj42t [750.10699ms]
Jun  1 16:38:43.123: INFO: Created: latency-svc-s6qrn
Jun  1 16:38:43.168: INFO: Got endpoints: latency-svc-nk6xq [750.263023ms]
Jun  1 16:38:43.174: INFO: Created: latency-svc-pdvqd
Jun  1 16:38:43.218: INFO: Got endpoints: latency-svc-54hw7 [749.968997ms]
Jun  1 16:38:43.223: INFO: Created: latency-svc-fhv7z
Jun  1 16:38:43.267: INFO: Got endpoints: latency-svc-2nk25 [750.020994ms]
Jun  1 16:38:43.274: INFO: Created: latency-svc-l28m7
Jun  1 16:38:43.318: INFO: Got endpoints: latency-svc-cjfth [750.24528ms]
Jun  1 16:38:43.323: INFO: Created: latency-svc-8bgw5
Jun  1 16:38:43.367: INFO: Got endpoints: latency-svc-wbwzk [749.882446ms]
Jun  1 16:38:43.418: INFO: Got endpoints: latency-svc-lwqfn [750.611268ms]
Jun  1 16:38:43.468: INFO: Got endpoints: latency-svc-cb4m9 [750.276342ms]
Jun  1 16:38:43.518: INFO: Got endpoints: latency-svc-c4knc [750.086221ms]
Jun  1 16:38:43.568: INFO: Got endpoints: latency-svc-5kmfb [749.864636ms]
Jun  1 16:38:43.617: INFO: Got endpoints: latency-svc-xr9bc [749.924074ms]
Jun  1 16:38:43.668: INFO: Got endpoints: latency-svc-nth54 [750.171331ms]
Jun  1 16:38:43.717: INFO: Got endpoints: latency-svc-85glp [749.92357ms]
Jun  1 16:38:43.768: INFO: Got endpoints: latency-svc-6wfns [749.960983ms]
Jun  1 16:38:43.817: INFO: Got endpoints: latency-svc-2ltnf [749.939689ms]
Jun  1 16:38:43.867: INFO: Got endpoints: latency-svc-s6qrn [749.805183ms]
Jun  1 16:38:43.918: INFO: Got endpoints: latency-svc-pdvqd [750.013508ms]
Jun  1 16:38:43.968: INFO: Got endpoints: latency-svc-fhv7z [750.014379ms]
Jun  1 16:38:44.018: INFO: Got endpoints: latency-svc-l28m7 [750.144811ms]
Jun  1 16:38:44.068: INFO: Got endpoints: latency-svc-8bgw5 [750.141085ms]
Jun  1 16:38:44.068: INFO: Latencies: [9.248258ms 12.004376ms 14.384508ms 16.870434ms 19.454546ms 21.709792ms 27.400075ms 27.850045ms 30.919589ms 32.61694ms 32.994131ms 34.586749ms 35.106722ms 35.286358ms 35.424746ms 35.683573ms 35.948845ms 36.490366ms 36.495461ms 37.393271ms 38.404764ms 38.501989ms 38.605335ms 39.125232ms 39.176672ms 39.201604ms 39.52364ms 40.141073ms 43.495063ms 46.53754ms 50.177904ms 97.509804ms 145.493332ms 193.41769ms 241.351813ms 289.011112ms 336.472204ms 384.335738ms 432.353206ms 480.151198ms 527.189924ms 575.736617ms 623.396441ms 671.511404ms 719.823384ms 729.461856ms 747.208602ms 748.910689ms 749.265291ms 749.324564ms 749.358806ms 749.396393ms 749.413334ms 749.417597ms 749.441131ms 749.455815ms 749.463961ms 749.46911ms 749.470497ms 749.489833ms 749.520694ms 749.544643ms 749.550523ms 749.58677ms 749.623545ms 749.63141ms 749.634767ms 749.65507ms 749.658206ms 749.661385ms 749.672857ms 749.685038ms 749.697005ms 749.702251ms 749.702928ms 749.716863ms 749.722353ms 749.729398ms 749.745301ms 749.750135ms 749.756526ms 749.75816ms 749.758898ms 749.76267ms 749.763976ms 749.765114ms 749.77601ms 749.782291ms 749.786516ms 749.788337ms 749.791621ms 749.793677ms 749.800036ms 749.805183ms 749.833178ms 749.833447ms 749.839283ms 749.864636ms 749.868752ms 749.877634ms 749.882446ms 749.886508ms 749.913285ms 749.92357ms 749.924074ms 749.926042ms 749.931422ms 749.933392ms 749.935017ms 749.939689ms 749.942861ms 749.943487ms 749.944674ms 749.948474ms 749.959169ms 749.960983ms 749.961555ms 749.968997ms 749.971315ms 749.973305ms 749.981076ms 749.999302ms 750.009836ms 750.011398ms 750.013016ms 750.013508ms 750.013757ms 750.014379ms 750.020994ms 750.03558ms 750.038938ms 750.039985ms 750.053722ms 750.058551ms 750.060351ms 750.061094ms 750.065387ms 750.065857ms 750.068845ms 750.070915ms 750.086221ms 750.098136ms 750.099852ms 750.102806ms 750.10699ms 750.118387ms 750.120373ms 750.13193ms 750.140586ms 750.141085ms 750.144811ms 750.158781ms 750.164828ms 750.167282ms 750.171331ms 750.172821ms 750.185938ms 750.190623ms 750.194512ms 750.198732ms 750.206417ms 750.20972ms 750.21108ms 750.228679ms 750.229011ms 750.23141ms 750.232125ms 750.24528ms 750.261041ms 750.263023ms 750.276342ms 750.284408ms 750.297019ms 750.311129ms 750.339657ms 750.341109ms 750.343933ms 750.352479ms 750.359258ms 750.364445ms 750.376758ms 750.390581ms 750.408092ms 750.410482ms 750.426877ms 750.435478ms 750.455143ms 750.470177ms 750.491395ms 750.517739ms 750.541802ms 750.555724ms 750.558461ms 750.611268ms 750.682529ms 750.840908ms 750.842799ms 750.973687ms 752.0891ms 769.761299ms]
Jun  1 16:38:44.068: INFO: 50 %ile: 749.882446ms
Jun  1 16:38:44.068: INFO: 90 %ile: 750.376758ms
Jun  1 16:38:44.068: INFO: 99 %ile: 752.0891ms
Jun  1 16:38:44.068: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:38:44.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9970" for this suite.
Jun  1 16:39:14.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:39:14.204: INFO: namespace svc-latency-9970 deletion completed in 30.132220821s

• [SLOW TEST:40.984 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:39:14.204: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9552
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:39:14.356: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  1 16:39:18.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-9552 create -f -'
Jun  1 16:39:18.348: INFO: stderr: ""
Jun  1 16:39:18.348: INFO: stdout: "e2e-test-crd-publish-openapi-6362-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  1 16:39:18.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-9552 delete e2e-test-crd-publish-openapi-6362-crds test-cr'
Jun  1 16:39:18.467: INFO: stderr: ""
Jun  1 16:39:18.467: INFO: stdout: "e2e-test-crd-publish-openapi-6362-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  1 16:39:18.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-9552 apply -f -'
Jun  1 16:39:18.660: INFO: stderr: ""
Jun  1 16:39:18.660: INFO: stdout: "e2e-test-crd-publish-openapi-6362-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  1 16:39:18.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-9552 delete e2e-test-crd-publish-openapi-6362-crds test-cr'
Jun  1 16:39:18.775: INFO: stderr: ""
Jun  1 16:39:18.775: INFO: stdout: "e2e-test-crd-publish-openapi-6362-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  1 16:39:18.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-6362-crds'
Jun  1 16:39:18.959: INFO: stderr: ""
Jun  1 16:39:18.959: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6362-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:39:22.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9552" for this suite.
Jun  1 16:39:28.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:39:28.615: INFO: namespace crd-publish-openapi-9552 deletion completed in 6.119349617s

• [SLOW TEST:14.410 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:39:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-30a1265c-988a-4ed3-90dc-dbbc1309393c
STEP: Creating a pod to test consume secrets
Jun  1 16:39:28.754: INFO: Waiting up to 5m0s for pod "pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c" in namespace "secrets-6815" to be "success or failure"
Jun  1 16:39:28.757: INFO: Pod "pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348567ms
Jun  1 16:39:30.759: INFO: Pod "pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005195614s
Jun  1 16:39:32.763: INFO: Pod "pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008505721s
STEP: Saw pod success
Jun  1 16:39:32.763: INFO: Pod "pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c" satisfied condition "success or failure"
Jun  1 16:39:32.765: INFO: Trying to get logs from node appserv11 pod pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 16:39:32.783: INFO: Waiting for pod pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c to disappear
Jun  1 16:39:32.785: INFO: Pod pod-secrets-6241745b-e14a-4094-8347-544ba91ec62c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:39:32.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6815" for this suite.
Jun  1 16:39:38.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:39:38.871: INFO: namespace secrets-6815 deletion completed in 6.082333717s

• [SLOW TEST:10.256 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:39:38.871: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1148
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-4dc21215-c839-4019-aeab-04d622561a2d
STEP: Creating configMap with name cm-test-opt-upd-870a9972-e7ec-4337-be5c-fff4d97187b4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4dc21215-c839-4019-aeab-04d622561a2d
STEP: Updating configmap cm-test-opt-upd-870a9972-e7ec-4337-be5c-fff4d97187b4
STEP: Creating configMap with name cm-test-opt-create-2aa15392-2156-40f1-8efb-3f3acb87890d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:40:57.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1148" for this suite.
Jun  1 16:41:09.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:41:09.742: INFO: namespace configmap-1148 deletion completed in 12.095101699s

• [SLOW TEST:90.707 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:41:09.743: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1099
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1099
I0601 16:41:09.897959      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1099, replica count: 2
I0601 16:41:12.948430      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  1 16:41:12.948: INFO: Creating new exec pod
Jun  1 16:41:15.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1099 execpodmcnhp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  1 16:41:16.240: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  1 16:41:16.240: INFO: stdout: ""
Jun  1 16:41:16.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1099 execpodmcnhp -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.241 80'
Jun  1 16:41:16.420: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.241 80\nConnection to 10.0.0.241 80 port [tcp/http] succeeded!\n"
Jun  1 16:41:16.420: INFO: stdout: ""
Jun  1 16:41:16.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1099 execpodmcnhp -- /bin/sh -x -c nc -zv -t -w 2 172.16.6.110 31687'
Jun  1 16:41:16.627: INFO: stderr: "+ nc -zv -t -w 2 172.16.6.110 31687\nConnection to 172.16.6.110 31687 port [tcp/31687] succeeded!\n"
Jun  1 16:41:16.627: INFO: stdout: ""
Jun  1 16:41:16.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-1099 execpodmcnhp -- /bin/sh -x -c nc -zv -t -w 2 172.16.6.111 31687'
Jun  1 16:41:16.854: INFO: stderr: "+ nc -zv -t -w 2 172.16.6.111 31687\nConnection to 172.16.6.111 31687 port [tcp/31687] succeeded!\n"
Jun  1 16:41:16.854: INFO: stdout: ""
Jun  1 16:41:16.854: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:41:16.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1099" for this suite.
Jun  1 16:41:22.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:41:22.966: INFO: namespace services-1099 deletion completed in 6.093117453s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.224 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:41:22.967: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  1 16:41:28.128: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:41:28.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6055" for this suite.
Jun  1 16:41:34.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:41:34.234: INFO: namespace container-runtime-6055 deletion completed in 6.093659994s

• [SLOW TEST:11.268 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:41:34.235: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:41:45.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-707" for this suite.
Jun  1 16:41:51.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:41:51.492: INFO: namespace resourcequota-707 deletion completed in 6.089809906s

• [SLOW TEST:17.258 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:41:51.493: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-57
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-35111719-7ce3-40c3-b620-01e0a799f08c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:41:51.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-57" for this suite.
Jun  1 16:41:57.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:41:57.764: INFO: namespace secrets-57 deletion completed in 6.131935506s

• [SLOW TEST:6.271 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:41:57.764: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:41:59.921: INFO: Waiting up to 5m0s for pod "client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359" in namespace "pods-9289" to be "success or failure"
Jun  1 16:41:59.923: INFO: Pod "client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413721ms
Jun  1 16:42:01.926: INFO: Pod "client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005626266s
STEP: Saw pod success
Jun  1 16:42:01.926: INFO: Pod "client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359" satisfied condition "success or failure"
Jun  1 16:42:01.928: INFO: Trying to get logs from node appserv9 pod client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359 container env3cont: <nil>
STEP: delete the pod
Jun  1 16:42:01.952: INFO: Waiting for pod client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359 to disappear
Jun  1 16:42:01.955: INFO: Pod client-envvars-de1d0b09-ddf0-495a-8a36-a8a2ab95a359 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:42:01.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9289" for this suite.
Jun  1 16:42:29.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:42:30.053: INFO: namespace pods-9289 deletion completed in 28.094048924s

• [SLOW TEST:32.289 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:42:30.053: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun  1 16:42:30.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-1626'
Jun  1 16:42:30.520: INFO: stderr: ""
Jun  1 16:42:30.520: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 16:42:30.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:30.634: INFO: stderr: ""
Jun  1 16:42:30.634: INFO: stdout: "update-demo-nautilus-ndnz6 update-demo-nautilus-xl77v "
Jun  1 16:42:30.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:30.735: INFO: stderr: ""
Jun  1 16:42:30.735: INFO: stdout: ""
Jun  1 16:42:30.735: INFO: update-demo-nautilus-ndnz6 is created but not running
Jun  1 16:42:35.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:35.859: INFO: stderr: ""
Jun  1 16:42:35.859: INFO: stdout: "update-demo-nautilus-ndnz6 update-demo-nautilus-xl77v "
Jun  1 16:42:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:35.978: INFO: stderr: ""
Jun  1 16:42:35.978: INFO: stdout: "true"
Jun  1 16:42:35.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:36.080: INFO: stderr: ""
Jun  1 16:42:36.080: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:36.080: INFO: validating pod update-demo-nautilus-ndnz6
Jun  1 16:42:36.089: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:36.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:36.089: INFO: update-demo-nautilus-ndnz6 is verified up and running
Jun  1 16:42:36.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-xl77v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:36.192: INFO: stderr: ""
Jun  1 16:42:36.192: INFO: stdout: "true"
Jun  1 16:42:36.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-xl77v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:36.308: INFO: stderr: ""
Jun  1 16:42:36.308: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:36.308: INFO: validating pod update-demo-nautilus-xl77v
Jun  1 16:42:36.317: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:36.317: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:36.317: INFO: update-demo-nautilus-xl77v is verified up and running
STEP: scaling down the replication controller
Jun  1 16:42:36.319: INFO: scanned /root for discovery docs: <nil>
Jun  1 16:42:36.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1626'
Jun  1 16:42:37.458: INFO: stderr: ""
Jun  1 16:42:37.458: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 16:42:37.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:37.583: INFO: stderr: ""
Jun  1 16:42:37.583: INFO: stdout: "update-demo-nautilus-ndnz6 update-demo-nautilus-xl77v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun  1 16:42:42.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:42.712: INFO: stderr: ""
Jun  1 16:42:42.712: INFO: stdout: "update-demo-nautilus-ndnz6 "
Jun  1 16:42:42.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:42.832: INFO: stderr: ""
Jun  1 16:42:42.832: INFO: stdout: "true"
Jun  1 16:42:42.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:42.938: INFO: stderr: ""
Jun  1 16:42:42.938: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:42.938: INFO: validating pod update-demo-nautilus-ndnz6
Jun  1 16:42:42.942: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:42.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:42.942: INFO: update-demo-nautilus-ndnz6 is verified up and running
STEP: scaling up the replication controller
Jun  1 16:42:42.946: INFO: scanned /root for discovery docs: <nil>
Jun  1 16:42:42.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1626'
Jun  1 16:42:44.081: INFO: stderr: ""
Jun  1 16:42:44.081: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 16:42:44.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:44.212: INFO: stderr: ""
Jun  1 16:42:44.212: INFO: stdout: "update-demo-nautilus-ndnz6 update-demo-nautilus-vftxh "
Jun  1 16:42:44.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:44.323: INFO: stderr: ""
Jun  1 16:42:44.323: INFO: stdout: "true"
Jun  1 16:42:44.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:44.424: INFO: stderr: ""
Jun  1 16:42:44.424: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:44.424: INFO: validating pod update-demo-nautilus-ndnz6
Jun  1 16:42:44.428: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:44.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:44.428: INFO: update-demo-nautilus-ndnz6 is verified up and running
Jun  1 16:42:44.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-vftxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:44.531: INFO: stderr: ""
Jun  1 16:42:44.531: INFO: stdout: ""
Jun  1 16:42:44.531: INFO: update-demo-nautilus-vftxh is created but not running
Jun  1 16:42:49.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1626'
Jun  1 16:42:49.653: INFO: stderr: ""
Jun  1 16:42:49.653: INFO: stdout: "update-demo-nautilus-ndnz6 update-demo-nautilus-vftxh "
Jun  1 16:42:49.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:49.752: INFO: stderr: ""
Jun  1 16:42:49.752: INFO: stdout: "true"
Jun  1 16:42:49.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-ndnz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:49.869: INFO: stderr: ""
Jun  1 16:42:49.869: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:49.869: INFO: validating pod update-demo-nautilus-ndnz6
Jun  1 16:42:49.874: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:49.874: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:49.874: INFO: update-demo-nautilus-ndnz6 is verified up and running
Jun  1 16:42:49.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-vftxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:49.986: INFO: stderr: ""
Jun  1 16:42:49.986: INFO: stdout: "true"
Jun  1 16:42:49.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-vftxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1626'
Jun  1 16:42:50.103: INFO: stderr: ""
Jun  1 16:42:50.103: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:42:50.103: INFO: validating pod update-demo-nautilus-vftxh
Jun  1 16:42:50.109: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:42:50.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:42:50.109: INFO: update-demo-nautilus-vftxh is verified up and running
STEP: using delete to clean up resources
Jun  1 16:42:50.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-1626'
Jun  1 16:42:50.223: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 16:42:50.223: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  1 16:42:50.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1626'
Jun  1 16:42:50.343: INFO: stderr: "No resources found in kubectl-1626 namespace.\n"
Jun  1 16:42:50.343: INFO: stdout: ""
Jun  1 16:42:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -l name=update-demo --namespace=kubectl-1626 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  1 16:42:50.454: INFO: stderr: ""
Jun  1 16:42:50.454: INFO: stdout: "update-demo-nautilus-ndnz6\nupdate-demo-nautilus-vftxh\n"
Jun  1 16:42:50.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1626'
Jun  1 16:42:51.073: INFO: stderr: "No resources found in kubectl-1626 namespace.\n"
Jun  1 16:42:51.073: INFO: stdout: ""
Jun  1 16:42:51.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -l name=update-demo --namespace=kubectl-1626 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  1 16:42:51.178: INFO: stderr: ""
Jun  1 16:42:51.178: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:42:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1626" for this suite.
Jun  1 16:42:57.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:42:57.570: INFO: namespace kubectl-1626 deletion completed in 6.387783329s

• [SLOW TEST:27.517 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:42:57.571: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun  1 16:42:57.701: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:43:00.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2498" for this suite.
Jun  1 16:43:06.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:43:06.696: INFO: namespace init-container-2498 deletion completed in 6.218497663s

• [SLOW TEST:9.125 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:43:06.696: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:43:06.836: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  1 16:43:11.839: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  1 16:43:11.840: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun  1 16:43:11.857: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2685 /apis/apps/v1/namespaces/deployment-2685/deployments/test-cleanup-deployment d259c56d-6e0b-4a99-86cd-208c9f8323ab 8186 1 2020-06-01 16:43:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002c52b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun  1 16:43:11.861: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-2685 /apis/apps/v1/namespaces/deployment-2685/replicasets/test-cleanup-deployment-65db99849b 235c700b-8590-405f-bd7c-cd483a55f361 8188 1 2020-06-01 16:43:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d259c56d-6e0b-4a99-86cd-208c9f8323ab 0xc002c53447 0xc002c53448}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002c53538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:43:11.861: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun  1 16:43:11.861: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2685 /apis/apps/v1/namespaces/deployment-2685/replicasets/test-cleanup-controller 8f190f70-d2ca-49eb-801c-e24ec71dbe02 8187 1 2020-06-01 16:43:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d259c56d-6e0b-4a99-86cd-208c9f8323ab 0xc002c532c7 0xc002c532c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c533c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  1 16:43:11.865: INFO: Pod "test-cleanup-controller-cp4ss" is available:
&Pod{ObjectMeta:{test-cleanup-controller-cp4ss test-cleanup-controller- deployment-2685 /api/v1/namespaces/deployment-2685/pods/test-cleanup-controller-cp4ss 32ab1827-d3f8-47bc-8df5-25080ed96ed6 8174 0 2020-06-01 16:43:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 8f190f70-d2ca-49eb-801c-e24ec71dbe02 0xc003887a7f 0xc003887a90}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c6qkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c6qkz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c6qkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:43:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:43:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:43:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 16:43:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.12,StartTime:2020-06-01 16:43:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 16:43:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b739254a05f51002d0daf3a7f73351583be30891085125750083f8e666283151,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  1 16:43:11.865: INFO: Pod "test-cleanup-deployment-65db99849b-h99xq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-h99xq test-cleanup-deployment-65db99849b- deployment-2685 /api/v1/namespaces/deployment-2685/pods/test-cleanup-deployment-65db99849b-h99xq 9dbc404e-a192-4a8a-bef7-5e20536d9806 8190 0 2020-06-01 16:43:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 235c700b-8590-405f-bd7c-cd483a55f361 0xc003887bdf 0xc003887c10}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c6qkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c6qkz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c6qkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:43:11.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2685" for this suite.
Jun  1 16:43:17.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:43:17.965: INFO: namespace deployment-2685 deletion completed in 6.095727064s

• [SLOW TEST:11.269 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:43:17.966: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:43:18.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2708" for this suite.
Jun  1 16:43:24.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:43:24.227: INFO: namespace kubelet-test-2708 deletion completed in 6.108213962s

• [SLOW TEST:6.262 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:43:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 16:43:24.737: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 16:43:26.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626604, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626604, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626604, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626604, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 16:43:29.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:43:29.759: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9274-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:43:30.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-661" for this suite.
Jun  1 16:43:36.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:43:36.948: INFO: namespace webhook-661 deletion completed in 6.089738341s
STEP: Destroying namespace "webhook-661-markers" for this suite.
Jun  1 16:43:42.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:43:43.038: INFO: namespace webhook-661-markers deletion completed in 6.090022809s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.822 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:43:43.050: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4346
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun  1 16:43:43.181: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:44:01.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4346" for this suite.
Jun  1 16:44:07.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:44:07.803: INFO: namespace crd-publish-openapi-4346 deletion completed in 6.091901215s

• [SLOW TEST:24.753 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:44:07.803: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Jun  1 16:44:07.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-8246'
Jun  1 16:44:08.217: INFO: stderr: ""
Jun  1 16:44:08.217: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 16:44:08.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8246'
Jun  1 16:44:08.305: INFO: stderr: ""
Jun  1 16:44:08.305: INFO: stdout: "update-demo-nautilus-4g6gb update-demo-nautilus-h9n6s "
Jun  1 16:44:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-4g6gb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:08.395: INFO: stderr: ""
Jun  1 16:44:08.395: INFO: stdout: ""
Jun  1 16:44:08.395: INFO: update-demo-nautilus-4g6gb is created but not running
Jun  1 16:44:13.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8246'
Jun  1 16:44:13.513: INFO: stderr: ""
Jun  1 16:44:13.513: INFO: stdout: "update-demo-nautilus-4g6gb update-demo-nautilus-h9n6s "
Jun  1 16:44:13.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-4g6gb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:13.620: INFO: stderr: ""
Jun  1 16:44:13.620: INFO: stdout: "true"
Jun  1 16:44:13.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-4g6gb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:13.722: INFO: stderr: ""
Jun  1 16:44:13.722: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:44:13.722: INFO: validating pod update-demo-nautilus-4g6gb
Jun  1 16:44:13.729: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:44:13.729: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:44:13.729: INFO: update-demo-nautilus-4g6gb is verified up and running
Jun  1 16:44:13.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-h9n6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:13.845: INFO: stderr: ""
Jun  1 16:44:13.845: INFO: stdout: "true"
Jun  1 16:44:13.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-h9n6s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:13.956: INFO: stderr: ""
Jun  1 16:44:13.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 16:44:13.956: INFO: validating pod update-demo-nautilus-h9n6s
Jun  1 16:44:13.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 16:44:13.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 16:44:13.962: INFO: update-demo-nautilus-h9n6s is verified up and running
STEP: rolling-update to new replication controller
Jun  1 16:44:13.964: INFO: scanned /root for discovery docs: <nil>
Jun  1 16:44:13.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8246'
Jun  1 16:44:36.355: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun  1 16:44:36.355: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 16:44:36.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8246'
Jun  1 16:44:36.482: INFO: stderr: ""
Jun  1 16:44:36.482: INFO: stdout: "update-demo-kitten-tlrfv update-demo-kitten-vq8x2 "
Jun  1 16:44:36.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-kitten-tlrfv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:36.592: INFO: stderr: ""
Jun  1 16:44:36.592: INFO: stdout: "true"
Jun  1 16:44:36.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-kitten-tlrfv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:36.712: INFO: stderr: ""
Jun  1 16:44:36.712: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun  1 16:44:36.712: INFO: validating pod update-demo-kitten-tlrfv
Jun  1 16:44:36.720: INFO: got data: {
  "image": "kitten.jpg"
}

Jun  1 16:44:36.720: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun  1 16:44:36.720: INFO: update-demo-kitten-tlrfv is verified up and running
Jun  1 16:44:36.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-kitten-vq8x2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:36.843: INFO: stderr: ""
Jun  1 16:44:36.843: INFO: stdout: "true"
Jun  1 16:44:36.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-kitten-vq8x2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8246'
Jun  1 16:44:36.955: INFO: stderr: ""
Jun  1 16:44:36.955: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun  1 16:44:36.955: INFO: validating pod update-demo-kitten-vq8x2
Jun  1 16:44:36.961: INFO: got data: {
  "image": "kitten.jpg"
}

Jun  1 16:44:36.961: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun  1 16:44:36.961: INFO: update-demo-kitten-vq8x2 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:44:36.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8246" for this suite.
Jun  1 16:45:04.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:45:05.055: INFO: namespace kubectl-8246 deletion completed in 28.09041677s

• [SLOW TEST:57.252 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:45:05.056: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun  1 16:45:05.193: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:45:14.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3289" for this suite.
Jun  1 16:45:20.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:45:20.904: INFO: namespace pods-3289 deletion completed in 6.094405289s

• [SLOW TEST:15.849 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:45:20.905: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4845
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun  1 16:45:21.037: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 16:45:24.745: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:45:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4845" for this suite.
Jun  1 16:45:44.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:45:45.056: INFO: namespace crd-publish-openapi-4845 deletion completed in 6.089403342s

• [SLOW TEST:24.152 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:45:45.057: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-aa68e4d6-5e57-4457-af36-cc2b5e9db652
STEP: Creating a pod to test consume configMaps
Jun  1 16:45:45.197: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6" in namespace "projected-1508" to be "success or failure"
Jun  1 16:45:45.199: INFO: Pod "pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468597ms
Jun  1 16:45:47.202: INFO: Pod "pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005705138s
STEP: Saw pod success
Jun  1 16:45:47.202: INFO: Pod "pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6" satisfied condition "success or failure"
Jun  1 16:45:47.205: INFO: Trying to get logs from node appserv11 pod pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 16:45:47.242: INFO: Waiting for pod pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6 to disappear
Jun  1 16:45:47.244: INFO: Pod pod-projected-configmaps-bbd43323-701e-41e1-bb63-3ae34b6896c6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:45:47.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1508" for this suite.
Jun  1 16:45:53.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:45:53.382: INFO: namespace projected-1508 deletion completed in 6.134482013s

• [SLOW TEST:8.325 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:45:53.382: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  1 16:45:53.520: INFO: Waiting up to 5m0s for pod "pod-11b70bea-ceda-4638-a8eb-7331ea76a199" in namespace "emptydir-7342" to be "success or failure"
Jun  1 16:45:53.522: INFO: Pod "pod-11b70bea-ceda-4638-a8eb-7331ea76a199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400846ms
Jun  1 16:45:55.526: INFO: Pod "pod-11b70bea-ceda-4638-a8eb-7331ea76a199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005834807s
Jun  1 16:45:57.529: INFO: Pod "pod-11b70bea-ceda-4638-a8eb-7331ea76a199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009153919s
STEP: Saw pod success
Jun  1 16:45:57.529: INFO: Pod "pod-11b70bea-ceda-4638-a8eb-7331ea76a199" satisfied condition "success or failure"
Jun  1 16:45:57.532: INFO: Trying to get logs from node appserv11 pod pod-11b70bea-ceda-4638-a8eb-7331ea76a199 container test-container: <nil>
STEP: delete the pod
Jun  1 16:45:57.550: INFO: Waiting for pod pod-11b70bea-ceda-4638-a8eb-7331ea76a199 to disappear
Jun  1 16:45:57.553: INFO: Pod pod-11b70bea-ceda-4638-a8eb-7331ea76a199 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:45:57.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7342" for this suite.
Jun  1 16:46:03.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:03.651: INFO: namespace emptydir-7342 deletion completed in 6.09435098s

• [SLOW TEST:10.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:46:03.651: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 16:46:03.801: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"29076b35-7708-4123-b4c3-7026d7063ace", Controller:(*bool)(0xc006094406), BlockOwnerDeletion:(*bool)(0xc006094407)}}
Jun  1 16:46:03.805: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"05666aed-e3d5-4467-8cfb-33f8e1f6c775", Controller:(*bool)(0xc0060cdb96), BlockOwnerDeletion:(*bool)(0xc0060cdb97)}}
Jun  1 16:46:03.809: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7a3fbdf1-0b95-4068-acac-a14aadd17097", Controller:(*bool)(0xc006070ad6), BlockOwnerDeletion:(*bool)(0xc006070ad7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:46:08.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9030" for this suite.
Jun  1 16:46:14.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:14.914: INFO: namespace gc-9030 deletion completed in 6.093787234s

• [SLOW TEST:11.263 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:46:14.914: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  1 16:46:15.052: INFO: Waiting up to 5m0s for pod "pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b" in namespace "emptydir-969" to be "success or failure"
Jun  1 16:46:15.054: INFO: Pod "pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289918ms
Jun  1 16:46:17.057: INFO: Pod "pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005588657s
Jun  1 16:46:19.060: INFO: Pod "pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008441208s
STEP: Saw pod success
Jun  1 16:46:19.060: INFO: Pod "pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b" satisfied condition "success or failure"
Jun  1 16:46:19.063: INFO: Trying to get logs from node appserv11 pod pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b container test-container: <nil>
STEP: delete the pod
Jun  1 16:46:19.079: INFO: Waiting for pod pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b to disappear
Jun  1 16:46:19.082: INFO: Pod pod-cb1a9c4c-b0d5-423b-bd7a-4eaa8029b86b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:46:19.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-969" for this suite.
Jun  1 16:46:25.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:25.234: INFO: namespace emptydir-969 deletion completed in 6.149118348s

• [SLOW TEST:10.320 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:46:25.234: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  1 16:46:25.374: INFO: Waiting up to 5m0s for pod "pod-e993f9be-a406-418f-a50e-056ae83655d1" in namespace "emptydir-3588" to be "success or failure"
Jun  1 16:46:25.377: INFO: Pod "pod-e993f9be-a406-418f-a50e-056ae83655d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570452ms
Jun  1 16:46:27.380: INFO: Pod "pod-e993f9be-a406-418f-a50e-056ae83655d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005970768s
Jun  1 16:46:29.383: INFO: Pod "pod-e993f9be-a406-418f-a50e-056ae83655d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009117711s
STEP: Saw pod success
Jun  1 16:46:29.383: INFO: Pod "pod-e993f9be-a406-418f-a50e-056ae83655d1" satisfied condition "success or failure"
Jun  1 16:46:29.386: INFO: Trying to get logs from node appserv11 pod pod-e993f9be-a406-418f-a50e-056ae83655d1 container test-container: <nil>
STEP: delete the pod
Jun  1 16:46:29.403: INFO: Waiting for pod pod-e993f9be-a406-418f-a50e-056ae83655d1 to disappear
Jun  1 16:46:29.405: INFO: Pod pod-e993f9be-a406-418f-a50e-056ae83655d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:46:29.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3588" for this suite.
Jun  1 16:46:35.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:35.499: INFO: namespace emptydir-3588 deletion completed in 6.09022796s

• [SLOW TEST:10.264 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:46:35.499: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 16:46:36.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 16:46:38.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626796, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626796, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626796, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726626796, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 16:46:41.729: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:46:41.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1757" for this suite.
Jun  1 16:46:47.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:47.917: INFO: namespace webhook-1757 deletion completed in 6.079385663s
STEP: Destroying namespace "webhook-1757-markers" for this suite.
Jun  1 16:46:53.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:46:54.006: INFO: namespace webhook-1757-markers deletion completed in 6.089355516s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.519 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:46:54.018: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c6b7774f-ac59-4962-8dd0-56e6d90a7643
STEP: Creating a pod to test consume configMaps
Jun  1 16:46:54.161: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7" in namespace "projected-9287" to be "success or failure"
Jun  1 16:46:54.164: INFO: Pod "pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403972ms
Jun  1 16:46:56.167: INFO: Pod "pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005633066s
Jun  1 16:46:58.170: INFO: Pod "pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008860321s
STEP: Saw pod success
Jun  1 16:46:58.170: INFO: Pod "pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7" satisfied condition "success or failure"
Jun  1 16:46:58.172: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 16:46:58.196: INFO: Waiting for pod pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7 to disappear
Jun  1 16:46:58.198: INFO: Pod pod-projected-configmaps-6cca06dd-6266-4e8c-8a8e-a652d6fe41c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:46:58.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9287" for this suite.
Jun  1 16:47:04.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:47:04.286: INFO: namespace projected-9287 deletion completed in 6.084087495s

• [SLOW TEST:10.268 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:47:04.286: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun  1 16:47:06.955: INFO: Successfully updated pod "annotationupdated9bc8f82-d05f-46cf-a2b5-75338dd6f7d5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:47:08.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6708" for this suite.
Jun  1 16:47:36.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:47:37.068: INFO: namespace downward-api-6708 deletion completed in 28.0926958s

• [SLOW TEST:32.783 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:47:37.069: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Jun  1 16:47:37.208: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8973" to be "success or failure"
Jun  1 16:47:37.211: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456008ms
Jun  1 16:47:39.214: INFO: Pod "pod-host-path-test": Phase="Running", Reason="", readiness=false. Elapsed: 2.005310159s
Jun  1 16:47:41.218: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009143437s
STEP: Saw pod success
Jun  1 16:47:41.218: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun  1 16:47:41.220: INFO: Trying to get logs from node appserv9 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun  1 16:47:41.239: INFO: Waiting for pod pod-host-path-test to disappear
Jun  1 16:47:41.241: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:47:41.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8973" for this suite.
Jun  1 16:47:47.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:47:47.415: INFO: namespace hostpath-8973 deletion completed in 6.169566506s

• [SLOW TEST:10.346 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:47:47.415: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-dfc68003-6c59-4faa-8709-b8e3e160a094 in namespace container-probe-7584
Jun  1 16:47:49.560: INFO: Started pod busybox-dfc68003-6c59-4faa-8709-b8e3e160a094 in namespace container-probe-7584
STEP: checking the pod's current state and verifying that restartCount is present
Jun  1 16:47:49.563: INFO: Initial restart count of pod busybox-dfc68003-6c59-4faa-8709-b8e3e160a094 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:51:49.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7584" for this suite.
Jun  1 16:51:55.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:51:56.072: INFO: namespace container-probe-7584 deletion completed in 6.086554279s

• [SLOW TEST:248.657 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:51:56.072: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:52:03.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4343" for this suite.
Jun  1 16:52:09.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:52:09.600: INFO: namespace resourcequota-4343 deletion completed in 6.38093656s

• [SLOW TEST:13.528 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:52:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun  1 16:52:09.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-6148'
Jun  1 16:52:10.116: INFO: stderr: ""
Jun  1 16:52:10.116: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun  1 16:52:11.120: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 16:52:11.120: INFO: Found 0 / 1
Jun  1 16:52:12.120: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 16:52:12.120: INFO: Found 1 / 1
Jun  1 16:52:12.120: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun  1 16:52:12.123: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 16:52:12.123: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  1 16:52:12.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 patch pod redis-master-xhdjd --namespace=kubectl-6148 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  1 16:52:12.251: INFO: stderr: ""
Jun  1 16:52:12.251: INFO: stdout: "pod/redis-master-xhdjd patched\n"
STEP: checking annotations
Jun  1 16:52:12.254: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 16:52:12.254: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:52:12.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6148" for this suite.
Jun  1 16:52:40.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:52:40.349: INFO: namespace kubectl-6148 deletion completed in 28.091112295s

• [SLOW TEST:30.749 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:52:40.350: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  1 16:52:40.489: INFO: Waiting up to 5m0s for pod "pod-4caeb319-15d0-4915-832c-d542a63b9944" in namespace "emptydir-2815" to be "success or failure"
Jun  1 16:52:40.492: INFO: Pod "pod-4caeb319-15d0-4915-832c-d542a63b9944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410841ms
Jun  1 16:52:42.495: INFO: Pod "pod-4caeb319-15d0-4915-832c-d542a63b9944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006169313s
STEP: Saw pod success
Jun  1 16:52:42.495: INFO: Pod "pod-4caeb319-15d0-4915-832c-d542a63b9944" satisfied condition "success or failure"
Jun  1 16:52:42.498: INFO: Trying to get logs from node appserv11 pod pod-4caeb319-15d0-4915-832c-d542a63b9944 container test-container: <nil>
STEP: delete the pod
Jun  1 16:52:42.568: INFO: Waiting for pod pod-4caeb319-15d0-4915-832c-d542a63b9944 to disappear
Jun  1 16:52:42.570: INFO: Pod pod-4caeb319-15d0-4915-832c-d542a63b9944 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:52:42.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2815" for this suite.
Jun  1 16:52:48.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:52:48.656: INFO: namespace emptydir-2815 deletion completed in 6.081014557s

• [SLOW TEST:8.306 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:52:48.656: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 16:52:48.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7" in namespace "downward-api-4414" to be "success or failure"
Jun  1 16:52:48.795: INFO: Pod "downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330824ms
Jun  1 16:52:50.798: INFO: Pod "downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005673439s
STEP: Saw pod success
Jun  1 16:52:50.798: INFO: Pod "downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7" satisfied condition "success or failure"
Jun  1 16:52:50.801: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7 container client-container: <nil>
STEP: delete the pod
Jun  1 16:52:50.818: INFO: Waiting for pod downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7 to disappear
Jun  1 16:52:50.820: INFO: Pod downwardapi-volume-27256cb1-36e8-4e24-a2bd-877d0a65b5c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:52:50.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4414" for this suite.
Jun  1 16:52:56.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:52:56.918: INFO: namespace downward-api-4414 deletion completed in 6.094280634s

• [SLOW TEST:8.262 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:52:56.919: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun  1 16:53:01.595: INFO: Successfully updated pod "annotationupdate403f5c46-0d93-490a-8695-ccb5a354e231"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:53:03.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9414" for this suite.
Jun  1 16:53:31.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:53:31.749: INFO: namespace projected-9414 deletion completed in 28.130637063s

• [SLOW TEST:34.830 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:53:31.749: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-eaff6196-8811-4ee1-9265-a6495afa74a5 in namespace container-probe-5458
Jun  1 16:53:33.895: INFO: Started pod busybox-eaff6196-8811-4ee1-9265-a6495afa74a5 in namespace container-probe-5458
STEP: checking the pod's current state and verifying that restartCount is present
Jun  1 16:53:33.898: INFO: Initial restart count of pod busybox-eaff6196-8811-4ee1-9265-a6495afa74a5 is 0
Jun  1 16:54:21.980: INFO: Restart count of pod container-probe-5458/busybox-eaff6196-8811-4ee1-9265-a6495afa74a5 is now 1 (48.081638159s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:54:21.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5458" for this suite.
Jun  1 16:54:28.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:54:28.091: INFO: namespace container-probe-5458 deletion completed in 6.099686193s

• [SLOW TEST:56.342 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:54:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 16:54:28.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6127'
Jun  1 16:54:28.351: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  1 16:54:28.351: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Jun  1 16:54:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete jobs e2e-test-httpd-job --namespace=kubectl-6127'
Jun  1 16:54:28.463: INFO: stderr: ""
Jun  1 16:54:28.463: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:54:28.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6127" for this suite.
Jun  1 16:54:34.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:54:34.555: INFO: namespace kubectl-6127 deletion completed in 6.087705366s

• [SLOW TEST:6.462 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:54:34.555: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:54:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1386" for this suite.
Jun  1 16:54:47.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:54:47.576: INFO: namespace watch-1386 deletion completed in 6.125001805s

• [SLOW TEST:13.021 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:54:47.577: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2f98cd83-a4da-4184-8ca8-004018edfdd3
STEP: Creating a pod to test consume secrets
Jun  1 16:54:47.717: INFO: Waiting up to 5m0s for pod "pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02" in namespace "secrets-671" to be "success or failure"
Jun  1 16:54:47.719: INFO: Pod "pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232753ms
Jun  1 16:54:49.723: INFO: Pod "pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005401526s
STEP: Saw pod success
Jun  1 16:54:49.723: INFO: Pod "pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02" satisfied condition "success or failure"
Jun  1 16:54:49.725: INFO: Trying to get logs from node appserv9 pod pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02 container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 16:54:49.752: INFO: Waiting for pod pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02 to disappear
Jun  1 16:54:49.754: INFO: Pod pod-secrets-3372e613-938c-49d3-beb0-aa67fb84ab02 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:54:49.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-671" for this suite.
Jun  1 16:54:55.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:54:55.843: INFO: namespace secrets-671 deletion completed in 6.084867183s

• [SLOW TEST:8.266 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:54:55.843: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5398
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-bd785258-7ec2-4184-83ea-53ac6d6e6bdc
STEP: Creating configMap with name cm-test-opt-upd-afd2e50d-00fc-4e25-b0ad-6c75adb562a9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bd785258-7ec2-4184-83ea-53ac6d6e6bdc
STEP: Updating configmap cm-test-opt-upd-afd2e50d-00fc-4e25-b0ad-6c75adb562a9
STEP: Creating configMap with name cm-test-opt-create-b14774ff-d7f3-4cb6-bbf8-be17492f23bb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 16:56:10.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5398" for this suite.
Jun  1 16:56:38.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 16:56:38.569: INFO: namespace projected-5398 deletion completed in 28.133505295s

• [SLOW TEST:102.726 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 16:56:38.569: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun  1 16:56:39.027: INFO: Pod name wrapped-volume-race-583e0b99-fa7b-4dd2-8503-db2f0e0dbe46: Found 0 pods out of 5
Jun  1 16:56:44.033: INFO: Pod name wrapped-volume-race-583e0b99-fa7b-4dd2-8503-db2f0e0dbe46: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-583e0b99-fa7b-4dd2-8503-db2f0e0dbe46 in namespace emptydir-wrapper-2212, will wait for the garbage collector to delete the pods
Jun  1 16:56:54.118: INFO: Deleting ReplicationController wrapped-volume-race-583e0b99-fa7b-4dd2-8503-db2f0e0dbe46 took: 6.629916ms
Jun  1 16:56:54.718: INFO: Terminating ReplicationController wrapped-volume-race-583e0b99-fa7b-4dd2-8503-db2f0e0dbe46 pods took: 600.205645ms
STEP: Creating RC which spawns configmap-volume pods
Jun  1 16:57:45.934: INFO: Pod name wrapped-volume-race-8e9d99f1-693c-4cc6-9c8d-937d1443e94c: Found 0 pods out of 5
Jun  1 16:57:50.940: INFO: Pod name wrapped-volume-race-8e9d99f1-693c-4cc6-9c8d-937d1443e94c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8e9d99f1-693c-4cc6-9c8d-937d1443e94c in namespace emptydir-wrapper-2212, will wait for the garbage collector to delete the pods
Jun  1 16:58:01.017: INFO: Deleting ReplicationController wrapped-volume-race-8e9d99f1-693c-4cc6-9c8d-937d1443e94c took: 6.858929ms
Jun  1 16:58:01.618: INFO: Terminating ReplicationController wrapped-volume-race-8e9d99f1-693c-4cc6-9c8d-937d1443e94c pods took: 600.190007ms
STEP: Creating RC which spawns configmap-volume pods
Jun  1 16:58:55.335: INFO: Pod name wrapped-volume-race-d8989fef-4fc6-40f5-93ba-e9c598e51c11: Found 0 pods out of 5
Jun  1 16:59:00.342: INFO: Pod name wrapped-volume-race-d8989fef-4fc6-40f5-93ba-e9c598e51c11: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d8989fef-4fc6-40f5-93ba-e9c598e51c11 in namespace emptydir-wrapper-2212, will wait for the garbage collector to delete the pods
Jun  1 16:59:12.426: INFO: Deleting ReplicationController wrapped-volume-race-d8989fef-4fc6-40f5-93ba-e9c598e51c11 took: 6.758591ms
Jun  1 16:59:13.026: INFO: Terminating ReplicationController wrapped-volume-race-d8989fef-4fc6-40f5-93ba-e9c598e51c11 pods took: 600.257907ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:00:05.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2212" for this suite.
Jun  1 17:00:11.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:00:11.402: INFO: namespace emptydir-wrapper-2212 deletion completed in 6.17260933s

• [SLOW TEST:212.833 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:00:11.402: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun  1 17:00:16.562: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:00:17.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6946" for this suite.
Jun  1 17:00:29.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:00:29.677: INFO: namespace replicaset-6946 deletion completed in 12.099388174s

• [SLOW TEST:18.274 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:00:29.677: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  1 17:00:29.813: INFO: Waiting up to 5m0s for pod "pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e" in namespace "emptydir-3544" to be "success or failure"
Jun  1 17:00:29.815: INFO: Pod "pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307964ms
Jun  1 17:00:31.819: INFO: Pod "pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005813061s
Jun  1 17:00:33.823: INFO: Pod "pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009375621s
STEP: Saw pod success
Jun  1 17:00:33.823: INFO: Pod "pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e" satisfied condition "success or failure"
Jun  1 17:00:33.825: INFO: Trying to get logs from node appserv9 pod pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e container test-container: <nil>
STEP: delete the pod
Jun  1 17:00:33.852: INFO: Waiting for pod pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e to disappear
Jun  1 17:00:33.854: INFO: Pod pod-e0f711fd-b120-476a-a1bb-3ee39b7fe50e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:00:33.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3544" for this suite.
Jun  1 17:00:39.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:00:39.989: INFO: namespace emptydir-3544 deletion completed in 6.130666586s

• [SLOW TEST:10.312 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:00:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Jun  1 17:00:40.125: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-621748879 proxy --unix-socket=/tmp/kubectl-proxy-unix532131526/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:00:40.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6567" for this suite.
Jun  1 17:00:46.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:00:46.318: INFO: namespace kubectl-6567 deletion completed in 6.089291884s

• [SLOW TEST:6.329 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:00:46.318: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Jun  1 17:00:46.966: INFO: created pod pod-service-account-defaultsa
Jun  1 17:00:46.966: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  1 17:00:46.970: INFO: created pod pod-service-account-mountsa
Jun  1 17:00:46.970: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  1 17:00:46.973: INFO: created pod pod-service-account-nomountsa
Jun  1 17:00:46.973: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  1 17:00:46.977: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  1 17:00:46.977: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  1 17:00:46.981: INFO: created pod pod-service-account-mountsa-mountspec
Jun  1 17:00:46.981: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  1 17:00:46.984: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  1 17:00:46.984: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  1 17:00:46.987: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  1 17:00:46.987: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  1 17:00:46.990: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  1 17:00:46.990: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  1 17:00:46.993: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  1 17:00:46.993: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:00:46.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8861" for this suite.
Jun  1 17:00:59.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:00:59.070: INFO: namespace svcaccounts-8861 deletion completed in 12.07335263s

• [SLOW TEST:12.752 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:00:59.070: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:01:07.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4221" for this suite.
Jun  1 17:01:13.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:01:13.299: INFO: namespace job-4221 deletion completed in 6.088778589s

• [SLOW TEST:14.229 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:01:13.299: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  1 17:01:17.526: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:17.528: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:19.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:19.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:21.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:21.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:23.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:23.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:25.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:25.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:27.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:27.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:29.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:29.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:31.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:31.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:33.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:33.532: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  1 17:01:35.528: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  1 17:01:35.532: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:01:35.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3441" for this suite.
Jun  1 17:02:03.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:02:03.882: INFO: namespace container-lifecycle-hook-3441 deletion completed in 28.346251435s

• [SLOW TEST:50.583 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:02:03.883: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun  1 17:02:06.540: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-355 pod-service-account-f3cd2ca4-5f5e-444c-a24c-9282290d3d35 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun  1 17:02:06.801: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-355 pod-service-account-f3cd2ca4-5f5e-444c-a24c-9282290d3d35 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun  1 17:02:07.037: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-355 pod-service-account-f3cd2ca4-5f5e-444c-a24c-9282290d3d35 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:02:07.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-355" for this suite.
Jun  1 17:02:13.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:02:13.415: INFO: namespace svcaccounts-355 deletion completed in 6.130171189s

• [SLOW TEST:9.533 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:02:13.416: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun  1 17:02:13.551: INFO: Waiting up to 5m0s for pod "downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d" in namespace "downward-api-5602" to be "success or failure"
Jun  1 17:02:13.553: INFO: Pod "downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385447ms
Jun  1 17:02:15.557: INFO: Pod "downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005806536s
Jun  1 17:02:17.560: INFO: Pod "downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009353775s
STEP: Saw pod success
Jun  1 17:02:17.560: INFO: Pod "downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d" satisfied condition "success or failure"
Jun  1 17:02:17.563: INFO: Trying to get logs from node appserv9 pod downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d container dapi-container: <nil>
STEP: delete the pod
Jun  1 17:02:17.590: INFO: Waiting for pod downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d to disappear
Jun  1 17:02:17.593: INFO: Pod downward-api-da54d952-a796-4b6e-b41b-6f6e4b433b6d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:02:17.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5602" for this suite.
Jun  1 17:02:23.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:02:23.729: INFO: namespace downward-api-5602 deletion completed in 6.132847265s

• [SLOW TEST:10.314 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:02:23.730: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-56a36482-49cb-4a9a-97f7-c9f08fc57629
STEP: Creating a pod to test consume secrets
Jun  1 17:02:23.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7" in namespace "projected-443" to be "success or failure"
Jun  1 17:02:23.876: INFO: Pod "pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.509752ms
Jun  1 17:02:25.879: INFO: Pod "pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005754604s
Jun  1 17:02:27.882: INFO: Pod "pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008830474s
STEP: Saw pod success
Jun  1 17:02:27.882: INFO: Pod "pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7" satisfied condition "success or failure"
Jun  1 17:02:27.885: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7 container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:02:27.910: INFO: Waiting for pod pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7 to disappear
Jun  1 17:02:27.912: INFO: Pod pod-projected-secrets-a4bce570-5daf-4452-83cc-000f63397ec7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:02:27.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-443" for this suite.
Jun  1 17:02:33.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:02:34.015: INFO: namespace projected-443 deletion completed in 6.100007782s

• [SLOW TEST:10.286 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:02:34.015: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun  1 17:02:34.150: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  1 17:02:34.161: INFO: Waiting for terminating namespaces to be deleted...
Jun  1 17:02:34.164: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Jun  1 17:02:34.173: INFO: csi-diamanti-driver-lqbpt from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container diamanticsidriver ready: true, restart count 1
Jun  1 17:02:34.173: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.173: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 17:02:34.173: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 17:02:34.173: INFO: nfs-csi-diamanti-driver-cht54 from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 17:02:34.173: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.173: INFO: metrics-server-v1-7659784467-v66bd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container metrics-server ready: true, restart count 0
Jun  1 17:02:34.173: INFO: sonobuoy-e2e-job-5a0f7ba7dfe14684 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container e2e ready: true, restart count 0
Jun  1 17:02:34.173: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 17:02:34.173: INFO: csi-external-snapshotter-5f96b86db9-kzt8f from diamanti-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Jun  1 17:02:34.173: INFO: prometheus-v1-2 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 17:02:34.173: INFO: provisioner-654c8c4db6-zxcsz from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container provisioner ready: true, restart count 0
Jun  1 17:02:34.173: INFO: sonobuoy from sonobuoy started at 2020-06-01 16:25:01 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.173: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  1 17:02:34.173: INFO: collectd-v0.8-hxgzj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 17:02:34.174: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 17:02:34.174: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 17:02:34.174: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 17:02:34.174: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 17:02:34.174: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 17:02:34.174: INFO: coredns-6d6bd8df7c-2zcwd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.174: INFO: 	Container coredns ready: true, restart count 0
Jun  1 17:02:34.174: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Jun  1 17:02:34.182: INFO: coredns-6d6bd8df7c-bnq4n from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container coredns ready: true, restart count 0
Jun  1 17:02:34.182: INFO: prometheus-v1-1 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 17:02:34.182: INFO: tiller-deploy-57f5b6fd78-574p5 from kube-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container tiller ready: true, restart count 0
Jun  1 17:02:34.182: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 17:02:34.182: INFO: nfs-csi-diamanti-driver-h2dts from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.182: INFO: csi-diamanti-driver-s59hb from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 17:02:34.182: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.182: INFO: collectd-v0.8-8fjnl from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (5 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 17:02:34.182: INFO: alertmanager-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container alertmanager ready: true, restart count 0
Jun  1 17:02:34.182: INFO: snapshot-controller-798c69596d-nsc4s from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.182: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jun  1 17:02:34.182: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Jun  1 17:02:34.190: INFO: csi-external-resizer-65f576d548-skb55 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container csi-external-resizer ready: true, restart count 0
Jun  1 17:02:34.190: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 from sonobuoy started at 2020-06-01 16:25:03 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 17:02:34.190: INFO: collectd-v0.8-m7sbv from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 17:02:34.190: INFO: csi-external-provisioner-7f56896f9-csfz4 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Jun  1 17:02:34.190: INFO: helm-chart-7fcf79f88c-6pv8f from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container helm-chart ready: true, restart count 0
Jun  1 17:02:34.190: INFO: coredns-6d6bd8df7c-ghgls from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container coredns ready: true, restart count 0
Jun  1 17:02:34.190: INFO: csi-external-attacher-846d47f8d6-7zcsv from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  1 17:02:34.190: INFO: nfs-csi-diamanti-driver-dh2xj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 17:02:34.190: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.190: INFO: csi-diamanti-driver-cg4qz from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 17:02:34.190: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 17:02:34.190: INFO: prometheus-v1-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 17:02:34.190: INFO: 	Container prometheus ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6380e3ed-b276-47e1-a65f-12d258b7746c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6380e3ed-b276-47e1-a65f-12d258b7746c off the node appserv9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6380e3ed-b276-47e1-a65f-12d258b7746c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:02:42.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7063" for this suite.
Jun  1 17:03:00.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:03:00.388: INFO: namespace sched-pred-7063 deletion completed in 18.133800734s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:26.373 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:03:00.388: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:03:00.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5518" for this suite.
Jun  1 17:03:06.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:03:06.612: INFO: namespace custom-resource-definition-5518 deletion completed in 6.083187159s

• [SLOW TEST:6.224 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:03:06.612: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9526
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-61338909-ff89-4d7c-8c4f-0ab5ab2327ef
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:03:10.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9526" for this suite.
Jun  1 17:03:38.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:03:38.863: INFO: namespace configmap-9526 deletion completed in 28.077186253s

• [SLOW TEST:32.251 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:03:38.863: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:03:39.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:03:42.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:03:42.358: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:03:43.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3671" for this suite.
Jun  1 17:03:49.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:03:49.612: INFO: namespace webhook-3671 deletion completed in 6.126350345s
STEP: Destroying namespace "webhook-3671-markers" for this suite.
Jun  1 17:03:55.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:03:55.785: INFO: namespace webhook-3671-markers deletion completed in 6.172454742s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.933 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:03:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-ebd75f9c-a450-452e-8dfe-a87d85ef5802
STEP: Creating a pod to test consume configMaps
Jun  1 17:03:55.939: INFO: Waiting up to 5m0s for pod "pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0" in namespace "configmap-7462" to be "success or failure"
Jun  1 17:03:55.942: INFO: Pod "pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.241281ms
Jun  1 17:03:57.945: INFO: Pod "pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005479257s
Jun  1 17:03:59.948: INFO: Pod "pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008905457s
STEP: Saw pod success
Jun  1 17:03:59.948: INFO: Pod "pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0" satisfied condition "success or failure"
Jun  1 17:03:59.951: INFO: Trying to get logs from node appserv9 pod pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:03:59.968: INFO: Waiting for pod pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0 to disappear
Jun  1 17:03:59.971: INFO: Pod pod-configmaps-87d30f73-325c-40cd-ae2c-6173d8679ee0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:03:59.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7462" for this suite.
Jun  1 17:04:05.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:04:06.066: INFO: namespace configmap-7462 deletion completed in 6.091609038s

• [SLOW TEST:10.270 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:04:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:04:06.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739" in namespace "projected-8983" to be "success or failure"
Jun  1 17:04:06.208: INFO: Pod "downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547892ms
Jun  1 17:04:08.212: INFO: Pod "downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00678246s
STEP: Saw pod success
Jun  1 17:04:08.212: INFO: Pod "downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739" satisfied condition "success or failure"
Jun  1 17:04:08.215: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739 container client-container: <nil>
STEP: delete the pod
Jun  1 17:04:08.230: INFO: Waiting for pod downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739 to disappear
Jun  1 17:04:08.232: INFO: Pod downwardapi-volume-a2a896b2-dcab-479b-a918-c3ebf7ded739 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:04:08.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8983" for this suite.
Jun  1 17:04:14.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:04:14.320: INFO: namespace projected-8983 deletion completed in 6.084697442s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:04:14.320: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a
Jun  1 17:04:14.459: INFO: Pod name my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a: Found 0 pods out of 1
Jun  1 17:04:19.463: INFO: Pod name my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a: Found 1 pods out of 1
Jun  1 17:04:19.463: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a" are running
Jun  1 17:04:19.465: INFO: Pod "my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a-2qsmz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:04:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:04:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:04:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:04:14 +0000 UTC Reason: Message:}])
Jun  1 17:04:19.465: INFO: Trying to dial the pod
Jun  1 17:04:24.477: INFO: Controller my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a: Got expected result from replica 1 [my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a-2qsmz]: "my-hostname-basic-e720e6db-6a4d-4662-b720-71049719fd0a-2qsmz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:04:24.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5209" for this suite.
Jun  1 17:04:30.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:04:30.637: INFO: namespace replication-controller-5209 deletion completed in 6.156196126s

• [SLOW TEST:16.317 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:04:30.637: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:04:30.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-6670'
Jun  1 17:04:31.158: INFO: stderr: ""
Jun  1 17:04:31.158: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun  1 17:04:31.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-6670'
Jun  1 17:04:31.345: INFO: stderr: ""
Jun  1 17:04:31.345: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun  1 17:04:32.349: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:04:32.349: INFO: Found 0 / 1
Jun  1 17:04:33.349: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:04:33.349: INFO: Found 1 / 1
Jun  1 17:04:33.349: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  1 17:04:33.352: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:04:33.352: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  1 17:04:33.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 describe pod redis-master-8gtzw --namespace=kubectl-6670'
Jun  1 17:04:33.494: INFO: stderr: ""
Jun  1 17:04:33.494: INFO: stdout: "Name:         redis-master-8gtzw\nNamespace:    kubectl-6670\nPriority:     0\nNode:         appserv9/172.16.6.109\nStart Time:   Mon, 01 Jun 2020 17:04:31 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           172.16.141.12\nIPs:\n  IP:           172.16.141.12\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3ca76bcabdd8d0422b11814948f9186fe6c493c2b69bed35585c69b79a02d82e\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 01 Jun 2020 17:04:32 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6rx24 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-6rx24:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6rx24\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-6670/redis-master-8gtzw to appserv9\n  Normal  Pulled     1s         kubelet, appserv9  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, appserv9  Created container redis-master\n  Normal  Started    1s         kubelet, appserv9  Started container redis-master\n"
Jun  1 17:04:33.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 describe rc redis-master --namespace=kubectl-6670'
Jun  1 17:04:33.649: INFO: stderr: ""
Jun  1 17:04:33.649: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6670\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-8gtzw\n"
Jun  1 17:04:33.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 describe service redis-master --namespace=kubectl-6670'
Jun  1 17:04:33.789: INFO: stderr: ""
Jun  1 17:04:33.789: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6670\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.0.2\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.141.12:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  1 17:04:33.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 describe node appserv10'
Jun  1 17:04:33.963: INFO: stderr: ""
Jun  1 17:04:33.963: INFO: stdout: "Name:               appserv10\nRoles:              <none>\nLabels:             beta.diamanti.com/runc=true\n                    beta.diamanti.com/runtime-engine=docker\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=appserv10\n                    kubernetes.io/os=linux\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"dcx.csi.diamanti.com\":\"appserv10\"}\n                    node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Mon, 01 Jun 2020 16:23:54 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 01 Jun 2020 17:03:57 +0000   Mon, 01 Jun 2020 16:23:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 01 Jun 2020 17:03:57 +0000   Mon, 01 Jun 2020 16:23:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 01 Jun 2020 17:03:57 +0000   Mon, 01 Jun 2020 16:23:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 01 Jun 2020 17:03:57 +0000   Mon, 01 Jun 2020 16:24:04 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.16.6.110\n  Hostname:    appserv10\nCapacity:\n cpu:                32\n ephemeral-storage:  65504Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65696548Ki\n pods:               110\nAllocatable:\n cpu:                32\n ephemeral-storage:  61817329972\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65594148Ki\n pods:               110\nSystem Info:\n Machine ID:                 7d84295c670a448fa5037ea9027fbe07\n System UUID:                8D3B0B1C-7BF9-1000-AE0C-54AB3A29191F\n Boot ID:                    ab14bf9b-3bf5-4527-ae3e-7b4096ecf475\n Kernel Version:             3.10.0-957.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.16.8\n Kube-Proxy Version:         v1.16.8\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  diamanti-system            collectd-v0.8-hxgzj                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  diamanti-system            csi-diamanti-driver-lqbpt                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  diamanti-system            csi-external-snapshotter-5f96b86db9-kzt8f                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  diamanti-system            nfs-csi-diamanti-driver-cht54                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  diamanti-system            prometheus-v1-2                                            0 (0%)        0 (0%)      1Gi (1%)         1Gi (1%)       40m\n  diamanti-system            provisioner-654c8c4db6-zxcsz                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                coredns-6d6bd8df7c-2zcwd                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     40m\n  kube-system                metrics-server-v1-7659784467-v66bd                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  sonobuoy                   sonobuoy-e2e-job-5a0f7ba7dfe14684                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                100m (0%)    0 (0%)\n  memory             1094Mi (1%)  1194Mi (1%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age   From                   Message\n  ----    ------                   ----  ----                   -------\n  Normal  Starting                 40m   kube-proxy, appserv10  Starting kube-proxy.\n  Normal  Starting                 40m   kubelet, appserv10     Starting kubelet.\n  Normal  NodeHasSufficientMemory  40m   kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    40m   kubelet, appserv10     Node appserv10 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     40m   kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  40m   kubelet, appserv10     Updated Node Allocatable limit across pods\n  Normal  NodeReady                40m   kubelet, appserv10     Node appserv10 status is now: NodeReady\n"
Jun  1 17:04:33.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 describe namespace kubectl-6670'
Jun  1 17:04:34.096: INFO: stderr: ""
Jun  1 17:04:34.097: INFO: stdout: "Name:         kubectl-6670\nLabels:       e2e-framework=kubectl\n              e2e-run=edc48979-7dfd-4125-a002-6fec5feb9fc3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:04:34.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6670" for this suite.
Jun  1 17:05:02.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:05:02.234: INFO: namespace kubectl-6670 deletion completed in 28.132927248s

• [SLOW TEST:31.596 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:05:02.234: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3784
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun  1 17:05:02.368: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:05:22.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3784" for this suite.
Jun  1 17:05:28.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:05:28.683: INFO: namespace crd-publish-openapi-3784 deletion completed in 6.085517824s

• [SLOW TEST:26.449 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:05:28.683: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-b0a36435-cbe9-4617-abab-50da7129de9f
STEP: Creating a pod to test consume secrets
Jun  1 17:05:28.823: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82" in namespace "projected-5454" to be "success or failure"
Jun  1 17:05:28.825: INFO: Pod "pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.176663ms
Jun  1 17:05:30.828: INFO: Pod "pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005577006s
Jun  1 17:05:32.832: INFO: Pod "pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009161838s
STEP: Saw pod success
Jun  1 17:05:32.832: INFO: Pod "pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82" satisfied condition "success or failure"
Jun  1 17:05:32.834: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:05:32.862: INFO: Waiting for pod pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82 to disappear
Jun  1 17:05:32.864: INFO: Pod pod-projected-secrets-732b62a4-5ed8-4eb4-a1ba-8f8d3e9e1f82 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:05:32.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5454" for this suite.
Jun  1 17:05:38.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:05:38.957: INFO: namespace projected-5454 deletion completed in 6.088658099s

• [SLOW TEST:10.274 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:05:38.957: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:05:39.087: INFO: Creating ReplicaSet my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935
Jun  1 17:05:39.093: INFO: Pod name my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935: Found 0 pods out of 1
Jun  1 17:05:44.097: INFO: Pod name my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935: Found 1 pods out of 1
Jun  1 17:05:44.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935" is running
Jun  1 17:05:44.100: INFO: Pod "my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935-qbdqp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:05:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:05:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:05:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-01 17:05:39 +0000 UTC Reason: Message:}])
Jun  1 17:05:44.100: INFO: Trying to dial the pod
Jun  1 17:05:49.110: INFO: Controller my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935: Got expected result from replica 1 [my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935-qbdqp]: "my-hostname-basic-abf811ee-7440-41cd-8af8-382f1a104935-qbdqp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:05:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2534" for this suite.
Jun  1 17:05:55.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:05:55.202: INFO: namespace replicaset-2534 deletion completed in 6.087995899s

• [SLOW TEST:16.245 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:05:55.202: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-71ea4555-dcbf-4753-9bee-ed86fb9e84ea
STEP: Creating a pod to test consume secrets
Jun  1 17:05:55.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4" in namespace "projected-4340" to be "success or failure"
Jun  1 17:05:55.345: INFO: Pod "pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389903ms
Jun  1 17:05:57.348: INFO: Pod "pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005603468s
STEP: Saw pod success
Jun  1 17:05:57.348: INFO: Pod "pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4" satisfied condition "success or failure"
Jun  1 17:05:57.351: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:05:57.368: INFO: Waiting for pod pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4 to disappear
Jun  1 17:05:57.371: INFO: Pod pod-projected-secrets-46d02ea8-f0db-4947-998e-d8bd7d9576b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:05:57.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4340" for this suite.
Jun  1 17:06:03.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:06:03.460: INFO: namespace projected-4340 deletion completed in 6.085755963s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:06:03.460: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:06:03.600: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-aec00e6a-1905-4a1d-a470-0774cfb3d8e7" in namespace "security-context-test-846" to be "success or failure"
Jun  1 17:06:03.602: INFO: Pod "alpine-nnp-false-aec00e6a-1905-4a1d-a470-0774cfb3d8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401895ms
Jun  1 17:06:05.606: INFO: Pod "alpine-nnp-false-aec00e6a-1905-4a1d-a470-0774cfb3d8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006016251s
Jun  1 17:06:07.609: INFO: Pod "alpine-nnp-false-aec00e6a-1905-4a1d-a470-0774cfb3d8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009223546s
Jun  1 17:06:07.609: INFO: Pod "alpine-nnp-false-aec00e6a-1905-4a1d-a470-0774cfb3d8e7" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:06:07.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-846" for this suite.
Jun  1 17:06:13.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:06:13.715: INFO: namespace security-context-test-846 deletion completed in 6.092931528s

• [SLOW TEST:10.255 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:06:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 17:06:13.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-2578'
Jun  1 17:06:13.992: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  1 17:06:13.992: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Jun  1 17:06:15.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete deployment e2e-test-httpd-deployment --namespace=kubectl-2578'
Jun  1 17:06:16.163: INFO: stderr: ""
Jun  1 17:06:16.163: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:06:16.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2578" for this suite.
Jun  1 17:06:28.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:06:28.289: INFO: namespace kubectl-2578 deletion completed in 12.121109675s

• [SLOW TEST:14.573 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:06:28.289: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  1 17:06:32.942: INFO: Successfully updated pod "pod-update-b7ae5527-a842-40bb-a720-a572a1b4822e"
STEP: verifying the updated pod is in kubernetes
Jun  1 17:06:32.948: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:06:32.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8271" for this suite.
Jun  1 17:07:00.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:07:01.041: INFO: namespace pods-8271 deletion completed in 28.089166608s

• [SLOW TEST:32.752 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:07:01.042: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-mr65
STEP: Creating a pod to test atomic-volume-subpath
Jun  1 17:07:01.184: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mr65" in namespace "subpath-9416" to be "success or failure"
Jun  1 17:07:01.187: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36983ms
Jun  1 17:07:03.190: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 2.00577546s
Jun  1 17:07:05.194: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 4.009361587s
Jun  1 17:07:07.197: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 6.012892108s
Jun  1 17:07:09.200: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 8.015987285s
Jun  1 17:07:11.204: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 10.019654327s
Jun  1 17:07:13.208: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 12.023265664s
Jun  1 17:07:15.211: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 14.026824831s
Jun  1 17:07:17.215: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 16.030538775s
Jun  1 17:07:19.218: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 18.033909645s
Jun  1 17:07:21.222: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Running", Reason="", readiness=true. Elapsed: 20.037124583s
Jun  1 17:07:23.229: INFO: Pod "pod-subpath-test-secret-mr65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044540047s
STEP: Saw pod success
Jun  1 17:07:23.229: INFO: Pod "pod-subpath-test-secret-mr65" satisfied condition "success or failure"
Jun  1 17:07:23.232: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-secret-mr65 container test-container-subpath-secret-mr65: <nil>
STEP: delete the pod
Jun  1 17:07:23.257: INFO: Waiting for pod pod-subpath-test-secret-mr65 to disappear
Jun  1 17:07:23.259: INFO: Pod pod-subpath-test-secret-mr65 no longer exists
STEP: Deleting pod pod-subpath-test-secret-mr65
Jun  1 17:07:23.259: INFO: Deleting pod "pod-subpath-test-secret-mr65" in namespace "subpath-9416"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:07:23.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9416" for this suite.
Jun  1 17:07:29.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:07:29.351: INFO: namespace subpath-9416 deletion completed in 6.086682665s

• [SLOW TEST:28.310 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:07:29.351: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:07:30.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:07:33.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun  1 17:07:37.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 attach --namespace=webhook-9828 to-be-attached-pod -i -c=container1'
Jun  1 17:07:37.540: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:07:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9828" for this suite.
Jun  1 17:07:49.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:07:49.663: INFO: namespace webhook-9828 deletion completed in 12.113506332s
STEP: Destroying namespace "webhook-9828-markers" for this suite.
Jun  1 17:07:55.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:07:55.753: INFO: namespace webhook-9828-markers deletion completed in 6.089806655s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.412 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:07:55.764: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  1 17:07:58.915: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:07:58.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1950" for this suite.
Jun  1 17:08:04.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:08:05.016: INFO: namespace container-runtime-1950 deletion completed in 6.089147654s

• [SLOW TEST:9.253 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:08:05.016: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:08:05.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff" in namespace "downward-api-1092" to be "success or failure"
Jun  1 17:08:05.160: INFO: Pod "downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324557ms
Jun  1 17:08:07.163: INFO: Pod "downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005735331s
STEP: Saw pod success
Jun  1 17:08:07.163: INFO: Pod "downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff" satisfied condition "success or failure"
Jun  1 17:08:07.166: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff container client-container: <nil>
STEP: delete the pod
Jun  1 17:08:07.182: INFO: Waiting for pod downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff to disappear
Jun  1 17:08:07.185: INFO: Pod downwardapi-volume-73547a27-1e8c-4552-a0e8-da3bacd708ff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:08:07.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1092" for this suite.
Jun  1 17:08:13.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:08:13.271: INFO: namespace downward-api-1092 deletion completed in 6.082796525s

• [SLOW TEST:8.255 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:08:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-104c28ea-0f06-47bf-bbcb-e89fa83678ba
STEP: Creating a pod to test consume secrets
Jun  1 17:08:13.412: INFO: Waiting up to 5m0s for pod "pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36" in namespace "secrets-9125" to be "success or failure"
Jun  1 17:08:13.414: INFO: Pod "pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353191ms
Jun  1 17:08:15.417: INFO: Pod "pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005578162s
Jun  1 17:08:17.421: INFO: Pod "pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009094482s
STEP: Saw pod success
Jun  1 17:08:17.421: INFO: Pod "pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36" satisfied condition "success or failure"
Jun  1 17:08:17.424: INFO: Trying to get logs from node appserv11 pod pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36 container secret-env-test: <nil>
STEP: delete the pod
Jun  1 17:08:17.453: INFO: Waiting for pod pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36 to disappear
Jun  1 17:08:17.456: INFO: Pod pod-secrets-1454b836-5f7d-4279-ac71-6066107abd36 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:08:17.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9125" for this suite.
Jun  1 17:08:23.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:08:23.596: INFO: namespace secrets-9125 deletion completed in 6.136041282s

• [SLOW TEST:10.324 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:08:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5526
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:08:23.730: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Creating first CR 
Jun  1 17:08:24.300: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:24Z generation:1 name:name1 resourceVersion:14869 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4a5e13b8-362e-4743-8460-11dbce0c78c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun  1 17:08:34.307: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:34Z generation:1 name:name2 resourceVersion:14890 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:75afef88-d6d4-4845-9655-818accc993d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun  1 17:08:44.312: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:24Z generation:2 name:name1 resourceVersion:14912 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4a5e13b8-362e-4743-8460-11dbce0c78c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun  1 17:08:54.317: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:34Z generation:2 name:name2 resourceVersion:14935 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:75afef88-d6d4-4845-9655-818accc993d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun  1 17:09:04.324: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:24Z generation:2 name:name1 resourceVersion:14957 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4a5e13b8-362e-4743-8460-11dbce0c78c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun  1 17:09:14.331: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-01T17:08:34Z generation:2 name:name2 resourceVersion:14980 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:75afef88-d6d4-4845-9655-818accc993d4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:09:24.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5526" for this suite.
Jun  1 17:09:30.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:09:30.973: INFO: namespace crd-watch-5526 deletion completed in 6.125201126s

• [SLOW TEST:67.377 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:09:30.973: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  1 17:09:31.138: INFO: Number of nodes with available pods: 0
Jun  1 17:09:31.138: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:09:32.146: INFO: Number of nodes with available pods: 0
Jun  1 17:09:32.146: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:09:33.146: INFO: Number of nodes with available pods: 2
Jun  1 17:09:33.146: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:09:34.146: INFO: Number of nodes with available pods: 3
Jun  1 17:09:34.146: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun  1 17:09:34.163: INFO: Number of nodes with available pods: 2
Jun  1 17:09:34.163: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:35.171: INFO: Number of nodes with available pods: 2
Jun  1 17:09:35.171: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:36.171: INFO: Number of nodes with available pods: 2
Jun  1 17:09:36.171: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:37.171: INFO: Number of nodes with available pods: 2
Jun  1 17:09:37.171: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:38.170: INFO: Number of nodes with available pods: 2
Jun  1 17:09:38.170: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:39.171: INFO: Number of nodes with available pods: 2
Jun  1 17:09:39.172: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:40.171: INFO: Number of nodes with available pods: 2
Jun  1 17:09:40.171: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:41.172: INFO: Number of nodes with available pods: 2
Jun  1 17:09:41.172: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:09:42.171: INFO: Number of nodes with available pods: 3
Jun  1 17:09:42.171: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6130, will wait for the garbage collector to delete the pods
Jun  1 17:09:42.233: INFO: Deleting DaemonSet.extensions daemon-set took: 5.916308ms
Jun  1 17:09:42.833: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.289319ms
Jun  1 17:09:54.936: INFO: Number of nodes with available pods: 0
Jun  1 17:09:54.936: INFO: Number of running nodes: 0, number of available pods: 0
Jun  1 17:09:54.939: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6130/daemonsets","resourceVersion":"15173"},"items":null}

Jun  1 17:09:54.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6130/pods","resourceVersion":"15173"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:09:54.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6130" for this suite.
Jun  1 17:10:00.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:10:01.160: INFO: namespace daemonsets-6130 deletion completed in 6.201956588s

• [SLOW TEST:30.187 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:10:01.161: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun  1 17:10:01.294: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Jun  1 17:10:02.166: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  1 17:10:05.030: INFO: Waited 828.858323ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:10:05.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1465" for this suite.
Jun  1 17:10:12.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:10:12.204: INFO: namespace aggregator-1465 deletion completed in 6.184204044s

• [SLOW TEST:11.044 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:10:12.205: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:10:34.352: INFO: Container started at 2020-06-01 17:10:13 +0000 UTC, pod became ready at 2020-06-01 17:10:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:10:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8522" for this suite.
Jun  1 17:11:02.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:02.743: INFO: namespace container-probe-8522 deletion completed in 28.38642579s

• [SLOW TEST:50.538 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:11:02.743: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Jun  1 17:11:02.883: INFO: Waiting up to 5m0s for pod "var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9" in namespace "var-expansion-7645" to be "success or failure"
Jun  1 17:11:02.885: INFO: Pod "var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351899ms
Jun  1 17:11:04.889: INFO: Pod "var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005806373s
Jun  1 17:11:06.892: INFO: Pod "var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009551638s
STEP: Saw pod success
Jun  1 17:11:06.892: INFO: Pod "var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9" satisfied condition "success or failure"
Jun  1 17:11:06.895: INFO: Trying to get logs from node appserv9 pod var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9 container dapi-container: <nil>
STEP: delete the pod
Jun  1 17:11:06.922: INFO: Waiting for pod var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9 to disappear
Jun  1 17:11:06.924: INFO: Pod var-expansion-38b58c3d-ac6a-414e-b6e4-392c921e4ae9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:11:06.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7645" for this suite.
Jun  1 17:11:12.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:13.344: INFO: namespace var-expansion-7645 deletion completed in 6.415646842s

• [SLOW TEST:10.601 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:11:13.344: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1232
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7107
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:11:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9128" for this suite.
Jun  1 17:11:25.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:25.851: INFO: namespace namespaces-9128 deletion completed in 6.092214728s
STEP: Destroying namespace "nsdeletetest-1232" for this suite.
Jun  1 17:11:25.853: INFO: Namespace nsdeletetest-1232 was already deleted
STEP: Destroying namespace "nsdeletetest-7107" for this suite.
Jun  1 17:11:31.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:31.946: INFO: namespace nsdeletetest-7107 deletion completed in 6.092385934s

• [SLOW TEST:18.602 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:11:31.946: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:11:32.897: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 17:11:34.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628292, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628292, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628292, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628292, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:11:37.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:11:37.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4303" for this suite.
Jun  1 17:11:49.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:50.056: INFO: namespace webhook-4303 deletion completed in 12.090845437s
STEP: Destroying namespace "webhook-4303-markers" for this suite.
Jun  1 17:11:56.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:11:56.143: INFO: namespace webhook-4303-markers deletion completed in 6.087275534s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.207 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:11:56.154: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-d178eab3-92fb-4d32-a63a-5516a7bdb671 in namespace container-probe-4745
Jun  1 17:11:58.295: INFO: Started pod test-webserver-d178eab3-92fb-4d32-a63a-5516a7bdb671 in namespace container-probe-4745
STEP: checking the pod's current state and verifying that restartCount is present
Jun  1 17:11:58.297: INFO: Initial restart count of pod test-webserver-d178eab3-92fb-4d32-a63a-5516a7bdb671 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:15:58.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4745" for this suite.
Jun  1 17:16:04.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:16:04.806: INFO: namespace container-probe-4745 deletion completed in 6.088869717s

• [SLOW TEST:248.652 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:16:04.806: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:16:04.939: INFO: Creating deployment "test-recreate-deployment"
Jun  1 17:16:04.943: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  1 17:16:04.949: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun  1 17:16:06.954: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  1 17:16:06.956: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  1 17:16:06.962: INFO: Updating deployment test-recreate-deployment
Jun  1 17:16:06.962: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun  1 17:16:06.997: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7995 /apis/apps/v1/namespaces/deployment-7995/deployments/test-recreate-deployment f0e456c6-e2ff-4e63-8dc1-3e18b599678d 16328 2 2020-06-01 17:16:04 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038ea748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-01 17:16:06 +0000 UTC,LastTransitionTime:2020-06-01 17:16:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-01 17:16:06 +0000 UTC,LastTransitionTime:2020-06-01 17:16:04 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  1 17:16:07.001: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7995 /apis/apps/v1/namespaces/deployment-7995/replicasets/test-recreate-deployment-5f94c574ff 46174639-530f-4dcc-ab77-486bb7543bf0 16326 1 2020-06-01 17:16:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f0e456c6-e2ff-4e63-8dc1-3e18b599678d 0xc0024489a7 0xc0024489a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002448a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 17:16:07.001: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  1 17:16:07.001: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-7995 /apis/apps/v1/namespaces/deployment-7995/replicasets/test-recreate-deployment-68fc85c7bb ecfe781a-b40b-4bf8-a133-e318b9d64014 16318 2 2020-06-01 17:16:04 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f0e456c6-e2ff-4e63-8dc1-3e18b599678d 0xc002448a77 0xc002448a78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002448ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 17:16:07.004: INFO: Pod "test-recreate-deployment-5f94c574ff-jdtvt" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-jdtvt test-recreate-deployment-5f94c574ff- deployment-7995 /api/v1/namespaces/deployment-7995/pods/test-recreate-deployment-5f94c574ff-jdtvt 32fe4954-8928-4ceb-8298-a799696aab18 16321 0 2020-06-01 17:16:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 46174639-530f-4dcc-ab77-486bb7543bf0 0xc002448f47 0xc002448f48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ntjkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ntjkn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ntjkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:16:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7995" for this suite.
Jun  1 17:16:13.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:16:13.384: INFO: namespace deployment-7995 deletion completed in 6.376986007s

• [SLOW TEST:8.578 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:16:13.384: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9492.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9492.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9492.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9492.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:16:15.555: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-9492/dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0: the server could not find the requested resource (get pods dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0)
Jun  1 17:16:15.558: INFO: Unable to read jessie_udp@PodARecord from pod dns-9492/dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0: the server could not find the requested resource (get pods dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0)
Jun  1 17:16:15.560: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9492/dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0: the server could not find the requested resource (get pods dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0)
Jun  1 17:16:15.560: INFO: Lookups using dns-9492/dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0 failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  1 17:16:20.589: INFO: DNS probes using dns-9492/dns-test-3fb90221-3379-4a1f-9605-0ab89d4fbce0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:16:20.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9492" for this suite.
Jun  1 17:16:26.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:16:26.707: INFO: namespace dns-9492 deletion completed in 6.093575115s

• [SLOW TEST:13.322 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:16:26.707: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9548
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun  1 17:16:26.909: INFO: Found 0 stateful pods, waiting for 3
Jun  1 17:16:36.913: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:16:36.914: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:16:36.914: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  1 17:16:36.940: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun  1 17:16:46.970: INFO: Updating stateful set ss2
Jun  1 17:16:46.976: INFO: Waiting for Pod statefulset-9548/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun  1 17:16:57.003: INFO: Found 1 stateful pods, waiting for 3
Jun  1 17:17:07.007: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:17:07.007: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:17:07.007: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun  1 17:17:07.030: INFO: Updating stateful set ss2
Jun  1 17:17:07.036: INFO: Waiting for Pod statefulset-9548/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 17:17:17.062: INFO: Updating stateful set ss2
Jun  1 17:17:17.067: INFO: Waiting for StatefulSet statefulset-9548/ss2 to complete update
Jun  1 17:17:17.067: INFO: Waiting for Pod statefulset-9548/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 17:17:27.075: INFO: Deleting all statefulset in ns statefulset-9548
Jun  1 17:17:27.078: INFO: Scaling statefulset ss2 to 0
Jun  1 17:17:57.092: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 17:17:57.095: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:17:57.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9548" for this suite.
Jun  1 17:18:03.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:03.206: INFO: namespace statefulset-9548 deletion completed in 6.095860161s

• [SLOW TEST:96.500 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:03.207: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:18:03.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de" in namespace "projected-2694" to be "success or failure"
Jun  1 17:18:03.354: INFO: Pod "downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426485ms
Jun  1 17:18:05.357: INFO: Pod "downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005670682s
Jun  1 17:18:07.361: INFO: Pod "downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009141113s
STEP: Saw pod success
Jun  1 17:18:07.361: INFO: Pod "downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de" satisfied condition "success or failure"
Jun  1 17:18:07.363: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de container client-container: <nil>
STEP: delete the pod
Jun  1 17:18:07.389: INFO: Waiting for pod downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de to disappear
Jun  1 17:18:07.392: INFO: Pod downwardapi-volume-02ab32d7-e648-478f-b35a-a71a0156f0de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:07.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2694" for this suite.
Jun  1 17:18:13.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:13.487: INFO: namespace projected-2694 deletion completed in 6.091461613s

• [SLOW TEST:10.280 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:18:13.645: INFO: (0) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 19.896163ms)
Jun  1 17:18:13.649: INFO: (1) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.953314ms)
Jun  1 17:18:13.652: INFO: (2) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.445157ms)
Jun  1 17:18:13.656: INFO: (3) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.678143ms)
Jun  1 17:18:13.659: INFO: (4) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.617909ms)
Jun  1 17:18:13.663: INFO: (5) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.600032ms)
Jun  1 17:18:13.667: INFO: (6) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.67432ms)
Jun  1 17:18:13.670: INFO: (7) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.637431ms)
Jun  1 17:18:13.674: INFO: (8) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.343934ms)
Jun  1 17:18:13.677: INFO: (9) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.659534ms)
Jun  1 17:18:13.681: INFO: (10) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.515702ms)
Jun  1 17:18:13.685: INFO: (11) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.491952ms)
Jun  1 17:18:13.688: INFO: (12) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.531571ms)
Jun  1 17:18:13.692: INFO: (13) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.598647ms)
Jun  1 17:18:13.695: INFO: (14) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.604191ms)
Jun  1 17:18:13.699: INFO: (15) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.414369ms)
Jun  1 17:18:13.702: INFO: (16) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.515202ms)
Jun  1 17:18:13.706: INFO: (17) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.45144ms)
Jun  1 17:18:13.709: INFO: (18) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.536631ms)
Jun  1 17:18:13.713: INFO: (19) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.400146ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:13.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3055" for this suite.
Jun  1 17:18:19.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:19.805: INFO: namespace proxy-3055 deletion completed in 6.088308439s

• [SLOW TEST:6.317 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:19.805: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-121
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:18:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  1 17:18:23.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-121 create -f -'
Jun  1 17:18:24.085: INFO: stderr: ""
Jun  1 17:18:24.085: INFO: stdout: "e2e-test-crd-publish-openapi-6522-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  1 17:18:24.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-121 delete e2e-test-crd-publish-openapi-6522-crds test-cr'
Jun  1 17:18:24.204: INFO: stderr: ""
Jun  1 17:18:24.204: INFO: stdout: "e2e-test-crd-publish-openapi-6522-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  1 17:18:24.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-121 apply -f -'
Jun  1 17:18:24.412: INFO: stderr: ""
Jun  1 17:18:24.412: INFO: stdout: "e2e-test-crd-publish-openapi-6522-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  1 17:18:24.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-121 delete e2e-test-crd-publish-openapi-6522-crds test-cr'
Jun  1 17:18:24.528: INFO: stderr: ""
Jun  1 17:18:24.528: INFO: stdout: "e2e-test-crd-publish-openapi-6522-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  1 17:18:24.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-6522-crds'
Jun  1 17:18:24.719: INFO: stderr: ""
Jun  1 17:18:24.720: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6522-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:28.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-121" for this suite.
Jun  1 17:18:34.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:34.435: INFO: namespace crd-publish-openapi-121 deletion completed in 6.091118973s

• [SLOW TEST:14.630 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:34.435: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Jun  1 17:18:34.574: INFO: Waiting up to 5m0s for pod "var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35" in namespace "var-expansion-9125" to be "success or failure"
Jun  1 17:18:34.587: INFO: Pod "var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35": Phase="Pending", Reason="", readiness=false. Elapsed: 12.202584ms
Jun  1 17:18:36.590: INFO: Pod "var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015724226s
STEP: Saw pod success
Jun  1 17:18:36.590: INFO: Pod "var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35" satisfied condition "success or failure"
Jun  1 17:18:36.593: INFO: Trying to get logs from node appserv9 pod var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35 container dapi-container: <nil>
STEP: delete the pod
Jun  1 17:18:36.610: INFO: Waiting for pod var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35 to disappear
Jun  1 17:18:36.612: INFO: Pod var-expansion-9900ef05-8d1c-4a9c-b4b5-a31e2a982f35 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:36.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9125" for this suite.
Jun  1 17:18:42.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:42.998: INFO: namespace var-expansion-9125 deletion completed in 6.381249141s

• [SLOW TEST:8.563 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:42.998: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  1 17:18:43.138: INFO: Waiting up to 5m0s for pod "pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf" in namespace "emptydir-3856" to be "success or failure"
Jun  1 17:18:43.141: INFO: Pod "pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4006ms
Jun  1 17:18:45.144: INFO: Pod "pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005763661s
STEP: Saw pod success
Jun  1 17:18:45.144: INFO: Pod "pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf" satisfied condition "success or failure"
Jun  1 17:18:45.147: INFO: Trying to get logs from node appserv9 pod pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf container test-container: <nil>
STEP: delete the pod
Jun  1 17:18:45.164: INFO: Waiting for pod pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf to disappear
Jun  1 17:18:45.166: INFO: Pod pod-e561c0a2-936a-47f8-9286-09e9bf7c33bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:45.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3856" for this suite.
Jun  1 17:18:51.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:18:51.259: INFO: namespace emptydir-3856 deletion completed in 6.089796662s

• [SLOW TEST:8.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:18:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:18:52.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 17:18:54.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726628732, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:18:57.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun  1 17:18:57.291: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:18:57.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4762" for this suite.
Jun  1 17:19:03.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:19:03.400: INFO: namespace webhook-4762 deletion completed in 6.091806253s
STEP: Destroying namespace "webhook-4762-markers" for this suite.
Jun  1 17:19:09.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:19:09.490: INFO: namespace webhook-4762-markers deletion completed in 6.089849359s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:19:09.501: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-19a332e6-69cb-407b-9f09-915148e14a7b
STEP: Creating a pod to test consume configMaps
Jun  1 17:19:09.648: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899" in namespace "configmap-9878" to be "success or failure"
Jun  1 17:19:09.650: INFO: Pod "pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353035ms
Jun  1 17:19:11.654: INFO: Pod "pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899": Phase="Running", Reason="", readiness=true. Elapsed: 2.005630537s
Jun  1 17:19:13.657: INFO: Pod "pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008841919s
STEP: Saw pod success
Jun  1 17:19:13.657: INFO: Pod "pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899" satisfied condition "success or failure"
Jun  1 17:19:13.660: INFO: Trying to get logs from node appserv9 pod pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:19:13.676: INFO: Waiting for pod pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899 to disappear
Jun  1 17:19:13.678: INFO: Pod pod-configmaps-6c0c9b17-2b1b-43b0-8d09-1c1d88b84899 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:19:13.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9878" for this suite.
Jun  1 17:19:19.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:19:19.812: INFO: namespace configmap-9878 deletion completed in 6.129376827s

• [SLOW TEST:10.311 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:19:19.812: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun  1 17:19:19.992: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17403 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  1 17:19:19.992: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17403 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun  1 17:19:29.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17423 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun  1 17:19:29.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17423 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun  1 17:19:40.006: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17443 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  1 17:19:40.007: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17443 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun  1 17:19:50.013: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17465 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  1 17:19:50.013: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-a a86cb35b-7ddb-4171-8dae-c49647aaaa17 17465 0 2020-06-01 17:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun  1 17:20:00.019: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-b 1f48690a-efcb-4fc8-92e5-6015d2e2e218 17486 0 2020-06-01 17:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  1 17:20:00.019: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-b 1f48690a-efcb-4fc8-92e5-6015d2e2e218 17486 0 2020-06-01 17:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun  1 17:20:10.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-b 1f48690a-efcb-4fc8-92e5-6015d2e2e218 17508 0 2020-06-01 17:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  1 17:20:10.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6031 /api/v1/namespaces/watch-6031/configmaps/e2e-watch-test-configmap-b 1f48690a-efcb-4fc8-92e5-6015d2e2e218 17508 0 2020-06-01 17:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:20:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6031" for this suite.
Jun  1 17:20:26.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:20:26.161: INFO: namespace watch-6031 deletion completed in 6.128662381s

• [SLOW TEST:66.349 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:20:26.162: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7436
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:20:26.296: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:20:32.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7436" for this suite.
Jun  1 17:20:38.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:20:38.567: INFO: namespace custom-resource-definition-7436 deletion completed in 6.080651363s

• [SLOW TEST:12.405 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:20:38.567: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-d7bd3cac-44f7-4267-bad7-fb492f55fb5e
STEP: Creating secret with name secret-projected-all-test-volume-06665216-467d-476e-a110-60c48086106e
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun  1 17:20:38.705: INFO: Waiting up to 5m0s for pod "projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9" in namespace "projected-7029" to be "success or failure"
Jun  1 17:20:38.707: INFO: Pod "projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122331ms
Jun  1 17:20:40.711: INFO: Pod "projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005608419s
STEP: Saw pod success
Jun  1 17:20:40.711: INFO: Pod "projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9" satisfied condition "success or failure"
Jun  1 17:20:40.713: INFO: Trying to get logs from node appserv11 pod projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun  1 17:20:40.783: INFO: Waiting for pod projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9 to disappear
Jun  1 17:20:40.785: INFO: Pod projected-volume-0e8f2aae-379a-411d-9379-38746feab0f9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:20:40.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7029" for this suite.
Jun  1 17:20:46.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:20:46.925: INFO: namespace projected-7029 deletion completed in 6.135895831s

• [SLOW TEST:8.358 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:20:46.926: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4169.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4169.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4169.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4169.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 153.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.153_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4169.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4169.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4169.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4169.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4169.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4169.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 153.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.153_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:20:51.089: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.092: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.098: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.119: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.122: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.124: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.127: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:51.144: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:20:56.148: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.154: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.157: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.178: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.181: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:20:56.205: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:21:01.148: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.154: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.158: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.160: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.179: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.182: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:01.203: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:21:06.148: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.155: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.180: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.183: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.186: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.189: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:06.206: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:21:11.149: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.152: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.155: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.180: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.183: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.186: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.189: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:11.207: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:21:16.148: INFO: Unable to read wheezy_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.155: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.178: INFO: Unable to read jessie_udp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.181: INFO: Unable to read jessie_tcp@dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local from pod dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8: the server could not find the requested resource (get pods dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8)
Jun  1 17:21:16.205: INFO: Lookups using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 failed for: [wheezy_udp@dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@dns-test-service.dns-4169.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_udp@dns-test-service.dns-4169.svc.cluster.local jessie_tcp@dns-test-service.dns-4169.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4169.svc.cluster.local]

Jun  1 17:21:21.208: INFO: DNS probes using dns-4169/dns-test-95925a3c-b0af-42c2-85b5-1bd563b95ed8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:21:21.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4169" for this suite.
Jun  1 17:21:27.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:21:27.333: INFO: namespace dns-4169 deletion completed in 6.088973671s

• [SLOW TEST:40.408 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:21:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:21:27.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3943" for this suite.
Jun  1 17:21:33.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:21:33.580: INFO: namespace resourcequota-3943 deletion completed in 6.088698781s

• [SLOW TEST:6.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:21:33.580: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-2f9c4aff-c242-45b0-a421-9ede61b05d8a
STEP: Creating a pod to test consume secrets
Jun  1 17:21:33.754: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a" in namespace "projected-8246" to be "success or failure"
Jun  1 17:21:33.756: INFO: Pod "pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389452ms
Jun  1 17:21:35.760: INFO: Pod "pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005881486s
Jun  1 17:21:37.764: INFO: Pod "pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009511563s
STEP: Saw pod success
Jun  1 17:21:37.764: INFO: Pod "pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a" satisfied condition "success or failure"
Jun  1 17:21:37.766: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:21:37.782: INFO: Waiting for pod pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a to disappear
Jun  1 17:21:37.784: INFO: Pod pod-projected-secrets-af79da05-0d56-4a9b-a971-1c8fb88e5f2a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:21:37.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8246" for this suite.
Jun  1 17:21:43.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:21:43.879: INFO: namespace projected-8246 deletion completed in 6.091083376s

• [SLOW TEST:10.298 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:21:43.879: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:21:44.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92" in namespace "projected-1399" to be "success or failure"
Jun  1 17:21:44.021: INFO: Pod "downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404786ms
Jun  1 17:21:46.025: INFO: Pod "downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92": Phase="Running", Reason="", readiness=true. Elapsed: 2.005465388s
Jun  1 17:21:48.028: INFO: Pod "downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008924879s
STEP: Saw pod success
Jun  1 17:21:48.028: INFO: Pod "downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92" satisfied condition "success or failure"
Jun  1 17:21:48.031: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92 container client-container: <nil>
STEP: delete the pod
Jun  1 17:21:48.064: INFO: Waiting for pod downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92 to disappear
Jun  1 17:21:48.066: INFO: Pod downwardapi-volume-a54d0465-0b3a-49e5-a8d4-efa63f38fa92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:21:48.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1399" for this suite.
Jun  1 17:21:54.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:21:54.233: INFO: namespace projected-1399 deletion completed in 6.163123109s

• [SLOW TEST:10.354 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:21:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun  1 17:21:56.908: INFO: Successfully updated pod "labelsupdate44f76c66-58f3-4d68-9d75-cc5dd5454574"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:21:58.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4476" for this suite.
Jun  1 17:22:26.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:22:27.017: INFO: namespace projected-4476 deletion completed in 28.087277475s

• [SLOW TEST:32.783 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:22:27.017: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Jun  1 17:22:27.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=kubectl-6436 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun  1 17:22:28.765: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun  1 17:22:28.765: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:22:30.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6436" for this suite.
Jun  1 17:22:36.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:22:36.863: INFO: namespace kubectl-6436 deletion completed in 6.087918807s

• [SLOW TEST:9.846 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:22:36.863: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:22:36.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2" in namespace "downward-api-1490" to be "success or failure"
Jun  1 17:22:37.002: INFO: Pod "downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.605091ms
Jun  1 17:22:39.005: INFO: Pod "downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005498604s
Jun  1 17:22:41.008: INFO: Pod "downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009000606s
STEP: Saw pod success
Jun  1 17:22:41.008: INFO: Pod "downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2" satisfied condition "success or failure"
Jun  1 17:22:41.011: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2 container client-container: <nil>
STEP: delete the pod
Jun  1 17:22:41.029: INFO: Waiting for pod downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2 to disappear
Jun  1 17:22:41.031: INFO: Pod downwardapi-volume-b89443ff-5358-432a-898a-94a3ee0d54a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:22:41.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1490" for this suite.
Jun  1 17:22:47.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:22:47.170: INFO: namespace downward-api-1490 deletion completed in 6.134843382s

• [SLOW TEST:10.307 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:22:47.170: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:22:47.310: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8575da9c-6718-48d1-88e8-ff9a471f2deb" in namespace "security-context-test-897" to be "success or failure"
Jun  1 17:22:47.312: INFO: Pod "busybox-readonly-false-8575da9c-6718-48d1-88e8-ff9a471f2deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559294ms
Jun  1 17:22:49.315: INFO: Pod "busybox-readonly-false-8575da9c-6718-48d1-88e8-ff9a471f2deb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005565784s
Jun  1 17:22:51.319: INFO: Pod "busybox-readonly-false-8575da9c-6718-48d1-88e8-ff9a471f2deb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009257332s
Jun  1 17:22:51.319: INFO: Pod "busybox-readonly-false-8575da9c-6718-48d1-88e8-ff9a471f2deb" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:22:51.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-897" for this suite.
Jun  1 17:22:57.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:22:57.448: INFO: namespace security-context-test-897 deletion completed in 6.124394856s

• [SLOW TEST:10.277 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:22:57.448: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e5d88209-3317-45f1-9ed9-89548f1a3fd0
STEP: Creating a pod to test consume secrets
Jun  1 17:22:57.589: INFO: Waiting up to 5m0s for pod "pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a" in namespace "secrets-188" to be "success or failure"
Jun  1 17:22:57.591: INFO: Pod "pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325415ms
Jun  1 17:22:59.595: INFO: Pod "pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005963973s
STEP: Saw pod success
Jun  1 17:22:59.595: INFO: Pod "pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a" satisfied condition "success or failure"
Jun  1 17:22:59.598: INFO: Trying to get logs from node appserv11 pod pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:22:59.615: INFO: Waiting for pod pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a to disappear
Jun  1 17:22:59.618: INFO: Pod pod-secrets-b2524ccd-1e5c-4d04-9273-3fc5969f232a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:22:59.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-188" for this suite.
Jun  1 17:23:05.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:23:05.711: INFO: namespace secrets-188 deletion completed in 6.089120927s

• [SLOW TEST:8.263 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:23:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5734
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-63acfeb8-b4b1-461b-8391-edf9ff7b57fc
STEP: Creating a pod to test consume secrets
Jun  1 17:23:05.854: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944" in namespace "projected-5734" to be "success or failure"
Jun  1 17:23:05.857: INFO: Pod "pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424552ms
Jun  1 17:23:07.860: INFO: Pod "pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005541372s
Jun  1 17:23:09.863: INFO: Pod "pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0088899s
STEP: Saw pod success
Jun  1 17:23:09.863: INFO: Pod "pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944" satisfied condition "success or failure"
Jun  1 17:23:09.866: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:23:09.883: INFO: Waiting for pod pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944 to disappear
Jun  1 17:23:09.885: INFO: Pod pod-projected-secrets-653d0a51-ef73-447d-a1c4-0750f3057944 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:23:09.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5734" for this suite.
Jun  1 17:23:15.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:23:16.107: INFO: namespace projected-5734 deletion completed in 6.218259873s

• [SLOW TEST:10.395 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:23:16.107: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  1 17:23:24.273: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  1 17:23:24.276: INFO: Pod pod-with-prestop-http-hook still exists
Jun  1 17:23:26.276: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  1 17:23:26.280: INFO: Pod pod-with-prestop-http-hook still exists
Jun  1 17:23:28.276: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  1 17:23:28.280: INFO: Pod pod-with-prestop-http-hook still exists
Jun  1 17:23:30.276: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  1 17:23:30.280: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:23:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7187" for this suite.
Jun  1 17:23:52.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:23:52.383: INFO: namespace container-lifecycle-hook-7187 deletion completed in 22.089872824s

• [SLOW TEST:36.276 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:23:52.383: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:23:56.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3453" for this suite.
Jun  1 17:24:46.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:24:46.639: INFO: namespace kubelet-test-3453 deletion completed in 50.094036916s

• [SLOW TEST:54.256 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:24:46.640: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun  1 17:24:52.801: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0601 17:24:52.801345      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:24:52.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8064" for this suite.
Jun  1 17:24:58.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:24:58.882: INFO: namespace gc-8064 deletion completed in 6.07773441s

• [SLOW TEST:12.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:24:58.882: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 17:24:59.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4508'
Jun  1 17:24:59.131: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  1 17:24:59.131: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun  1 17:24:59.137: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-wxqf2]
Jun  1 17:24:59.137: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-wxqf2" in namespace "kubectl-4508" to be "running and ready"
Jun  1 17:24:59.139: INFO: Pod "e2e-test-httpd-rc-wxqf2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866871ms
Jun  1 17:25:01.142: INFO: Pod "e2e-test-httpd-rc-wxqf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005150127s
Jun  1 17:25:01.142: INFO: Pod "e2e-test-httpd-rc-wxqf2" satisfied condition "running and ready"
Jun  1 17:25:01.142: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-wxqf2]
Jun  1 17:25:01.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs rc/e2e-test-httpd-rc --namespace=kubectl-4508'
Jun  1 17:25:01.294: INFO: stderr: ""
Jun  1 17:25:01.294: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.141.11. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.16.141.11. Set the 'ServerName' directive globally to suppress this message\n[Mon Jun 01 17:25:00.656549 2020] [mpm_event:notice] [pid 1:tid 140063724661608] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Jun 01 17:25:00.656611 2020] [core:notice] [pid 1:tid 140063724661608] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Jun  1 17:25:01.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete rc e2e-test-httpd-rc --namespace=kubectl-4508'
Jun  1 17:25:01.417: INFO: stderr: ""
Jun  1 17:25:01.417: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4508" for this suite.
Jun  1 17:25:13.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:13.548: INFO: namespace kubectl-4508 deletion completed in 12.126156173s

• [SLOW TEST:14.665 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:13.548: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:25:13.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8" in namespace "projected-3202" to be "success or failure"
Jun  1 17:25:13.692: INFO: Pod "downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595588ms
Jun  1 17:25:15.695: INFO: Pod "downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006300547s
STEP: Saw pod success
Jun  1 17:25:15.695: INFO: Pod "downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8" satisfied condition "success or failure"
Jun  1 17:25:15.698: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8 container client-container: <nil>
STEP: delete the pod
Jun  1 17:25:15.765: INFO: Waiting for pod downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8 to disappear
Jun  1 17:25:15.767: INFO: Pod downwardapi-volume-921e6a93-b58d-440c-bc90-701e349c7ed8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3202" for this suite.
Jun  1 17:25:21.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:21.900: INFO: namespace projected-3202 deletion completed in 6.12894587s

• [SLOW TEST:8.353 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:21.901: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  1 17:25:22.039: INFO: Waiting up to 5m0s for pod "pod-55b75be2-0a23-448c-a59b-34fd80d5fb01" in namespace "emptydir-2149" to be "success or failure"
Jun  1 17:25:22.042: INFO: Pod "pod-55b75be2-0a23-448c-a59b-34fd80d5fb01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366727ms
Jun  1 17:25:24.045: INFO: Pod "pod-55b75be2-0a23-448c-a59b-34fd80d5fb01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00569727s
STEP: Saw pod success
Jun  1 17:25:24.045: INFO: Pod "pod-55b75be2-0a23-448c-a59b-34fd80d5fb01" satisfied condition "success or failure"
Jun  1 17:25:24.048: INFO: Trying to get logs from node appserv9 pod pod-55b75be2-0a23-448c-a59b-34fd80d5fb01 container test-container: <nil>
STEP: delete the pod
Jun  1 17:25:24.064: INFO: Waiting for pod pod-55b75be2-0a23-448c-a59b-34fd80d5fb01 to disappear
Jun  1 17:25:24.066: INFO: Pod pod-55b75be2-0a23-448c-a59b-34fd80d5fb01 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:24.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2149" for this suite.
Jun  1 17:25:30.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:30.164: INFO: namespace emptydir-2149 deletion completed in 6.094520479s

• [SLOW TEST:8.264 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:30.165: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  1 17:25:32.824: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494"
Jun  1 17:25:32.824: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494" in namespace "pods-9130" to be "terminated due to deadline exceeded"
Jun  1 17:25:32.826: INFO: Pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494": Phase="Running", Reason="", readiness=true. Elapsed: 2.384751ms
Jun  1 17:25:34.829: INFO: Pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494": Phase="Running", Reason="", readiness=true. Elapsed: 2.005444416s
Jun  1 17:25:36.833: INFO: Pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008672622s
Jun  1 17:25:36.833: INFO: Pod "pod-update-activedeadlineseconds-2ff33a43-c5e7-430d-883b-08c00d991494" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:36.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9130" for this suite.
Jun  1 17:25:42.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:42.969: INFO: namespace pods-9130 deletion completed in 6.131397011s

• [SLOW TEST:12.804 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:42.969: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:25:43.109: INFO: Waiting up to 5m0s for pod "busybox-user-65534-abaae994-1bdc-4100-8dc9-45bf213b9b91" in namespace "security-context-test-7307" to be "success or failure"
Jun  1 17:25:43.114: INFO: Pod "busybox-user-65534-abaae994-1bdc-4100-8dc9-45bf213b9b91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576793ms
Jun  1 17:25:45.117: INFO: Pod "busybox-user-65534-abaae994-1bdc-4100-8dc9-45bf213b9b91": Phase="Running", Reason="", readiness=true. Elapsed: 2.007591744s
Jun  1 17:25:47.121: INFO: Pod "busybox-user-65534-abaae994-1bdc-4100-8dc9-45bf213b9b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011367013s
Jun  1 17:25:47.121: INFO: Pod "busybox-user-65534-abaae994-1bdc-4100-8dc9-45bf213b9b91" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:47.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7307" for this suite.
Jun  1 17:25:53.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:53.295: INFO: namespace security-context-test-7307 deletion completed in 6.170016115s

• [SLOW TEST:10.326 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:53.296: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Jun  1 17:25:53.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 cluster-info'
Jun  1 17:25:53.574: INFO: stderr: ""
Jun  1 17:25:53.574: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:25:53.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6940" for this suite.
Jun  1 17:25:59.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:25:59.667: INFO: namespace kubectl-6940 deletion completed in 6.088335026s

• [SLOW TEST:6.371 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:25:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3393
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:26:15.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3393" for this suite.
Jun  1 17:26:21.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:26:21.928: INFO: namespace resourcequota-3393 deletion completed in 6.090697859s

• [SLOW TEST:22.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:26:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:26:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5598" for this suite.
Jun  1 17:26:34.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:26:34.167: INFO: namespace pods-5598 deletion completed in 12.09461453s

• [SLOW TEST:12.240 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:26:34.168: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:26:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-478" for this suite.
Jun  1 17:27:04.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:27:04.421: INFO: namespace containers-478 deletion completed in 28.092646721s

• [SLOW TEST:30.253 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:27:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Jun  1 17:27:04.554: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun  1 17:27:04.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:04.875: INFO: stderr: ""
Jun  1 17:27:04.875: INFO: stdout: "service/redis-slave created\n"
Jun  1 17:27:04.876: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun  1 17:27:04.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:05.068: INFO: stderr: ""
Jun  1 17:27:05.068: INFO: stdout: "service/redis-master created\n"
Jun  1 17:27:05.068: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  1 17:27:05.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:05.247: INFO: stderr: ""
Jun  1 17:27:05.247: INFO: stdout: "service/frontend created\n"
Jun  1 17:27:05.247: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun  1 17:27:05.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:05.402: INFO: stderr: ""
Jun  1 17:27:05.403: INFO: stdout: "deployment.apps/frontend created\n"
Jun  1 17:27:05.403: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  1 17:27:05.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:05.587: INFO: stderr: ""
Jun  1 17:27:05.587: INFO: stdout: "deployment.apps/redis-master created\n"
Jun  1 17:27:05.587: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun  1 17:27:05.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-2705'
Jun  1 17:27:05.805: INFO: stderr: ""
Jun  1 17:27:05.805: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun  1 17:27:05.806: INFO: Waiting for all frontend pods to be Running.
Jun  1 17:27:10.856: INFO: Waiting for frontend to serve content.
Jun  1 17:27:10.889: INFO: Trying to add a new entry to the guestbook.
Jun  1 17:27:10.920: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun  1 17:27:10.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.091: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun  1 17:27:11.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.199: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.199: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun  1 17:27:11.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.305: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  1 17:27:11.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.399: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.399: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  1 17:27:11.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.513: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.513: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun  1 17:27:11.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-2705'
Jun  1 17:27:11.624: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:27:11.624: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:27:11.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2705" for this suite.
Jun  1 17:27:39.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:27:39.755: INFO: namespace kubectl-2705 deletion completed in 28.127314462s

• [SLOW TEST:35.334 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:27:39.755: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9926
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-7482d7ad-bfd8-4d82-a979-cfb65beceba8
STEP: Creating secret with name s-test-opt-upd-9d4e34bd-022a-4b05-8e86-f1114b9abddd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7482d7ad-bfd8-4d82-a979-cfb65beceba8
STEP: Updating secret s-test-opt-upd-9d4e34bd-022a-4b05-8e86-f1114b9abddd
STEP: Creating secret with name s-test-opt-create-9112a69c-1c80-451d-8643-15e840678d15
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:27:43.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9926" for this suite.
Jun  1 17:28:12.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:28:12.086: INFO: namespace secrets-9926 deletion completed in 28.095567866s

• [SLOW TEST:32.330 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:28:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8485
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun  1 17:28:14.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec pod-sharedvolume-420f9439-1073-442a-9f9c-ab572347cb34 -c busybox-main-container --namespace=emptydir-8485 -- cat /usr/share/volumeshare/shareddata.txt'
Jun  1 17:28:14.489: INFO: stderr: ""
Jun  1 17:28:14.489: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:28:14.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8485" for this suite.
Jun  1 17:28:20.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:28:20.588: INFO: namespace emptydir-8485 deletion completed in 6.094368646s

• [SLOW TEST:8.502 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:28:20.589: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-93
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  1 17:28:20.752: INFO: Number of nodes with available pods: 0
Jun  1 17:28:20.752: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:21.759: INFO: Number of nodes with available pods: 0
Jun  1 17:28:21.759: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:22.760: INFO: Number of nodes with available pods: 1
Jun  1 17:28:22.760: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:23.761: INFO: Number of nodes with available pods: 2
Jun  1 17:28:23.761: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:28:24.764: INFO: Number of nodes with available pods: 2
Jun  1 17:28:24.764: INFO: Node appserv11 is running more than one daemon pod
Jun  1 17:28:25.760: INFO: Number of nodes with available pods: 3
Jun  1 17:28:25.760: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun  1 17:28:25.777: INFO: Number of nodes with available pods: 2
Jun  1 17:28:25.777: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:26.785: INFO: Number of nodes with available pods: 2
Jun  1 17:28:26.786: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:27.785: INFO: Number of nodes with available pods: 2
Jun  1 17:28:27.785: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:28.784: INFO: Number of nodes with available pods: 2
Jun  1 17:28:28.784: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:28:29.785: INFO: Number of nodes with available pods: 3
Jun  1 17:28:29.785: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-93, will wait for the garbage collector to delete the pods
Jun  1 17:28:29.848: INFO: Deleting DaemonSet.extensions daemon-set took: 6.051551ms
Jun  1 17:28:30.448: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.186681ms
Jun  1 17:28:44.951: INFO: Number of nodes with available pods: 0
Jun  1 17:28:44.952: INFO: Number of running nodes: 0, number of available pods: 0
Jun  1 17:28:44.955: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-93/daemonsets","resourceVersion":"20150"},"items":null}

Jun  1 17:28:44.997: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-93/pods","resourceVersion":"20150"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:28:45.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-93" for this suite.
Jun  1 17:28:51.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:28:51.104: INFO: namespace daemonsets-93 deletion completed in 6.091864515s

• [SLOW TEST:30.516 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:28:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  1 17:28:55.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:28:55.294: INFO: Pod pod-with-poststart-http-hook still exists
Jun  1 17:28:57.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:28:57.297: INFO: Pod pod-with-poststart-http-hook still exists
Jun  1 17:28:59.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:28:59.297: INFO: Pod pod-with-poststart-http-hook still exists
Jun  1 17:29:01.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:29:01.298: INFO: Pod pod-with-poststart-http-hook still exists
Jun  1 17:29:03.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:29:03.298: INFO: Pod pod-with-poststart-http-hook still exists
Jun  1 17:29:05.294: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  1 17:29:05.297: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:29:05.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3132" for this suite.
Jun  1 17:29:17.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:29:17.387: INFO: namespace container-lifecycle-hook-3132 deletion completed in 12.086501117s

• [SLOW TEST:26.283 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:29:17.388: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-de896e90-d56a-473a-a683-977acae3313d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:29:17.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1374" for this suite.
Jun  1 17:29:23.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:29:23.613: INFO: namespace configmap-1374 deletion completed in 6.083932869s

• [SLOW TEST:6.225 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:29:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun  1 17:29:23.751: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  1 17:29:28.755: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:29:29.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5601" for this suite.
Jun  1 17:29:35.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:29:35.905: INFO: namespace replication-controller-5601 deletion completed in 6.133642491s

• [SLOW TEST:12.292 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:29:35.905: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:29:36.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024" in namespace "downward-api-3293" to be "success or failure"
Jun  1 17:29:36.049: INFO: Pod "downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463941ms
Jun  1 17:29:38.053: INFO: Pod "downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024": Phase="Running", Reason="", readiness=true. Elapsed: 2.006460872s
Jun  1 17:29:40.056: INFO: Pod "downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010325286s
STEP: Saw pod success
Jun  1 17:29:40.056: INFO: Pod "downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024" satisfied condition "success or failure"
Jun  1 17:29:40.059: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024 container client-container: <nil>
STEP: delete the pod
Jun  1 17:29:40.126: INFO: Waiting for pod downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024 to disappear
Jun  1 17:29:40.128: INFO: Pod downwardapi-volume-125892c6-b17a-4ae5-9c84-f815b1d74024 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:29:40.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3293" for this suite.
Jun  1 17:29:46.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:29:46.221: INFO: namespace downward-api-3293 deletion completed in 6.088568203s

• [SLOW TEST:10.316 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:29:46.222: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:29:46.749: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:29:49.765: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:29:49.768: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:29:50.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9595" for this suite.
Jun  1 17:29:56.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:29:57.022: INFO: namespace crd-webhook-9595 deletion completed in 6.129720338s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.811 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:29:57.033: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9261
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9261
STEP: creating replication controller externalsvc in namespace services-9261
I0601 17:29:57.185448      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-9261, replica count: 2
I0601 17:30:00.235879      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun  1 17:30:00.253: INFO: Creating new exec pod
Jun  1 17:30:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-9261 execpodl6rj6 -- /bin/sh -x -c nslookup nodeport-service'
Jun  1 17:30:04.808: INFO: stderr: "+ nslookup nodeport-service\n"
Jun  1 17:30:04.808: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-9261.svc.cluster.local\tcanonical name = externalsvc.services-9261.svc.cluster.local.\nName:\texternalsvc.services-9261.svc.cluster.local\nAddress: 10.0.0.209\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9261, will wait for the garbage collector to delete the pods
Jun  1 17:30:04.868: INFO: Deleting ReplicationController externalsvc took: 6.235915ms
Jun  1 17:30:04.968: INFO: Terminating ReplicationController externalsvc pods took: 100.243449ms
Jun  1 17:30:14.881: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:30:14.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9261" for this suite.
Jun  1 17:30:20.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:30:20.984: INFO: namespace services-9261 deletion completed in 6.089123151s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.951 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:30:20.984: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:30:23.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4141" for this suite.
Jun  1 17:31:07.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:31:07.364: INFO: namespace kubelet-test-4141 deletion completed in 44.174129145s

• [SLOW TEST:46.380 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:31:07.365: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:31:07.511: INFO: (0) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.794638ms)
Jun  1 17:31:07.515: INFO: (1) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.80548ms)
Jun  1 17:31:07.519: INFO: (2) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.809677ms)
Jun  1 17:31:07.522: INFO: (3) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.682518ms)
Jun  1 17:31:07.526: INFO: (4) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.479739ms)
Jun  1 17:31:07.529: INFO: (5) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.467695ms)
Jun  1 17:31:07.533: INFO: (6) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.372594ms)
Jun  1 17:31:07.536: INFO: (7) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.579777ms)
Jun  1 17:31:07.540: INFO: (8) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.271686ms)
Jun  1 17:31:07.543: INFO: (9) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.394251ms)
Jun  1 17:31:07.547: INFO: (10) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.428081ms)
Jun  1 17:31:07.550: INFO: (11) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.251133ms)
Jun  1 17:31:07.554: INFO: (12) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.773055ms)
Jun  1 17:31:07.557: INFO: (13) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.794924ms)
Jun  1 17:31:07.561: INFO: (14) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.714728ms)
Jun  1 17:31:07.565: INFO: (15) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.764155ms)
Jun  1 17:31:07.569: INFO: (16) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.80393ms)
Jun  1 17:31:07.573: INFO: (17) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.656983ms)
Jun  1 17:31:07.576: INFO: (18) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.506672ms)
Jun  1 17:31:07.580: INFO: (19) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.621342ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:31:07.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7268" for this suite.
Jun  1 17:31:13.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:31:13.683: INFO: namespace proxy-7268 deletion completed in 6.099963264s

• [SLOW TEST:6.319 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:31:13.684: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9998
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  1 17:31:13.817: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  1 17:31:37.979: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9998 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:31:37.979: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:31:39.091: INFO: Found all expected endpoints: [netserver-0]
Jun  1 17:31:39.094: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9998 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:31:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:31:40.204: INFO: Found all expected endpoints: [netserver-1]
Jun  1 17:31:40.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9998 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:31:40.207: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:31:41.325: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:31:41.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9998" for this suite.
Jun  1 17:31:53.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:31:53.708: INFO: namespace pod-network-test-9998 deletion completed in 12.378674796s

• [SLOW TEST:40.024 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:31:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0601 17:32:03.897288      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun  1 17:32:03.897: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:32:03.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1522" for this suite.
Jun  1 17:32:09.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:10.021: INFO: namespace gc-1522 deletion completed in 6.120743634s

• [SLOW TEST:16.313 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:32:10.022: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-10486785-4852-492b-9524-59b25a4a6650
STEP: Creating a pod to test consume secrets
Jun  1 17:32:10.200: INFO: Waiting up to 5m0s for pod "pod-secrets-10af684c-7885-4228-9653-0d5ebd251800" in namespace "secrets-4019" to be "success or failure"
Jun  1 17:32:10.202: INFO: Pod "pod-secrets-10af684c-7885-4228-9653-0d5ebd251800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51986ms
Jun  1 17:32:12.206: INFO: Pod "pod-secrets-10af684c-7885-4228-9653-0d5ebd251800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005793397s
Jun  1 17:32:14.210: INFO: Pod "pod-secrets-10af684c-7885-4228-9653-0d5ebd251800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009856932s
STEP: Saw pod success
Jun  1 17:32:14.210: INFO: Pod "pod-secrets-10af684c-7885-4228-9653-0d5ebd251800" satisfied condition "success or failure"
Jun  1 17:32:14.212: INFO: Trying to get logs from node appserv10 pod pod-secrets-10af684c-7885-4228-9653-0d5ebd251800 container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:32:14.230: INFO: Waiting for pod pod-secrets-10af684c-7885-4228-9653-0d5ebd251800 to disappear
Jun  1 17:32:14.232: INFO: Pod pod-secrets-10af684c-7885-4228-9653-0d5ebd251800 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:32:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4019" for this suite.
Jun  1 17:32:20.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:20.353: INFO: namespace secrets-4019 deletion completed in 6.117748998s

• [SLOW TEST:10.331 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:32:20.353: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:32:21.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 17:32:23.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629541, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629541, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629541, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629541, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:32:26.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:32:26.321: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5846-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:32:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1865" for this suite.
Jun  1 17:32:33.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:33.508: INFO: namespace webhook-1865 deletion completed in 6.087164404s
STEP: Destroying namespace "webhook-1865-markers" for this suite.
Jun  1 17:32:39.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:39.638: INFO: namespace webhook-1865-markers deletion completed in 6.129946423s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.297 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:32:39.650: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4654
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Jun  1 17:32:39.789: INFO: Waiting up to 5m0s for pod "pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6" in namespace "emptydir-4654" to be "success or failure"
Jun  1 17:32:39.792: INFO: Pod "pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409038ms
Jun  1 17:32:41.795: INFO: Pod "pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005854308s
Jun  1 17:32:43.799: INFO: Pod "pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009359821s
STEP: Saw pod success
Jun  1 17:32:43.800: INFO: Pod "pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6" satisfied condition "success or failure"
Jun  1 17:32:43.802: INFO: Trying to get logs from node appserv11 pod pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6 container test-container: <nil>
STEP: delete the pod
Jun  1 17:32:43.871: INFO: Waiting for pod pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6 to disappear
Jun  1 17:32:43.873: INFO: Pod pod-318d0930-4ce9-4fc5-8af9-cb1c73a21cc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:32:43.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4654" for this suite.
Jun  1 17:32:49.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:49.971: INFO: namespace emptydir-4654 deletion completed in 6.094320504s

• [SLOW TEST:10.321 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:32:49.971: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5548
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:32:50.106: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:32:50.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5548" for this suite.
Jun  1 17:32:56.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:32:56.729: INFO: namespace custom-resource-definition-5548 deletion completed in 6.088565678s

• [SLOW TEST:6.758 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:32:56.730: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun  1 17:32:58.397: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0601 17:32:58.397487      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun  1 17:32:58.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4712" for this suite.
Jun  1 17:33:04.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:33:04.482: INFO: namespace gc-4712 deletion completed in 6.082465579s

• [SLOW TEST:7.752 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:33:04.483: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:33:20.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6975" for this suite.
Jun  1 17:33:26.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:33:26.763: INFO: namespace resourcequota-6975 deletion completed in 6.080429692s

• [SLOW TEST:22.280 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:33:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:33:26.910: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun  1 17:33:26.916: INFO: Number of nodes with available pods: 0
Jun  1 17:33:26.916: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun  1 17:33:26.928: INFO: Number of nodes with available pods: 0
Jun  1 17:33:26.928: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:27.932: INFO: Number of nodes with available pods: 0
Jun  1 17:33:27.932: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:28.931: INFO: Number of nodes with available pods: 1
Jun  1 17:33:28.931: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun  1 17:33:28.943: INFO: Number of nodes with available pods: 1
Jun  1 17:33:28.943: INFO: Number of running nodes: 0, number of available pods: 1
Jun  1 17:33:29.946: INFO: Number of nodes with available pods: 0
Jun  1 17:33:29.946: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun  1 17:33:29.954: INFO: Number of nodes with available pods: 0
Jun  1 17:33:29.954: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:30.958: INFO: Number of nodes with available pods: 0
Jun  1 17:33:30.958: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:31.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:31.957: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:32.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:32.957: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:33.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:33.957: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:34.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:34.957: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:35.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:35.958: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:36.957: INFO: Number of nodes with available pods: 0
Jun  1 17:33:36.957: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:33:37.957: INFO: Number of nodes with available pods: 1
Jun  1 17:33:37.957: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6199, will wait for the garbage collector to delete the pods
Jun  1 17:33:38.039: INFO: Deleting DaemonSet.extensions daemon-set took: 26.005403ms
Jun  1 17:33:38.640: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.18755ms
Jun  1 17:33:44.943: INFO: Number of nodes with available pods: 0
Jun  1 17:33:44.943: INFO: Number of running nodes: 0, number of available pods: 0
Jun  1 17:33:44.946: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6199/daemonsets","resourceVersion":"21990"},"items":null}

Jun  1 17:33:44.948: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6199/pods","resourceVersion":"21990"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:33:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6199" for this suite.
Jun  1 17:33:50.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:33:51.085: INFO: namespace daemonsets-6199 deletion completed in 6.116884788s

• [SLOW TEST:24.322 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:33:51.086: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-81b68487-897f-4411-9b9e-6da2d7d28ca0
STEP: Creating a pod to test consume configMaps
Jun  1 17:33:51.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8" in namespace "configmap-8640" to be "success or failure"
Jun  1 17:33:51.229: INFO: Pod "pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385754ms
Jun  1 17:33:53.232: INFO: Pod "pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005987532s
Jun  1 17:33:55.235: INFO: Pod "pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009074342s
STEP: Saw pod success
Jun  1 17:33:55.235: INFO: Pod "pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8" satisfied condition "success or failure"
Jun  1 17:33:55.237: INFO: Trying to get logs from node appserv9 pod pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:33:55.263: INFO: Waiting for pod pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8 to disappear
Jun  1 17:33:55.265: INFO: Pod pod-configmaps-3c189f8a-b360-46a3-805f-469d72aefaf8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:33:55.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8640" for this suite.
Jun  1 17:34:01.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:34:01.352: INFO: namespace configmap-8640 deletion completed in 6.082873728s

• [SLOW TEST:10.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:34:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:34:02.541: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 17:34:04.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629642, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629642, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629642, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726629642, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:34:07.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:34:07.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4575" for this suite.
Jun  1 17:34:13.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:34:13.703: INFO: namespace webhook-4575 deletion completed in 6.099564003s
STEP: Destroying namespace "webhook-4575-markers" for this suite.
Jun  1 17:34:19.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:34:19.789: INFO: namespace webhook-4575-markers deletion completed in 6.086045582s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.448 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:34:19.800: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-fd368fc9-b979-4711-b100-439c4d59f254
STEP: Creating a pod to test consume configMaps
Jun  1 17:34:19.940: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41" in namespace "projected-6038" to be "success or failure"
Jun  1 17:34:19.942: INFO: Pod "pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.453334ms
Jun  1 17:34:21.946: INFO: Pod "pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836859s
Jun  1 17:34:23.949: INFO: Pod "pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009228386s
STEP: Saw pod success
Jun  1 17:34:23.949: INFO: Pod "pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41" satisfied condition "success or failure"
Jun  1 17:34:23.952: INFO: Trying to get logs from node appserv11 pod pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:34:23.979: INFO: Waiting for pod pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41 to disappear
Jun  1 17:34:23.981: INFO: Pod pod-projected-configmaps-b4dd80af-bb4e-4c78-921f-ed61f06b4f41 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:34:23.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6038" for this suite.
Jun  1 17:34:29.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:34:30.100: INFO: namespace projected-6038 deletion completed in 6.114866699s

• [SLOW TEST:10.299 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:34:30.101: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun  1 17:34:36.261: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.261: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.373: INFO: Exec stderr: ""
Jun  1 17:34:36.373: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.373: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.479: INFO: Exec stderr: ""
Jun  1 17:34:36.479: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.479: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.568: INFO: Exec stderr: ""
Jun  1 17:34:36.568: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.568: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.668: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun  1 17:34:36.668: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.668: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.774: INFO: Exec stderr: ""
Jun  1 17:34:36.774: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.774: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.861: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun  1 17:34:36.861: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.861: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:36.970: INFO: Exec stderr: ""
Jun  1 17:34:36.970: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:36.970: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:37.077: INFO: Exec stderr: ""
Jun  1 17:34:37.077: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:37.187: INFO: Exec stderr: ""
Jun  1 17:34:37.187: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1028 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:34:37.187: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:34:37.286: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:34:37.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1028" for this suite.
Jun  1 17:35:27.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:35:27.380: INFO: namespace e2e-kubelet-etc-hosts-1028 deletion completed in 50.090006187s

• [SLOW TEST:57.280 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:35:27.381: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0601 17:35:58.041026      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun  1 17:35:58.041: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:35:58.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4008" for this suite.
Jun  1 17:36:04.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:36:04.136: INFO: namespace gc-4008 deletion completed in 6.09274714s

• [SLOW TEST:36.756 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:36:04.137: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6832
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6832, will wait for the garbage collector to delete the pods
Jun  1 17:36:08.338: INFO: Deleting Job.batch foo took: 5.309049ms
Jun  1 17:36:08.939: INFO: Terminating Job.batch foo pods took: 600.23545ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:36:44.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6832" for this suite.
Jun  1 17:36:50.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:36:50.919: INFO: namespace job-6832 deletion completed in 6.073266379s

• [SLOW TEST:46.782 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:36:50.919: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-3b09f7da-04f5-461f-84e9-f102daf74e58
STEP: Creating a pod to test consume configMaps
Jun  1 17:36:51.056: INFO: Waiting up to 5m0s for pod "pod-configmaps-63646a71-86ab-4390-a753-606a8b577249" in namespace "configmap-8512" to be "success or failure"
Jun  1 17:36:51.059: INFO: Pod "pod-configmaps-63646a71-86ab-4390-a753-606a8b577249": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446695ms
Jun  1 17:36:53.062: INFO: Pod "pod-configmaps-63646a71-86ab-4390-a753-606a8b577249": Phase="Running", Reason="", readiness=true. Elapsed: 2.005739667s
Jun  1 17:36:55.065: INFO: Pod "pod-configmaps-63646a71-86ab-4390-a753-606a8b577249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009226658s
STEP: Saw pod success
Jun  1 17:36:55.065: INFO: Pod "pod-configmaps-63646a71-86ab-4390-a753-606a8b577249" satisfied condition "success or failure"
Jun  1 17:36:55.068: INFO: Trying to get logs from node appserv9 pod pod-configmaps-63646a71-86ab-4390-a753-606a8b577249 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:36:55.093: INFO: Waiting for pod pod-configmaps-63646a71-86ab-4390-a753-606a8b577249 to disappear
Jun  1 17:36:55.096: INFO: Pod pod-configmaps-63646a71-86ab-4390-a753-606a8b577249 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:36:55.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8512" for this suite.
Jun  1 17:37:01.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:01.230: INFO: namespace configmap-8512 deletion completed in 6.130606381s

• [SLOW TEST:10.311 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:01.231: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1878/secret-test-92b6213e-bfb3-4583-b215-175e569bbdae
STEP: Creating a pod to test consume secrets
Jun  1 17:37:01.374: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48" in namespace "secrets-1878" to be "success or failure"
Jun  1 17:37:01.377: INFO: Pod "pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466852ms
Jun  1 17:37:03.380: INFO: Pod "pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006001405s
STEP: Saw pod success
Jun  1 17:37:03.380: INFO: Pod "pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48" satisfied condition "success or failure"
Jun  1 17:37:03.383: INFO: Trying to get logs from node appserv9 pod pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48 container env-test: <nil>
STEP: delete the pod
Jun  1 17:37:03.400: INFO: Waiting for pod pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48 to disappear
Jun  1 17:37:03.402: INFO: Pod pod-configmaps-6e5d37f8-2aae-4e69-99c1-3795ca821e48 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:37:03.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1878" for this suite.
Jun  1 17:37:09.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:09.577: INFO: namespace secrets-1878 deletion completed in 6.170109015s

• [SLOW TEST:8.346 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:09.577: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4149
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:37:09.713: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun  1 17:37:13.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 create -f -'
Jun  1 17:37:13.810: INFO: stderr: ""
Jun  1 17:37:13.810: INFO: stdout: "e2e-test-crd-publish-openapi-5437-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  1 17:37:13.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 delete e2e-test-crd-publish-openapi-5437-crds test-foo'
Jun  1 17:37:13.933: INFO: stderr: ""
Jun  1 17:37:13.933: INFO: stdout: "e2e-test-crd-publish-openapi-5437-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  1 17:37:13.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 apply -f -'
Jun  1 17:37:14.126: INFO: stderr: ""
Jun  1 17:37:14.126: INFO: stdout: "e2e-test-crd-publish-openapi-5437-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  1 17:37:14.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 delete e2e-test-crd-publish-openapi-5437-crds test-foo'
Jun  1 17:37:14.240: INFO: stderr: ""
Jun  1 17:37:14.240: INFO: stdout: "e2e-test-crd-publish-openapi-5437-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun  1 17:37:14.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 create -f -'
Jun  1 17:37:14.425: INFO: rc: 1
Jun  1 17:37:14.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 apply -f -'
Jun  1 17:37:14.616: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun  1 17:37:14.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 create -f -'
Jun  1 17:37:14.816: INFO: rc: 1
Jun  1 17:37:14.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-4149 apply -f -'
Jun  1 17:37:15.006: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun  1 17:37:15.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5437-crds'
Jun  1 17:37:15.209: INFO: stderr: ""
Jun  1 17:37:15.209: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5437-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun  1 17:37:15.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5437-crds.metadata'
Jun  1 17:37:15.420: INFO: stderr: ""
Jun  1 17:37:15.420: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5437-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  1 17:37:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5437-crds.spec'
Jun  1 17:37:15.610: INFO: stderr: ""
Jun  1 17:37:15.610: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5437-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  1 17:37:15.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5437-crds.spec.bars'
Jun  1 17:37:15.815: INFO: stderr: ""
Jun  1 17:37:15.815: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5437-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun  1 17:37:15.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5437-crds.spec.bars2'
Jun  1 17:37:16.008: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:37:19.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4149" for this suite.
Jun  1 17:37:25.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:25.777: INFO: namespace crd-publish-openapi-4149 deletion completed in 6.220220712s

• [SLOW TEST:16.200 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:25.777: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun  1 17:37:25.911: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:37:29.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7581" for this suite.
Jun  1 17:37:41.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:41.972: INFO: namespace init-container-7581 deletion completed in 12.089160916s

• [SLOW TEST:16.195 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:41.972: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:37:42.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 version'
Jun  1 17:37:42.293: INFO: stderr: ""
Jun  1 17:37:42.293: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.8\", GitCommit:\"ec6eb119b81be488b030e849b9e64fda4caaf33c\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:00:06Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.8\", GitCommit:\"ec6eb119b81be488b030e849b9e64fda4caaf33c\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T20:52:22Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:37:42.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2592" for this suite.
Jun  1 17:37:48.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:48.377: INFO: namespace kubectl-2592 deletion completed in 6.079701913s

• [SLOW TEST:6.404 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:48.377: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Jun  1 17:37:48.508: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-621748879 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:37:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4643" for this suite.
Jun  1 17:37:54.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:37:54.993: INFO: namespace kubectl-4643 deletion completed in 6.383764033s

• [SLOW TEST:6.615 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:37:54.993: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun  1 17:37:55.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-7621'
Jun  1 17:37:55.448: INFO: stderr: ""
Jun  1 17:37:55.448: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  1 17:37:55.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7621'
Jun  1 17:37:55.570: INFO: stderr: ""
Jun  1 17:37:55.570: INFO: stdout: "update-demo-nautilus-2njlz update-demo-nautilus-tjbj5 "
Jun  1 17:37:55.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-2njlz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7621'
Jun  1 17:37:55.677: INFO: stderr: ""
Jun  1 17:37:55.677: INFO: stdout: ""
Jun  1 17:37:55.677: INFO: update-demo-nautilus-2njlz is created but not running
Jun  1 17:38:00.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7621'
Jun  1 17:38:00.800: INFO: stderr: ""
Jun  1 17:38:00.800: INFO: stdout: "update-demo-nautilus-2njlz update-demo-nautilus-tjbj5 "
Jun  1 17:38:00.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-2njlz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7621'
Jun  1 17:38:00.912: INFO: stderr: ""
Jun  1 17:38:00.912: INFO: stdout: "true"
Jun  1 17:38:00.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-2njlz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7621'
Jun  1 17:38:01.008: INFO: stderr: ""
Jun  1 17:38:01.008: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 17:38:01.008: INFO: validating pod update-demo-nautilus-2njlz
Jun  1 17:38:01.014: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 17:38:01.014: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 17:38:01.014: INFO: update-demo-nautilus-2njlz is verified up and running
Jun  1 17:38:01.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-tjbj5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7621'
Jun  1 17:38:01.122: INFO: stderr: ""
Jun  1 17:38:01.122: INFO: stdout: "true"
Jun  1 17:38:01.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods update-demo-nautilus-tjbj5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7621'
Jun  1 17:38:01.240: INFO: stderr: ""
Jun  1 17:38:01.240: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  1 17:38:01.240: INFO: validating pod update-demo-nautilus-tjbj5
Jun  1 17:38:01.245: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  1 17:38:01.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  1 17:38:01.245: INFO: update-demo-nautilus-tjbj5 is verified up and running
STEP: using delete to clean up resources
Jun  1 17:38:01.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete --grace-period=0 --force -f - --namespace=kubectl-7621'
Jun  1 17:38:01.369: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  1 17:38:01.369: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  1 17:38:01.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7621'
Jun  1 17:38:01.489: INFO: stderr: "No resources found in kubectl-7621 namespace.\n"
Jun  1 17:38:01.489: INFO: stdout: ""
Jun  1 17:38:01.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -l name=update-demo --namespace=kubectl-7621 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  1 17:38:01.610: INFO: stderr: ""
Jun  1 17:38:01.610: INFO: stdout: "update-demo-nautilus-2njlz\nupdate-demo-nautilus-tjbj5\n"
Jun  1 17:38:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7621'
Jun  1 17:38:02.242: INFO: stderr: "No resources found in kubectl-7621 namespace.\n"
Jun  1 17:38:02.242: INFO: stdout: ""
Jun  1 17:38:02.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -l name=update-demo --namespace=kubectl-7621 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  1 17:38:02.352: INFO: stderr: ""
Jun  1 17:38:02.352: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:38:02.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7621" for this suite.
Jun  1 17:38:30.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:38:30.448: INFO: namespace kubectl-7621 deletion completed in 28.09109024s

• [SLOW TEST:35.455 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:38:30.448: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:38:30.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44" in namespace "downward-api-4412" to be "success or failure"
Jun  1 17:38:30.590: INFO: Pod "downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462511ms
Jun  1 17:38:32.593: INFO: Pod "downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006160605s
Jun  1 17:38:34.597: INFO: Pod "downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009603021s
STEP: Saw pod success
Jun  1 17:38:34.597: INFO: Pod "downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44" satisfied condition "success or failure"
Jun  1 17:38:34.600: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44 container client-container: <nil>
STEP: delete the pod
Jun  1 17:38:34.627: INFO: Waiting for pod downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44 to disappear
Jun  1 17:38:34.630: INFO: Pod downwardapi-volume-f37f8419-7b30-4134-bdab-10fca5419a44 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:38:34.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4412" for this suite.
Jun  1 17:38:40.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:38:40.769: INFO: namespace downward-api-4412 deletion completed in 6.135861618s

• [SLOW TEST:10.321 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:38:40.770: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9034
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:38:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:38:41.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9034" for this suite.
Jun  1 17:38:47.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:38:47.999: INFO: namespace custom-resource-definition-9034 deletion completed in 6.073316488s

• [SLOW TEST:7.229 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:38:47.999: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun  1 17:38:52.164: INFO: &Pod{ObjectMeta:{send-events-b4f4a736-85ab-4fa7-bd6b-8541be1d73fd  events-144 /api/v1/namespaces/events-144/pods/send-events-b4f4a736-85ab-4fa7-bd6b-8541be1d73fd 809d6dd2-a8c5-4f50-9025-6a73d4f386f6 23312 0 2020-06-01 17:38:48 +0000 UTC <nil> <nil> map[name:foo time:147655234] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jbc9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jbc9b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jbc9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:38:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:38:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.11,StartTime:2020-06-01 17:38:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 17:38:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://e59e236b5bc3a749522d64ec5720231450c91b0d44626015359a5e52edf5b73e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun  1 17:38:54.168: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun  1 17:38:56.172: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:38:56.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-144" for this suite.
Jun  1 17:39:40.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:39:40.336: INFO: namespace events-144 deletion completed in 44.155723489s

• [SLOW TEST:52.337 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:39:40.337: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-79
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun  1 17:39:40.478: INFO: Waiting up to 5m0s for pod "pod-f84952d5-f2a3-4c97-9e9c-8827090054e8" in namespace "emptydir-79" to be "success or failure"
Jun  1 17:39:40.481: INFO: Pod "pod-f84952d5-f2a3-4c97-9e9c-8827090054e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946789ms
Jun  1 17:39:42.484: INFO: Pod "pod-f84952d5-f2a3-4c97-9e9c-8827090054e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006334492s
STEP: Saw pod success
Jun  1 17:39:42.484: INFO: Pod "pod-f84952d5-f2a3-4c97-9e9c-8827090054e8" satisfied condition "success or failure"
Jun  1 17:39:42.487: INFO: Trying to get logs from node appserv9 pod pod-f84952d5-f2a3-4c97-9e9c-8827090054e8 container test-container: <nil>
STEP: delete the pod
Jun  1 17:39:42.513: INFO: Waiting for pod pod-f84952d5-f2a3-4c97-9e9c-8827090054e8 to disappear
Jun  1 17:39:42.516: INFO: Pod pod-f84952d5-f2a3-4c97-9e9c-8827090054e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:39:42.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-79" for this suite.
Jun  1 17:39:48.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:39:48.605: INFO: namespace emptydir-79 deletion completed in 6.085135823s

• [SLOW TEST:8.268 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:39:48.606: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:39:49.014: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:39:52.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:40:04.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2997" for this suite.
Jun  1 17:40:10.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:40:10.251: INFO: namespace webhook-2997 deletion completed in 6.092546589s
STEP: Destroying namespace "webhook-2997-markers" for this suite.
Jun  1 17:40:16.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:40:16.384: INFO: namespace webhook-2997-markers deletion completed in 6.132637803s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.790 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:40:16.396: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zr6lr in namespace proxy-1758
I0601 17:40:16.538904      26 runners.go:184] Created replication controller with name: proxy-service-zr6lr, namespace: proxy-1758, replica count: 1
I0601 17:40:17.589264      26 runners.go:184] proxy-service-zr6lr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0601 17:40:18.589449      26 runners.go:184] proxy-service-zr6lr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0601 17:40:19.589660      26 runners.go:184] proxy-service-zr6lr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0601 17:40:20.589859      26 runners.go:184] proxy-service-zr6lr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  1 17:40:20.592: INFO: setup took 4.063952353s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 5.224181ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 5.233838ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 5.213212ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 5.434214ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 5.302862ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 5.613981ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 5.861226ms)
Jun  1 17:40:20.598: INFO: (0) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 5.779092ms)
Jun  1 17:40:20.599: INFO: (0) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 6.383028ms)
Jun  1 17:40:20.599: INFO: (0) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 6.368901ms)
Jun  1 17:40:20.599: INFO: (0) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 6.298839ms)
Jun  1 17:40:20.607: INFO: (0) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 13.950277ms)
Jun  1 17:40:20.607: INFO: (0) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 14.002921ms)
Jun  1 17:40:20.607: INFO: (0) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 14.117457ms)
Jun  1 17:40:20.607: INFO: (0) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 14.060127ms)
Jun  1 17:40:20.607: INFO: (0) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 14.123159ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 4.069427ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 4.434886ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 4.528218ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 4.517027ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 4.422905ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 4.477688ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 4.549454ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 4.469077ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 4.450473ms)
Jun  1 17:40:20.611: INFO: (1) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 4.485666ms)
Jun  1 17:40:20.612: INFO: (1) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.832819ms)
Jun  1 17:40:20.612: INFO: (1) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 5.430957ms)
Jun  1 17:40:20.612: INFO: (1) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 5.409155ms)
Jun  1 17:40:20.612: INFO: (1) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 5.512488ms)
Jun  1 17:40:20.613: INFO: (1) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 5.79238ms)
Jun  1 17:40:20.613: INFO: (1) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 5.682655ms)
Jun  1 17:40:20.616: INFO: (2) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.189417ms)
Jun  1 17:40:20.616: INFO: (2) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.350423ms)
Jun  1 17:40:20.616: INFO: (2) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.3553ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.860797ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.940496ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.838493ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.885705ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.955074ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.898888ms)
Jun  1 17:40:20.617: INFO: (2) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.951007ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.788613ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.744713ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 5.180784ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 5.276656ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 5.416003ms)
Jun  1 17:40:20.618: INFO: (2) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 5.339012ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.219456ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.781367ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.912033ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.927654ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.893177ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.929133ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.933248ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.902206ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.963998ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 4.000874ms)
Jun  1 17:40:20.622: INFO: (3) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.100132ms)
Jun  1 17:40:20.623: INFO: (3) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.279357ms)
Jun  1 17:40:20.623: INFO: (3) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.280565ms)
Jun  1 17:40:20.623: INFO: (3) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.651663ms)
Jun  1 17:40:20.623: INFO: (3) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.984648ms)
Jun  1 17:40:20.624: INFO: (3) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 5.019717ms)
Jun  1 17:40:20.626: INFO: (4) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.607433ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.16466ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.122896ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.221804ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.177898ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.222954ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.66892ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.677979ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.641926ms)
Jun  1 17:40:20.627: INFO: (4) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.605278ms)
Jun  1 17:40:20.628: INFO: (4) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.145589ms)
Jun  1 17:40:20.628: INFO: (4) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.324316ms)
Jun  1 17:40:20.628: INFO: (4) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.360261ms)
Jun  1 17:40:20.628: INFO: (4) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.892342ms)
Jun  1 17:40:20.629: INFO: (4) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.854766ms)
Jun  1 17:40:20.629: INFO: (4) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.820858ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.55058ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.589414ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.654333ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.668483ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.683739ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.670526ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.53875ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.676452ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.772691ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 3.845324ms)
Jun  1 17:40:20.632: INFO: (5) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.703236ms)
Jun  1 17:40:20.633: INFO: (5) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.29468ms)
Jun  1 17:40:20.633: INFO: (5) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.656907ms)
Jun  1 17:40:20.633: INFO: (5) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.674873ms)
Jun  1 17:40:20.633: INFO: (5) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.712731ms)
Jun  1 17:40:20.634: INFO: (5) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.934317ms)
Jun  1 17:40:20.636: INFO: (6) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.718428ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.388651ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.444317ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.397196ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.408998ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.454523ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.593861ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 3.744118ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.622979ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.671473ms)
Jun  1 17:40:20.637: INFO: (6) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.71212ms)
Jun  1 17:40:20.638: INFO: (6) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.192281ms)
Jun  1 17:40:20.638: INFO: (6) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.584648ms)
Jun  1 17:40:20.638: INFO: (6) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.66463ms)
Jun  1 17:40:20.638: INFO: (6) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.660922ms)
Jun  1 17:40:20.638: INFO: (6) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.668614ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.434692ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.418408ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.41104ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.543493ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.458939ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.414334ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.473164ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.452849ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.544095ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.487125ms)
Jun  1 17:40:20.642: INFO: (7) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.030772ms)
Jun  1 17:40:20.643: INFO: (7) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.247705ms)
Jun  1 17:40:20.643: INFO: (7) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.307684ms)
Jun  1 17:40:20.643: INFO: (7) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.681636ms)
Jun  1 17:40:20.643: INFO: (7) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.683464ms)
Jun  1 17:40:20.643: INFO: (7) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.6856ms)
Jun  1 17:40:20.646: INFO: (8) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 2.989581ms)
Jun  1 17:40:20.646: INFO: (8) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.027059ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.775643ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.824786ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.000089ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.818039ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.819737ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.907944ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.855248ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 3.998348ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.981611ms)
Jun  1 17:40:20.647: INFO: (8) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.886341ms)
Jun  1 17:40:20.648: INFO: (8) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.344643ms)
Jun  1 17:40:20.649: INFO: (8) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 5.945021ms)
Jun  1 17:40:20.650: INFO: (8) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 6.059837ms)
Jun  1 17:40:20.650: INFO: (8) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 6.719486ms)
Jun  1 17:40:20.653: INFO: (9) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.956031ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.287004ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.690657ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.709636ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.814709ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.795514ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.755437ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.796897ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.823623ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 3.954089ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.037799ms)
Jun  1 17:40:20.654: INFO: (9) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.886627ms)
Jun  1 17:40:20.655: INFO: (9) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.26412ms)
Jun  1 17:40:20.655: INFO: (9) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.315017ms)
Jun  1 17:40:20.655: INFO: (9) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.381633ms)
Jun  1 17:40:20.655: INFO: (9) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.4801ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 2.663133ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.15754ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.205985ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.266227ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.232104ms)
Jun  1 17:40:20.658: INFO: (10) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.272736ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.598458ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.652048ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 3.763664ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.830392ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.743472ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.221169ms)
Jun  1 17:40:20.659: INFO: (10) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.199717ms)
Jun  1 17:40:20.660: INFO: (10) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.607442ms)
Jun  1 17:40:20.660: INFO: (10) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.63489ms)
Jun  1 17:40:20.660: INFO: (10) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 5.005702ms)
Jun  1 17:40:20.663: INFO: (11) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.669594ms)
Jun  1 17:40:20.663: INFO: (11) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 2.95868ms)
Jun  1 17:40:20.663: INFO: (11) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 2.986653ms)
Jun  1 17:40:20.663: INFO: (11) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.937159ms)
Jun  1 17:40:20.663: INFO: (11) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 2.970534ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.46713ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.440582ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.465331ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.50866ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 3.641054ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.650327ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 3.821079ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.064234ms)
Jun  1 17:40:20.664: INFO: (11) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.09135ms)
Jun  1 17:40:20.665: INFO: (11) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.714419ms)
Jun  1 17:40:20.665: INFO: (11) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.695688ms)
Jun  1 17:40:20.667: INFO: (12) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.363466ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 2.797437ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.098793ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.081155ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.448557ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.397677ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.473566ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.481573ms)
Jun  1 17:40:20.668: INFO: (12) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.516939ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.642142ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.015102ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 3.981286ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 3.951523ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 3.971807ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 3.956683ms)
Jun  1 17:40:20.669: INFO: (12) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.136253ms)
Jun  1 17:40:20.672: INFO: (13) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.569277ms)
Jun  1 17:40:20.672: INFO: (13) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 2.831571ms)
Jun  1 17:40:20.672: INFO: (13) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.806645ms)
Jun  1 17:40:20.672: INFO: (13) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.031679ms)
Jun  1 17:40:20.672: INFO: (13) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.035915ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.432354ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.353878ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.361481ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.387536ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.420079ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 3.635635ms)
Jun  1 17:40:20.673: INFO: (13) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.328221ms)
Jun  1 17:40:20.674: INFO: (13) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.295807ms)
Jun  1 17:40:20.674: INFO: (13) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.30384ms)
Jun  1 17:40:20.674: INFO: (13) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.299212ms)
Jun  1 17:40:20.674: INFO: (13) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.386824ms)
Jun  1 17:40:20.676: INFO: (14) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 2.53365ms)
Jun  1 17:40:20.676: INFO: (14) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.786759ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.159615ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.184957ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.239227ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.279804ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.275511ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.310833ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.290695ms)
Jun  1 17:40:20.677: INFO: (14) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.295404ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 43.840786ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 43.909666ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 43.950237ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 43.933887ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 43.918202ms)
Jun  1 17:40:20.718: INFO: (14) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 43.909458ms)
Jun  1 17:40:20.721: INFO: (15) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.86118ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 4.027306ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 4.031834ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 4.162778ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 4.090286ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 4.135696ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 4.1717ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 4.091866ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 4.078694ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 4.158195ms)
Jun  1 17:40:20.722: INFO: (15) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.548598ms)
Jun  1 17:40:20.723: INFO: (15) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.919844ms)
Jun  1 17:40:20.723: INFO: (15) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 5.142044ms)
Jun  1 17:40:20.723: INFO: (15) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 5.041615ms)
Jun  1 17:40:20.723: INFO: (15) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 5.062984ms)
Jun  1 17:40:20.723: INFO: (15) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 5.425606ms)
Jun  1 17:40:20.726: INFO: (16) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 2.757083ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.61654ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.535015ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.62034ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.602926ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.849298ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.86577ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.953938ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 4.004476ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.958974ms)
Jun  1 17:40:20.727: INFO: (16) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.083219ms)
Jun  1 17:40:20.728: INFO: (16) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.527206ms)
Jun  1 17:40:20.728: INFO: (16) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.594317ms)
Jun  1 17:40:20.728: INFO: (16) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.559552ms)
Jun  1 17:40:20.728: INFO: (16) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.581443ms)
Jun  1 17:40:20.728: INFO: (16) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.582374ms)
Jun  1 17:40:20.731: INFO: (17) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 2.682527ms)
Jun  1 17:40:20.731: INFO: (17) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 2.991152ms)
Jun  1 17:40:20.731: INFO: (17) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.173482ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.56501ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.451276ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.463172ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.462607ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.535954ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.766575ms)
Jun  1 17:40:20.732: INFO: (17) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.699276ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.34917ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.320913ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.799944ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.812433ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 5.287396ms)
Jun  1 17:40:20.733: INFO: (17) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 5.204256ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.019008ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 2.961799ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.10366ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.11892ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.443961ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.396712ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.43579ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.527691ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.407671ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.41639ms)
Jun  1 17:40:20.737: INFO: (18) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 3.944312ms)
Jun  1 17:40:20.738: INFO: (18) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.483517ms)
Jun  1 17:40:20.738: INFO: (18) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.507575ms)
Jun  1 17:40:20.738: INFO: (18) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.54268ms)
Jun  1 17:40:20.738: INFO: (18) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.621661ms)
Jun  1 17:40:20.738: INFO: (18) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.501638ms)
Jun  1 17:40:20.741: INFO: (19) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 2.669738ms)
Jun  1 17:40:20.741: INFO: (19) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:162/proxy/: bar (200; 3.08525ms)
Jun  1 17:40:20.741: INFO: (19) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87/proxy/rewriteme">test</a> (200; 3.090773ms)
Jun  1 17:40:20.741: INFO: (19) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:460/proxy/: tls baz (200; 3.086037ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">test<... (200; 3.552357ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:462/proxy/: tls qux (200; 3.622534ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/https:proxy-service-zr6lr-r6v87:443/proxy/tlsrewritem... (200; 3.559178ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/: <a href="/api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:1080/proxy/rewriteme">... (200; 3.655226ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/http:proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.703374ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/pods/proxy-service-zr6lr-r6v87:160/proxy/: foo (200; 3.776426ms)
Jun  1 17:40:20.742: INFO: (19) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname2/proxy/: bar (200; 4.134922ms)
Jun  1 17:40:20.743: INFO: (19) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname1/proxy/: tls baz (200; 4.391421ms)
Jun  1 17:40:20.743: INFO: (19) /api/v1/namespaces/proxy-1758/services/http:proxy-service-zr6lr:portname1/proxy/: foo (200; 4.375997ms)
Jun  1 17:40:20.743: INFO: (19) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname2/proxy/: bar (200; 4.445134ms)
Jun  1 17:40:20.743: INFO: (19) /api/v1/namespaces/proxy-1758/services/https:proxy-service-zr6lr:tlsportname2/proxy/: tls qux (200; 4.57939ms)
Jun  1 17:40:20.743: INFO: (19) /api/v1/namespaces/proxy-1758/services/proxy-service-zr6lr:portname1/proxy/: foo (200; 4.592662ms)
STEP: deleting ReplicationController proxy-service-zr6lr in namespace proxy-1758, will wait for the garbage collector to delete the pods
Jun  1 17:40:20.801: INFO: Deleting ReplicationController proxy-service-zr6lr took: 5.83373ms
Jun  1 17:40:21.401: INFO: Terminating ReplicationController proxy-service-zr6lr pods took: 600.261593ms
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:40:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1758" for this suite.
Jun  1 17:40:40.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:40:40.996: INFO: namespace proxy-1758 deletion completed in 6.090447962s

• [SLOW TEST:24.600 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:40:40.997: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  1 17:40:43.185: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:40:43.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2463" for this suite.
Jun  1 17:40:49.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:40:49.324: INFO: namespace container-runtime-2463 deletion completed in 6.124451119s

• [SLOW TEST:8.327 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:40:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:40:49.457: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  1 17:40:49.463: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  1 17:40:54.467: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  1 17:40:54.467: INFO: Creating deployment "test-rolling-update-deployment"
Jun  1 17:40:54.472: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  1 17:40:54.477: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  1 17:40:56.484: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  1 17:40:56.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630054, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630054, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630054, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630054, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  1 17:40:58.490: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun  1 17:40:58.498: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2410 /apis/apps/v1/namespaces/deployment-2410/deployments/test-rolling-update-deployment ff2495d1-a607-405e-874c-72c8c9034372 23848 1 2020-06-01 17:40:54 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006cf6938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-01 17:40:54 +0000 UTC,LastTransitionTime:2020-06-01 17:40:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-06-01 17:40:56 +0000 UTC,LastTransitionTime:2020-06-01 17:40:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  1 17:40:58.501: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-2410 /apis/apps/v1/namespaces/deployment-2410/replicasets/test-rolling-update-deployment-55d946486 ad1c081b-8f40-4dde-9ec8-d3f21188d7e5 23836 1 2020-06-01 17:40:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ff2495d1-a607-405e-874c-72c8c9034372 0xc006cf6e30 0xc006cf6e31}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006cf6e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  1 17:40:58.501: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  1 17:40:58.501: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2410 /apis/apps/v1/namespaces/deployment-2410/replicasets/test-rolling-update-controller 00463119-9e76-4437-aaa4-27de0611bee3 23845 2 2020-06-01 17:40:49 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ff2495d1-a607-405e-874c-72c8c9034372 0xc006cf6d67 0xc006cf6d68}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006cf6dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  1 17:40:58.504: INFO: Pod "test-rolling-update-deployment-55d946486-vr5nh" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-vr5nh test-rolling-update-deployment-55d946486- deployment-2410 /api/v1/namespaces/deployment-2410/pods/test-rolling-update-deployment-55d946486-vr5nh 42bd0078-8057-4621-91e8-03bbd0243a67 23835 0 2020-06-01 17:40:54 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 ad1c081b-8f40-4dde-9ec8-d3f21188d7e5 0xc006cf7320 0xc006cf7321}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-czx88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-czx88,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-czx88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-01 17:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.12,StartTime:2020-06-01 17:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-01 17:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/redis:5.0.5-alpine,ImageID:docker-pullable://docker.io/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://376aace48feb1203e9bf65c19916e759561854dd17161e5a86aa18d8fd291a09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.141.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:40:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2410" for this suite.
Jun  1 17:41:04.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:41:04.600: INFO: namespace deployment-2410 deletion completed in 6.091888118s

• [SLOW TEST:15.275 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:41:04.601: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7611
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  1 17:41:04.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  1 17:41:28.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.12:8080/dial?request=hostName&protocol=udp&host=172.16.141.11&port=8081&tries=1'] Namespace:pod-network-test-7611 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:41:28.791: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:41:28.899: INFO: Waiting for endpoints: map[]
Jun  1 17:41:28.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.12:8080/dial?request=hostName&protocol=udp&host=172.16.141.13&port=8081&tries=1'] Namespace:pod-network-test-7611 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:41:28.901: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:41:29.003: INFO: Waiting for endpoints: map[]
Jun  1 17:41:29.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.12:8080/dial?request=hostName&protocol=udp&host=172.16.141.14&port=8081&tries=1'] Namespace:pod-network-test-7611 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:41:29.005: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:41:29.092: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:41:29.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7611" for this suite.
Jun  1 17:41:41.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:41:41.186: INFO: namespace pod-network-test-7611 deletion completed in 12.090115085s

• [SLOW TEST:36.585 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:41:41.186: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:41:41.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da" in namespace "downward-api-3465" to be "success or failure"
Jun  1 17:41:41.328: INFO: Pod "downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355659ms
Jun  1 17:41:43.332: INFO: Pod "downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da": Phase="Running", Reason="", readiness=true. Elapsed: 2.006168554s
Jun  1 17:41:45.336: INFO: Pod "downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009706199s
STEP: Saw pod success
Jun  1 17:41:45.336: INFO: Pod "downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da" satisfied condition "success or failure"
Jun  1 17:41:45.339: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da container client-container: <nil>
STEP: delete the pod
Jun  1 17:41:45.365: INFO: Waiting for pod downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da to disappear
Jun  1 17:41:45.368: INFO: Pod downwardapi-volume-293e475d-2dd9-4109-8b49-cf5d82ca84da no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:41:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3465" for this suite.
Jun  1 17:41:51.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:41:51.463: INFO: namespace downward-api-3465 deletion completed in 6.091372823s

• [SLOW TEST:10.276 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:41:51.463: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4233
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-4233
Jun  1 17:41:51.605: INFO: Found 0 stateful pods, waiting for 1
Jun  1 17:42:01.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 17:42:01.626: INFO: Deleting all statefulset in ns statefulset-4233
Jun  1 17:42:01.629: INFO: Scaling statefulset ss to 0
Jun  1 17:42:21.654: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 17:42:21.657: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:42:21.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4233" for this suite.
Jun  1 17:42:27.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:42:27.761: INFO: namespace statefulset-4233 deletion completed in 6.087953685s

• [SLOW TEST:36.298 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:42:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:42:27.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811" in namespace "projected-9634" to be "success or failure"
Jun  1 17:42:27.896: INFO: Pod "downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811": Phase="Pending", Reason="", readiness=false. Elapsed: 1.633506ms
Jun  1 17:42:29.900: INFO: Pod "downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00510541s
STEP: Saw pod success
Jun  1 17:42:29.900: INFO: Pod "downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811" satisfied condition "success or failure"
Jun  1 17:42:29.903: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811 container client-container: <nil>
STEP: delete the pod
Jun  1 17:42:29.968: INFO: Waiting for pod downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811 to disappear
Jun  1 17:42:29.971: INFO: Pod downwardapi-volume-93ce10ba-92ed-42d4-86f0-3033f9ed4811 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:42:29.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9634" for this suite.
Jun  1 17:42:35.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:42:36.062: INFO: namespace projected-9634 deletion completed in 6.087828056s

• [SLOW TEST:8.301 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:42:36.063: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:42:38.220: INFO: DNS probes using dns-test-14d22223-e2eb-4a9c-9970-1e0af115720b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:42:40.251: INFO: File wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:40.254: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:42:45.259: INFO: File wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:45.262: INFO: File jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:45.262: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:42:50.258: INFO: File wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:50.261: INFO: File jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:50.261: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:42:55.262: INFO: File jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:42:55.262: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:43:00.262: INFO: File jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains '' instead of 'bar.example.com.'
Jun  1 17:43:00.262: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:43:05.259: INFO: File wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local from pod  dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  1 17:43:05.262: INFO: Lookups using dns-7790/dns-test-84d23005-249f-4e50-95be-13662a04be48 failed for: [wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local]

Jun  1 17:43:10.261: INFO: DNS probes using dns-test-84d23005-249f-4e50-95be-13662a04be48 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7790.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7790.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:43:12.305: INFO: DNS probes using dns-test-d8f01d44-7721-4405-9c56-e0dee642b399 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:43:12.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7790" for this suite.
Jun  1 17:43:18.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:43:18.407: INFO: namespace dns-7790 deletion completed in 6.079141913s

• [SLOW TEST:42.344 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:43:18.407: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:43:19.320: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 17:43:21.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630199, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630199, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630199, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630199, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:43:24.339: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:43:24.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5345" for this suite.
Jun  1 17:43:30.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:43:30.472: INFO: namespace webhook-5345 deletion completed in 6.085367885s
STEP: Destroying namespace "webhook-5345-markers" for this suite.
Jun  1 17:43:36.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:43:36.556: INFO: namespace webhook-5345-markers deletion completed in 6.084345837s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.159 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:43:36.566: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:43:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4707" for this suite.
Jun  1 17:43:55.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:43:56.138: INFO: namespace resourcequota-4707 deletion completed in 6.388293472s

• [SLOW TEST:19.572 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:43:56.138: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:43:56.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20" in namespace "projected-5366" to be "success or failure"
Jun  1 17:43:56.280: INFO: Pod "downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.49011ms
Jun  1 17:43:58.283: INFO: Pod "downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005465029s
STEP: Saw pod success
Jun  1 17:43:58.283: INFO: Pod "downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20" satisfied condition "success or failure"
Jun  1 17:43:58.285: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20 container client-container: <nil>
STEP: delete the pod
Jun  1 17:43:58.301: INFO: Waiting for pod downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20 to disappear
Jun  1 17:43:58.303: INFO: Pod downwardapi-volume-aeb79bef-4ad4-4aba-8cff-4e563de37c20 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:43:58.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5366" for this suite.
Jun  1 17:44:04.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:44:04.396: INFO: namespace projected-5366 deletion completed in 6.08975819s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:44:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-5159
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5159 to expose endpoints map[]
Jun  1 17:44:04.536: INFO: Get endpoints failed (2.237314ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun  1 17:44:05.539: INFO: successfully validated that service endpoint-test2 in namespace services-5159 exposes endpoints map[] (1.005034658s elapsed)
STEP: Creating pod pod1 in namespace services-5159
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5159 to expose endpoints map[pod1:[80]]
Jun  1 17:44:08.566: INFO: successfully validated that service endpoint-test2 in namespace services-5159 exposes endpoints map[pod1:[80]] (3.020850451s elapsed)
STEP: Creating pod pod2 in namespace services-5159
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5159 to expose endpoints map[pod1:[80] pod2:[80]]
Jun  1 17:44:10.592: INFO: successfully validated that service endpoint-test2 in namespace services-5159 exposes endpoints map[pod1:[80] pod2:[80]] (2.023205835s elapsed)
STEP: Deleting pod pod1 in namespace services-5159
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5159 to expose endpoints map[pod2:[80]]
Jun  1 17:44:11.608: INFO: successfully validated that service endpoint-test2 in namespace services-5159 exposes endpoints map[pod2:[80]] (1.010671255s elapsed)
STEP: Deleting pod pod2 in namespace services-5159
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5159 to expose endpoints map[]
Jun  1 17:44:12.619: INFO: successfully validated that service endpoint-test2 in namespace services-5159 exposes endpoints map[] (1.005614782s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:44:12.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5159" for this suite.
Jun  1 17:44:40.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:44:40.725: INFO: namespace services-5159 deletion completed in 28.089999512s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:36.329 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:44:40.725: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8416.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8416.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 17:44:44.891: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:44.894: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:44.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:44.909: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:44.912: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:44.918: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:44:49.929: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:49.932: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:49.953: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:49.956: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:49.962: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:44:54.930: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:54.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:54.948: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:54.951: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:54.957: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:44:59.930: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:59.934: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:59.949: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:59.951: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:44:59.957: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:45:04.930: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:04.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:04.948: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:04.951: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:04.957: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:45:09.930: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:09.933: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:09.949: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:09.952: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local from pod dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982: the server could not find the requested resource (get pods dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982)
Jun  1 17:45:09.958: INFO: Lookups using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 failed for: [wheezy_udp@dns-test-service-2.dns-8416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8416.svc.cluster.local jessie_udp@dns-test-service-2.dns-8416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8416.svc.cluster.local]

Jun  1 17:45:14.956: INFO: DNS probes using dns-8416/dns-test-4c55983a-a81b-4330-94e9-b11ba91b2982 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:45:14.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8416" for this suite.
Jun  1 17:45:21.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:45:21.360: INFO: namespace dns-8416 deletion completed in 6.380028593s

• [SLOW TEST:40.635 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:45:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:45:21.515: INFO: Create a RollingUpdate DaemonSet
Jun  1 17:45:21.518: INFO: Check that daemon pods launch on every node of the cluster
Jun  1 17:45:21.524: INFO: Number of nodes with available pods: 0
Jun  1 17:45:21.524: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:45:22.531: INFO: Number of nodes with available pods: 0
Jun  1 17:45:22.531: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:45:23.532: INFO: Number of nodes with available pods: 0
Jun  1 17:45:23.532: INFO: Node appserv10 is running more than one daemon pod
Jun  1 17:45:24.532: INFO: Number of nodes with available pods: 3
Jun  1 17:45:24.532: INFO: Number of running nodes: 3, number of available pods: 3
Jun  1 17:45:24.532: INFO: Update the DaemonSet to trigger a rollout
Jun  1 17:45:24.538: INFO: Updating DaemonSet daemon-set
Jun  1 17:45:35.551: INFO: Roll back the DaemonSet before rollout is complete
Jun  1 17:45:35.556: INFO: Updating DaemonSet daemon-set
Jun  1 17:45:35.556: INFO: Make sure DaemonSet rollback is complete
Jun  1 17:45:35.559: INFO: Wrong image for pod: daemon-set-fxbp9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  1 17:45:35.559: INFO: Pod daemon-set-fxbp9 is not available
Jun  1 17:45:36.566: INFO: Wrong image for pod: daemon-set-fxbp9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  1 17:45:36.566: INFO: Pod daemon-set-fxbp9 is not available
Jun  1 17:45:37.566: INFO: Pod daemon-set-qt4hs is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3634, will wait for the garbage collector to delete the pods
Jun  1 17:45:37.643: INFO: Deleting DaemonSet.extensions daemon-set took: 14.27929ms
Jun  1 17:45:38.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.260937ms
Jun  1 17:45:44.947: INFO: Number of nodes with available pods: 0
Jun  1 17:45:44.947: INFO: Number of running nodes: 0, number of available pods: 0
Jun  1 17:45:44.949: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3634/daemonsets","resourceVersion":"25277"},"items":null}

Jun  1 17:45:44.951: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3634/pods","resourceVersion":"25277"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:45:44.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3634" for this suite.
Jun  1 17:45:50.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:45:51.101: INFO: namespace daemonsets-3634 deletion completed in 6.134821219s

• [SLOW TEST:29.741 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:45:51.102: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8214
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-8214
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8214
Jun  1 17:45:51.246: INFO: Found 0 stateful pods, waiting for 1
Jun  1 17:46:01.251: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun  1 17:46:01.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 17:46:01.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 17:46:01.518: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 17:46:01.518: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 17:46:01.521: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  1 17:46:11.525: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 17:46:11.525: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 17:46:11.537: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:11.537: INFO: ss-0  appserv11  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:11.537: INFO: 
Jun  1 17:46:11.537: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  1 17:46:12.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996933196s
Jun  1 17:46:13.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992541286s
Jun  1 17:46:14.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988615727s
Jun  1 17:46:15.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985063349s
Jun  1 17:46:16.557: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98111551s
Jun  1 17:46:17.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977251447s
Jun  1 17:46:18.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973322281s
Jun  1 17:46:19.568: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969827494s
Jun  1 17:46:20.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.139936ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8214
Jun  1 17:46:21.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 17:46:21.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 17:46:21.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 17:46:21.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 17:46:21.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 17:46:22.079: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  1 17:46:22.079: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 17:46:22.079: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 17:46:22.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 17:46:22.319: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  1 17:46:22.319: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 17:46:22.319: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 17:46:22.323: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun  1 17:46:32.327: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:46:32.327: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 17:46:32.327: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun  1 17:46:32.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 17:46:32.573: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 17:46:32.573: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 17:46:32.573: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 17:46:32.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 17:46:32.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 17:46:32.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 17:46:32.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 17:46:32.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-8214 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 17:46:33.060: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 17:46:33.060: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 17:46:33.060: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 17:46:33.060: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 17:46:33.062: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  1 17:46:43.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 17:46:43.069: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 17:46:43.069: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 17:46:43.079: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:43.079: INFO: ss-0  appserv11  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:43.079: INFO: ss-1  appserv9   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:43.079: INFO: ss-2  appserv10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:43.079: INFO: 
Jun  1 17:46:43.079: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  1 17:46:44.084: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:44.084: INFO: ss-0  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:44.084: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:44.084: INFO: ss-2  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:44.084: INFO: 
Jun  1 17:46:44.084: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  1 17:46:45.088: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:45.088: INFO: ss-0  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:45.088: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:45.088: INFO: ss-2  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:45.088: INFO: 
Jun  1 17:46:45.088: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  1 17:46:46.093: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:46.094: INFO: ss-0  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:46.094: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:46.094: INFO: ss-2  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:46.094: INFO: 
Jun  1 17:46:46.094: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  1 17:46:47.098: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jun  1 17:46:47.098: INFO: ss-0  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:45:51 +0000 UTC  }]
Jun  1 17:46:47.098: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:47.098: INFO: ss-2  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-01 17:46:11 +0000 UTC  }]
Jun  1 17:46:47.098: INFO: 
Jun  1 17:46:47.098: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  1 17:46:48.101: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977525124s
Jun  1 17:46:49.104: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974192307s
Jun  1 17:46:50.108: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971771174s
Jun  1 17:46:51.112: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.96721664s
Jun  1 17:46:52.116: INFO: Verifying statefulset ss doesn't scale past 0 for another 963.580451ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8214
Jun  1 17:46:53.119: INFO: Scaling statefulset ss to 0
Jun  1 17:46:53.128: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 17:46:53.130: INFO: Deleting all statefulset in ns statefulset-8214
Jun  1 17:46:53.133: INFO: Scaling statefulset ss to 0
Jun  1 17:46:53.142: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 17:46:53.144: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:46:53.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8214" for this suite.
Jun  1 17:46:59.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:46:59.243: INFO: namespace statefulset-8214 deletion completed in 6.083452363s

• [SLOW TEST:68.141 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:46:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  1 17:46:59.381: INFO: Waiting up to 5m0s for pod "pod-59f6785a-5e24-4732-8ac4-2a4be679436f" in namespace "emptydir-6241" to be "success or failure"
Jun  1 17:46:59.383: INFO: Pod "pod-59f6785a-5e24-4732-8ac4-2a4be679436f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365787ms
Jun  1 17:47:01.387: INFO: Pod "pod-59f6785a-5e24-4732-8ac4-2a4be679436f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515058s
Jun  1 17:47:03.390: INFO: Pod "pod-59f6785a-5e24-4732-8ac4-2a4be679436f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009191646s
STEP: Saw pod success
Jun  1 17:47:03.390: INFO: Pod "pod-59f6785a-5e24-4732-8ac4-2a4be679436f" satisfied condition "success or failure"
Jun  1 17:47:03.393: INFO: Trying to get logs from node appserv9 pod pod-59f6785a-5e24-4732-8ac4-2a4be679436f container test-container: <nil>
STEP: delete the pod
Jun  1 17:47:03.421: INFO: Waiting for pod pod-59f6785a-5e24-4732-8ac4-2a4be679436f to disappear
Jun  1 17:47:03.424: INFO: Pod pod-59f6785a-5e24-4732-8ac4-2a4be679436f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:47:03.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6241" for this suite.
Jun  1 17:47:09.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:47:09.518: INFO: namespace emptydir-6241 deletion completed in 6.090837827s

• [SLOW TEST:10.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:47:09.519: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:47:09.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d" in namespace "projected-3753" to be "success or failure"
Jun  1 17:47:09.659: INFO: Pod "downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225826ms
Jun  1 17:47:11.663: INFO: Pod "downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005716647s
Jun  1 17:47:13.666: INFO: Pod "downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009258302s
STEP: Saw pod success
Jun  1 17:47:13.667: INFO: Pod "downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d" satisfied condition "success or failure"
Jun  1 17:47:13.669: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d container client-container: <nil>
STEP: delete the pod
Jun  1 17:47:13.735: INFO: Waiting for pod downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d to disappear
Jun  1 17:47:13.737: INFO: Pod downwardapi-volume-9014d071-d616-450c-847f-f953b91b6e5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:47:13.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3753" for this suite.
Jun  1 17:47:19.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:47:19.833: INFO: namespace projected-3753 deletion completed in 6.090371785s

• [SLOW TEST:10.315 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:47:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  1 17:47:20.304: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  1 17:47:22.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630440, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630440, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630440, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726630440, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 17:47:25.324: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:47:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:47:26.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-361" for this suite.
Jun  1 17:47:32.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:47:32.879: INFO: namespace crd-webhook-361 deletion completed in 6.114724393s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.057 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:47:32.891: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 17:47:33.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7333'
Jun  1 17:47:33.241: INFO: stderr: ""
Jun  1 17:47:33.241: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Jun  1 17:47:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete pods e2e-test-httpd-pod --namespace=kubectl-7333'
Jun  1 17:47:44.810: INFO: stderr: ""
Jun  1 17:47:44.810: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:47:44.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7333" for this suite.
Jun  1 17:47:50.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:47:50.938: INFO: namespace kubectl-7333 deletion completed in 6.123822373s

• [SLOW TEST:18.047 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:47:50.938: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-3574
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3574
STEP: Deleting pre-stop pod
Jun  1 17:48:00.102: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:48:00.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3574" for this suite.
Jun  1 17:48:44.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:48:44.227: INFO: namespace prestop-3574 deletion completed in 44.115801831s

• [SLOW TEST:53.289 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:48:44.227: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:49:10.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9802" for this suite.
Jun  1 17:49:16.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:49:16.621: INFO: namespace container-runtime-9802 deletion completed in 6.090101755s

• [SLOW TEST:32.394 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:49:16.621: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Jun  1 17:49:16.759: INFO: Waiting up to 5m0s for pod "var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292" in namespace "var-expansion-1088" to be "success or failure"
Jun  1 17:49:16.762: INFO: Pod "var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191239ms
Jun  1 17:49:18.765: INFO: Pod "var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005169814s
Jun  1 17:49:20.768: INFO: Pod "var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008539007s
STEP: Saw pod success
Jun  1 17:49:20.768: INFO: Pod "var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292" satisfied condition "success or failure"
Jun  1 17:49:20.771: INFO: Trying to get logs from node appserv9 pod var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292 container dapi-container: <nil>
STEP: delete the pod
Jun  1 17:49:20.798: INFO: Waiting for pod var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292 to disappear
Jun  1 17:49:20.800: INFO: Pod var-expansion-76a0f8b1-1d93-4dc5-9e05-c44553fb6292 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:49:20.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1088" for this suite.
Jun  1 17:49:26.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:49:26.932: INFO: namespace var-expansion-1088 deletion completed in 6.127515639s

• [SLOW TEST:10.311 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:49:26.932: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8650
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  1 17:49:27.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  1 17:49:53.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.11:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8650 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:49:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:49:53.249: INFO: Found all expected endpoints: [netserver-0]
Jun  1 17:49:53.252: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8650 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:49:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:49:53.352: INFO: Found all expected endpoints: [netserver-1]
Jun  1 17:49:53.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8650 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 17:49:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 17:49:53.455: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:49:53.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8650" for this suite.
Jun  1 17:50:05.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:50:05.558: INFO: namespace pod-network-test-8650 deletion completed in 12.098487289s

• [SLOW TEST:38.626 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:50:05.558: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:50:16.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3554" for this suite.
Jun  1 17:50:22.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:50:22.899: INFO: namespace resourcequota-3554 deletion completed in 6.170150194s

• [SLOW TEST:17.341 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:50:22.900: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  1 17:50:29.064: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:29.067: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:31.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:31.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:33.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:33.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:35.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:35.071: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:37.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:37.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:39.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:39.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:41.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:41.071: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:43.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:43.070: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  1 17:50:45.067: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  1 17:50:45.070: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:50:45.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4027" for this suite.
Jun  1 17:51:13.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:51:13.292: INFO: namespace container-lifecycle-hook-4027 deletion completed in 28.20769551s

• [SLOW TEST:50.392 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:51:13.292: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6095
STEP: Creating secret with name secret-test-74685371-772b-4e70-a7fa-d5aded1263c7
STEP: Creating a pod to test consume secrets
Jun  1 17:51:13.566: INFO: Waiting up to 5m0s for pod "pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599" in namespace "secrets-7613" to be "success or failure"
Jun  1 17:51:13.568: INFO: Pod "pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214917ms
Jun  1 17:51:15.571: INFO: Pod "pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005535258s
STEP: Saw pod success
Jun  1 17:51:15.572: INFO: Pod "pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599" satisfied condition "success or failure"
Jun  1 17:51:15.574: INFO: Trying to get logs from node appserv11 pod pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599 container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:51:15.642: INFO: Waiting for pod pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599 to disappear
Jun  1 17:51:15.644: INFO: Pod pod-secrets-860cdff6-8b23-4b1e-9a6b-660b41d64599 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:51:15.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7613" for this suite.
Jun  1 17:51:21.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:51:21.738: INFO: namespace secrets-7613 deletion completed in 6.088953661s
STEP: Destroying namespace "secret-namespace-6095" for this suite.
Jun  1 17:51:27.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:51:28.107: INFO: namespace secret-namespace-6095 deletion completed in 6.36930736s

• [SLOW TEST:14.815 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:51:28.107: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:51:28.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84" in namespace "downward-api-4974" to be "success or failure"
Jun  1 17:51:28.245: INFO: Pod "downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199892ms
Jun  1 17:51:30.249: INFO: Pod "downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00572429s
STEP: Saw pod success
Jun  1 17:51:30.249: INFO: Pod "downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84" satisfied condition "success or failure"
Jun  1 17:51:30.252: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84 container client-container: <nil>
STEP: delete the pod
Jun  1 17:51:30.269: INFO: Waiting for pod downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84 to disappear
Jun  1 17:51:30.271: INFO: Pod downwardapi-volume-e6907702-8db8-4a49-a596-b9ecbcb72a84 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:51:30.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4974" for this suite.
Jun  1 17:51:36.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:51:36.404: INFO: namespace downward-api-4974 deletion completed in 6.128625295s

• [SLOW TEST:8.297 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:51:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:51:38.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6034" for this suite.
Jun  1 17:52:26.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:52:26.656: INFO: namespace kubelet-test-6034 deletion completed in 48.092838298s

• [SLOW TEST:50.252 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:52:26.656: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7621/configmap-test-4fafbda2-5887-483a-881e-ba734a3406bb
STEP: Creating a pod to test consume configMaps
Jun  1 17:52:26.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8" in namespace "configmap-7621" to be "success or failure"
Jun  1 17:52:26.842: INFO: Pod "pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400583ms
Jun  1 17:52:28.845: INFO: Pod "pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005393509s
Jun  1 17:52:30.848: INFO: Pod "pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00865854s
STEP: Saw pod success
Jun  1 17:52:30.848: INFO: Pod "pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8" satisfied condition "success or failure"
Jun  1 17:52:30.850: INFO: Trying to get logs from node appserv11 pod pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8 container env-test: <nil>
STEP: delete the pod
Jun  1 17:52:30.867: INFO: Waiting for pod pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8 to disappear
Jun  1 17:52:30.869: INFO: Pod pod-configmaps-de5402e7-7fd5-4c20-8d9e-e9c7ea1585b8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:52:30.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7621" for this suite.
Jun  1 17:52:36.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:52:36.983: INFO: namespace configmap-7621 deletion completed in 6.110615902s

• [SLOW TEST:10.327 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:52:36.983: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 17:52:37.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c" in namespace "downward-api-5625" to be "success or failure"
Jun  1 17:52:37.125: INFO: Pod "downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437826ms
Jun  1 17:52:39.129: INFO: Pod "downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005860154s
STEP: Saw pod success
Jun  1 17:52:39.129: INFO: Pod "downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c" satisfied condition "success or failure"
Jun  1 17:52:39.132: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c container client-container: <nil>
STEP: delete the pod
Jun  1 17:52:39.148: INFO: Waiting for pod downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c to disappear
Jun  1 17:52:39.150: INFO: Pod downwardapi-volume-969b3419-0a42-4f97-ad69-ba64bf9aa14c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:52:39.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5625" for this suite.
Jun  1 17:52:45.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:52:45.288: INFO: namespace downward-api-5625 deletion completed in 6.134260746s

• [SLOW TEST:8.304 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:52:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:52:48.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4440" for this suite.
Jun  1 17:53:16.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:53:16.580: INFO: namespace replication-controller-4440 deletion completed in 28.129322415s

• [SLOW TEST:31.291 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:53:16.580: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Jun  1 17:53:16.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-1906 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  1 17:53:16.856: INFO: stderr: ""
Jun  1 17:53:16.856: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Jun  1 17:53:16.856: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  1 17:53:16.856: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1906" to be "running and ready, or succeeded"
Jun  1 17:53:16.859: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457469ms
Jun  1 17:53:18.862: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006339068s
Jun  1 17:53:18.862: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  1 17:53:18.862: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun  1 17:53:18.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906'
Jun  1 17:53:18.996: INFO: stderr: ""
Jun  1 17:53:18.996: INFO: stdout: "I0601 17:53:18.220174       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c2nf 226\nI0601 17:53:18.420288       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/6jtd 582\nI0601 17:53:18.620288       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hrz 294\nI0601 17:53:18.820274       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ff8g 316\n"
STEP: limiting log lines
Jun  1 17:53:18.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906 --tail=1'
Jun  1 17:53:19.110: INFO: stderr: ""
Jun  1 17:53:19.110: INFO: stdout: "I0601 17:53:19.020311       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/nptb 258\n"
STEP: limiting log bytes
Jun  1 17:53:19.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906 --limit-bytes=1'
Jun  1 17:53:19.238: INFO: stderr: ""
Jun  1 17:53:19.238: INFO: stdout: "I"
STEP: exposing timestamps
Jun  1 17:53:19.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906 --tail=1 --timestamps'
Jun  1 17:53:19.368: INFO: stderr: ""
Jun  1 17:53:19.368: INFO: stdout: "2020-06-01T17:53:19.220478776Z I0601 17:53:19.220320       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t8c 297\n"
STEP: restricting to a time range
Jun  1 17:53:21.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906 --since=1s'
Jun  1 17:53:22.001: INFO: stderr: ""
Jun  1 17:53:22.001: INFO: stdout: "I0601 17:53:21.020353       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/fdb 424\nI0601 17:53:21.220326       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/b67 357\nI0601 17:53:21.420327       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/smsq 445\nI0601 17:53:21.620367       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/fr9 497\nI0601 17:53:21.820276       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/b8tv 473\n"
Jun  1 17:53:22.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs logs-generator logs-generator --namespace=kubectl-1906 --since=24h'
Jun  1 17:53:22.139: INFO: stderr: ""
Jun  1 17:53:22.139: INFO: stdout: "I0601 17:53:18.220174       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/c2nf 226\nI0601 17:53:18.420288       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/6jtd 582\nI0601 17:53:18.620288       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hrz 294\nI0601 17:53:18.820274       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ff8g 316\nI0601 17:53:19.020311       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/nptb 258\nI0601 17:53:19.220320       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t8c 297\nI0601 17:53:19.420321       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/vwm 539\nI0601 17:53:19.620360       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/lpn 443\nI0601 17:53:19.820281       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/6f7z 388\nI0601 17:53:20.020367       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/nrt 583\nI0601 17:53:20.220326       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fnk 301\nI0601 17:53:20.420323       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/92j 230\nI0601 17:53:20.620369       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/bn8 375\nI0601 17:53:20.820286       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/mpgw 266\nI0601 17:53:21.020353       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/fdb 424\nI0601 17:53:21.220326       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/b67 357\nI0601 17:53:21.420327       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/smsq 445\nI0601 17:53:21.620367       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/fr9 497\nI0601 17:53:21.820276       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/b8tv 473\nI0601 17:53:22.020311       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/bcz 444\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Jun  1 17:53:22.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete pod logs-generator --namespace=kubectl-1906'
Jun  1 17:53:34.811: INFO: stderr: ""
Jun  1 17:53:34.811: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:53:34.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1906" for this suite.
Jun  1 17:53:40.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:53:40.934: INFO: namespace kubectl-1906 deletion completed in 6.118573229s

• [SLOW TEST:24.354 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:53:40.934: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6910
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-93723852-f97a-4851-abb2-b41f720f5cb4
STEP: Creating secret with name s-test-opt-upd-842ef00c-34a5-4e60-9d76-a8186a763517
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-93723852-f97a-4851-abb2-b41f720f5cb4
STEP: Updating secret s-test-opt-upd-842ef00c-34a5-4e60-9d76-a8186a763517
STEP: Creating secret with name s-test-opt-create-e1437799-40d3-4232-b9ae-fd46a6640312
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:55:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6910" for this suite.
Jun  1 17:55:27.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:55:27.722: INFO: namespace projected-6910 deletion completed in 12.080915017s

• [SLOW TEST:106.788 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:55:27.722: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-ecc16a48-91b8-41be-81d2-c600b5d08157
STEP: Creating a pod to test consume configMaps
Jun  1 17:55:27.862: INFO: Waiting up to 5m0s for pod "pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af" in namespace "configmap-8424" to be "success or failure"
Jun  1 17:55:27.864: INFO: Pod "pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.241831ms
Jun  1 17:55:29.867: INFO: Pod "pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005747819s
STEP: Saw pod success
Jun  1 17:55:29.867: INFO: Pod "pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af" satisfied condition "success or failure"
Jun  1 17:55:29.870: INFO: Trying to get logs from node appserv11 pod pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:55:29.928: INFO: Waiting for pod pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af to disappear
Jun  1 17:55:29.931: INFO: Pod pod-configmaps-928267d1-c38f-46a7-8f12-0707659697af no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:55:29.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8424" for this suite.
Jun  1 17:55:35.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:55:36.072: INFO: namespace configmap-8424 deletion completed in 6.137043006s

• [SLOW TEST:8.349 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:55:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e9f3411e-09c8-47cb-a76c-01b1ec990e7c
STEP: Creating a pod to test consume configMaps
Jun  1 17:55:36.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae" in namespace "projected-9947" to be "success or failure"
Jun  1 17:55:36.215: INFO: Pod "pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466577ms
Jun  1 17:55:38.217: INFO: Pod "pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005076355s
Jun  1 17:55:40.221: INFO: Pod "pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008492454s
STEP: Saw pod success
Jun  1 17:55:40.221: INFO: Pod "pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae" satisfied condition "success or failure"
Jun  1 17:55:40.223: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:55:40.240: INFO: Waiting for pod pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae to disappear
Jun  1 17:55:40.243: INFO: Pod pod-projected-configmaps-59546b11-457f-4af0-9ca0-17fe8d1123ae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:55:40.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9947" for this suite.
Jun  1 17:55:46.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:55:46.335: INFO: namespace projected-9947 deletion completed in 6.088104169s

• [SLOW TEST:10.263 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:55:46.336: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1480
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 17:55:46.466: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  1 17:55:50.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-1480 create -f -'
Jun  1 17:55:50.486: INFO: stderr: ""
Jun  1 17:55:50.486: INFO: stdout: "e2e-test-crd-publish-openapi-5707-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  1 17:55:50.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-1480 delete e2e-test-crd-publish-openapi-5707-crds test-cr'
Jun  1 17:55:50.614: INFO: stderr: ""
Jun  1 17:55:50.614: INFO: stdout: "e2e-test-crd-publish-openapi-5707-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  1 17:55:50.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-1480 apply -f -'
Jun  1 17:55:50.842: INFO: stderr: ""
Jun  1 17:55:50.842: INFO: stdout: "e2e-test-crd-publish-openapi-5707-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  1 17:55:50.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 --namespace=crd-publish-openapi-1480 delete e2e-test-crd-publish-openapi-5707-crds test-cr'
Jun  1 17:55:50.959: INFO: stderr: ""
Jun  1 17:55:50.959: INFO: stdout: "e2e-test-crd-publish-openapi-5707-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun  1 17:55:50.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 explain e2e-test-crd-publish-openapi-5707-crds'
Jun  1 17:55:51.144: INFO: stderr: ""
Jun  1 17:55:51.144: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5707-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:55:54.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1480" for this suite.
Jun  1 17:56:00.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:56:00.878: INFO: namespace crd-publish-openapi-1480 deletion completed in 6.081474633s

• [SLOW TEST:14.542 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:56:00.878: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-pbzd
STEP: Creating a pod to test atomic-volume-subpath
Jun  1 17:56:01.020: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pbzd" in namespace "subpath-1102" to be "success or failure"
Jun  1 17:56:01.022: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072092ms
Jun  1 17:56:03.026: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005718315s
Jun  1 17:56:05.029: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 4.009002566s
Jun  1 17:56:07.032: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 6.012208746s
Jun  1 17:56:09.035: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 8.014889285s
Jun  1 17:56:11.038: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 10.018233014s
Jun  1 17:56:13.042: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 12.02196923s
Jun  1 17:56:15.045: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 14.025479222s
Jun  1 17:56:17.049: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 16.028802292s
Jun  1 17:56:19.051: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 18.031528291s
Jun  1 17:56:21.055: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Running", Reason="", readiness=true. Elapsed: 20.035102476s
Jun  1 17:56:23.058: INFO: Pod "pod-subpath-test-downwardapi-pbzd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038587724s
STEP: Saw pod success
Jun  1 17:56:23.058: INFO: Pod "pod-subpath-test-downwardapi-pbzd" satisfied condition "success or failure"
Jun  1 17:56:23.061: INFO: Trying to get logs from node appserv11 pod pod-subpath-test-downwardapi-pbzd container test-container-subpath-downwardapi-pbzd: <nil>
STEP: delete the pod
Jun  1 17:56:23.078: INFO: Waiting for pod pod-subpath-test-downwardapi-pbzd to disappear
Jun  1 17:56:23.081: INFO: Pod pod-subpath-test-downwardapi-pbzd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pbzd
Jun  1 17:56:23.081: INFO: Deleting pod "pod-subpath-test-downwardapi-pbzd" in namespace "subpath-1102"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:56:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1102" for this suite.
Jun  1 17:56:29.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:56:29.228: INFO: namespace subpath-1102 deletion completed in 6.141768395s

• [SLOW TEST:28.351 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:56:29.229: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  1 17:56:29.367: INFO: Waiting up to 5m0s for pod "pod-b767dfe5-5651-4c41-a308-36579699e271" in namespace "emptydir-3234" to be "success or failure"
Jun  1 17:56:29.369: INFO: Pod "pod-b767dfe5-5651-4c41-a308-36579699e271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.170768ms
Jun  1 17:56:31.372: INFO: Pod "pod-b767dfe5-5651-4c41-a308-36579699e271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005372069s
STEP: Saw pod success
Jun  1 17:56:31.372: INFO: Pod "pod-b767dfe5-5651-4c41-a308-36579699e271" satisfied condition "success or failure"
Jun  1 17:56:31.375: INFO: Trying to get logs from node appserv11 pod pod-b767dfe5-5651-4c41-a308-36579699e271 container test-container: <nil>
STEP: delete the pod
Jun  1 17:56:31.393: INFO: Waiting for pod pod-b767dfe5-5651-4c41-a308-36579699e271 to disappear
Jun  1 17:56:31.395: INFO: Pod pod-b767dfe5-5651-4c41-a308-36579699e271 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:56:31.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3234" for this suite.
Jun  1 17:56:37.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:56:37.483: INFO: namespace emptydir-3234 deletion completed in 6.084475214s

• [SLOW TEST:8.255 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:56:37.484: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:56:53.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7882" for this suite.
Jun  1 17:56:59.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:56:59.788: INFO: namespace resourcequota-7882 deletion completed in 6.090960405s

• [SLOW TEST:22.305 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:56:59.789: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:57:03.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7268" for this suite.
Jun  1 17:57:09.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:57:10.028: INFO: namespace kubelet-test-7268 deletion completed in 6.089904995s

• [SLOW TEST:10.240 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:57:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  1 17:57:10.169: INFO: Waiting up to 5m0s for pod "pod-55fc2296-c07a-4202-be43-004c0fe9e612" in namespace "emptydir-3360" to be "success or failure"
Jun  1 17:57:10.171: INFO: Pod "pod-55fc2296-c07a-4202-be43-004c0fe9e612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433503ms
Jun  1 17:57:12.175: INFO: Pod "pod-55fc2296-c07a-4202-be43-004c0fe9e612": Phase="Running", Reason="", readiness=true. Elapsed: 2.005690595s
Jun  1 17:57:14.178: INFO: Pod "pod-55fc2296-c07a-4202-be43-004c0fe9e612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009163392s
STEP: Saw pod success
Jun  1 17:57:14.178: INFO: Pod "pod-55fc2296-c07a-4202-be43-004c0fe9e612" satisfied condition "success or failure"
Jun  1 17:57:14.181: INFO: Trying to get logs from node appserv11 pod pod-55fc2296-c07a-4202-be43-004c0fe9e612 container test-container: <nil>
STEP: delete the pod
Jun  1 17:57:14.198: INFO: Waiting for pod pod-55fc2296-c07a-4202-be43-004c0fe9e612 to disappear
Jun  1 17:57:14.200: INFO: Pod pod-55fc2296-c07a-4202-be43-004c0fe9e612 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:57:14.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3360" for this suite.
Jun  1 17:57:20.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:57:20.619: INFO: namespace emptydir-3360 deletion completed in 6.415006106s

• [SLOW TEST:10.590 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:57:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-pjvj
STEP: Creating a pod to test atomic-volume-subpath
Jun  1 17:57:20.766: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjvj" in namespace "subpath-7485" to be "success or failure"
Jun  1 17:57:20.768: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.45139ms
Jun  1 17:57:22.771: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005725427s
Jun  1 17:57:24.775: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 4.009578147s
Jun  1 17:57:26.779: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 6.013489679s
Jun  1 17:57:28.782: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 8.016454226s
Jun  1 17:57:30.785: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 10.01969231s
Jun  1 17:57:32.789: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 12.023009773s
Jun  1 17:57:34.792: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 14.026270126s
Jun  1 17:57:36.795: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 16.029457026s
Jun  1 17:57:38.798: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 18.031967314s
Jun  1 17:57:40.801: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Running", Reason="", readiness=true. Elapsed: 20.035240407s
Jun  1 17:57:42.804: INFO: Pod "pod-subpath-test-projected-pjvj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038425045s
STEP: Saw pod success
Jun  1 17:57:42.804: INFO: Pod "pod-subpath-test-projected-pjvj" satisfied condition "success or failure"
Jun  1 17:57:42.847: INFO: Trying to get logs from node appserv10 pod pod-subpath-test-projected-pjvj container test-container-subpath-projected-pjvj: <nil>
STEP: delete the pod
Jun  1 17:57:42.884: INFO: Waiting for pod pod-subpath-test-projected-pjvj to disappear
Jun  1 17:57:42.886: INFO: Pod pod-subpath-test-projected-pjvj no longer exists
STEP: Deleting pod pod-subpath-test-projected-pjvj
Jun  1 17:57:42.886: INFO: Deleting pod "pod-subpath-test-projected-pjvj" in namespace "subpath-7485"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:57:42.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7485" for this suite.
Jun  1 17:57:48.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:57:48.963: INFO: namespace subpath-7485 deletion completed in 6.071205753s

• [SLOW TEST:28.344 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:57:48.964: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  1 17:57:49.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2566'
Jun  1 17:57:49.199: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  1 17:57:49.199: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Jun  1 17:57:49.206: INFO: scanned /root for discovery docs: <nil>
Jun  1 17:57:49.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2566'
Jun  1 17:58:04.988: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun  1 17:58:04.988: INFO: stdout: "Created e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32\nScaling up e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun  1 17:58:04.988: INFO: stdout: "Created e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32\nScaling up e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun  1 17:58:04.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2566'
Jun  1 17:58:05.108: INFO: stderr: ""
Jun  1 17:58:05.108: INFO: stdout: "e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32-jk7fj "
Jun  1 17:58:05.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32-jk7fj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2566'
Jun  1 17:58:05.231: INFO: stderr: ""
Jun  1 17:58:05.231: INFO: stdout: "true"
Jun  1 17:58:05.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 get pods e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32-jk7fj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2566'
Jun  1 17:58:05.342: INFO: stderr: ""
Jun  1 17:58:05.342: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun  1 17:58:05.342: INFO: e2e-test-httpd-rc-70f91a33331ac3d509299e5ae962bc32-jk7fj is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Jun  1 17:58:05.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 delete rc e2e-test-httpd-rc --namespace=kubectl-2566'
Jun  1 17:58:05.464: INFO: stderr: ""
Jun  1 17:58:05.464: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:58:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2566" for this suite.
Jun  1 17:58:17.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:58:17.560: INFO: namespace kubectl-2566 deletion completed in 12.091738502s

• [SLOW TEST:28.597 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:58:17.561: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-7sff
STEP: Creating a pod to test atomic-volume-subpath
Jun  1 17:58:17.706: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7sff" in namespace "subpath-242" to be "success or failure"
Jun  1 17:58:17.708: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692823ms
Jun  1 17:58:19.712: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 2.006035353s
Jun  1 17:58:21.715: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 4.009544181s
Jun  1 17:58:23.719: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 6.013160802s
Jun  1 17:58:25.722: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 8.016648834s
Jun  1 17:58:27.726: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 10.020632265s
Jun  1 17:58:29.730: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 12.023980591s
Jun  1 17:58:31.733: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 14.027435076s
Jun  1 17:58:33.737: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 16.031675532s
Jun  1 17:58:35.741: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 18.03503255s
Jun  1 17:58:37.744: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 20.038670362s
Jun  1 17:58:39.748: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Running", Reason="", readiness=true. Elapsed: 22.042437364s
Jun  1 17:58:41.752: INFO: Pod "pod-subpath-test-configmap-7sff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045878997s
STEP: Saw pod success
Jun  1 17:58:41.752: INFO: Pod "pod-subpath-test-configmap-7sff" satisfied condition "success or failure"
Jun  1 17:58:41.754: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-configmap-7sff container test-container-subpath-configmap-7sff: <nil>
STEP: delete the pod
Jun  1 17:58:41.782: INFO: Waiting for pod pod-subpath-test-configmap-7sff to disappear
Jun  1 17:58:41.785: INFO: Pod pod-subpath-test-configmap-7sff no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7sff
Jun  1 17:58:41.785: INFO: Deleting pod "pod-subpath-test-configmap-7sff" in namespace "subpath-242"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:58:41.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-242" for this suite.
Jun  1 17:58:47.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:58:47.868: INFO: namespace subpath-242 deletion completed in 6.077089034s

• [SLOW TEST:30.307 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:58:47.868: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-e7bd4fc3-f982-49fb-8b98-4192ff07c360
STEP: Creating a pod to test consume secrets
Jun  1 17:58:48.006: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e" in namespace "projected-4498" to be "success or failure"
Jun  1 17:58:48.008: INFO: Pod "pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.934888ms
Jun  1 17:58:50.011: INFO: Pod "pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005541299s
Jun  1 17:58:52.015: INFO: Pod "pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009172752s
STEP: Saw pod success
Jun  1 17:58:52.015: INFO: Pod "pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e" satisfied condition "success or failure"
Jun  1 17:58:52.017: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  1 17:58:52.083: INFO: Waiting for pod pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e to disappear
Jun  1 17:58:52.085: INFO: Pod pod-projected-secrets-d5936ecf-2047-4511-8ec2-454b5720867e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:58:52.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4498" for this suite.
Jun  1 17:58:58.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:58:58.167: INFO: namespace projected-4498 deletion completed in 6.07761039s

• [SLOW TEST:10.298 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:58:58.167: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-03e9b469-40a2-4e79-ab99-cb9fef1e9c73
STEP: Creating a pod to test consume configMaps
Jun  1 17:58:58.304: INFO: Waiting up to 5m0s for pod "pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb" in namespace "configmap-7862" to be "success or failure"
Jun  1 17:58:58.306: INFO: Pod "pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055965ms
Jun  1 17:59:00.310: INFO: Pod "pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005615473s
Jun  1 17:59:02.313: INFO: Pod "pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177029s
STEP: Saw pod success
Jun  1 17:59:02.313: INFO: Pod "pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb" satisfied condition "success or failure"
Jun  1 17:59:02.316: INFO: Trying to get logs from node appserv9 pod pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb container configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 17:59:02.335: INFO: Waiting for pod pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb to disappear
Jun  1 17:59:02.338: INFO: Pod pod-configmaps-21951572-ee2d-4485-8478-676189fad1eb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:59:02.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7862" for this suite.
Jun  1 17:59:08.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:59:08.465: INFO: namespace configmap-7862 deletion completed in 6.122708361s

• [SLOW TEST:10.298 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:59:08.465: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun  1 17:59:08.635: INFO: namespace kubectl-6242
Jun  1 17:59:08.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 create -f - --namespace=kubectl-6242'
Jun  1 17:59:08.941: INFO: stderr: ""
Jun  1 17:59:08.941: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun  1 17:59:09.945: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:59:09.945: INFO: Found 0 / 1
Jun  1 17:59:10.944: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:59:10.944: INFO: Found 0 / 1
Jun  1 17:59:11.944: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:59:11.944: INFO: Found 1 / 1
Jun  1 17:59:11.944: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  1 17:59:11.947: INFO: Selector matched 1 pods for map[app:redis]
Jun  1 17:59:11.947: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  1 17:59:11.947: INFO: wait on redis-master startup in kubectl-6242 
Jun  1 17:59:11.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 logs redis-master-q6vjl redis-master --namespace=kubectl-6242'
Jun  1 17:59:12.078: INFO: stderr: ""
Jun  1 17:59:12.078: INFO: stdout: "1:C 01 Jun 2020 17:59:10.559 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 01 Jun 2020 17:59:10.559 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 01 Jun 2020 17:59:10.559 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 01 Jun 2020 17:59:10.563 * Running mode=standalone, port=6379.\n1:M 01 Jun 2020 17:59:10.563 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Jun 2020 17:59:10.563 # Server initialized\n1:M 01 Jun 2020 17:59:10.563 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Jun 2020 17:59:10.564 * Ready to accept connections\n"
STEP: exposing RC
Jun  1 17:59:12.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6242'
Jun  1 17:59:12.209: INFO: stderr: ""
Jun  1 17:59:12.209: INFO: stdout: "service/rm2 exposed\n"
Jun  1 17:59:12.211: INFO: Service rm2 in namespace kubectl-6242 found.
STEP: exposing service
Jun  1 17:59:14.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6242'
Jun  1 17:59:14.358: INFO: stderr: ""
Jun  1 17:59:14.358: INFO: stdout: "service/rm3 exposed\n"
Jun  1 17:59:14.360: INFO: Service rm3 in namespace kubectl-6242 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:59:16.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6242" for this suite.
Jun  1 17:59:44.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:59:44.463: INFO: namespace kubectl-6242 deletion completed in 28.093130462s

• [SLOW TEST:35.998 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:59:44.464: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:59:46.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3833" for this suite.
Jun  1 17:59:52.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 17:59:52.803: INFO: namespace emptydir-wrapper-3833 deletion completed in 6.119144001s

• [SLOW TEST:8.339 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 17:59:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun  1 17:59:57.467: INFO: Successfully updated pod "labelsupdate92b6160f-32de-4ac4-b5f8-e5b38de60f59"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 17:59:59.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1543" for this suite.
Jun  1 18:00:27.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:00:27.584: INFO: namespace downward-api-1543 deletion completed in 28.094267051s

• [SLOW TEST:34.781 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:00:27.584: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8420/configmap-test-fd8a4900-88e1-4932-a5fa-b33b8ca62f2e
STEP: Creating a pod to test consume configMaps
Jun  1 18:00:27.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4" in namespace "configmap-8420" to be "success or failure"
Jun  1 18:00:27.728: INFO: Pod "pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117044ms
Jun  1 18:00:29.731: INFO: Pod "pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005181019s
STEP: Saw pod success
Jun  1 18:00:29.731: INFO: Pod "pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4" satisfied condition "success or failure"
Jun  1 18:00:29.733: INFO: Trying to get logs from node appserv9 pod pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4 container env-test: <nil>
STEP: delete the pod
Jun  1 18:00:29.750: INFO: Waiting for pod pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4 to disappear
Jun  1 18:00:29.752: INFO: Pod pod-configmaps-e2ce3122-31ff-467b-a018-a02020eabbc4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:00:29.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8420" for this suite.
Jun  1 18:00:35.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:00:35.848: INFO: namespace configmap-8420 deletion completed in 6.09245463s

• [SLOW TEST:8.264 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:00:35.849: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-0749a4ba-d613-4c5d-8253-a21ae6304c45
STEP: Creating a pod to test consume configMaps
Jun  1 18:00:35.991: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58" in namespace "projected-6283" to be "success or failure"
Jun  1 18:00:35.993: INFO: Pod "pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184846ms
Jun  1 18:00:37.996: INFO: Pod "pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005270669s
Jun  1 18:00:40.000: INFO: Pod "pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008819105s
STEP: Saw pod success
Jun  1 18:00:40.000: INFO: Pod "pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58" satisfied condition "success or failure"
Jun  1 18:00:40.002: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 18:00:40.020: INFO: Waiting for pod pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58 to disappear
Jun  1 18:00:40.022: INFO: Pod pod-projected-configmaps-3fbfcfd6-08d7-42e1-a7e6-e7a0cc278c58 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:00:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6283" for this suite.
Jun  1 18:00:46.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:00:46.154: INFO: namespace projected-6283 deletion completed in 6.128152825s

• [SLOW TEST:10.305 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:00:46.154: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Jun  1 18:00:46.295: INFO: Waiting up to 5m0s for pod "client-containers-14f02ee7-8487-4927-b348-7a64ead81901" in namespace "containers-6944" to be "success or failure"
Jun  1 18:00:46.297: INFO: Pod "client-containers-14f02ee7-8487-4927-b348-7a64ead81901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.495488ms
Jun  1 18:00:48.301: INFO: Pod "client-containers-14f02ee7-8487-4927-b348-7a64ead81901": Phase="Running", Reason="", readiness=true. Elapsed: 2.005713195s
Jun  1 18:00:50.304: INFO: Pod "client-containers-14f02ee7-8487-4927-b348-7a64ead81901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009446555s
STEP: Saw pod success
Jun  1 18:00:50.304: INFO: Pod "client-containers-14f02ee7-8487-4927-b348-7a64ead81901" satisfied condition "success or failure"
Jun  1 18:00:50.307: INFO: Trying to get logs from node appserv9 pod client-containers-14f02ee7-8487-4927-b348-7a64ead81901 container test-container: <nil>
STEP: delete the pod
Jun  1 18:00:50.325: INFO: Waiting for pod client-containers-14f02ee7-8487-4927-b348-7a64ead81901 to disappear
Jun  1 18:00:50.328: INFO: Pod client-containers-14f02ee7-8487-4927-b348-7a64ead81901 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:00:50.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6944" for this suite.
Jun  1 18:00:56.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:00:56.499: INFO: namespace containers-6944 deletion completed in 6.167575477s

• [SLOW TEST:10.345 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:00:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0540c058-51cd-4f52-8672-96c85b0d6713
STEP: Creating a pod to test consume configMaps
Jun  1 18:00:56.642: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593" in namespace "projected-4111" to be "success or failure"
Jun  1 18:00:56.644: INFO: Pod "pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450213ms
Jun  1 18:00:58.648: INFO: Pod "pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005523002s
Jun  1 18:01:00.651: INFO: Pod "pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009043607s
STEP: Saw pod success
Jun  1 18:01:00.651: INFO: Pod "pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593" satisfied condition "success or failure"
Jun  1 18:01:00.654: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 18:01:00.672: INFO: Waiting for pod pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593 to disappear
Jun  1 18:01:00.674: INFO: Pod pod-projected-configmaps-261ec6cb-8432-4fb6-a84b-0647bc647593 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:01:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4111" for this suite.
Jun  1 18:01:06.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:01:06.815: INFO: namespace projected-4111 deletion completed in 6.136922003s

• [SLOW TEST:10.316 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:01:06.816: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun  1 18:01:06.954: INFO: Waiting up to 5m0s for pod "downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67" in namespace "downward-api-341" to be "success or failure"
Jun  1 18:01:06.957: INFO: Pod "downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.606968ms
Jun  1 18:01:08.960: INFO: Pod "downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005326042s
STEP: Saw pod success
Jun  1 18:01:08.960: INFO: Pod "downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67" satisfied condition "success or failure"
Jun  1 18:01:08.962: INFO: Trying to get logs from node appserv11 pod downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67 container dapi-container: <nil>
STEP: delete the pod
Jun  1 18:01:08.976: INFO: Waiting for pod downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67 to disappear
Jun  1 18:01:08.978: INFO: Pod downward-api-51c6f40f-bf5e-4f52-92da-56c9f1c5fb67 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:01:08.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-341" for this suite.
Jun  1 18:01:14.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:01:15.073: INFO: namespace downward-api-341 deletion completed in 6.09084378s

• [SLOW TEST:8.257 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:01:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Jun  1 18:01:15.213: INFO: Waiting up to 5m0s for pod "client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064" in namespace "containers-1491" to be "success or failure"
Jun  1 18:01:15.216: INFO: Pod "client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25711ms
Jun  1 18:01:17.219: INFO: Pod "client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064": Phase="Running", Reason="", readiness=true. Elapsed: 2.005369809s
Jun  1 18:01:19.222: INFO: Pod "client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008212334s
STEP: Saw pod success
Jun  1 18:01:19.222: INFO: Pod "client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064" satisfied condition "success or failure"
Jun  1 18:01:19.224: INFO: Trying to get logs from node appserv9 pod client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064 container test-container: <nil>
STEP: delete the pod
Jun  1 18:01:19.242: INFO: Waiting for pod client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064 to disappear
Jun  1 18:01:19.244: INFO: Pod client-containers-0c7774b3-c194-4fdf-adf0-7ca04e005064 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:01:19.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1491" for this suite.
Jun  1 18:01:25.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:01:25.374: INFO: namespace containers-1491 deletion completed in 6.125689909s

• [SLOW TEST:10.300 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:01:25.374: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun  1 18:01:25.506: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  1 18:01:25.517: INFO: Waiting for terminating namespaces to be deleted...
Jun  1 18:01:25.520: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Jun  1 18:01:25.578: INFO: csi-diamanti-driver-lqbpt from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container diamanticsidriver ready: true, restart count 1
Jun  1 18:01:25.578: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.578: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:01:25.578: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:01:25.578: INFO: nfs-csi-diamanti-driver-cht54 from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.578: INFO: metrics-server-v1-7659784467-v66bd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container metrics-server ready: true, restart count 0
Jun  1 18:01:25.578: INFO: sonobuoy-e2e-job-5a0f7ba7dfe14684 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container e2e ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 18:01:25.578: INFO: csi-external-snapshotter-5f96b86db9-kzt8f from diamanti-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Jun  1 18:01:25.578: INFO: prometheus-v1-2 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:01:25.578: INFO: provisioner-654c8c4db6-zxcsz from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container provisioner ready: true, restart count 0
Jun  1 18:01:25.578: INFO: sonobuoy from sonobuoy started at 2020-06-01 16:25:01 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  1 18:01:25.578: INFO: collectd-v0.8-hxgzj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:01:25.578: INFO: coredns-6d6bd8df7c-2zcwd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.578: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:01:25.578: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Jun  1 18:01:25.587: INFO: nfs-csi-diamanti-driver-h2dts from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.587: INFO: csi-diamanti-driver-s59hb from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:01:25.587: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.587: INFO: collectd-v0.8-8fjnl from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (5 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:01:25.587: INFO: alertmanager-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container alertmanager ready: true, restart count 0
Jun  1 18:01:25.587: INFO: snapshot-controller-798c69596d-nsc4s from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  1 18:01:25.587: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jun  1 18:01:25.587: INFO: coredns-6d6bd8df7c-bnq4n from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:01:25.587: INFO: prometheus-v1-1 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.587: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:01:25.588: INFO: tiller-deploy-57f5b6fd78-574p5 from kube-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.588: INFO: 	Container tiller ready: true, restart count 0
Jun  1 18:01:25.588: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.588: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:01:25.588: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:01:25.588: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Jun  1 18:01:25.596: INFO: collectd-v0.8-m7sbv from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:01:25.596: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:01:25.596: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:01:25.596: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:01:25.596: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:01:25.596: INFO: csi-external-provisioner-7f56896f9-csfz4 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Jun  1 18:01:25.596: INFO: helm-chart-7fcf79f88c-6pv8f from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container helm-chart ready: true, restart count 0
Jun  1 18:01:25.596: INFO: coredns-6d6bd8df7c-ghgls from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:01:25.596: INFO: csi-external-attacher-846d47f8d6-7zcsv from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  1 18:01:25.596: INFO: nfs-csi-diamanti-driver-dh2xj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:01:25.596: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.596: INFO: csi-diamanti-driver-cg4qz from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:01:25.596: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:01:25.596: INFO: prometheus-v1-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:01:25.596: INFO: csi-external-resizer-65f576d548-skb55 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container csi-external-resizer ready: true, restart count 0
Jun  1 18:01:25.596: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 from sonobuoy started at 2020-06-01 16:25:03 +0000 UTC (2 container statuses recorded)
Jun  1 18:01:25.596: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:01:25.596: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16147d6f5c81f1b0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:01:26.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2019" for this suite.
Jun  1 18:01:32.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:01:32.709: INFO: namespace sched-pred-2019 deletion completed in 6.083599635s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.335 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:01:32.710: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:02:32.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7013" for this suite.
Jun  1 18:03:00.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:03:00.945: INFO: namespace container-probe-7013 deletion completed in 28.090180066s

• [SLOW TEST:88.235 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:03:00.945: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 18:03:01.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 18:03:03.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631381, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631381, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631381, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631381, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 18:03:06.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:03:06.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4869" for this suite.
Jun  1 18:03:12.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:03:12.873: INFO: namespace webhook-4869 deletion completed in 6.135549172s
STEP: Destroying namespace "webhook-4869-markers" for this suite.
Jun  1 18:03:18.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:03:18.994: INFO: namespace webhook-4869-markers deletion completed in 6.120967261s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:03:19.005: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7414
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun  1 18:03:19.144: INFO: Found 0 stateful pods, waiting for 3
Jun  1 18:03:29.148: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:03:29.149: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:03:29.149: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:03:29.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-7414 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:03:29.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:03:29.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:03:29.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  1 18:03:39.435: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun  1 18:03:49.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-7414 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:03:49.690: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:03:49.690: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:03:49.690: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:03:59.711: INFO: Waiting for StatefulSet statefulset-7414/ss2 to complete update
Jun  1 18:03:59.711: INFO: Waiting for Pod statefulset-7414/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 18:03:59.711: INFO: Waiting for Pod statefulset-7414/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 18:03:59.711: INFO: Waiting for Pod statefulset-7414/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 18:04:09.721: INFO: Waiting for StatefulSet statefulset-7414/ss2 to complete update
Jun  1 18:04:09.721: INFO: Waiting for Pod statefulset-7414/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 18:04:09.721: INFO: Waiting for Pod statefulset-7414/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  1 18:04:19.719: INFO: Waiting for StatefulSet statefulset-7414/ss2 to complete update
Jun  1 18:04:19.719: INFO: Waiting for Pod statefulset-7414/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun  1 18:04:29.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-7414 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:04:29.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:04:29.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:04:29.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 18:04:39.995: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun  1 18:04:50.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-7414 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:04:50.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:04:50.252: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:04:50.252: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:05:10.271: INFO: Waiting for StatefulSet statefulset-7414/ss2 to complete update
Jun  1 18:05:10.272: INFO: Waiting for Pod statefulset-7414/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 18:05:20.278: INFO: Deleting all statefulset in ns statefulset-7414
Jun  1 18:05:20.281: INFO: Scaling statefulset ss2 to 0
Jun  1 18:06:00.294: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 18:06:00.297: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:06:00.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7414" for this suite.
Jun  1 18:06:06.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:06:06.444: INFO: namespace statefulset-7414 deletion completed in 6.131406648s

• [SLOW TEST:167.439 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:06:06.445: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 18:06:07.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 18:06:09.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631567, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631567, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631567, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631567, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 18:06:12.359: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:06:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2165" for this suite.
Jun  1 18:06:18.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:06:18.605: INFO: namespace webhook-2165 deletion completed in 6.078372738s
STEP: Destroying namespace "webhook-2165-markers" for this suite.
Jun  1 18:06:24.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:06:24.694: INFO: namespace webhook-2165-markers deletion completed in 6.089208708s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.261 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:06:24.706: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4023
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3f4d8d64-7c06-4c98-a573-08b9f30f85db
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3f4d8d64-7c06-4c98-a573-08b9f30f85db
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:06:28.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4023" for this suite.
Jun  1 18:06:56.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:06:57.324: INFO: namespace projected-4023 deletion completed in 28.381580129s

• [SLOW TEST:32.618 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:06:57.325: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4271
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  1 18:06:57.458: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  1 18:07:17.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.11&port=8080&tries=1'] Namespace:pod-network-test-4271 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 18:07:17.523: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 18:07:17.657: INFO: Waiting for endpoints: map[]
Jun  1 18:07:17.660: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.12&port=8080&tries=1'] Namespace:pod-network-test-4271 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 18:07:17.660: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 18:07:17.757: INFO: Waiting for endpoints: map[]
Jun  1 18:07:17.759: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.13&port=8080&tries=1'] Namespace:pod-network-test-4271 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  1 18:07:17.759: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 18:07:17.866: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:07:17.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4271" for this suite.
Jun  1 18:07:29.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:07:29.965: INFO: namespace pod-network-test-4271 deletion completed in 12.094623551s

• [SLOW TEST:32.640 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:07:29.965: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun  1 18:07:30.097: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:07:33.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1100" for this suite.
Jun  1 18:07:39.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:07:40.254: INFO: namespace init-container-1100 deletion completed in 6.353813571s

• [SLOW TEST:10.288 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:07:40.254: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3915
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3915
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3915
Jun  1 18:07:40.400: INFO: Found 0 stateful pods, waiting for 1
Jun  1 18:07:50.404: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun  1 18:07:50.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:07:50.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:07:50.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:07:50.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 18:07:50.724: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  1 18:08:00.727: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 18:08:00.728: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 18:08:00.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999528s
Jun  1 18:08:01.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996835765s
Jun  1 18:08:02.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992988953s
Jun  1 18:08:03.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989458479s
Jun  1 18:08:04.755: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985746335s
Jun  1 18:08:05.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982164261s
Jun  1 18:08:06.763: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978257362s
Jun  1 18:08:07.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974538994s
Jun  1 18:08:08.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971008276s
Jun  1 18:08:09.773: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.830729ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3915
Jun  1 18:08:10.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:08:11.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:08:11.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:08:11.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:08:11.030: INFO: Found 1 stateful pods, waiting for 3
Jun  1 18:08:21.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:21.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:21.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:08:31.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:31.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:31.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:08:41.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:41.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:41.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:08:51.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:51.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:08:51.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:09:01.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:01.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:01.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:09:11.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:11.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:11.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:09:21.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:21.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:21.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:09:31.035: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:31.035: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:31.035: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  1 18:09:41.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:41.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  1 18:09:41.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun  1 18:09:41.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:09:41.316: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:09:41.316: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:09:41.316: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 18:09:41.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:09:41.573: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:09:41.573: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:09:41.573: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 18:09:41.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  1 18:09:41.815: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  1 18:09:41.815: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  1 18:09:41.815: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  1 18:09:41.815: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 18:09:41.818: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  1 18:09:51.825: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 18:09:51.826: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 18:09:51.826: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  1 18:09:51.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999534s
Jun  1 18:09:52.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996650914s
Jun  1 18:09:53.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992755774s
Jun  1 18:09:54.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988928897s
Jun  1 18:09:55.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98519246s
Jun  1 18:09:56.855: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980901852s
Jun  1 18:09:57.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977102909s
Jun  1 18:09:58.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973351106s
Jun  1 18:09:59.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970165746s
Jun  1 18:10:00.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.239984ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3915
Jun  1 18:10:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:10:02.123: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:10:02.123: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:10:02.123: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:10:02.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:10:02.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:10:02.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:10:02.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:10:02.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=statefulset-3915 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  1 18:10:02.635: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  1 18:10:02.635: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  1 18:10:02.635: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  1 18:10:02.635: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun  1 18:10:32.649: INFO: Deleting all statefulset in ns statefulset-3915
Jun  1 18:10:32.652: INFO: Scaling statefulset ss to 0
Jun  1 18:10:32.660: INFO: Waiting for statefulset status.replicas updated to 0
Jun  1 18:10:32.663: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:10:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3915" for this suite.
Jun  1 18:10:38.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:10:38.797: INFO: namespace statefulset-3915 deletion completed in 6.118218152s

• [SLOW TEST:178.543 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:10:38.797: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 18:10:39.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 18:10:41.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631839, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631839, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631839, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726631839, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 18:10:44.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:10:44.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9103" for this suite.
Jun  1 18:10:50.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:10:50.851: INFO: namespace webhook-9103 deletion completed in 6.087951106s
STEP: Destroying namespace "webhook-9103-markers" for this suite.
Jun  1 18:10:56.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:10:56.943: INFO: namespace webhook-9103-markers deletion completed in 6.091470218s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.157 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:10:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun  1 18:10:57.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2620 /api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-watch-closed 659a1137-884b-4ea6-bb66-afd1ce1cce87 31564 0 2020-06-01 18:10:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  1 18:10:57.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2620 /api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-watch-closed 659a1137-884b-4ea6-bb66-afd1ce1cce87 31565 0 2020-06-01 18:10:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun  1 18:10:57.109: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2620 /api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-watch-closed 659a1137-884b-4ea6-bb66-afd1ce1cce87 31566 0 2020-06-01 18:10:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  1 18:10:57.109: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2620 /api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-watch-closed 659a1137-884b-4ea6-bb66-afd1ce1cce87 31567 0 2020-06-01 18:10:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:10:57.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2620" for this suite.
Jun  1 18:11:03.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:11:03.202: INFO: namespace watch-2620 deletion completed in 6.089223341s

• [SLOW TEST:6.248 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:11:03.203: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-74aefb72-90a3-45e7-a549-ff0cdbbc5267
STEP: Creating a pod to test consume secrets
Jun  1 18:11:03.345: INFO: Waiting up to 5m0s for pod "pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb" in namespace "secrets-752" to be "success or failure"
Jun  1 18:11:03.347: INFO: Pod "pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284043ms
Jun  1 18:11:05.351: INFO: Pod "pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005540281s
Jun  1 18:11:07.354: INFO: Pod "pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008907281s
STEP: Saw pod success
Jun  1 18:11:07.354: INFO: Pod "pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb" satisfied condition "success or failure"
Jun  1 18:11:07.357: INFO: Trying to get logs from node appserv9 pod pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 18:11:07.383: INFO: Waiting for pod pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb to disappear
Jun  1 18:11:07.386: INFO: Pod pod-secrets-59c45be1-34aa-4e00-a22a-31f34c2380eb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:11:07.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-752" for this suite.
Jun  1 18:11:13.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:11:13.484: INFO: namespace secrets-752 deletion completed in 6.094237455s

• [SLOW TEST:10.281 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:11:13.484: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1200
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun  1 18:11:13.616: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
Jun  1 18:11:17.167: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:11:31.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1200" for this suite.
Jun  1 18:11:37.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:11:37.466: INFO: namespace crd-publish-openapi-1200 deletion completed in 6.28041684s

• [SLOW TEST:23.981 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:11:37.466: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun  1 18:11:37.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31722 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  1 18:11:37.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31723 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun  1 18:11:37.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31724 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun  1 18:11:47.638: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31745 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  1 18:11:47.638: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31746 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun  1 18:11:47.638: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7787 /api/v1/namespaces/watch-7787/configmaps/e2e-watch-test-label-changed 1e983e85-e9e7-404c-b4bc-e807535abcf1 31747 0 2020-06-01 18:11:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:11:47.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7787" for this suite.
Jun  1 18:11:53.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:11:53.731: INFO: namespace watch-7787 deletion completed in 6.089197738s

• [SLOW TEST:16.265 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:11:53.731: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun  1 18:12:03.886: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0601 18:12:03.886592      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:12:03.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-183" for this suite.
Jun  1 18:12:09.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:12:10.021: INFO: namespace gc-183 deletion completed in 6.130757432s

• [SLOW TEST:16.290 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:12:10.021: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun  1 18:12:10.187: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  1 18:12:10.198: INFO: Waiting for terminating namespaces to be deleted...
Jun  1 18:12:10.201: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Jun  1 18:12:10.259: INFO: nfs-csi-diamanti-driver-cht54 from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.259: INFO: metrics-server-v1-7659784467-v66bd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container metrics-server ready: true, restart count 0
Jun  1 18:12:10.259: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:12:10.259: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:12:10.259: INFO: sonobuoy-e2e-job-5a0f7ba7dfe14684 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container e2e ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 18:12:10.259: INFO: collectd-v0.8-hxgzj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:12:10.259: INFO: coredns-6d6bd8df7c-2zcwd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:12:10.259: INFO: csi-external-snapshotter-5f96b86db9-kzt8f from diamanti-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Jun  1 18:12:10.259: INFO: prometheus-v1-2 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:12:10.259: INFO: provisioner-654c8c4db6-zxcsz from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container provisioner ready: true, restart count 0
Jun  1 18:12:10.259: INFO: sonobuoy from sonobuoy started at 2020-06-01 16:25:01 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  1 18:12:10.259: INFO: csi-diamanti-driver-lqbpt from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.259: INFO: 	Container diamanticsidriver ready: true, restart count 1
Jun  1 18:12:10.259: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.259: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Jun  1 18:12:10.282: INFO: collectd-v0.8-8fjnl from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (5 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:12:10.282: INFO: alertmanager-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container alertmanager ready: true, restart count 0
Jun  1 18:12:10.282: INFO: snapshot-controller-798c69596d-nsc4s from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jun  1 18:12:10.282: INFO: coredns-6d6bd8df7c-bnq4n from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:12:10.282: INFO: prometheus-v1-1 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:12:10.282: INFO: tiller-deploy-57f5b6fd78-574p5 from kube-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container tiller ready: true, restart count 0
Jun  1 18:12:10.282: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:12:10.282: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:12:10.282: INFO: nfs-csi-diamanti-driver-h2dts from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.282: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:12:10.282: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.283: INFO: csi-diamanti-driver-s59hb from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.283: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:12:10.283: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.283: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Jun  1 18:12:10.290: INFO: nfs-csi-diamanti-driver-dh2xj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:12:10.290: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.290: INFO: csi-diamanti-driver-cg4qz from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:12:10.290: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:12:10.290: INFO: prometheus-v1-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:12:10.290: INFO: csi-external-resizer-65f576d548-skb55 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container csi-external-resizer ready: true, restart count 0
Jun  1 18:12:10.290: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 from sonobuoy started at 2020-06-01 16:25:03 +0000 UTC (2 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:12:10.290: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:12:10.290: INFO: collectd-v0.8-m7sbv from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:12:10.290: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:12:10.290: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:12:10.290: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:12:10.290: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:12:10.290: INFO: csi-external-provisioner-7f56896f9-csfz4 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Jun  1 18:12:10.290: INFO: helm-chart-7fcf79f88c-6pv8f from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container helm-chart ready: true, restart count 0
Jun  1 18:12:10.290: INFO: coredns-6d6bd8df7c-ghgls from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:12:10.290: INFO: csi-external-attacher-846d47f8d6-7zcsv from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:12:10.290: INFO: 	Container csi-attacher ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-64f79ccd-e5bb-4c65-8ca1-5da9d3df807f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-64f79ccd-e5bb-4c65-8ca1-5da9d3df807f off the node appserv9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-64f79ccd-e5bb-4c65-8ca1-5da9d3df807f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:12:22.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2070" for this suite.
Jun  1 18:12:50.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:12:50.456: INFO: namespace sched-pred-2070 deletion completed in 28.093386604s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:40.434 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:12:50.456: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 18:12:50.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065" in namespace "projected-8439" to be "success or failure"
Jun  1 18:12:50.621: INFO: Pod "downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295979ms
Jun  1 18:12:52.624: INFO: Pod "downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005676936s
STEP: Saw pod success
Jun  1 18:12:52.624: INFO: Pod "downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065" satisfied condition "success or failure"
Jun  1 18:12:52.627: INFO: Trying to get logs from node appserv11 pod downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065 container client-container: <nil>
STEP: delete the pod
Jun  1 18:12:52.644: INFO: Waiting for pod downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065 to disappear
Jun  1 18:12:52.646: INFO: Pod downwardapi-volume-9e6b3a1d-edb3-41fe-b510-e4d7e7176065 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:12:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8439" for this suite.
Jun  1 18:12:58.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:12:58.731: INFO: namespace projected-8439 deletion completed in 6.080591457s

• [SLOW TEST:8.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:12:58.731: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 18:12:58.862: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun  1 18:13:00.884: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:13:01.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4535" for this suite.
Jun  1 18:13:07.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:13:07.974: INFO: namespace replication-controller-4535 deletion completed in 6.080260104s

• [SLOW TEST:9.242 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:13:07.974: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  1 18:13:12.132: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:13:12.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9943" for this suite.
Jun  1 18:13:18.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:13:18.556: INFO: namespace container-runtime-9943 deletion completed in 6.409325296s

• [SLOW TEST:10.582 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:13:18.556: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun  1 18:13:21.205: INFO: Successfully updated pod "adopt-release-6q84p"
STEP: Checking that the Job readopts the Pod
Jun  1 18:13:21.205: INFO: Waiting up to 15m0s for pod "adopt-release-6q84p" in namespace "job-1240" to be "adopted"
Jun  1 18:13:21.208: INFO: Pod "adopt-release-6q84p": Phase="Running", Reason="", readiness=true. Elapsed: 2.932ms
Jun  1 18:13:23.211: INFO: Pod "adopt-release-6q84p": Phase="Running", Reason="", readiness=true. Elapsed: 2.006524589s
Jun  1 18:13:23.211: INFO: Pod "adopt-release-6q84p" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun  1 18:13:23.719: INFO: Successfully updated pod "adopt-release-6q84p"
STEP: Checking that the Job releases the Pod
Jun  1 18:13:23.719: INFO: Waiting up to 15m0s for pod "adopt-release-6q84p" in namespace "job-1240" to be "released"
Jun  1 18:13:23.721: INFO: Pod "adopt-release-6q84p": Phase="Running", Reason="", readiness=true. Elapsed: 2.361494ms
Jun  1 18:13:25.724: INFO: Pod "adopt-release-6q84p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005410769s
Jun  1 18:13:25.724: INFO: Pod "adopt-release-6q84p" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:13:25.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1240" for this suite.
Jun  1 18:14:07.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:14:07.800: INFO: namespace job-1240 deletion completed in 42.071651021s

• [SLOW TEST:49.244 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:14:07.800: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 18:14:07.972: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:14:10.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-538" for this suite.
Jun  1 18:14:56.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:14:56.092: INFO: namespace pods-538 deletion completed in 46.07545222s

• [SLOW TEST:48.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:14:56.092: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-53c7cc2a-c8ee-4911-ad85-0076b6a9f860
STEP: Creating a pod to test consume configMaps
Jun  1 18:14:56.231: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3" in namespace "projected-5723" to be "success or failure"
Jun  1 18:14:56.233: INFO: Pod "pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281764ms
Jun  1 18:14:58.237: INFO: Pod "pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005440575s
STEP: Saw pod success
Jun  1 18:14:58.237: INFO: Pod "pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3" satisfied condition "success or failure"
Jun  1 18:14:58.239: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  1 18:14:58.256: INFO: Waiting for pod pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3 to disappear
Jun  1 18:14:58.258: INFO: Pod pod-projected-configmaps-d2b966b2-ab8c-45ad-aa97-58cbd7da20a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:14:58.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5723" for this suite.
Jun  1 18:15:04.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:15:04.353: INFO: namespace projected-5723 deletion completed in 6.091486513s

• [SLOW TEST:8.261 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:15:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Jun  1 18:15:04.492: INFO: Waiting up to 5m0s for pod "client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70" in namespace "containers-592" to be "success or failure"
Jun  1 18:15:04.494: INFO: Pod "client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129378ms
Jun  1 18:15:06.497: INFO: Pod "client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004841665s
STEP: Saw pod success
Jun  1 18:15:06.497: INFO: Pod "client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70" satisfied condition "success or failure"
Jun  1 18:15:06.499: INFO: Trying to get logs from node appserv9 pod client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70 container test-container: <nil>
STEP: delete the pod
Jun  1 18:15:06.516: INFO: Waiting for pod client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70 to disappear
Jun  1 18:15:06.518: INFO: Pod client-containers-b0c5cb95-0eb6-402d-9667-325e06dc0f70 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:15:06.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-592" for this suite.
Jun  1 18:15:12.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:15:12.617: INFO: namespace containers-592 deletion completed in 6.095044408s

• [SLOW TEST:8.264 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:15:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun  1 18:15:12.787: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  1 18:15:12.798: INFO: Waiting for terminating namespaces to be deleted...
Jun  1 18:15:12.801: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Jun  1 18:15:12.859: INFO: collectd-v0.8-hxgzj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:15:12.859: INFO: coredns-6d6bd8df7c-2zcwd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:15:12.859: INFO: csi-external-snapshotter-5f96b86db9-kzt8f from diamanti-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Jun  1 18:15:12.859: INFO: prometheus-v1-2 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:15:12.859: INFO: provisioner-654c8c4db6-zxcsz from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container provisioner ready: true, restart count 0
Jun  1 18:15:12.859: INFO: sonobuoy from sonobuoy started at 2020-06-01 16:25:01 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  1 18:15:12.859: INFO: csi-diamanti-driver-lqbpt from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container diamanticsidriver ready: true, restart count 1
Jun  1 18:15:12.859: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.859: INFO: nfs-csi-diamanti-driver-cht54 from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.859: INFO: metrics-server-v1-7659784467-v66bd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container metrics-server ready: true, restart count 0
Jun  1 18:15:12.859: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:15:12.859: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:15:12.859: INFO: sonobuoy-e2e-job-5a0f7ba7dfe14684 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.859: INFO: 	Container e2e ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 18:15:12.859: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Jun  1 18:15:12.916: INFO: coredns-6d6bd8df7c-bnq4n from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:15:12.916: INFO: prometheus-v1-1 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:15:12.916: INFO: tiller-deploy-57f5b6fd78-574p5 from kube-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container tiller ready: true, restart count 0
Jun  1 18:15:12.916: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:15:12.916: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:15:12.916: INFO: nfs-csi-diamanti-driver-h2dts from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.916: INFO: csi-diamanti-driver-s59hb from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:15:12.916: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.916: INFO: collectd-v0.8-8fjnl from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (5 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:15:12.916: INFO: alertmanager-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container alertmanager ready: true, restart count 0
Jun  1 18:15:12.916: INFO: snapshot-controller-798c69596d-nsc4s from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.916: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jun  1 18:15:12.916: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Jun  1 18:15:12.924: INFO: csi-external-resizer-65f576d548-skb55 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container csi-external-resizer ready: true, restart count 0
Jun  1 18:15:12.924: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 from sonobuoy started at 2020-06-01 16:25:03 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:15:12.924: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:15:12.924: INFO: collectd-v0.8-m7sbv from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:15:12.924: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:15:12.924: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:15:12.924: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:15:12.924: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:15:12.924: INFO: csi-external-provisioner-7f56896f9-csfz4 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Jun  1 18:15:12.924: INFO: helm-chart-7fcf79f88c-6pv8f from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container helm-chart ready: true, restart count 0
Jun  1 18:15:12.924: INFO: coredns-6d6bd8df7c-ghgls from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:15:12.924: INFO: csi-external-attacher-846d47f8d6-7zcsv from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  1 18:15:12.924: INFO: nfs-csi-diamanti-driver-dh2xj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:15:12.924: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.924: INFO: csi-diamanti-driver-cg4qz from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:15:12.924: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:15:12.924: INFO: prometheus-v1-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:15:12.924: INFO: 	Container prometheus ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bd5955fe-6e02-45c4-bff7-d6c74cdbbc51 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-bd5955fe-6e02-45c4-bff7-d6c74cdbbc51 off the node appserv9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bd5955fe-6e02-45c4-bff7-d6c74cdbbc51
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:15:18.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8743" for this suite.
Jun  1 18:15:36.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:15:37.064: INFO: namespace sched-pred-8743 deletion completed in 18.085184402s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:24.447 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:15:37.064: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9926
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9926 to expose endpoints map[]
Jun  1 18:15:37.205: INFO: Get endpoints failed (2.4515ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun  1 18:15:38.208: INFO: successfully validated that service multi-endpoint-test in namespace services-9926 exposes endpoints map[] (1.00516481s elapsed)
STEP: Creating pod pod1 in namespace services-9926
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9926 to expose endpoints map[pod1:[100]]
Jun  1 18:15:40.229: INFO: successfully validated that service multi-endpoint-test in namespace services-9926 exposes endpoints map[pod1:[100]] (2.015081601s elapsed)
STEP: Creating pod pod2 in namespace services-9926
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9926 to expose endpoints map[pod1:[100] pod2:[101]]
Jun  1 18:15:42.256: INFO: successfully validated that service multi-endpoint-test in namespace services-9926 exposes endpoints map[pod1:[100] pod2:[101]] (2.023783376s elapsed)
STEP: Deleting pod pod1 in namespace services-9926
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9926 to expose endpoints map[pod2:[101]]
Jun  1 18:15:43.273: INFO: successfully validated that service multi-endpoint-test in namespace services-9926 exposes endpoints map[pod2:[101]] (1.011317699s elapsed)
STEP: Deleting pod pod2 in namespace services-9926
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9926 to expose endpoints map[]
Jun  1 18:15:44.283: INFO: successfully validated that service multi-endpoint-test in namespace services-9926 exposes endpoints map[] (1.005488847s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:15:44.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9926" for this suite.
Jun  1 18:16:12.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:16:12.471: INFO: namespace services-9926 deletion completed in 28.171483646s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:35.407 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:16:12.471: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4176
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4176
I0601 18:16:12.620666      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4176, replica count: 2
Jun  1 18:16:15.671: INFO: Creating new exec pod
I0601 18:16:15.671119      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  1 18:16:20.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-4176 execpod4pksr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  1 18:16:20.927: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  1 18:16:20.928: INFO: stdout: ""
Jun  1 18:16:20.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-621748879 exec --namespace=services-4176 execpod4pksr -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.8 80'
Jun  1 18:16:21.142: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.8 80\nConnection to 10.0.0.8 80 port [tcp/http] succeeded!\n"
Jun  1 18:16:21.142: INFO: stdout: ""
Jun  1 18:16:21.142: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:16:21.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4176" for this suite.
Jun  1 18:16:27.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:16:27.274: INFO: namespace services-4176 deletion completed in 6.114702702s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:14.802 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:16:27.274: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun  1 18:16:27.412: INFO: Waiting up to 5m0s for pod "downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31" in namespace "downward-api-4123" to be "success or failure"
Jun  1 18:16:27.415: INFO: Pod "downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.394736ms
Jun  1 18:16:29.418: INFO: Pod "downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006263297s
Jun  1 18:16:31.422: INFO: Pod "downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009936618s
STEP: Saw pod success
Jun  1 18:16:31.422: INFO: Pod "downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31" satisfied condition "success or failure"
Jun  1 18:16:31.425: INFO: Trying to get logs from node appserv9 pod downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31 container dapi-container: <nil>
STEP: delete the pod
Jun  1 18:16:31.445: INFO: Waiting for pod downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31 to disappear
Jun  1 18:16:31.447: INFO: Pod downward-api-6b1a6772-8dae-4686-963e-2f45cd23bc31 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:16:31.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4123" for this suite.
Jun  1 18:16:37.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:16:37.535: INFO: namespace downward-api-4123 deletion completed in 6.08370737s

• [SLOW TEST:10.261 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:16:37.535: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-33675e93-d0db-42f2-8832-39e103a02c5a in namespace container-probe-8816
Jun  1 18:16:41.677: INFO: Started pod liveness-33675e93-d0db-42f2-8832-39e103a02c5a in namespace container-probe-8816
STEP: checking the pod's current state and verifying that restartCount is present
Jun  1 18:16:41.680: INFO: Initial restart count of pod liveness-33675e93-d0db-42f2-8832-39e103a02c5a is 0
Jun  1 18:16:53.703: INFO: Restart count of pod container-probe-8816/liveness-33675e93-d0db-42f2-8832-39e103a02c5a is now 1 (12.02258409s elapsed)
Jun  1 18:17:13.737: INFO: Restart count of pod container-probe-8816/liveness-33675e93-d0db-42f2-8832-39e103a02c5a is now 2 (32.056732279s elapsed)
Jun  1 18:17:33.770: INFO: Restart count of pod container-probe-8816/liveness-33675e93-d0db-42f2-8832-39e103a02c5a is now 3 (52.089927668s elapsed)
Jun  1 18:17:55.807: INFO: Restart count of pod container-probe-8816/liveness-33675e93-d0db-42f2-8832-39e103a02c5a is now 4 (1m14.126641094s elapsed)
Jun  1 18:19:07.932: INFO: Restart count of pod container-probe-8816/liveness-33675e93-d0db-42f2-8832-39e103a02c5a is now 5 (2m26.251162792s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:19:07.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8816" for this suite.
Jun  1 18:19:13.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:19:14.071: INFO: namespace container-probe-8816 deletion completed in 6.128810117s

• [SLOW TEST:156.535 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:19:14.071: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:19:14.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9796" for this suite.
Jun  1 18:19:20.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:19:20.337: INFO: namespace services-9796 deletion completed in 6.127050267s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.266 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:19:20.337: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun  1 18:19:20.469: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  1 18:19:20.480: INFO: Waiting for terminating namespaces to be deleted...
Jun  1 18:19:20.482: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Jun  1 18:19:20.541: INFO: csi-diamanti-driver-lqbpt from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container diamanticsidriver ready: true, restart count 1
Jun  1 18:19:20.541: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.541: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:19:20.541: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:19:20.541: INFO: nfs-csi-diamanti-driver-cht54 from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.541: INFO: metrics-server-v1-7659784467-v66bd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container metrics-server ready: true, restart count 0
Jun  1 18:19:20.541: INFO: sonobuoy-e2e-job-5a0f7ba7dfe14684 from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container e2e ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  1 18:19:20.541: INFO: csi-external-snapshotter-5f96b86db9-kzt8f from diamanti-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Jun  1 18:19:20.541: INFO: prometheus-v1-2 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:19:20.541: INFO: provisioner-654c8c4db6-zxcsz from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container provisioner ready: true, restart count 0
Jun  1 18:19:20.541: INFO: sonobuoy from sonobuoy started at 2020-06-01 16:25:01 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  1 18:19:20.541: INFO: collectd-v0.8-hxgzj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:19:20.541: INFO: coredns-6d6bd8df7c-2zcwd from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.541: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:19:20.541: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Jun  1 18:19:20.559: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k from sonobuoy started at 2020-06-01 16:25:02 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:19:20.559: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  1 18:19:20.559: INFO: coredns-6d6bd8df7c-bnq4n from kube-system started at 2020-06-01 16:24:11 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:19:20.559: INFO: prometheus-v1-1 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:19:20.559: INFO: tiller-deploy-57f5b6fd78-574p5 from kube-system started at 2020-06-01 16:24:13 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container tiller ready: true, restart count 0
Jun  1 18:19:20.559: INFO: nfs-csi-diamanti-driver-h2dts from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.559: INFO: csi-diamanti-driver-s59hb from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:19:20.559: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.559: INFO: collectd-v0.8-8fjnl from diamanti-system started at 2020-06-01 16:24:04 +0000 UTC (5 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:19:20.559: INFO: alertmanager-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container alertmanager ready: true, restart count 0
Jun  1 18:19:20.559: INFO: snapshot-controller-798c69596d-nsc4s from diamanti-system started at 2020-06-01 16:24:13 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.559: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jun  1 18:19:20.559: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Jun  1 18:19:20.577: INFO: collectd-v0.8-m7sbv from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (5 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container cadvisor ready: true, restart count 0
Jun  1 18:19:20.577: INFO: 	Container collectd-es ready: true, restart count 0
Jun  1 18:19:20.577: INFO: 	Container collectd-exporter ready: true, restart count 0
Jun  1 18:19:20.577: INFO: 	Container node-exporter ready: true, restart count 0
Jun  1 18:19:20.577: INFO: 	Container nvidia-dcgm-exporter ready: true, restart count 0
Jun  1 18:19:20.577: INFO: csi-external-provisioner-7f56896f9-csfz4 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Jun  1 18:19:20.577: INFO: helm-chart-7fcf79f88c-6pv8f from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container helm-chart ready: true, restart count 0
Jun  1 18:19:20.577: INFO: coredns-6d6bd8df7c-ghgls from kube-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container coredns ready: true, restart count 0
Jun  1 18:19:20.577: INFO: csi-external-attacher-846d47f8d6-7zcsv from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  1 18:19:20.577: INFO: nfs-csi-diamanti-driver-dh2xj from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container diamantinfscsidriver ready: true, restart count 0
Jun  1 18:19:20.577: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.577: INFO: csi-diamanti-driver-cg4qz from diamanti-system started at 2020-06-01 16:24:05 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container diamanticsidriver ready: true, restart count 2
Jun  1 18:19:20.577: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun  1 18:19:20.577: INFO: prometheus-v1-0 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container prometheus ready: true, restart count 0
Jun  1 18:19:20.577: INFO: csi-external-resizer-65f576d548-skb55 from diamanti-system started at 2020-06-01 16:24:12 +0000 UTC (1 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container csi-external-resizer ready: true, restart count 0
Jun  1 18:19:20.577: INFO: sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 from sonobuoy started at 2020-06-01 16:25:03 +0000 UTC (2 container statuses recorded)
Jun  1 18:19:20.577: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  1 18:19:20.577: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node appserv10
STEP: verifying the node has the label node appserv11
STEP: verifying the node has the label node appserv9
Jun  1 18:19:20.616: INFO: Pod alertmanager-0 requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod collectd-v0.8-8fjnl requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod collectd-v0.8-hxgzj requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod collectd-v0.8-m7sbv requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod csi-diamanti-driver-cg4qz requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod csi-diamanti-driver-lqbpt requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod csi-diamanti-driver-s59hb requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod csi-external-attacher-846d47f8d6-7zcsv requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod csi-external-provisioner-7f56896f9-csfz4 requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod csi-external-resizer-65f576d548-skb55 requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod csi-external-snapshotter-5f96b86db9-kzt8f requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod nfs-csi-diamanti-driver-cht54 requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod nfs-csi-diamanti-driver-dh2xj requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod nfs-csi-diamanti-driver-h2dts requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod prometheus-v1-0 requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod prometheus-v1-1 requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod prometheus-v1-2 requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod provisioner-654c8c4db6-zxcsz requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod snapshot-controller-798c69596d-nsc4s requesting resource cpu=0m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod coredns-6d6bd8df7c-2zcwd requesting resource cpu=100m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod coredns-6d6bd8df7c-bnq4n requesting resource cpu=100m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod coredns-6d6bd8df7c-ghgls requesting resource cpu=100m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod helm-chart-7fcf79f88c-6pv8f requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod metrics-server-v1-7659784467-v66bd requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod tiller-deploy-57f5b6fd78-574p5 requesting resource cpu=250m on Node appserv11
Jun  1 18:19:20.616: INFO: Pod sonobuoy requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod sonobuoy-e2e-job-5a0f7ba7dfe14684 requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6cwq4 requesting resource cpu=0m on Node appserv9
Jun  1 18:19:20.616: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-6nn72 requesting resource cpu=0m on Node appserv10
Jun  1 18:19:20.616: INFO: Pod sonobuoy-systemd-logs-daemon-set-ff2a55b97bdc4fe3-hf75k requesting resource cpu=0m on Node appserv11
STEP: Starting Pods to consume most of the cluster CPU.
Jun  1 18:19:20.616: INFO: Creating a pod which consumes cpu=22330m on Node appserv10
Jun  1 18:19:20.621: INFO: Creating a pod which consumes cpu=22155m on Node appserv11
Jun  1 18:19:20.624: INFO: Creating a pod which consumes cpu=22330m on Node appserv9
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6.16147e69ab860f4f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7145/filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6 to appserv10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6.16147e69ee7688a8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6.16147e69f10d0daa], Reason = [Created], Message = [Created container filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6.16147e69fc2c1190], Reason = [Started], Message = [Started container filler-pod-2f911867-af97-4e53-ad4e-4df24721acd6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec.16147e69b2adef7c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7145/filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec to appserv9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec.16147e69f51d9f7c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec.16147e69f7946157], Reason = [Created], Message = [Created container filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec.16147e6a028db7f1], Reason = [Started], Message = [Started container filler-pod-519f8b75-7319-4d96-999e-ac3c4d785bec]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0.16147e69af030b0c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7145/filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0 to appserv11]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0.16147e69f1c1c261], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0.16147e69f43a034b], Reason = [Created], Message = [Created container filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0.16147e69ff4e9d84], Reason = [Started], Message = [Started container filler-pod-dc17c9c1-bf5a-4a8c-a84f-6db192401ac0]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16147e6a20a6dbf1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node appserv10
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv11
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:19:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7145" for this suite.
Jun  1 18:19:29.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:19:29.779: INFO: namespace sched-pred-7145 deletion completed in 6.09248682s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:9.442 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:19:29.779: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4200
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4548
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:19:45.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2775" for this suite.
Jun  1 18:19:51.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:19:51.291: INFO: namespace namespaces-2775 deletion completed in 6.090373542s
STEP: Destroying namespace "nsdeletetest-4200" for this suite.
Jun  1 18:19:51.294: INFO: Namespace nsdeletetest-4200 was already deleted
STEP: Destroying namespace "nsdeletetest-4548" for this suite.
Jun  1 18:19:57.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:19:57.462: INFO: namespace nsdeletetest-4548 deletion completed in 6.168412516s

• [SLOW TEST:27.683 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:19:57.462: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  1 18:19:58.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  1 18:20:00.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726632398, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726632398, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63726632398, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63726632398, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  1 18:20:03.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun  1 18:20:03.226: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3895-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:20:04.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6586" for this suite.
Jun  1 18:20:10.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:20:10.505: INFO: namespace webhook-6586 deletion completed in 6.087177332s
STEP: Destroying namespace "webhook-6586-markers" for this suite.
Jun  1 18:20:16.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:20:16.645: INFO: namespace webhook-6586-markers deletion completed in 6.140091862s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.195 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:20:16.658: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  1 18:20:16.795: INFO: Waiting up to 5m0s for pod "pod-532f3b3e-029f-454d-8d74-bda01ec460ba" in namespace "emptydir-971" to be "success or failure"
Jun  1 18:20:16.797: INFO: Pod "pod-532f3b3e-029f-454d-8d74-bda01ec460ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168418ms
Jun  1 18:20:18.800: INFO: Pod "pod-532f3b3e-029f-454d-8d74-bda01ec460ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.004839243s
Jun  1 18:20:20.803: INFO: Pod "pod-532f3b3e-029f-454d-8d74-bda01ec460ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007925137s
STEP: Saw pod success
Jun  1 18:20:20.803: INFO: Pod "pod-532f3b3e-029f-454d-8d74-bda01ec460ba" satisfied condition "success or failure"
Jun  1 18:20:20.806: INFO: Trying to get logs from node appserv11 pod pod-532f3b3e-029f-454d-8d74-bda01ec460ba container test-container: <nil>
STEP: delete the pod
Jun  1 18:20:20.821: INFO: Waiting for pod pod-532f3b3e-029f-454d-8d74-bda01ec460ba to disappear
Jun  1 18:20:20.823: INFO: Pod pod-532f3b3e-029f-454d-8d74-bda01ec460ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:20:20.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-971" for this suite.
Jun  1 18:20:26.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:20:26.914: INFO: namespace emptydir-971 deletion completed in 6.087296937s

• [SLOW TEST:10.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:20:26.915: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3888.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3888.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3888.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3888.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3888.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3888.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 18:20:29.118: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3888/dns-test-20626503-85b4-4752-8194-097cd1e6b914: the server could not find the requested resource (get pods dns-test-20626503-85b4-4752-8194-097cd1e6b914)
Jun  1 18:20:29.121: INFO: Unable to read jessie_udp@PodARecord from pod dns-3888/dns-test-20626503-85b4-4752-8194-097cd1e6b914: the server could not find the requested resource (get pods dns-test-20626503-85b4-4752-8194-097cd1e6b914)
Jun  1 18:20:29.123: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3888/dns-test-20626503-85b4-4752-8194-097cd1e6b914: the server could not find the requested resource (get pods dns-test-20626503-85b4-4752-8194-097cd1e6b914)
Jun  1 18:20:29.123: INFO: Lookups using dns-3888/dns-test-20626503-85b4-4752-8194-097cd1e6b914 failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  1 18:20:34.150: INFO: DNS probes using dns-3888/dns-test-20626503-85b4-4752-8194-097cd1e6b914 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:20:34.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3888" for this suite.
Jun  1 18:20:40.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:20:40.255: INFO: namespace dns-3888 deletion completed in 6.090534477s

• [SLOW TEST:13.341 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:20:40.256: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-85
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-b5e4469b-72f5-4315-bbc8-a532ea96aaac
STEP: Creating a pod to test consume secrets
Jun  1 18:20:40.398: INFO: Waiting up to 5m0s for pod "pod-secrets-afba799d-1ec4-470d-8041-e2404088a457" in namespace "secrets-85" to be "success or failure"
Jun  1 18:20:40.401: INFO: Pod "pod-secrets-afba799d-1ec4-470d-8041-e2404088a457": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271995ms
Jun  1 18:20:42.404: INFO: Pod "pod-secrets-afba799d-1ec4-470d-8041-e2404088a457": Phase="Running", Reason="", readiness=true. Elapsed: 2.005704449s
Jun  1 18:20:44.408: INFO: Pod "pod-secrets-afba799d-1ec4-470d-8041-e2404088a457": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009550562s
STEP: Saw pod success
Jun  1 18:20:44.408: INFO: Pod "pod-secrets-afba799d-1ec4-470d-8041-e2404088a457" satisfied condition "success or failure"
Jun  1 18:20:44.411: INFO: Trying to get logs from node appserv9 pod pod-secrets-afba799d-1ec4-470d-8041-e2404088a457 container secret-volume-test: <nil>
STEP: delete the pod
Jun  1 18:20:44.429: INFO: Waiting for pod pod-secrets-afba799d-1ec4-470d-8041-e2404088a457 to disappear
Jun  1 18:20:44.431: INFO: Pod pod-secrets-afba799d-1ec4-470d-8041-e2404088a457 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:20:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-85" for this suite.
Jun  1 18:20:50.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:20:50.526: INFO: namespace secrets-85 deletion completed in 6.090715388s

• [SLOW TEST:10.270 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:20:50.527: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun  1 18:20:50.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019" in namespace "downward-api-6255" to be "success or failure"
Jun  1 18:20:50.667: INFO: Pod "downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019": Phase="Pending", Reason="", readiness=false. Elapsed: 3.240957ms
Jun  1 18:20:52.671: INFO: Pod "downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006966831s
Jun  1 18:20:54.675: INFO: Pod "downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010603792s
STEP: Saw pod success
Jun  1 18:20:54.675: INFO: Pod "downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019" satisfied condition "success or failure"
Jun  1 18:20:54.677: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019 container client-container: <nil>
STEP: delete the pod
Jun  1 18:20:54.694: INFO: Waiting for pod downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019 to disappear
Jun  1 18:20:54.697: INFO: Pod downwardapi-volume-47144115-7224-4b7d-922c-abfe1bc9c019 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:20:54.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6255" for this suite.
Jun  1 18:21:00.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:21:00.792: INFO: namespace downward-api-6255 deletion completed in 6.091014625s

• [SLOW TEST:10.265 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:21:00.792: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6697.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6697.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  1 18:21:04.965: INFO: DNS probes using dns-6697/dns-test-ccfffe23-c442-42b3-9612-1ea0a73d09c9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:21:04.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6697" for this suite.
Jun  1 18:21:10.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:21:11.061: INFO: namespace dns-6697 deletion completed in 6.083549301s

• [SLOW TEST:10.269 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  1 18:21:11.061: INFO: >>> kubeConfig: /tmp/kubeconfig-621748879
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  1 18:21:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8276" for this suite.
Jun  1 18:21:17.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun  1 18:21:17.975: INFO: namespace tables-8276 deletion completed in 6.287373859s

• [SLOW TEST:6.914 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSJun  1 18:21:17.975: INFO: Running AfterSuite actions on all nodes
Jun  1 18:21:17.975: INFO: Running AfterSuite actions on node 1
Jun  1 18:21:17.975: INFO: Skipping dumping logs from cluster

Ran 274 of 4731 Specs in 6946.149 seconds
SUCCESS! -- 274 Passed | 0 Failed | 0 Pending | 4457 Skipped
PASS

Ginkgo ran 1 suite in 1h55m48.030394514s
Test Suite Passed
