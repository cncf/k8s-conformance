I0229 23:35:54.991061      21 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-585087022
I0229 23:35:54.991236      21 e2e.go:92] Starting e2e run "e95d0896-83db-42a2-8ffd-8d7987332da2" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583019352 - Will randomize all specs
Will run 276 of 4732 specs

Feb 29 23:35:55.015: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:35:55.019: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 29 23:35:55.051: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 29 23:35:55.129: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 29 23:35:55.130: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Feb 29 23:35:55.130: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 29 23:35:55.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 29 23:35:55.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 29 23:35:55.142: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Feb 29 23:35:55.142: INFO: e2e test version: v1.16.3
Feb 29 23:35:55.144: INFO: kube-apiserver version: v1.16.3
Feb 29 23:35:55.144: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:35:55.151: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:35:55.153: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
Feb 29 23:35:55.253: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2093
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2093
I0229 23:35:55.309944      21 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2093, replica count: 2
I0229 23:35:58.364514      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0229 23:36:01.364980      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 29 23:36:04.365: INFO: Creating new exec pod
I0229 23:36:04.365558      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 29 23:36:09.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 29 23:36:10.259: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 29 23:36:10.259: INFO: stdout: ""
Feb 29 23:36:10.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 10.96.146.255 80'
Feb 29 23:36:10.546: INFO: stderr: "+ nc -zv -t -w 2 10.96.146.255 80\nConnection to 10.96.146.255 80 port [tcp/http] succeeded!\n"
Feb 29 23:36:10.546: INFO: stdout: ""
Feb 29 23:36:10.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.22 30575'
Feb 29 23:36:10.834: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.22 30575\nConnection to 10.10.128.22 30575 port [tcp/30575] succeeded!\n"
Feb 29 23:36:10.834: INFO: stdout: ""
Feb 29 23:36:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.24 30575'
Feb 29 23:36:11.191: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.24 30575\nConnection to 10.10.128.24 30575 port [tcp/30575] succeeded!\n"
Feb 29 23:36:11.191: INFO: stdout: ""
Feb 29 23:36:11.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.22 30575'
Feb 29 23:36:11.487: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.22 30575\nConnection to 10.10.128.22 30575 port [tcp/30575] succeeded!\n"
Feb 29 23:36:11.487: INFO: stdout: ""
Feb 29 23:36:11.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-2093 execpod2kvw4 -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.24 30575'
Feb 29 23:36:11.763: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.24 30575\nConnection to 10.10.128.24 30575 port [tcp/30575] succeeded!\n"
Feb 29 23:36:11.763: INFO: stdout: ""
Feb 29 23:36:11.763: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:36:11.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2093" for this suite.
Feb 29 23:36:17.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:36:17.944: INFO: namespace services-2093 deletion completed in 6.144915388s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:22.792 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:36:17.947: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:36:17.987: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 29 23:36:23.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-7305 create -f -'
Feb 29 23:36:24.112: INFO: stderr: ""
Feb 29 23:36:24.112: INFO: stdout: "e2e-test-crd-publish-openapi-1871-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 29 23:36:24.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-7305 delete e2e-test-crd-publish-openapi-1871-crds test-cr'
Feb 29 23:36:24.233: INFO: stderr: ""
Feb 29 23:36:24.233: INFO: stdout: "e2e-test-crd-publish-openapi-1871-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 29 23:36:24.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-7305 apply -f -'
Feb 29 23:36:24.513: INFO: stderr: ""
Feb 29 23:36:24.513: INFO: stdout: "e2e-test-crd-publish-openapi-1871-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 29 23:36:24.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-7305 delete e2e-test-crd-publish-openapi-1871-crds test-cr'
Feb 29 23:36:24.627: INFO: stderr: ""
Feb 29 23:36:24.627: INFO: stdout: "e2e-test-crd-publish-openapi-1871-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 29 23:36:24.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1871-crds'
Feb 29 23:36:24.936: INFO: stderr: ""
Feb 29 23:36:24.936: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1871-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:36:30.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7305" for this suite.
Feb 29 23:36:36.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:36:36.906: INFO: namespace crd-publish-openapi-7305 deletion completed in 6.252152907s

• [SLOW TEST:18.959 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:36:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 29 23:36:36.983: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 29 23:36:37.039: INFO: Waiting for terminating namespaces to be deleted...
Feb 29 23:36:37.043: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-14d9760cdc before test
Feb 29 23:36:37.081: INFO: nginx-ingress-default-backend-8465968b95-q4zrk from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 29 23:36:37.081: INFO: metallb-controller-6c8c5fd7fd-vzjj8 from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container metallb-controller ready: true, restart count 0
Feb 29 23:36:37.081: INFO: coredns-65f5bff896-v5nsg from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container coredns ready: true, restart count 0
Feb 29 23:36:37.081: INFO: calico-node-mgknx from kube-system started at 2020-02-29 23:14:56 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:36:37.081: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:36:37.081: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:36:37.081: INFO: metallb-speaker-f4prv from ccp started at 2020-02-29 23:15:14 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.081: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:36:37.082: INFO: nginx-ingress-controller-7q5kc from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.082: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 29 23:36:37.082: INFO: calico-kube-controllers-67878b79d6-4qj8x from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.082: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 29 23:36:37.082: INFO: kube-proxy-kwv7g from kube-system started at 2020-02-29 23:14:35 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.082: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:36:37.082: INFO: nvidia-device-plugin-daemonset-ptdg5 from kube-system started at 2020-02-29 23:14:59 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.082: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:36:37.082: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-5b74768057 before test
Feb 29 23:36:37.126: INFO: ccp-helm-operator-5ff4f4944c-xqbq6 from ccp started at 2020-02-29 23:26:54 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.126: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Feb 29 23:36:37.127: INFO: sonobuoy-e2e-job-a8e644cf92284639 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:36:37.127: INFO: 	Container e2e ready: true, restart count 0
Feb 29 23:36:37.127: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:36:37.127: INFO: kube-proxy-n5h6f from kube-system started at 2020-02-29 23:16:04 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.127: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:36:37.127: INFO: calico-node-xvtvr from kube-system started at 2020-02-29 23:16:05 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.127: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:36:37.128: INFO: cert-manager-7c4fdf69b7-cxg76 from ccp started at 2020-02-29 23:26:26 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container cert-manager ready: true, restart count 0
Feb 29 23:36:37.128: INFO: sonobuoy from sonobuoy started at 2020-02-29 23:35:00 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 29 23:36:37.128: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:36:37.128: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:36:37.128: INFO: nvidia-device-plugin-daemonset-vx9dx from kube-system started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:36:37.128: INFO: metallb-speaker-gfzhh from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:36:37.128: INFO: nginx-ingress-controller-bhmwg from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:36:37.128: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f803c0fdcd2365], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:36:38.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2381" for this suite.
Feb 29 23:36:44.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:36:44.317: INFO: namespace sched-pred-2381 deletion completed in 6.135033213s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.410 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:36:44.323: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 29 23:36:44.376: INFO: Waiting up to 5m0s for pod "pod-40cc9fef-6087-4bc8-a48f-1257657aeae8" in namespace "emptydir-4497" to be "success or failure"
Feb 29 23:36:44.387: INFO: Pod "pod-40cc9fef-6087-4bc8-a48f-1257657aeae8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.991367ms
Feb 29 23:36:46.392: INFO: Pod "pod-40cc9fef-6087-4bc8-a48f-1257657aeae8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01623643s
Feb 29 23:36:48.396: INFO: Pod "pod-40cc9fef-6087-4bc8-a48f-1257657aeae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020375201s
STEP: Saw pod success
Feb 29 23:36:48.396: INFO: Pod "pod-40cc9fef-6087-4bc8-a48f-1257657aeae8" satisfied condition "success or failure"
Feb 29 23:36:48.400: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-40cc9fef-6087-4bc8-a48f-1257657aeae8 container test-container: <nil>
STEP: delete the pod
Feb 29 23:36:48.436: INFO: Waiting for pod pod-40cc9fef-6087-4bc8-a48f-1257657aeae8 to disappear
Feb 29 23:36:48.457: INFO: Pod pod-40cc9fef-6087-4bc8-a48f-1257657aeae8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:36:48.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4497" for this suite.
Feb 29 23:36:54.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:36:54.670: INFO: namespace emptydir-4497 deletion completed in 6.199893758s

• [SLOW TEST:10.348 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:36:54.673: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6179
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 29 23:36:54.717: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 29 23:37:20.847: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.4.14:8080/dial?request=hostName&protocol=udp&host=192.168.5.10&port=8081&tries=1'] Namespace:pod-network-test-6179 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:37:20.847: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:37:21.015: INFO: Waiting for endpoints: map[]
Feb 29 23:37:21.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.4.14:8080/dial?request=hostName&protocol=udp&host=192.168.4.13&port=8081&tries=1'] Namespace:pod-network-test-6179 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:37:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:37:21.227: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:37:21.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6179" for this suite.
Feb 29 23:37:33.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:37:33.491: INFO: namespace pod-network-test-6179 deletion completed in 12.257781574s

• [SLOW TEST:38.818 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:37:33.492: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:37:33.621: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 29 23:37:35.714: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:37:35.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7290" for this suite.
Feb 29 23:37:41.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:37:42.010: INFO: namespace replication-controller-7290 deletion completed in 6.286104998s

• [SLOW TEST:8.518 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:37:42.010: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-c8a2b8fb-b86e-4d6a-b82c-cc0df5017103
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:37:48.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9917" for this suite.
Feb 29 23:38:00.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:38:00.365: INFO: namespace configmap-9917 deletion completed in 12.143210558s

• [SLOW TEST:18.355 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:38:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:38:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9810" for this suite.
Feb 29 23:38:16.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:38:16.593: INFO: namespace job-9810 deletion completed in 6.165544554s

• [SLOW TEST:16.223 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:38:16.598: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:38:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4033" for this suite.
Feb 29 23:38:22.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:38:22.853: INFO: namespace custom-resource-definition-4033 deletion completed in 6.189629856s

• [SLOW TEST:6.255 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:38:22.857: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 29 23:38:23.066: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 29 23:38:23.113: INFO: Waiting for terminating namespaces to be deleted...
Feb 29 23:38:23.116: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-14d9760cdc before test
Feb 29 23:38:23.127: INFO: kube-proxy-kwv7g from kube-system started at 2020-02-29 23:14:35 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:38:23.128: INFO: nvidia-device-plugin-daemonset-ptdg5 from kube-system started at 2020-02-29 23:14:59 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:38:23.128: INFO: metallb-speaker-f4prv from ccp started at 2020-02-29 23:15:14 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:38:23.128: INFO: nginx-ingress-controller-7q5kc from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 29 23:38:23.128: INFO: calico-kube-controllers-67878b79d6-4qj8x from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 29 23:38:23.128: INFO: calico-node-mgknx from kube-system started at 2020-02-29 23:14:56 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:38:23.128: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:38:23.128: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:38:23.128: INFO: nginx-ingress-default-backend-8465968b95-q4zrk from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 29 23:38:23.128: INFO: metallb-controller-6c8c5fd7fd-vzjj8 from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container metallb-controller ready: true, restart count 0
Feb 29 23:38:23.128: INFO: coredns-65f5bff896-v5nsg from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.128: INFO: 	Container coredns ready: true, restart count 0
Feb 29 23:38:23.128: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-5b74768057 before test
Feb 29 23:38:23.148: INFO: nvidia-device-plugin-daemonset-vx9dx from kube-system started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.149: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:38:23.149: INFO: metallb-speaker-gfzhh from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.149: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:38:23.149: INFO: nginx-ingress-controller-bhmwg from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.150: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 29 23:38:23.150: INFO: sonobuoy from sonobuoy started at 2020-02-29 23:35:00 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.150: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 29 23:38:23.150: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:38:23.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:38:23.150: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:38:23.151: INFO: kube-proxy-n5h6f from kube-system started at 2020-02-29 23:16:04 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.151: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:38:23.151: INFO: calico-node-xvtvr from kube-system started at 2020-02-29 23:16:05 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.151: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:38:23.152: INFO: cert-manager-7c4fdf69b7-cxg76 from ccp started at 2020-02-29 23:26:26 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.152: INFO: 	Container cert-manager ready: true, restart count 0
Feb 29 23:38:23.152: INFO: ccp-helm-operator-5ff4f4944c-xqbq6 from ccp started at 2020-02-29 23:26:54 +0000 UTC (1 container statuses recorded)
Feb 29 23:38:23.152: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Feb 29 23:38:23.152: INFO: sonobuoy-e2e-job-a8e644cf92284639 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:38:23.152: INFO: 	Container e2e ready: true, restart count 0
Feb 29 23:38:23.153: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6f6040f1-adbe-4c86-8413-0a51123331d5 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-6f6040f1-adbe-4c86-8413-0a51123331d5 off the node alex-slot1-v3-vsp1-node-group-5b74768057
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6f6040f1-adbe-4c86-8413-0a51123331d5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:38:39.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5184" for this suite.
Feb 29 23:39:09.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:39:09.513: INFO: namespace sched-pred-5184 deletion completed in 30.171293448s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:46.657 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:39:09.514: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-688
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 29 23:39:09.566: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 29 23:39:31.690: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.5.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:39:31.690: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:39:31.876: INFO: Found all expected endpoints: [netserver-0]
Feb 29 23:39:31.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.4.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-688 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:39:31.879: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:39:32.046: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:39:32.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-688" for this suite.
Feb 29 23:39:44.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:39:44.198: INFO: namespace pod-network-test-688 deletion completed in 12.146482614s

• [SLOW TEST:34.684 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:39:44.198: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5485/configmap-test-e886c06e-095b-44a6-ba4b-c4b90d2c0411
STEP: Creating a pod to test consume configMaps
Feb 29 23:39:44.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e" in namespace "configmap-5485" to be "success or failure"
Feb 29 23:39:44.265: INFO: Pod "pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538771ms
Feb 29 23:39:46.269: INFO: Pod "pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009720678s
Feb 29 23:39:48.275: INFO: Pod "pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014866802s
STEP: Saw pod success
Feb 29 23:39:48.275: INFO: Pod "pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e" satisfied condition "success or failure"
Feb 29 23:39:48.280: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e container env-test: <nil>
STEP: delete the pod
Feb 29 23:39:48.305: INFO: Waiting for pod pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e to disappear
Feb 29 23:39:48.308: INFO: Pod pod-configmaps-4e2e4907-6c29-49b9-a244-7be99b42526e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:39:48.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5485" for this suite.
Feb 29 23:39:54.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:39:54.483: INFO: namespace configmap-5485 deletion completed in 6.170932074s

• [SLOW TEST:10.285 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:39:54.484: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 29 23:39:54.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104" in namespace "projected-9770" to be "success or failure"
Feb 29 23:39:54.562: INFO: Pod "downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104": Phase="Pending", Reason="", readiness=false. Elapsed: 10.524678ms
Feb 29 23:39:56.565: INFO: Pod "downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014055782s
Feb 29 23:39:58.577: INFO: Pod "downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02607465s
STEP: Saw pod success
Feb 29 23:39:58.577: INFO: Pod "downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104" satisfied condition "success or failure"
Feb 29 23:39:58.583: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104 container client-container: <nil>
STEP: delete the pod
Feb 29 23:39:58.614: INFO: Waiting for pod downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104 to disappear
Feb 29 23:39:58.617: INFO: Pod downwardapi-volume-13d9b622-1046-49c0-abb6-438f58a2b104 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:39:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9770" for this suite.
Feb 29 23:40:04.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:40:04.942: INFO: namespace projected-9770 deletion completed in 6.318384494s

• [SLOW TEST:10.459 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:40:04.943: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:40:05.001: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 29 23:40:10.008: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 29 23:40:10.008: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 29 23:40:10.047: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3397 /apis/apps/v1/namespaces/deployment-3397/deployments/test-cleanup-deployment cc2db7b7-681e-4e37-b28d-5cbf9e0cc765 14633 1 2020-02-29 23:40:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0064d3078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 29 23:40:10.060: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-3397 /apis/apps/v1/namespaces/deployment-3397/replicasets/test-cleanup-deployment-65db99849b f70deb83-bec7-4faa-bb5f-2f4addf1c353 14635 1 2020-02-29 23:40:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cc2db7b7-681e-4e37-b28d-5cbf9e0cc765 0xc0064d34a7 0xc0064d34a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0064d3508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 29 23:40:10.060: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 29 23:40:10.061: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3397 /apis/apps/v1/namespaces/deployment-3397/replicasets/test-cleanup-controller 9d8d0474-f8c9-4aab-a53d-01049919434c 14634 1 2020-02-29 23:40:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cc2db7b7-681e-4e37-b28d-5cbf9e0cc765 0xc0064d33d7 0xc0064d33d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0064d3438 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 29 23:40:10.095: INFO: Pod "test-cleanup-controller-w8xkn" is available:
&Pod{ObjectMeta:{test-cleanup-controller-w8xkn test-cleanup-controller- deployment-3397 /api/v1/namespaces/deployment-3397/pods/test-cleanup-controller-w8xkn 8e08aa20-3ffa-4fb4-991e-ea53e8f6a41d 14629 0 2020-02-29 23:40:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.4.22/32] [{apps/v1 ReplicaSet test-cleanup-controller 9d8d0474-f8c9-4aab-a53d-01049919434c 0xc0064d3967 0xc0064d3968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-slfrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-slfrk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-slfrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:40:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:40:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:40:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:40:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.22,StartTime:2020-02-29 23:40:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-29 23:40:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://01b1512a293f7f8ce5805a1c7eeef7550ff1ef515942d5823ac2f547ab960e5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 29 23:40:10.095: INFO: Pod "test-cleanup-deployment-65db99849b-khfs6" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-khfs6 test-cleanup-deployment-65db99849b- deployment-3397 /api/v1/namespaces/deployment-3397/pods/test-cleanup-deployment-65db99849b-khfs6 acaf22c6-f1cc-4639-b154-c6de0f3bd8b0 14641 0 2020-02-29 23:40:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b f70deb83-bec7-4faa-bb5f-2f4addf1c353 0xc0064d3ae7 0xc0064d3ae8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-slfrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-slfrk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-slfrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:40:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:40:10.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3397" for this suite.
Feb 29 23:40:16.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:40:16.229: INFO: namespace deployment-3397 deletion completed in 6.121430578s

• [SLOW TEST:11.286 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:40:16.230: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:40:40.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6149" for this suite.
Feb 29 23:40:46.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:40:46.665: INFO: namespace container-runtime-6149 deletion completed in 6.133823787s

• [SLOW TEST:30.436 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:40:46.666: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 29 23:40:46.723: INFO: Waiting up to 5m0s for pod "downward-api-12223cf3-b011-47f0-bc17-372f123b6472" in namespace "downward-api-2132" to be "success or failure"
Feb 29 23:40:46.729: INFO: Pod "downward-api-12223cf3-b011-47f0-bc17-372f123b6472": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925286ms
Feb 29 23:40:48.737: INFO: Pod "downward-api-12223cf3-b011-47f0-bc17-372f123b6472": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014497068s
Feb 29 23:40:50.741: INFO: Pod "downward-api-12223cf3-b011-47f0-bc17-372f123b6472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0179286s
STEP: Saw pod success
Feb 29 23:40:50.741: INFO: Pod "downward-api-12223cf3-b011-47f0-bc17-372f123b6472" satisfied condition "success or failure"
Feb 29 23:40:50.746: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downward-api-12223cf3-b011-47f0-bc17-372f123b6472 container dapi-container: <nil>
STEP: delete the pod
Feb 29 23:40:50.775: INFO: Waiting for pod downward-api-12223cf3-b011-47f0-bc17-372f123b6472 to disappear
Feb 29 23:40:50.777: INFO: Pod downward-api-12223cf3-b011-47f0-bc17-372f123b6472 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:40:50.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2132" for this suite.
Feb 29 23:40:56.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:40:56.916: INFO: namespace downward-api-2132 deletion completed in 6.135181563s

• [SLOW TEST:10.251 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:40:56.917: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:40:56.979: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 29 23:40:56.987: INFO: Number of nodes with available pods: 0
Feb 29 23:40:56.987: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 29 23:40:57.019: INFO: Number of nodes with available pods: 0
Feb 29 23:40:57.019: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:40:58.024: INFO: Number of nodes with available pods: 0
Feb 29 23:40:58.024: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:40:59.023: INFO: Number of nodes with available pods: 0
Feb 29 23:40:59.024: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:00.023: INFO: Number of nodes with available pods: 1
Feb 29 23:41:00.023: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 29 23:41:00.059: INFO: Number of nodes with available pods: 1
Feb 29 23:41:00.059: INFO: Number of running nodes: 0, number of available pods: 1
Feb 29 23:41:01.063: INFO: Number of nodes with available pods: 0
Feb 29 23:41:01.063: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 29 23:41:01.102: INFO: Number of nodes with available pods: 0
Feb 29 23:41:01.102: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:02.108: INFO: Number of nodes with available pods: 0
Feb 29 23:41:02.109: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:03.106: INFO: Number of nodes with available pods: 0
Feb 29 23:41:03.106: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:04.107: INFO: Number of nodes with available pods: 0
Feb 29 23:41:04.107: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:05.109: INFO: Number of nodes with available pods: 0
Feb 29 23:41:05.109: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:06.106: INFO: Number of nodes with available pods: 0
Feb 29 23:41:06.106: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:07.106: INFO: Number of nodes with available pods: 0
Feb 29 23:41:07.106: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:08.106: INFO: Number of nodes with available pods: 0
Feb 29 23:41:08.106: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:09.106: INFO: Number of nodes with available pods: 0
Feb 29 23:41:09.106: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:10.107: INFO: Number of nodes with available pods: 0
Feb 29 23:41:10.107: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:11.108: INFO: Number of nodes with available pods: 0
Feb 29 23:41:11.108: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:41:12.106: INFO: Number of nodes with available pods: 1
Feb 29 23:41:12.107: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5497, will wait for the garbage collector to delete the pods
Feb 29 23:41:12.182: INFO: Deleting DaemonSet.extensions daemon-set took: 17.490156ms
Feb 29 23:41:12.683: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.553289ms
Feb 29 23:41:15.886: INFO: Number of nodes with available pods: 0
Feb 29 23:41:15.886: INFO: Number of running nodes: 0, number of available pods: 0
Feb 29 23:41:15.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5497/daemonsets","resourceVersion":"14947"},"items":null}

Feb 29 23:41:15.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5497/pods","resourceVersion":"14947"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:41:15.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5497" for this suite.
Feb 29 23:41:21.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:41:22.104: INFO: namespace daemonsets-5497 deletion completed in 6.17522284s

• [SLOW TEST:25.188 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:41:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:41:22.166: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7c2acf01-9184-4e1e-adfd-0e4dd23aad46" in namespace "security-context-test-5599" to be "success or failure"
Feb 29 23:41:22.175: INFO: Pod "busybox-readonly-false-7c2acf01-9184-4e1e-adfd-0e4dd23aad46": Phase="Pending", Reason="", readiness=false. Elapsed: 8.715351ms
Feb 29 23:41:24.180: INFO: Pod "busybox-readonly-false-7c2acf01-9184-4e1e-adfd-0e4dd23aad46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013837055s
Feb 29 23:41:26.242: INFO: Pod "busybox-readonly-false-7c2acf01-9184-4e1e-adfd-0e4dd23aad46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075834889s
Feb 29 23:41:26.242: INFO: Pod "busybox-readonly-false-7c2acf01-9184-4e1e-adfd-0e4dd23aad46" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:41:26.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5599" for this suite.
Feb 29 23:41:32.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:41:32.514: INFO: namespace security-context-test-5599 deletion completed in 6.243197121s

• [SLOW TEST:10.405 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:41:32.519: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Feb 29 23:41:32.592: INFO: Waiting up to 5m0s for pod "client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d" in namespace "containers-9902" to be "success or failure"
Feb 29 23:41:32.602: INFO: Pod "client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.450088ms
Feb 29 23:41:34.611: INFO: Pod "client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019266384s
Feb 29 23:41:36.693: INFO: Pod "client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101091119s
STEP: Saw pod success
Feb 29 23:41:36.693: INFO: Pod "client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d" satisfied condition "success or failure"
Feb 29 23:41:36.793: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d container test-container: <nil>
STEP: delete the pod
Feb 29 23:41:36.863: INFO: Waiting for pod client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d to disappear
Feb 29 23:41:36.866: INFO: Pod client-containers-3d025c95-fd03-4d14-8a33-99aedee8d05d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:41:36.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9902" for this suite.
Feb 29 23:41:42.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:41:42.976: INFO: namespace containers-9902 deletion completed in 6.104845063s

• [SLOW TEST:10.457 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:41:42.982: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4736.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4736.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4736.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.79.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.79.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.79.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.79.231_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4736.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4736.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4736.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4736.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4736.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.79.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.79.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.79.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.79.231_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 29 23:41:57.072: INFO: Unable to read wheezy_udp@dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.078: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.080: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.102: INFO: Unable to read jessie_udp@dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.106: INFO: Unable to read jessie_tcp@dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.110: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.116: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local from pod dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31: the server could not find the requested resource (get pods dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31)
Feb 29 23:41:57.135: INFO: Lookups using dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31 failed for: [wheezy_udp@dns-test-service.dns-4736.svc.cluster.local wheezy_tcp@dns-test-service.dns-4736.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local jessie_udp@dns-test-service.dns-4736.svc.cluster.local jessie_tcp@dns-test-service.dns-4736.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4736.svc.cluster.local]

Feb 29 23:42:02.345: INFO: DNS probes using dns-4736/dns-test-fdf818a5-cb86-47db-983d-e093d79e6a31 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:42:02.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4736" for this suite.
Feb 29 23:42:08.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:42:08.521: INFO: namespace dns-4736 deletion completed in 6.08273217s

• [SLOW TEST:25.539 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:42:08.522: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:42:12.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3284" for this suite.
Feb 29 23:42:18.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:42:18.669: INFO: namespace kubelet-test-3284 deletion completed in 6.08101843s

• [SLOW TEST:10.148 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:42:18.675: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-51f882be-2fe0-45da-abaa-caff81f5cf36
STEP: Creating a pod to test consume configMaps
Feb 29 23:42:18.712: INFO: Waiting up to 5m0s for pod "pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de" in namespace "configmap-9092" to be "success or failure"
Feb 29 23:42:18.730: INFO: Pod "pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de": Phase="Pending", Reason="", readiness=false. Elapsed: 17.51183ms
Feb 29 23:42:20.733: INFO: Pod "pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020075951s
Feb 29 23:42:22.736: INFO: Pod "pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023951065s
STEP: Saw pod success
Feb 29 23:42:22.736: INFO: Pod "pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de" satisfied condition "success or failure"
Feb 29 23:42:22.738: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de container configmap-volume-test: <nil>
STEP: delete the pod
Feb 29 23:42:22.766: INFO: Waiting for pod pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de to disappear
Feb 29 23:42:22.767: INFO: Pod pod-configmaps-e05617a7-06f9-4e79-b28c-0476fbd307de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:42:22.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9092" for this suite.
Feb 29 23:42:28.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:42:28.867: INFO: namespace configmap-9092 deletion completed in 6.096463049s

• [SLOW TEST:10.193 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:42:28.868: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 29 23:42:28.898: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:42:33.439: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:42:53.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9385" for this suite.
Feb 29 23:42:59.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:42:59.306: INFO: namespace crd-publish-openapi-9385 deletion completed in 6.087792463s

• [SLOW TEST:30.438 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:42:59.308: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-fd849c92-0fcb-42ed-84eb-6e5dcd3ca1d0
STEP: Creating a pod to test consume secrets
Feb 29 23:42:59.406: INFO: Waiting up to 5m0s for pod "pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93" in namespace "secrets-4564" to be "success or failure"
Feb 29 23:42:59.413: INFO: Pod "pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425529ms
Feb 29 23:43:01.418: INFO: Pod "pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012777434s
STEP: Saw pod success
Feb 29 23:43:01.419: INFO: Pod "pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93" satisfied condition "success or failure"
Feb 29 23:43:01.422: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-14d9760cdc pod pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93 container secret-volume-test: <nil>
STEP: delete the pod
Feb 29 23:43:01.471: INFO: Waiting for pod pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93 to disappear
Feb 29 23:43:01.474: INFO: Pod pod-secrets-98fc5cd9-f7dd-42a5-a736-82d503d79b93 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:43:01.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4564" for this suite.
Feb 29 23:43:07.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:43:07.587: INFO: namespace secrets-4564 deletion completed in 6.108126704s
STEP: Destroying namespace "secret-namespace-9406" for this suite.
Feb 29 23:43:13.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:43:13.696: INFO: namespace secret-namespace-9406 deletion completed in 6.108824119s

• [SLOW TEST:14.388 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:43:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0229 23:43:43.799059      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 29 23:43:43.799: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:43:43.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2693" for this suite.
Feb 29 23:43:49.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:43:49.902: INFO: namespace gc-2693 deletion completed in 6.100009224s

• [SLOW TEST:36.204 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:43:49.907: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 29 23:43:53.969: INFO: &Pod{ObjectMeta:{send-events-69f6264f-9609-40b3-8006-1f53a4bcaea0  events-7700 /api/v1/namespaces/events-7700/pods/send-events-69f6264f-9609-40b3-8006-1f53a4bcaea0 0ddee013-a96c-4496-8b5a-a3f56b9221c7 15565 0 2020-02-29 23:43:48 +0000 UTC <nil> <nil> map[name:foo time:940950208] map[cni.projectcalico.org/podIP:192.168.4.30/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kwkc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kwkc8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kwkc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:43:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:43:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:43:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-29 23:43:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.30,StartTime:2020-02-29 23:43:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-29 23:43:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://fb63109d8e263ef9d63e2db8aaba65025458730eca0e75c07a56289b0a17ff95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 29 23:43:55.977: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 29 23:43:57.980: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:43:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7700" for this suite.
Feb 29 23:44:42.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:44:42.096: INFO: namespace events-7700 deletion completed in 44.106624183s

• [SLOW TEST:52.189 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:44:42.098: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 29 23:44:42.145: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3128 /api/v1/namespaces/watch-3128/configmaps/e2e-watch-test-watch-closed 725e8bac-3259-42ec-b0a9-cfb2e2b2bcc9 15679 0 2020-02-29 23:44:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 29 23:44:42.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3128 /api/v1/namespaces/watch-3128/configmaps/e2e-watch-test-watch-closed 725e8bac-3259-42ec-b0a9-cfb2e2b2bcc9 15680 0 2020-02-29 23:44:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 29 23:44:42.155: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3128 /api/v1/namespaces/watch-3128/configmaps/e2e-watch-test-watch-closed 725e8bac-3259-42ec-b0a9-cfb2e2b2bcc9 15681 0 2020-02-29 23:44:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 29 23:44:42.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3128 /api/v1/namespaces/watch-3128/configmaps/e2e-watch-test-watch-closed 725e8bac-3259-42ec-b0a9-cfb2e2b2bcc9 15682 0 2020-02-29 23:44:41 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:44:42.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3128" for this suite.
Feb 29 23:44:48.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:44:48.249: INFO: namespace watch-3128 deletion completed in 6.087400724s

• [SLOW TEST:6.152 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:44:48.251: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 29 23:44:56.330: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:44:56.349: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:44:58.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:44:58.353: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:00.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:00.355: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:02.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:02.353: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:04.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:04.353: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:06.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:06.353: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:08.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:08.353: INFO: Pod pod-with-prestop-http-hook still exists
Feb 29 23:45:10.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 29 23:45:10.353: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:45:10.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6828" for this suite.
Feb 29 23:45:22.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:45:22.459: INFO: namespace container-lifecycle-hook-6828 deletion completed in 12.085947593s

• [SLOW TEST:34.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:45:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2201
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 29 23:45:22.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 29 23:45:44.570: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.5.30 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2201 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:45:44.570: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:45:45.694: INFO: Found all expected endpoints: [netserver-0]
Feb 29 23:45:45.696: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.4.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2201 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 29 23:45:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Feb 29 23:45:46.822: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:45:46.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2201" for this suite.
Feb 29 23:45:58.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:45:58.917: INFO: namespace pod-network-test-2201 deletion completed in 12.090618069s

• [SLOW TEST:36.455 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:45:58.920: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 29 23:45:58.950: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 29 23:45:58.960: INFO: Waiting for terminating namespaces to be deleted...
Feb 29 23:45:58.962: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-14d9760cdc before test
Feb 29 23:45:58.978: INFO: calico-kube-controllers-67878b79d6-4qj8x from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 29 23:45:58.978: INFO: kube-proxy-kwv7g from kube-system started at 2020-02-29 23:14:35 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:45:58.978: INFO: nvidia-device-plugin-daemonset-ptdg5 from kube-system started at 2020-02-29 23:14:59 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:45:58.978: INFO: metallb-speaker-f4prv from ccp started at 2020-02-29 23:15:14 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:45:58.978: INFO: nginx-ingress-controller-7q5kc from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 29 23:45:58.978: INFO: coredns-65f5bff896-v5nsg from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container coredns ready: true, restart count 0
Feb 29 23:45:58.978: INFO: calico-node-mgknx from kube-system started at 2020-02-29 23:14:56 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.978: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:45:58.978: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:45:58.979: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:45:58.979: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:45:58.979: INFO: nginx-ingress-default-backend-8465968b95-q4zrk from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.979: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 29 23:45:58.979: INFO: metallb-controller-6c8c5fd7fd-vzjj8 from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.979: INFO: 	Container metallb-controller ready: true, restart count 0
Feb 29 23:45:58.979: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-5b74768057 before test
Feb 29 23:45:58.988: INFO: nvidia-device-plugin-daemonset-vx9dx from kube-system started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Feb 29 23:45:58.988: INFO: metallb-speaker-gfzhh from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container metallb-speaker ready: true, restart count 0
Feb 29 23:45:58.988: INFO: nginx-ingress-controller-bhmwg from ccp started at 2020-02-29 23:16:19 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 29 23:45:58.988: INFO: sonobuoy from sonobuoy started at 2020-02-29 23:35:00 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 29 23:45:58.988: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 29 23:45:58.988: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 29 23:45:58.988: INFO: kube-proxy-n5h6f from kube-system started at 2020-02-29 23:16:04 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 29 23:45:58.988: INFO: calico-node-xvtvr from kube-system started at 2020-02-29 23:16:05 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.988: INFO: 	Container calico-node ready: true, restart count 0
Feb 29 23:45:58.989: INFO: cert-manager-7c4fdf69b7-cxg76 from ccp started at 2020-02-29 23:26:26 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.989: INFO: 	Container cert-manager ready: true, restart count 0
Feb 29 23:45:58.989: INFO: ccp-helm-operator-5ff4f4944c-xqbq6 from ccp started at 2020-02-29 23:26:54 +0000 UTC (1 container statuses recorded)
Feb 29 23:45:58.989: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Feb 29 23:45:58.989: INFO: sonobuoy-e2e-job-a8e644cf92284639 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Feb 29 23:45:58.989: INFO: 	Container e2e ready: true, restart count 0
Feb 29 23:45:58.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b5439a5e-82e6-4f6a-99a0-781228dd3796 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b5439a5e-82e6-4f6a-99a0-781228dd3796 off the node alex-slot1-v3-vsp1-node-group-14d9760cdc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b5439a5e-82e6-4f6a-99a0-781228dd3796
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:46:05.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2293" for this suite.
Feb 29 23:46:13.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:46:13.183: INFO: namespace sched-pred-2293 deletion completed in 8.117714002s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:14.263 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:46:13.184: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 29 23:46:16.234: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:46:16.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8874" for this suite.
Feb 29 23:46:22.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:46:22.394: INFO: namespace container-runtime-8874 deletion completed in 6.132125806s

• [SLOW TEST:9.209 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:46:22.395: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-3d3b4a5d-a370-4e48-8c1b-e3fb274f3653
STEP: Creating secret with name secret-projected-all-test-volume-15babb4b-d91b-4891-a4fc-694c81fc2449
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 29 23:46:22.444: INFO: Waiting up to 5m0s for pod "projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276" in namespace "projected-776" to be "success or failure"
Feb 29 23:46:22.456: INFO: Pod "projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276": Phase="Pending", Reason="", readiness=false. Elapsed: 11.549783ms
Feb 29 23:46:24.459: INFO: Pod "projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014595393s
Feb 29 23:46:26.462: INFO: Pod "projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017668278s
STEP: Saw pod success
Feb 29 23:46:26.462: INFO: Pod "projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276" satisfied condition "success or failure"
Feb 29 23:46:26.464: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 29 23:46:26.486: INFO: Waiting for pod projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276 to disappear
Feb 29 23:46:26.489: INFO: Pod projected-volume-a26f8da8-14b3-41c0-a3c1-5cbdb2c8f276 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:46:26.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-776" for this suite.
Feb 29 23:46:32.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:46:32.593: INFO: namespace projected-776 deletion completed in 6.099558845s

• [SLOW TEST:10.198 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:46:32.594: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344
Feb 29 23:46:32.631: INFO: Pod name my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344: Found 0 pods out of 1
Feb 29 23:46:37.634: INFO: Pod name my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344: Found 1 pods out of 1
Feb 29 23:46:37.634: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344" are running
Feb 29 23:46:37.637: INFO: Pod "my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344-pcxpw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-29 23:46:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-29 23:46:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-29 23:46:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-29 23:46:31 +0000 UTC Reason: Message:}])
Feb 29 23:46:37.637: INFO: Trying to dial the pod
Feb 29 23:46:42.646: INFO: Controller my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344: Got expected result from replica 1 [my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344-pcxpw]: "my-hostname-basic-feb5697b-cba1-4731-a1dc-d345a5b6c344-pcxpw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:46:42.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-182" for this suite.
Feb 29 23:46:48.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:46:48.785: INFO: namespace replication-controller-182 deletion completed in 6.134907918s

• [SLOW TEST:16.191 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:46:48.791: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 29 23:46:52.865: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-585087022 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 29 23:46:58.008: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:46:58.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2094" for this suite.
Feb 29 23:47:04.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:47:04.110: INFO: namespace pods-2094 deletion completed in 6.094867137s

• [SLOW TEST:15.319 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:47:04.110: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:47:04.157: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-da5d80da-4dd8-4d70-9ae5-70d5a0dee779" in namespace "security-context-test-6995" to be "success or failure"
Feb 29 23:47:04.169: INFO: Pod "alpine-nnp-false-da5d80da-4dd8-4d70-9ae5-70d5a0dee779": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920846ms
Feb 29 23:47:06.174: INFO: Pod "alpine-nnp-false-da5d80da-4dd8-4d70-9ae5-70d5a0dee779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016991841s
Feb 29 23:47:08.177: INFO: Pod "alpine-nnp-false-da5d80da-4dd8-4d70-9ae5-70d5a0dee779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020494658s
Feb 29 23:47:08.177: INFO: Pod "alpine-nnp-false-da5d80da-4dd8-4d70-9ae5-70d5a0dee779" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:47:08.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6995" for this suite.
Feb 29 23:47:14.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:47:14.290: INFO: namespace security-context-test-6995 deletion completed in 6.098997818s

• [SLOW TEST:10.180 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:47:14.292: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-84b7e9aa-e78c-4bab-9338-30885990acdc
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:47:14.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7426" for this suite.
Feb 29 23:47:20.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:47:20.425: INFO: namespace configmap-7426 deletion completed in 6.09897097s

• [SLOW TEST:6.133 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:47:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 29 23:47:20.510: INFO: Waiting up to 5m0s for pod "pod-c4265c7e-cae0-4c49-a723-8c9221984af0" in namespace "emptydir-1230" to be "success or failure"
Feb 29 23:47:20.517: INFO: Pod "pod-c4265c7e-cae0-4c49-a723-8c9221984af0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.486917ms
Feb 29 23:47:22.520: INFO: Pod "pod-c4265c7e-cae0-4c49-a723-8c9221984af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010269729s
Feb 29 23:47:24.523: INFO: Pod "pod-c4265c7e-cae0-4c49-a723-8c9221984af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013319437s
STEP: Saw pod success
Feb 29 23:47:24.524: INFO: Pod "pod-c4265c7e-cae0-4c49-a723-8c9221984af0" satisfied condition "success or failure"
Feb 29 23:47:24.526: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-c4265c7e-cae0-4c49-a723-8c9221984af0 container test-container: <nil>
STEP: delete the pod
Feb 29 23:47:24.552: INFO: Waiting for pod pod-c4265c7e-cae0-4c49-a723-8c9221984af0 to disappear
Feb 29 23:47:24.554: INFO: Pod pod-c4265c7e-cae0-4c49-a723-8c9221984af0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:47:24.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1230" for this suite.
Feb 29 23:47:30.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:47:30.648: INFO: namespace emptydir-1230 deletion completed in 6.090617648s

• [SLOW TEST:10.222 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:47:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-qx9k
STEP: Creating a pod to test atomic-volume-subpath
Feb 29 23:47:30.687: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qx9k" in namespace "subpath-5317" to be "success or failure"
Feb 29 23:47:30.695: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233195ms
Feb 29 23:47:32.697: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009788228s
Feb 29 23:47:34.700: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 4.012773661s
Feb 29 23:47:36.703: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 6.016025624s
Feb 29 23:47:38.706: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 8.018845398s
Feb 29 23:47:40.709: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 10.022052262s
Feb 29 23:47:42.712: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 12.024573331s
Feb 29 23:47:44.715: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 14.02770251s
Feb 29 23:47:46.718: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 16.030585058s
Feb 29 23:47:48.721: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 18.033630077s
Feb 29 23:47:50.724: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 20.036201218s
Feb 29 23:47:52.728: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Running", Reason="", readiness=true. Elapsed: 22.040256616s
Feb 29 23:47:54.731: INFO: Pod "pod-subpath-test-configmap-qx9k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.04351379s
STEP: Saw pod success
Feb 29 23:47:54.731: INFO: Pod "pod-subpath-test-configmap-qx9k" satisfied condition "success or failure"
Feb 29 23:47:54.733: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-14d9760cdc pod pod-subpath-test-configmap-qx9k container test-container-subpath-configmap-qx9k: <nil>
STEP: delete the pod
Feb 29 23:47:54.770: INFO: Waiting for pod pod-subpath-test-configmap-qx9k to disappear
Feb 29 23:47:54.771: INFO: Pod pod-subpath-test-configmap-qx9k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qx9k
Feb 29 23:47:54.771: INFO: Deleting pod "pod-subpath-test-configmap-qx9k" in namespace "subpath-5317"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:47:54.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5317" for this suite.
Feb 29 23:48:00.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:48:00.883: INFO: namespace subpath-5317 deletion completed in 6.106122873s

• [SLOW TEST:30.232 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:48:00.883: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 29 23:48:00.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3076'
Feb 29 23:48:01.486: INFO: stderr: ""
Feb 29 23:48:01.486: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Feb 29 23:48:01.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete pods e2e-test-httpd-pod --namespace=kubectl-3076'
Feb 29 23:48:19.619: INFO: stderr: ""
Feb 29 23:48:19.619: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:48:19.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3076" for this suite.
Feb 29 23:48:25.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:48:25.726: INFO: namespace kubectl-3076 deletion completed in 6.103134825s

• [SLOW TEST:24.843 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:48:25.729: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 29 23:48:26.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 29 23:48:29.500: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:48:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5326" for this suite.
Feb 29 23:48:35.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:48:35.786: INFO: namespace webhook-5326 deletion completed in 6.086950969s
STEP: Destroying namespace "webhook-5326-markers" for this suite.
Feb 29 23:48:41.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:48:41.878: INFO: namespace webhook-5326-markers deletion completed in 6.091008465s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.157 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:48:41.890: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 29 23:48:41.929: INFO: Waiting up to 5m0s for pod "pod-6f777acf-0e85-4d87-ae52-459bd092051c" in namespace "emptydir-7167" to be "success or failure"
Feb 29 23:48:41.933: INFO: Pod "pod-6f777acf-0e85-4d87-ae52-459bd092051c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639452ms
Feb 29 23:48:43.935: INFO: Pod "pod-6f777acf-0e85-4d87-ae52-459bd092051c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006400631s
Feb 29 23:48:45.938: INFO: Pod "pod-6f777acf-0e85-4d87-ae52-459bd092051c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009520796s
STEP: Saw pod success
Feb 29 23:48:45.938: INFO: Pod "pod-6f777acf-0e85-4d87-ae52-459bd092051c" satisfied condition "success or failure"
Feb 29 23:48:45.941: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-14d9760cdc pod pod-6f777acf-0e85-4d87-ae52-459bd092051c container test-container: <nil>
STEP: delete the pod
Feb 29 23:48:45.967: INFO: Waiting for pod pod-6f777acf-0e85-4d87-ae52-459bd092051c to disappear
Feb 29 23:48:45.970: INFO: Pod pod-6f777acf-0e85-4d87-ae52-459bd092051c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:48:45.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7167" for this suite.
Feb 29 23:48:51.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:48:52.080: INFO: namespace emptydir-7167 deletion completed in 6.106622916s

• [SLOW TEST:10.191 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:48:52.088: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8638.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8638.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8638.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8638.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 29 23:48:56.170: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-8638/dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5: the server could not find the requested resource (get pods dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5)
Feb 29 23:48:56.172: INFO: Unable to read jessie_udp@PodARecord from pod dns-8638/dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5: the server could not find the requested resource (get pods dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5)
Feb 29 23:48:56.176: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8638/dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5: the server could not find the requested resource (get pods dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5)
Feb 29 23:48:56.176: INFO: Lookups using dns-8638/dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5 failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 29 23:49:01.201: INFO: DNS probes using dns-8638/dns-test-47374b72-264d-43d3-a9ba-5d70c6b9def5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:49:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8638" for this suite.
Feb 29 23:49:07.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:49:07.345: INFO: namespace dns-8638 deletion completed in 6.093875013s

• [SLOW TEST:15.258 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:49:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Feb 29 23:49:07.377: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 29 23:50:07.397: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:50:07.400: INFO: Starting informer...
STEP: Starting pods...
Feb 29 23:50:07.616: INFO: Pod1 is running on alex-slot1-v3-vsp1-node-group-5b74768057. Tainting Node
Feb 29 23:50:11.832: INFO: Pod2 is running on alex-slot1-v3-vsp1-node-group-5b74768057. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 29 23:50:28.945: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 29 23:50:39.243: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:50:39.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9110" for this suite.
Feb 29 23:50:45.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:50:45.439: INFO: namespace taint-multiple-pods-9110 deletion completed in 6.125181824s

• [SLOW TEST:98.092 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:50:45.441: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:50:45.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-8921'
Feb 29 23:50:45.867: INFO: stderr: ""
Feb 29 23:50:45.867: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 29 23:50:45.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-8921'
Feb 29 23:50:46.173: INFO: stderr: ""
Feb 29 23:50:46.173: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 29 23:50:47.179: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:47.179: INFO: Found 0 / 1
Feb 29 23:50:48.178: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:48.178: INFO: Found 0 / 1
Feb 29 23:50:49.178: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:49.178: INFO: Found 0 / 1
Feb 29 23:50:50.176: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:50.176: INFO: Found 0 / 1
Feb 29 23:50:51.181: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:51.181: INFO: Found 1 / 1
Feb 29 23:50:51.181: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 29 23:50:51.184: INFO: Selector matched 1 pods for map[app:redis]
Feb 29 23:50:51.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 29 23:50:51.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 describe pod redis-master-bs87z --namespace=kubectl-8921'
Feb 29 23:50:51.316: INFO: stderr: ""
Feb 29 23:50:51.316: INFO: stdout: "Name:         redis-master-bs87z\nNamespace:    kubectl-8921\nPriority:     0\nNode:         alex-slot1-v3-vsp1-node-group-5b74768057/10.10.128.24\nStart Time:   Sat, 29 Feb 2020 23:50:45 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.5.39/32\nStatus:       Running\nIP:           192.168.5.39\nIPs:\n  IP:           192.168.5.39\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://69c14edd6f146730a26ca712820e034c71e3b1bfea14888037ce1c8843a48dc3\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Feb 2020 23:50:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9qm4l (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9qm4l:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9qm4l\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                               Message\n  ----    ------     ----       ----                                               -------\n  Normal  Scheduled  <unknown>  default-scheduler                                  Successfully assigned kubectl-8921/redis-master-bs87z to alex-slot1-v3-vsp1-node-group-5b74768057\n  Normal  Pulling    4s         kubelet, alex-slot1-v3-vsp1-node-group-5b74768057  Pulling image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Pulled     1s         kubelet, alex-slot1-v3-vsp1-node-group-5b74768057  Successfully pulled image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Created    1s         kubelet, alex-slot1-v3-vsp1-node-group-5b74768057  Created container redis-master\n  Normal  Started    0s         kubelet, alex-slot1-v3-vsp1-node-group-5b74768057  Started container redis-master\n"
Feb 29 23:50:51.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 describe rc redis-master --namespace=kubectl-8921'
Feb 29 23:50:51.457: INFO: stderr: ""
Feb 29 23:50:51.457: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8921\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-bs87z\n"
Feb 29 23:50:51.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 describe service redis-master --namespace=kubectl-8921'
Feb 29 23:50:51.575: INFO: stderr: ""
Feb 29 23:50:51.575: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8921\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.109.64.123\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.5.39:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 29 23:50:51.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 describe node alex-slot1-v3-vsp1-master-gro-22271c3398'
Feb 29 23:50:51.762: INFO: stderr: ""
Feb 29 23:50:51.762: INFO: stdout: "Name:               alex-slot1-v3-vsp1-master-gro-22271c3398\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=alex-slot1-v3-vsp1-master-gro-22271c3398\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.128.20/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.183.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Feb 2020 23:12:03 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 29 Feb 2020 23:15:01 +0000   Sat, 29 Feb 2020 23:15:01 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 29 Feb 2020 23:49:56 +0000   Sat, 29 Feb 2020 23:11:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 29 Feb 2020 23:49:56 +0000   Sat, 29 Feb 2020 23:11:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 29 Feb 2020 23:49:56 +0000   Sat, 29 Feb 2020 23:11:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 29 Feb 2020 23:49:56 +0000   Sat, 29 Feb 2020 23:13:03 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  10.10.128.20\n  InternalIP:  10.10.128.20\n  Hostname:    alex-slot1-v3-vsp1-master-gro-22271c3398\nCapacity:\n cpu:                2\n ephemeral-storage:  40470732Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16426416Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37297826550\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16324016Ki\n pods:               110\nSystem Info:\n Machine ID:                 6f85343d31aa4911912d0024fba5ba8f\n System UUID:                4215A995-8926-6AEE-8641-DB485933A6DE\n Boot ID:                    e4ca1303-5ecb-4588-8083-ec64de93f58d\n Kernel Version:             4.15.0-64-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.16.3\n Kube-Proxy Version:         v1.16.3\nPodCIDR:                     192.168.3.0/24\nPodCIDRs:                    192.168.3.0/24\nProviderID:                  vsphere://4215a995-8926-6aee-8641-db485933a6de\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                ------------  ----------  ---------------  -------------  ---\n  ccp                        ccp-vip-manager-alex-slot1-v3-vsp1-master-gro-22271c3398            0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\n  kube-system                calico-node-dfbmh                                                   250m (12%)    0 (0%)      0 (0%)           0 (0%)         35m\n  kube-system                coredns-65f5bff896-pw4bq                                            100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     37m\n  kube-system                etcd-alex-slot1-v3-vsp1-master-gro-22271c3398                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\n  kube-system                kube-apiserver-alex-slot1-v3-vsp1-master-gro-22271c3398             250m (12%)    0 (0%)      0 (0%)           0 (0%)         37m\n  kube-system                kube-controller-manager-alex-slot1-v3-vsp1-master-gro-22271c3398    200m (10%)    0 (0%)      0 (0%)           0 (0%)         37m\n  kube-system                kube-proxy-lt6dm                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  kube-system                kube-scheduler-alex-slot1-v3-vsp1-master-gro-22271c3398             100m (5%)     0 (0%)      0 (0%)           0 (0%)         37m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lpqmw             0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                900m (45%)  0 (0%)\n  memory             70Mi (0%)   170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                                                  Message\n  ----    ------                   ----               ----                                                  -------\n  Normal  Starting                 38m                kubelet, alex-slot1-v3-vsp1-master-gro-22271c3398     Starting kubelet.\n  Normal  NodeHasSufficientMemory  38m (x8 over 38m)  kubelet, alex-slot1-v3-vsp1-master-gro-22271c3398     Node alex-slot1-v3-vsp1-master-gro-22271c3398 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    38m (x7 over 38m)  kubelet, alex-slot1-v3-vsp1-master-gro-22271c3398     Node alex-slot1-v3-vsp1-master-gro-22271c3398 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     38m (x8 over 38m)  kubelet, alex-slot1-v3-vsp1-master-gro-22271c3398     Node alex-slot1-v3-vsp1-master-gro-22271c3398 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  38m                kubelet, alex-slot1-v3-vsp1-master-gro-22271c3398     Updated Node Allocatable limit across pods\n  Normal  Starting                 38m                kube-proxy, alex-slot1-v3-vsp1-master-gro-22271c3398  Starting kube-proxy.\n"
Feb 29 23:50:51.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 describe namespace kubectl-8921'
Feb 29 23:50:51.900: INFO: stderr: ""
Feb 29 23:50:51.900: INFO: stdout: "Name:         kubectl-8921\nLabels:       e2e-framework=kubectl\n              e2e-run=e95d0896-83db-42a2-8ffd-8d7987332da2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:50:51.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8921" for this suite.
Feb 29 23:51:03.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:51:04.011: INFO: namespace kubectl-8921 deletion completed in 12.105091743s

• [SLOW TEST:18.570 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:51:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6456.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6456.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 29 23:51:18.110: INFO: DNS probes using dns-6456/dns-test-2004e9c4-d4b4-475b-890a-d48ad8f180fc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:51:18.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6456" for this suite.
Feb 29 23:51:24.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:51:24.244: INFO: namespace dns-6456 deletion completed in 6.103802447s

• [SLOW TEST:20.232 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:51:24.252: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 29 23:51:24.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3507'
Feb 29 23:51:24.421: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 29 23:51:24.421: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Feb 29 23:51:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3507'
Feb 29 23:51:24.581: INFO: stderr: ""
Feb 29 23:51:24.581: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:51:24.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3507" for this suite.
Feb 29 23:51:30.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:51:30.682: INFO: namespace kubectl-3507 deletion completed in 6.09663873s

• [SLOW TEST:6.430 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:51:30.684: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 29 23:51:30.734: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:30.737: INFO: Number of nodes with available pods: 0
Feb 29 23:51:30.737: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:31.749: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:31.751: INFO: Number of nodes with available pods: 0
Feb 29 23:51:31.751: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:32.742: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:32.745: INFO: Number of nodes with available pods: 0
Feb 29 23:51:32.745: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:33.742: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:33.745: INFO: Number of nodes with available pods: 2
Feb 29 23:51:33.745: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 29 23:51:33.764: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:33.766: INFO: Number of nodes with available pods: 1
Feb 29 23:51:33.766: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:34.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:34.774: INFO: Number of nodes with available pods: 1
Feb 29 23:51:34.774: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:35.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:35.773: INFO: Number of nodes with available pods: 1
Feb 29 23:51:35.773: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:36.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:36.773: INFO: Number of nodes with available pods: 1
Feb 29 23:51:36.773: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:37.772: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:37.774: INFO: Number of nodes with available pods: 1
Feb 29 23:51:37.774: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:38.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:38.774: INFO: Number of nodes with available pods: 1
Feb 29 23:51:38.774: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:39.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:39.774: INFO: Number of nodes with available pods: 1
Feb 29 23:51:39.774: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:40.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:40.773: INFO: Number of nodes with available pods: 1
Feb 29 23:51:40.773: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:41.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:41.775: INFO: Number of nodes with available pods: 1
Feb 29 23:51:41.775: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Feb 29 23:51:42.771: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 29 23:51:42.774: INFO: Number of nodes with available pods: 2
Feb 29 23:51:42.774: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6577, will wait for the garbage collector to delete the pods
Feb 29 23:51:42.837: INFO: Deleting DaemonSet.extensions daemon-set took: 6.47106ms
Feb 29 23:51:42.937: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.289596ms
Feb 29 23:51:46.440: INFO: Number of nodes with available pods: 0
Feb 29 23:51:46.440: INFO: Number of running nodes: 0, number of available pods: 0
Feb 29 23:51:46.443: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6577/daemonsets","resourceVersion":"17399"},"items":null}

Feb 29 23:51:46.446: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6577/pods","resourceVersion":"17399"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:51:46.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6577" for this suite.
Feb 29 23:51:52.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:51:52.562: INFO: namespace daemonsets-6577 deletion completed in 6.101801489s

• [SLOW TEST:21.878 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:51:52.562: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-4509/secret-test-f8265f5e-d2cc-427b-a727-03b4b567ed9a
STEP: Creating a pod to test consume secrets
Feb 29 23:51:52.600: INFO: Waiting up to 5m0s for pod "pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab" in namespace "secrets-4509" to be "success or failure"
Feb 29 23:51:52.606: INFO: Pod "pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.017932ms
Feb 29 23:51:54.608: INFO: Pod "pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00760948s
Feb 29 23:51:56.612: INFO: Pod "pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011510537s
STEP: Saw pod success
Feb 29 23:51:56.612: INFO: Pod "pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab" satisfied condition "success or failure"
Feb 29 23:51:56.615: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab container env-test: <nil>
STEP: delete the pod
Feb 29 23:51:56.645: INFO: Waiting for pod pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab to disappear
Feb 29 23:51:56.653: INFO: Pod pod-configmaps-036f913c-fe9b-4afa-8587-cbf57baae6ab no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:51:56.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4509" for this suite.
Feb 29 23:52:02.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:52:02.762: INFO: namespace secrets-4509 deletion completed in 6.10637937s

• [SLOW TEST:10.200 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:52:02.763: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-09c8a4ae-82f2-4a24-9373-fc967c0b7532
STEP: Creating a pod to test consume secrets
Feb 29 23:52:02.847: INFO: Waiting up to 5m0s for pod "pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de" in namespace "secrets-1775" to be "success or failure"
Feb 29 23:52:02.860: INFO: Pod "pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de": Phase="Pending", Reason="", readiness=false. Elapsed: 12.626875ms
Feb 29 23:52:04.867: INFO: Pod "pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019540041s
Feb 29 23:52:06.871: INFO: Pod "pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023543001s
STEP: Saw pod success
Feb 29 23:52:06.871: INFO: Pod "pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de" satisfied condition "success or failure"
Feb 29 23:52:06.873: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de container secret-volume-test: <nil>
STEP: delete the pod
Feb 29 23:52:06.896: INFO: Waiting for pod pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de to disappear
Feb 29 23:52:06.898: INFO: Pod pod-secrets-9ae6d360-7275-4791-a80d-b7ce25f3c4de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:52:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1775" for this suite.
Feb 29 23:52:12.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:52:12.995: INFO: namespace secrets-1775 deletion completed in 6.092057075s

• [SLOW TEST:10.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:52:12.996: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 29 23:52:13.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 29 23:52:16.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617133, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617133, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617133, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 29 23:52:19.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:52:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5696" for this suite.
Feb 29 23:52:25.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:52:25.142: INFO: namespace webhook-5696 deletion completed in 6.106400781s
STEP: Destroying namespace "webhook-5696-markers" for this suite.
Feb 29 23:52:31.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:52:31.236: INFO: namespace webhook-5696-markers deletion completed in 6.093129938s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.249 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:52:31.245: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 29 23:52:35.868: INFO: Successfully updated pod "pod-update-799c9799-ab7a-49ab-bfa5-89e47c2b475b"
STEP: verifying the updated pod is in kubernetes
Feb 29 23:52:35.878: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:52:35.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2847" for this suite.
Feb 29 23:52:47.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:52:47.977: INFO: namespace pods-2847 deletion completed in 12.095904036s

• [SLOW TEST:16.731 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:52:47.977: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Feb 29 23:52:48.007: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 29 23:53:48.026: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 29 23:53:48.029: INFO: Starting informer...
STEP: Starting pod...
Feb 29 23:53:48.239: INFO: Pod is running on alex-slot1-v3-vsp1-node-group-5b74768057. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 29 23:53:48.264: INFO: Pod wasn't evicted. Proceeding
Feb 29 23:53:48.264: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 29 23:55:03.301: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:55:03.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8834" for this suite.
Feb 29 23:55:31.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:55:31.405: INFO: namespace taint-single-pod-8834 deletion completed in 28.099666978s

• [SLOW TEST:163.428 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:55:31.408: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 29 23:55:31.921: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 29 23:55:33.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617330, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617330, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617330, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617330, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 29 23:55:36.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:55:37.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-726" for this suite.
Feb 29 23:55:43.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:55:43.219: INFO: namespace webhook-726 deletion completed in 6.094387128s
STEP: Destroying namespace "webhook-726-markers" for this suite.
Feb 29 23:55:49.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:55:49.304: INFO: namespace webhook-726-markers deletion completed in 6.085066871s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.912 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:55:49.319: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0229 23:55:59.429933      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 29 23:55:59.430: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:55:59.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7264" for this suite.
Feb 29 23:56:05.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:56:05.530: INFO: namespace gc-7264 deletion completed in 6.097367361s

• [SLOW TEST:16.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:56:05.532: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 29 23:56:05.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3833'
Feb 29 23:56:05.704: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 29 23:56:05.704: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Feb 29 23:56:05.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete jobs e2e-test-httpd-job --namespace=kubectl-3833'
Feb 29 23:56:05.841: INFO: stderr: ""
Feb 29 23:56:05.841: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:56:05.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3833" for this suite.
Feb 29 23:56:11.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:56:11.960: INFO: namespace kubectl-3833 deletion completed in 6.110735463s

• [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:56:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Feb 29 23:56:11.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-9475'
Feb 29 23:56:12.288: INFO: stderr: ""
Feb 29 23:56:12.288: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 29 23:56:12.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9475'
Feb 29 23:56:12.412: INFO: stderr: ""
Feb 29 23:56:12.412: INFO: stdout: "update-demo-nautilus-g2gtm update-demo-nautilus-qcf52 "
Feb 29 23:56:12.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-g2gtm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:12.541: INFO: stderr: ""
Feb 29 23:56:12.541: INFO: stdout: ""
Feb 29 23:56:12.541: INFO: update-demo-nautilus-g2gtm is created but not running
Feb 29 23:56:17.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9475'
Feb 29 23:56:17.644: INFO: stderr: ""
Feb 29 23:56:17.644: INFO: stdout: "update-demo-nautilus-g2gtm update-demo-nautilus-qcf52 "
Feb 29 23:56:17.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-g2gtm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:17.752: INFO: stderr: ""
Feb 29 23:56:17.752: INFO: stdout: "true"
Feb 29 23:56:17.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-g2gtm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:17.853: INFO: stderr: ""
Feb 29 23:56:17.853: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 29 23:56:17.853: INFO: validating pod update-demo-nautilus-g2gtm
Feb 29 23:56:17.858: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 29 23:56:17.858: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 29 23:56:17.858: INFO: update-demo-nautilus-g2gtm is verified up and running
Feb 29 23:56:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-qcf52 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:17.951: INFO: stderr: ""
Feb 29 23:56:17.951: INFO: stdout: "true"
Feb 29 23:56:17.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-qcf52 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:18.042: INFO: stderr: ""
Feb 29 23:56:18.042: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 29 23:56:18.042: INFO: validating pod update-demo-nautilus-qcf52
Feb 29 23:56:18.046: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 29 23:56:18.046: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 29 23:56:18.046: INFO: update-demo-nautilus-qcf52 is verified up and running
STEP: rolling-update to new replication controller
Feb 29 23:56:18.048: INFO: scanned /root for discovery docs: <nil>
Feb 29 23:56:18.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9475'
Feb 29 23:56:37.637: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 29 23:56:37.637: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 29 23:56:37.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9475'
Feb 29 23:56:37.751: INFO: stderr: ""
Feb 29 23:56:37.751: INFO: stdout: "update-demo-kitten-bpwww update-demo-kitten-fk5xx update-demo-nautilus-g2gtm "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 29 23:56:42.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9475'
Feb 29 23:56:42.861: INFO: stderr: ""
Feb 29 23:56:42.861: INFO: stdout: "update-demo-kitten-bpwww update-demo-kitten-fk5xx "
Feb 29 23:56:42.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-kitten-bpwww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:42.988: INFO: stderr: ""
Feb 29 23:56:42.988: INFO: stdout: "true"
Feb 29 23:56:42.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-kitten-bpwww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:43.095: INFO: stderr: ""
Feb 29 23:56:43.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 29 23:56:43.095: INFO: validating pod update-demo-kitten-bpwww
Feb 29 23:56:43.099: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 29 23:56:43.099: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 29 23:56:43.099: INFO: update-demo-kitten-bpwww is verified up and running
Feb 29 23:56:43.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-kitten-fk5xx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:43.199: INFO: stderr: ""
Feb 29 23:56:43.199: INFO: stdout: "true"
Feb 29 23:56:43.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-kitten-fk5xx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9475'
Feb 29 23:56:43.320: INFO: stderr: ""
Feb 29 23:56:43.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 29 23:56:43.320: INFO: validating pod update-demo-kitten-fk5xx
Feb 29 23:56:43.325: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 29 23:56:43.325: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 29 23:56:43.325: INFO: update-demo-kitten-fk5xx is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:56:43.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9475" for this suite.
Feb 29 23:56:55.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:56:55.403: INFO: namespace kubectl-9475 deletion completed in 12.074718465s

• [SLOW TEST:43.442 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:56:55.410: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:56:55.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1335" for this suite.
Feb 29 23:57:23.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:57:23.583: INFO: namespace pods-1335 deletion completed in 28.115707558s

• [SLOW TEST:28.173 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:57:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-378f87c8-b5b4-4ebe-aed6-13aca57326e8
STEP: Creating a pod to test consume secrets
Feb 29 23:57:23.649: INFO: Waiting up to 5m0s for pod "pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a" in namespace "secrets-7760" to be "success or failure"
Feb 29 23:57:23.669: INFO: Pod "pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.480634ms
Feb 29 23:57:25.672: INFO: Pod "pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022898302s
Feb 29 23:57:27.677: INFO: Pod "pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028054935s
STEP: Saw pod success
Feb 29 23:57:27.677: INFO: Pod "pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a" satisfied condition "success or failure"
Feb 29 23:57:27.684: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a container secret-volume-test: <nil>
STEP: delete the pod
Feb 29 23:57:27.726: INFO: Waiting for pod pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a to disappear
Feb 29 23:57:27.736: INFO: Pod pod-secrets-8c1d79ce-641c-420d-9317-b8827f19fe4a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:57:27.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7760" for this suite.
Feb 29 23:57:33.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:57:33.865: INFO: namespace secrets-7760 deletion completed in 6.120807119s

• [SLOW TEST:10.279 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:57:33.866: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0229 23:57:39.929330      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 29 23:57:39.929: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:57:39.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3768" for this suite.
Feb 29 23:57:45.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:57:46.030: INFO: namespace gc-3768 deletion completed in 6.096178579s

• [SLOW TEST:12.164 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:57:46.030: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Feb 29 23:57:46.072: INFO: Waiting up to 5m0s for pod "client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da" in namespace "containers-4771" to be "success or failure"
Feb 29 23:57:46.078: INFO: Pod "client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59084ms
Feb 29 23:57:48.082: INFO: Pod "client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009821257s
Feb 29 23:57:50.086: INFO: Pod "client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014386826s
STEP: Saw pod success
Feb 29 23:57:50.086: INFO: Pod "client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da" satisfied condition "success or failure"
Feb 29 23:57:50.090: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da container test-container: <nil>
STEP: delete the pod
Feb 29 23:57:50.113: INFO: Waiting for pod client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da to disappear
Feb 29 23:57:50.116: INFO: Pod client-containers-84ab2ba6-33a4-4b16-b18a-86c3e71b09da no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:57:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4771" for this suite.
Feb 29 23:57:56.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:57:56.215: INFO: namespace containers-4771 deletion completed in 6.094737642s

• [SLOW TEST:10.185 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:57:56.215: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3345
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3345
STEP: creating replication controller externalsvc in namespace services-3345
I0229 23:57:56.302955      21 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3345, replica count: 2
I0229 23:57:59.353561      21 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 29 23:57:59.373: INFO: Creating new exec pod
Feb 29 23:58:03.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3345 execpodzbzrc -- /bin/sh -x -c nslookup clusterip-service'
Feb 29 23:58:04.140: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 29 23:58:04.140: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3345.svc.cluster.local\tcanonical name = externalsvc.services-3345.svc.cluster.local.\nName:\texternalsvc.services-3345.svc.cluster.local\nAddress: 10.104.164.166\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3345, will wait for the garbage collector to delete the pods
Feb 29 23:58:04.203: INFO: Deleting ReplicationController externalsvc took: 10.09764ms
Feb 29 23:58:04.703: INFO: Terminating ReplicationController externalsvc pods took: 500.220131ms
Feb 29 23:58:19.735: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:58:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3345" for this suite.
Feb 29 23:58:25.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:58:25.864: INFO: namespace services-3345 deletion completed in 6.105751915s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.649 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:58:25.865: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 29 23:58:25.897: INFO: PodSpec: initContainers in spec.initContainers
Feb 29 23:59:09.744: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-280ab37e-51a1-48b5-b34b-931caad89196", GenerateName:"", Namespace:"init-container-4214", SelfLink:"/api/v1/namespaces/init-container-4214/pods/pod-init-280ab37e-51a1-48b5-b34b-931caad89196", UID:"9a82f33c-5041-44d3-ab87-024ac438d3a7", ResourceVersion:"19410", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718617504, loc:(*time.Location)(0x84bfb00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"897202821"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.5.69/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8kgd2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0058d2100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kgd2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kgd2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8kgd2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ba7418), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"alex-slot1-v3-vsp1-node-group-5b74768057", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00441c1e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba74c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002ba7550)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002ba7558), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002ba755c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617505, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617505, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617505, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617504, loc:(*time.Location)(0x84bfb00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.128.24", PodIP:"192.168.5.69", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.5.69"}}, StartTime:(*v1.Time)(0xc0030f4b60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ff1110)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ff11f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://67cdbc1812d7a18e05d61a1f95e36f69d2f988d7ed3ea90aeb86c90c3fd635ab", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030f4ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030f4b80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002ba767f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:59:09.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4214" for this suite.
Feb 29 23:59:21.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:59:21.863: INFO: namespace init-container-4214 deletion completed in 12.105133972s

• [SLOW TEST:55.998 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:59:21.864: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 29 23:59:26.461: INFO: Successfully updated pod "annotationupdate32a8856e-299c-43f8-abfe-72d882e67b43"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:59:28.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1373" for this suite.
Feb 29 23:59:40.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:59:40.586: INFO: namespace downward-api-1373 deletion completed in 12.100037863s

• [SLOW TEST:18.722 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:59:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 29 23:59:40.628: INFO: Waiting up to 5m0s for pod "pod-37e58813-153c-4b1a-8df5-c1e153ae3def" in namespace "emptydir-8044" to be "success or failure"
Feb 29 23:59:40.633: INFO: Pod "pod-37e58813-153c-4b1a-8df5-c1e153ae3def": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830768ms
Feb 29 23:59:42.636: INFO: Pod "pod-37e58813-153c-4b1a-8df5-c1e153ae3def": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007975643s
Feb 29 23:59:44.640: INFO: Pod "pod-37e58813-153c-4b1a-8df5-c1e153ae3def": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011537612s
STEP: Saw pod success
Feb 29 23:59:44.640: INFO: Pod "pod-37e58813-153c-4b1a-8df5-c1e153ae3def" satisfied condition "success or failure"
Feb 29 23:59:44.646: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-37e58813-153c-4b1a-8df5-c1e153ae3def container test-container: <nil>
STEP: delete the pod
Feb 29 23:59:44.668: INFO: Waiting for pod pod-37e58813-153c-4b1a-8df5-c1e153ae3def to disappear
Feb 29 23:59:44.670: INFO: Pod pod-37e58813-153c-4b1a-8df5-c1e153ae3def no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:59:44.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8044" for this suite.
Feb 29 23:59:50.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 29 23:59:50.759: INFO: namespace emptydir-8044 deletion completed in 6.085288225s

• [SLOW TEST:10.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 29 23:59:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 29 23:59:50.801: INFO: Waiting up to 5m0s for pod "pod-9361a31f-0e63-4003-aeb1-9ec5200d0482" in namespace "emptydir-6829" to be "success or failure"
Feb 29 23:59:50.808: INFO: Pod "pod-9361a31f-0e63-4003-aeb1-9ec5200d0482": Phase="Pending", Reason="", readiness=false. Elapsed: 6.316539ms
Feb 29 23:59:52.811: INFO: Pod "pod-9361a31f-0e63-4003-aeb1-9ec5200d0482": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010023202s
Feb 29 23:59:54.814: INFO: Pod "pod-9361a31f-0e63-4003-aeb1-9ec5200d0482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012620875s
STEP: Saw pod success
Feb 29 23:59:54.814: INFO: Pod "pod-9361a31f-0e63-4003-aeb1-9ec5200d0482" satisfied condition "success or failure"
Feb 29 23:59:54.816: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-9361a31f-0e63-4003-aeb1-9ec5200d0482 container test-container: <nil>
STEP: delete the pod
Feb 29 23:59:54.843: INFO: Waiting for pod pod-9361a31f-0e63-4003-aeb1-9ec5200d0482 to disappear
Feb 29 23:59:54.845: INFO: Pod pod-9361a31f-0e63-4003-aeb1-9ec5200d0482 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 29 23:59:54.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6829" for this suite.
Mar  1 00:00:00.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:00:00.978: INFO: namespace emptydir-6829 deletion completed in 6.121389516s

• [SLOW TEST:10.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:00:00.978: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar  1 00:00:01.010: INFO: namespace kubectl-6754
Mar  1 00:00:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6754'
Mar  1 00:00:01.283: INFO: stderr: ""
Mar  1 00:00:01.284: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  1 00:00:02.287: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:00:02.287: INFO: Found 0 / 1
Mar  1 00:00:03.287: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:00:03.287: INFO: Found 0 / 1
Mar  1 00:00:04.287: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:00:04.287: INFO: Found 1 / 1
Mar  1 00:00:04.287: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 00:00:04.291: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:00:04.291: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 00:00:04.291: INFO: wait on redis-master startup in kubectl-6754 
Mar  1 00:00:04.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs redis-master-kkkv7 redis-master --namespace=kubectl-6754'
Mar  1 00:00:04.418: INFO: stderr: ""
Mar  1 00:00:04.418: INFO: stdout: "1:C 01 Mar 2020 00:00:02.871 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 01 Mar 2020 00:00:02.871 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 01 Mar 2020 00:00:02.871 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 01 Mar 2020 00:00:02.873 * Running mode=standalone, port=6379.\n1:M 01 Mar 2020 00:00:02.873 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Mar 2020 00:00:02.873 # Server initialized\n1:M 01 Mar 2020 00:00:02.873 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Mar 2020 00:00:02.873 * Ready to accept connections\n"
STEP: exposing RC
Mar  1 00:00:04.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6754'
Mar  1 00:00:04.579: INFO: stderr: ""
Mar  1 00:00:04.579: INFO: stdout: "service/rm2 exposed\n"
Mar  1 00:00:04.582: INFO: Service rm2 in namespace kubectl-6754 found.
STEP: exposing service
Mar  1 00:00:06.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6754'
Mar  1 00:00:06.741: INFO: stderr: ""
Mar  1 00:00:06.741: INFO: stdout: "service/rm3 exposed\n"
Mar  1 00:00:06.748: INFO: Service rm3 in namespace kubectl-6754 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:00:08.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6754" for this suite.
Mar  1 00:00:20.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:00:20.839: INFO: namespace kubectl-6754 deletion completed in 12.081088859s

• [SLOW TEST:19.861 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:00:20.839: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 00:00:23.894: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:00:23.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9892" for this suite.
Mar  1 00:00:29.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:00:30.018: INFO: namespace container-runtime-9892 deletion completed in 6.107843514s

• [SLOW TEST:9.179 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:00:30.018: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  1 00:00:38.170: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:38.185: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:40.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:40.199: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:42.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:42.189: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:44.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:44.189: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:46.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:46.189: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:48.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:48.189: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 00:00:50.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 00:00:50.188: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:00:50.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8450" for this suite.
Mar  1 00:01:02.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:01:02.303: INFO: namespace container-lifecycle-hook-8450 deletion completed in 12.11129104s

• [SLOW TEST:32.285 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:01:02.304: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:01:02.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751" in namespace "downward-api-1674" to be "success or failure"
Mar  1 00:01:02.351: INFO: Pod "downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751": Phase="Pending", Reason="", readiness=false. Elapsed: 9.928116ms
Mar  1 00:01:04.355: INFO: Pod "downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014697347s
Mar  1 00:01:06.359: INFO: Pod "downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017759668s
STEP: Saw pod success
Mar  1 00:01:06.359: INFO: Pod "downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751" satisfied condition "success or failure"
Mar  1 00:01:06.360: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751 container client-container: <nil>
STEP: delete the pod
Mar  1 00:01:06.384: INFO: Waiting for pod downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751 to disappear
Mar  1 00:01:06.387: INFO: Pod downwardapi-volume-15734a44-6825-4fa9-9f6a-c7349b390751 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:01:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1674" for this suite.
Mar  1 00:01:12.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:01:12.520: INFO: namespace downward-api-1674 deletion completed in 6.127890837s

• [SLOW TEST:10.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:01:12.520: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1835
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-1835
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1835
Mar  1 00:01:12.581: INFO: Found 0 stateful pods, waiting for 1
Mar  1 00:01:22.585: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  1 00:01:22.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 00:01:22.859: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 00:01:22.859: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 00:01:22.859: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 00:01:22.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 00:01:32.865: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 00:01:32.865: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 00:01:32.876: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:01:32.876: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:01:32.876: INFO: 
Mar  1 00:01:32.876: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  1 00:01:33.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996993648s
Mar  1 00:01:34.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991235851s
Mar  1 00:01:35.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987205473s
Mar  1 00:01:36.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982962384s
Mar  1 00:01:37.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978475615s
Mar  1 00:01:38.903: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974750716s
Mar  1 00:01:39.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970580478s
Mar  1 00:01:40.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966649514s
Mar  1 00:01:41.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.863454ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1835
Mar  1 00:01:42.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 00:01:43.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 00:01:43.187: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 00:01:43.187: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 00:01:43.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 00:01:43.449: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 00:01:43.450: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 00:01:43.450: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 00:01:43.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 00:01:43.705: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 00:01:43.705: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 00:01:43.705: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 00:01:43.708: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  1 00:01:53.712: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:01:53.712: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:01:53.712: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  1 00:01:53.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 00:01:53.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 00:01:53.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 00:01:53.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 00:01:53.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 00:01:54.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 00:01:54.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 00:01:54.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 00:01:54.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-1835 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 00:01:54.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 00:01:54.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 00:01:54.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 00:01:54.528: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 00:01:54.532: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  1 00:02:04.539: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 00:02:04.539: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 00:02:04.539: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 00:02:04.553: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:04.553: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:02:04.553: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:04.553: INFO: ss-2  alex-slot1-v3-vsp1-node-group-5b74768057  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:04.553: INFO: 
Mar  1 00:02:04.553: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 00:02:05.567: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:05.567: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:02:05.574: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:05.574: INFO: ss-2  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:05.574: INFO: 
Mar  1 00:02:05.574: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 00:02:06.577: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:06.577: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:02:06.577: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:06.577: INFO: ss-2  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:06.577: INFO: 
Mar  1 00:02:06.577: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 00:02:07.581: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:07.581: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:02:07.581: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:07.581: INFO: ss-2  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:07.581: INFO: 
Mar  1 00:02:07.581: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 00:02:08.587: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:08.587: INFO: ss-0  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:11 +0000 UTC  }]
Mar  1 00:02:08.587: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:08.587: INFO: ss-2  alex-slot1-v3-vsp1-node-group-5b74768057  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:08.587: INFO: 
Mar  1 00:02:08.587: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 00:02:09.590: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar  1 00:02:09.590: INFO: ss-1  alex-slot1-v3-vsp1-node-group-14d9760cdc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 00:01:31 +0000 UTC  }]
Mar  1 00:02:09.590: INFO: 
Mar  1 00:02:09.590: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  1 00:02:10.593: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.955072926s
Mar  1 00:02:11.598: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.950322022s
Mar  1 00:02:12.602: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.947006695s
Mar  1 00:02:13.607: INFO: Verifying statefulset ss doesn't scale past 0 for another 943.173212ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1835
Mar  1 00:02:14.611: INFO: Scaling statefulset ss to 0
Mar  1 00:02:14.620: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 00:02:14.622: INFO: Deleting all statefulset in ns statefulset-1835
Mar  1 00:02:14.624: INFO: Scaling statefulset ss to 0
Mar  1 00:02:14.631: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 00:02:14.632: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:02:14.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1835" for this suite.
Mar  1 00:02:20.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:02:20.760: INFO: namespace statefulset-1835 deletion completed in 6.100001503s

• [SLOW TEST:68.240 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:02:20.760: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:02:33.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1743" for this suite.
Mar  1 00:02:39.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:02:39.949: INFO: namespace resourcequota-1743 deletion completed in 6.087316343s

• [SLOW TEST:19.188 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:02:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:02:39.978: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  1 00:02:39.996: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  1 00:02:44.009: INFO: Creating deployment "test-rolling-update-deployment"
Mar  1 00:02:44.014: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  1 00:02:44.019: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  1 00:02:46.024: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  1 00:02:46.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617763, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617763, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617763, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617763, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:02:48.029: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  1 00:02:48.036: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7473 /apis/apps/v1/namespaces/deployment-7473/deployments/test-rolling-update-deployment 31b47d85-e7eb-4e32-a374-9fc86e876840 20288 1 2020-03-01 00:02:43 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004838248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-01 00:02:43 +0000 UTC,LastTransitionTime:2020-03-01 00:02:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-03-01 00:02:45 +0000 UTC,LastTransitionTime:2020-03-01 00:02:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 00:02:48.039: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-7473 /apis/apps/v1/namespaces/deployment-7473/replicasets/test-rolling-update-deployment-55d946486 155c5287-fda3-41c2-9cd6-292b8e7ae431 20277 1 2020-03-01 00:02:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 31b47d85-e7eb-4e32-a374-9fc86e876840 0xc005dc73c0 0xc005dc73c1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005dc7428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 00:02:48.039: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  1 00:02:48.040: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7473 /apis/apps/v1/namespaces/deployment-7473/replicasets/test-rolling-update-controller 1a29aca7-2c52-463b-bbcf-a1d8e8ee919d 20287 2 2020-03-01 00:02:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 31b47d85-e7eb-4e32-a374-9fc86e876840 0xc005dc72f7 0xc005dc72f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005dc7358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 00:02:48.042: INFO: Pod "test-rolling-update-deployment-55d946486-t4njj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-t4njj test-rolling-update-deployment-55d946486- deployment-7473 /api/v1/namespaces/deployment-7473/pods/test-rolling-update-deployment-55d946486-t4njj 573b57a3-6316-4b05-96f2-6f45df788692 20276 0 2020-03-01 00:02:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:192.168.5.81/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 155c5287-fda3-41c2-9cd6-292b8e7ae431 0xc005dc78b0 0xc005dc78b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j9nx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j9nx2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j9nx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 00:02:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 00:02:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 00:02:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 00:02:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.81,StartTime:2020-03-01 00:02:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 00:02:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://ea409e55418297243e19f5def451c1e16bec835ac2c83fd6dfa987297c67cadf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:02:48.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7473" for this suite.
Mar  1 00:02:54.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:02:54.146: INFO: namespace deployment-7473 deletion completed in 6.100466757s

• [SLOW TEST:14.197 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:02:54.146: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8690/configmap-test-104c1064-17e0-43b0-a5b6-fae72a3b8a4c
STEP: Creating a pod to test consume configMaps
Mar  1 00:02:54.192: INFO: Waiting up to 5m0s for pod "pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252" in namespace "configmap-8690" to be "success or failure"
Mar  1 00:02:54.199: INFO: Pod "pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5805ms
Mar  1 00:02:56.206: INFO: Pod "pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010517704s
Mar  1 00:02:58.209: INFO: Pod "pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013372797s
STEP: Saw pod success
Mar  1 00:02:58.209: INFO: Pod "pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252" satisfied condition "success or failure"
Mar  1 00:02:58.211: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252 container env-test: <nil>
STEP: delete the pod
Mar  1 00:02:58.250: INFO: Waiting for pod pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252 to disappear
Mar  1 00:02:58.252: INFO: Pod pod-configmaps-a51eb3a6-c77b-478d-a8c7-d661ab49e252 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:02:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8690" for this suite.
Mar  1 00:03:04.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:03:04.363: INFO: namespace configmap-8690 deletion completed in 6.106396805s

• [SLOW TEST:10.217 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:03:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-01023df3-6252-401c-b312-35a2684f8a5a
STEP: Creating a pod to test consume secrets
Mar  1 00:03:04.411: INFO: Waiting up to 5m0s for pod "pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d" in namespace "secrets-9823" to be "success or failure"
Mar  1 00:03:04.424: INFO: Pod "pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.648723ms
Mar  1 00:03:06.428: INFO: Pod "pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016096597s
Mar  1 00:03:08.431: INFO: Pod "pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019215599s
STEP: Saw pod success
Mar  1 00:03:08.431: INFO: Pod "pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d" satisfied condition "success or failure"
Mar  1 00:03:08.433: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:03:08.460: INFO: Waiting for pod pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d to disappear
Mar  1 00:03:08.464: INFO: Pod pod-secrets-9e9eb2c9-1951-4a2c-9ae5-b0f721dc834d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:03:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9823" for this suite.
Mar  1 00:03:14.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:03:14.584: INFO: namespace secrets-9823 deletion completed in 6.115421945s

• [SLOW TEST:10.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:03:14.584: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  1 00:03:22.663: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 00:03:22.672: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 00:03:24.672: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 00:03:24.675: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 00:03:26.672: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 00:03:26.675: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 00:03:28.673: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 00:03:28.676: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 00:03:30.672: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 00:03:30.675: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:03:30.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-605" for this suite.
Mar  1 00:03:58.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:03:58.793: INFO: namespace container-lifecycle-hook-605 deletion completed in 28.100906685s

• [SLOW TEST:44.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:03:58.797: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-30f9439b-718b-48f8-ac45-4964181bc497
STEP: Creating a pod to test consume secrets
Mar  1 00:03:58.840: INFO: Waiting up to 5m0s for pod "pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab" in namespace "secrets-3868" to be "success or failure"
Mar  1 00:03:58.847: INFO: Pod "pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.381063ms
Mar  1 00:04:00.852: INFO: Pod "pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012683302s
Mar  1 00:04:02.855: INFO: Pod "pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015874548s
STEP: Saw pod success
Mar  1 00:04:02.855: INFO: Pod "pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab" satisfied condition "success or failure"
Mar  1 00:04:02.858: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:04:02.881: INFO: Waiting for pod pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab to disappear
Mar  1 00:04:02.884: INFO: Pod pod-secrets-c0c2fc6c-8898-4161-b146-a1b48550d1ab no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:04:02.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3868" for this suite.
Mar  1 00:04:08.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:04:08.993: INFO: namespace secrets-3868 deletion completed in 6.106008146s

• [SLOW TEST:10.196 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:04:08.994: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:04:10.101: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 00:04:12.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617849, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617849, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617849, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718617849, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:04:15.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:04:15.132: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:04:15.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7819" for this suite.
Mar  1 00:04:21.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:04:21.879: INFO: namespace webhook-7819 deletion completed in 6.116087087s
STEP: Destroying namespace "webhook-7819-markers" for this suite.
Mar  1 00:04:27.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:04:27.965: INFO: namespace webhook-7819-markers deletion completed in 6.085340783s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.980 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:04:27.976: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:04:28.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df" in namespace "projected-8982" to be "success or failure"
Mar  1 00:04:28.028: INFO: Pod "downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333346ms
Mar  1 00:04:30.031: INFO: Pod "downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013791517s
Mar  1 00:04:32.034: INFO: Pod "downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016788707s
STEP: Saw pod success
Mar  1 00:04:32.034: INFO: Pod "downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df" satisfied condition "success or failure"
Mar  1 00:04:32.036: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df container client-container: <nil>
STEP: delete the pod
Mar  1 00:04:32.052: INFO: Waiting for pod downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df to disappear
Mar  1 00:04:32.054: INFO: Pod downwardapi-volume-b1c6fbc3-c89a-48b3-836d-244f29a2d9df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:04:32.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8982" for this suite.
Mar  1 00:04:38.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:04:38.164: INFO: namespace projected-8982 deletion completed in 6.107648144s

• [SLOW TEST:10.188 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:04:38.164: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9076, will wait for the garbage collector to delete the pods
Mar  1 00:04:42.261: INFO: Deleting Job.batch foo took: 4.986544ms
Mar  1 00:04:42.361: INFO: Terminating Job.batch foo pods took: 100.25425ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:05:28.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9076" for this suite.
Mar  1 00:05:34.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:05:35.077: INFO: namespace job-9076 deletion completed in 6.100982098s

• [SLOW TEST:56.913 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:05:35.078: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-65ec022d-fcd8-4d0b-9cf9-b8ad7126581c
STEP: Creating a pod to test consume secrets
Mar  1 00:05:35.116: INFO: Waiting up to 5m0s for pod "pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2" in namespace "secrets-6093" to be "success or failure"
Mar  1 00:05:35.119: INFO: Pod "pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097541ms
Mar  1 00:05:37.122: INFO: Pod "pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006301859s
Mar  1 00:05:39.126: INFO: Pod "pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009925718s
STEP: Saw pod success
Mar  1 00:05:39.126: INFO: Pod "pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2" satisfied condition "success or failure"
Mar  1 00:05:39.128: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2 container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:05:39.149: INFO: Waiting for pod pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2 to disappear
Mar  1 00:05:39.152: INFO: Pod pod-secrets-6472b0f8-6e4e-4b9b-87fe-c663476548f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:05:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6093" for this suite.
Mar  1 00:05:45.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:05:45.242: INFO: namespace secrets-6093 deletion completed in 6.086304028s

• [SLOW TEST:10.164 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:05:45.242: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-2c761ca4-d329-44e8-a4c3-74cb09a74660
STEP: Creating a pod to test consume secrets
Mar  1 00:05:45.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710" in namespace "projected-5523" to be "success or failure"
Mar  1 00:05:45.318: INFO: Pod "pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710": Phase="Pending", Reason="", readiness=false. Elapsed: 9.112474ms
Mar  1 00:05:47.321: INFO: Pod "pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012104748s
Mar  1 00:05:49.324: INFO: Pod "pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015327212s
STEP: Saw pod success
Mar  1 00:05:49.325: INFO: Pod "pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710" satisfied condition "success or failure"
Mar  1 00:05:49.327: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710 container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:05:49.346: INFO: Waiting for pod pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710 to disappear
Mar  1 00:05:49.349: INFO: Pod pod-projected-secrets-4aa0eedc-e804-40e4-a24f-0ff6b0b15710 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:05:49.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5523" for this suite.
Mar  1 00:05:55.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:05:55.445: INFO: namespace projected-5523 deletion completed in 6.092325071s

• [SLOW TEST:10.203 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:05:55.447: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:05:59.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9215" for this suite.
Mar  1 00:06:43.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:06:43.637: INFO: namespace kubelet-test-9215 deletion completed in 44.090637493s

• [SLOW TEST:48.190 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:06:43.639: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  1 00:06:43.679: INFO: Waiting up to 5m0s for pod "downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a" in namespace "downward-api-3341" to be "success or failure"
Mar  1 00:06:43.684: INFO: Pod "downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417531ms
Mar  1 00:06:45.687: INFO: Pod "downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008639045s
Mar  1 00:06:47.694: INFO: Pod "downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015748594s
STEP: Saw pod success
Mar  1 00:06:47.695: INFO: Pod "downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a" satisfied condition "success or failure"
Mar  1 00:06:47.697: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a container dapi-container: <nil>
STEP: delete the pod
Mar  1 00:06:47.717: INFO: Waiting for pod downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a to disappear
Mar  1 00:06:47.719: INFO: Pod downward-api-a158f09a-b6d8-40c5-8126-47f5a6999e6a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:06:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3341" for this suite.
Mar  1 00:06:53.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:06:53.841: INFO: namespace downward-api-3341 deletion completed in 6.096198943s

• [SLOW TEST:10.202 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:06:53.841: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:06:57.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2182" for this suite.
Mar  1 00:07:37.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:07:38.024: INFO: namespace kubelet-test-2182 deletion completed in 40.115352083s

• [SLOW TEST:44.183 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:07:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 00:07:38.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1503'
Mar  1 00:07:38.170: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 00:07:38.170: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar  1 00:07:38.190: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-sddnz]
Mar  1 00:07:38.190: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-sddnz" in namespace "kubectl-1503" to be "running and ready"
Mar  1 00:07:38.194: INFO: Pod "e2e-test-httpd-rc-sddnz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282588ms
Mar  1 00:07:40.197: INFO: Pod "e2e-test-httpd-rc-sddnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0065143s
Mar  1 00:07:42.200: INFO: Pod "e2e-test-httpd-rc-sddnz": Phase="Running", Reason="", readiness=true. Elapsed: 4.009564785s
Mar  1 00:07:42.200: INFO: Pod "e2e-test-httpd-rc-sddnz" satisfied condition "running and ready"
Mar  1 00:07:42.200: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-sddnz]
Mar  1 00:07:42.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs rc/e2e-test-httpd-rc --namespace=kubectl-1503'
Mar  1 00:07:42.329: INFO: stderr: ""
Mar  1 00:07:42.329: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.5.96. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.5.96. Set the 'ServerName' directive globally to suppress this message\n[Sun Mar 01 00:07:39.785290 2020] [mpm_event:notice] [pid 1:tid 139637935766376] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sun Mar 01 00:07:39.785439 2020] [core:notice] [pid 1:tid 139637935766376] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Mar  1 00:07:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete rc e2e-test-httpd-rc --namespace=kubectl-1503'
Mar  1 00:07:42.439: INFO: stderr: ""
Mar  1 00:07:42.439: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:07:42.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1503" for this suite.
Mar  1 00:07:54.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:07:54.526: INFO: namespace kubectl-1503 deletion completed in 12.083374491s

• [SLOW TEST:16.499 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:07:54.528: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-81602104-52e0-457e-b24d-4346fe08dfa7
STEP: Creating configMap with name cm-test-opt-upd-b58184c5-4134-4297-a68d-e630fe39d9cd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-81602104-52e0-457e-b24d-4346fe08dfa7
STEP: Updating configmap cm-test-opt-upd-b58184c5-4134-4297-a68d-e630fe39d9cd
STEP: Creating configMap with name cm-test-opt-create-7744ed00-7d6a-43c1-82ce-5ca301c58d30
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:08:02.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2640" for this suite.
Mar  1 00:08:20.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:08:20.745: INFO: namespace configmap-2640 deletion completed in 18.085157784s

• [SLOW TEST:26.217 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:08:20.746: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:08:24.830: INFO: Waiting up to 5m0s for pod "client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0" in namespace "pods-4354" to be "success or failure"
Mar  1 00:08:24.838: INFO: Pod "client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.844494ms
Mar  1 00:08:26.841: INFO: Pod "client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011035207s
Mar  1 00:08:28.845: INFO: Pod "client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014426105s
STEP: Saw pod success
Mar  1 00:08:28.845: INFO: Pod "client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0" satisfied condition "success or failure"
Mar  1 00:08:28.847: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0 container env3cont: <nil>
STEP: delete the pod
Mar  1 00:08:28.864: INFO: Waiting for pod client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0 to disappear
Mar  1 00:08:28.867: INFO: Pod client-envvars-329de771-81ae-40cb-bc16-70bb61567bb0 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:08:28.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4354" for this suite.
Mar  1 00:08:40.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:08:40.971: INFO: namespace pods-4354 deletion completed in 12.100374237s

• [SLOW TEST:20.225 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:08:40.972: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:08:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Creating first CR 
Mar  1 00:08:41.868: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:40Z generation:1 name:name1 resourceVersion:21528 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6cda3fc6-c2f5-4c47-8b12-2f3e4fcc5df9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  1 00:08:51.873: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:50Z generation:1 name:name2 resourceVersion:21548 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3a3a0800-cf1d-41f6-97fe-40a8dfc534ee] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  1 00:09:01.878: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:40Z generation:2 name:name1 resourceVersion:21568 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6cda3fc6-c2f5-4c47-8b12-2f3e4fcc5df9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  1 00:09:11.881: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:50Z generation:2 name:name2 resourceVersion:21587 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3a3a0800-cf1d-41f6-97fe-40a8dfc534ee] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  1 00:09:21.887: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:40Z generation:2 name:name1 resourceVersion:21607 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6cda3fc6-c2f5-4c47-8b12-2f3e4fcc5df9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  1 00:09:31.894: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-01T00:08:50Z generation:2 name:name2 resourceVersion:21626 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3a3a0800-cf1d-41f6-97fe-40a8dfc534ee] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:09:42.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9613" for this suite.
Mar  1 00:09:48.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:09:48.487: INFO: namespace crd-watch-9613 deletion completed in 6.079614079s

• [SLOW TEST:67.516 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:09:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-27d84741-04aa-4cf1-bdee-bd4d191e6880
STEP: Creating a pod to test consume secrets
Mar  1 00:09:48.528: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19" in namespace "projected-3596" to be "success or failure"
Mar  1 00:09:48.538: INFO: Pod "pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19": Phase="Pending", Reason="", readiness=false. Elapsed: 9.472151ms
Mar  1 00:09:50.543: INFO: Pod "pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014372439s
Mar  1 00:09:52.546: INFO: Pod "pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017914686s
STEP: Saw pod success
Mar  1 00:09:52.546: INFO: Pod "pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19" satisfied condition "success or failure"
Mar  1 00:09:52.548: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:09:52.563: INFO: Waiting for pod pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19 to disappear
Mar  1 00:09:52.564: INFO: Pod pod-projected-secrets-e2251493-eafd-41df-b29f-82b7f08cdf19 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:09:52.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3596" for this suite.
Mar  1 00:09:58.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:09:58.652: INFO: namespace projected-3596 deletion completed in 6.085600423s

• [SLOW TEST:10.163 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:09:58.652: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:09:58.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191" in namespace "downward-api-9458" to be "success or failure"
Mar  1 00:09:58.689: INFO: Pod "downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191": Phase="Pending", Reason="", readiness=false. Elapsed: 6.762187ms
Mar  1 00:10:00.693: INFO: Pod "downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184384s
Mar  1 00:10:02.697: INFO: Pod "downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01486034s
STEP: Saw pod success
Mar  1 00:10:02.698: INFO: Pod "downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191" satisfied condition "success or failure"
Mar  1 00:10:02.700: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191 container client-container: <nil>
STEP: delete the pod
Mar  1 00:10:02.716: INFO: Waiting for pod downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191 to disappear
Mar  1 00:10:02.723: INFO: Pod downwardapi-volume-4f701e06-a8a4-45db-9505-0af63c418191 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:10:02.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9458" for this suite.
Mar  1 00:10:08.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:10:08.809: INFO: namespace downward-api-9458 deletion completed in 6.081804503s

• [SLOW TEST:10.156 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:10:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:10:25.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6141" for this suite.
Mar  1 00:10:31.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:10:31.971: INFO: namespace resourcequota-6141 deletion completed in 6.087858369s

• [SLOW TEST:23.161 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:10:31.974: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0301 00:10:42.080837      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  1 00:10:42.080: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:10:42.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1457" for this suite.
Mar  1 00:10:48.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:10:48.183: INFO: namespace gc-1457 deletion completed in 6.098735246s

• [SLOW TEST:16.210 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:10:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar  1 00:10:48.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-9999'
Mar  1 00:10:49.063: INFO: stderr: ""
Mar  1 00:10:49.063: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 00:10:49.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9999'
Mar  1 00:10:49.181: INFO: stderr: ""
Mar  1 00:10:49.181: INFO: stdout: "update-demo-nautilus-gv95p update-demo-nautilus-vpg22 "
Mar  1 00:10:49.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gv95p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9999'
Mar  1 00:10:49.302: INFO: stderr: ""
Mar  1 00:10:49.302: INFO: stdout: ""
Mar  1 00:10:49.302: INFO: update-demo-nautilus-gv95p is created but not running
Mar  1 00:10:54.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9999'
Mar  1 00:10:54.404: INFO: stderr: ""
Mar  1 00:10:54.404: INFO: stdout: "update-demo-nautilus-gv95p update-demo-nautilus-vpg22 "
Mar  1 00:10:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gv95p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9999'
Mar  1 00:10:54.498: INFO: stderr: ""
Mar  1 00:10:54.498: INFO: stdout: "true"
Mar  1 00:10:54.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gv95p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9999'
Mar  1 00:10:54.582: INFO: stderr: ""
Mar  1 00:10:54.582: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:10:54.582: INFO: validating pod update-demo-nautilus-gv95p
Mar  1 00:10:54.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:10:54.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:10:54.586: INFO: update-demo-nautilus-gv95p is verified up and running
Mar  1 00:10:54.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-vpg22 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9999'
Mar  1 00:10:54.699: INFO: stderr: ""
Mar  1 00:10:54.699: INFO: stdout: "true"
Mar  1 00:10:54.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-vpg22 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9999'
Mar  1 00:10:54.810: INFO: stderr: ""
Mar  1 00:10:54.810: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:10:54.810: INFO: validating pod update-demo-nautilus-vpg22
Mar  1 00:10:54.814: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:10:54.814: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:10:54.814: INFO: update-demo-nautilus-vpg22 is verified up and running
STEP: using delete to clean up resources
Mar  1 00:10:54.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-9999'
Mar  1 00:10:54.911: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:10:54.911: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 00:10:54.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9999'
Mar  1 00:10:55.041: INFO: stderr: "No resources found in kubectl-9999 namespace.\n"
Mar  1 00:10:55.041: INFO: stdout: ""
Mar  1 00:10:55.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -l name=update-demo --namespace=kubectl-9999 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 00:10:55.145: INFO: stderr: ""
Mar  1 00:10:55.145: INFO: stdout: "update-demo-nautilus-gv95p\nupdate-demo-nautilus-vpg22\n"
Mar  1 00:10:55.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9999'
Mar  1 00:10:55.777: INFO: stderr: "No resources found in kubectl-9999 namespace.\n"
Mar  1 00:10:55.777: INFO: stdout: ""
Mar  1 00:10:55.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -l name=update-demo --namespace=kubectl-9999 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 00:10:55.912: INFO: stderr: ""
Mar  1 00:10:55.912: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:10:55.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9999" for this suite.
Mar  1 00:11:01.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:11:01.998: INFO: namespace kubectl-9999 deletion completed in 6.080898618s

• [SLOW TEST:13.815 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:11:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 00:11:02.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-9874'
Mar  1 00:11:02.142: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 00:11:02.142: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Mar  1 00:11:04.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9874'
Mar  1 00:11:04.310: INFO: stderr: ""
Mar  1 00:11:04.310: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:11:04.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9874" for this suite.
Mar  1 00:11:32.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:11:32.417: INFO: namespace kubectl-9874 deletion completed in 28.097404049s

• [SLOW TEST:30.419 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:11:32.417: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  1 00:11:32.457: INFO: Waiting up to 5m0s for pod "downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b" in namespace "downward-api-2228" to be "success or failure"
Mar  1 00:11:32.464: INFO: Pod "downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294582ms
Mar  1 00:11:34.467: INFO: Pod "downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009348228s
Mar  1 00:11:36.470: INFO: Pod "downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012293539s
STEP: Saw pod success
Mar  1 00:11:36.470: INFO: Pod "downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b" satisfied condition "success or failure"
Mar  1 00:11:36.472: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b container dapi-container: <nil>
STEP: delete the pod
Mar  1 00:11:36.509: INFO: Waiting for pod downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b to disappear
Mar  1 00:11:36.511: INFO: Pod downward-api-e83c4dec-9f33-4b35-807d-bf47f4919a0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:11:36.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2228" for this suite.
Mar  1 00:11:42.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:11:42.595: INFO: namespace downward-api-2228 deletion completed in 6.081779065s

• [SLOW TEST:10.178 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:11:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:11:42.634: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6" in namespace "projected-8301" to be "success or failure"
Mar  1 00:11:42.639: INFO: Pod "downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361142ms
Mar  1 00:11:44.642: INFO: Pod "downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007455389s
Mar  1 00:11:46.644: INFO: Pod "downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010240835s
STEP: Saw pod success
Mar  1 00:11:46.645: INFO: Pod "downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6" satisfied condition "success or failure"
Mar  1 00:11:46.646: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6 container client-container: <nil>
STEP: delete the pod
Mar  1 00:11:46.663: INFO: Waiting for pod downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6 to disappear
Mar  1 00:11:46.668: INFO: Pod downwardapi-volume-1b448914-854d-4ac0-a154-9b707d51b1a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:11:46.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8301" for this suite.
Mar  1 00:11:52.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:11:52.765: INFO: namespace projected-8301 deletion completed in 6.094180064s

• [SLOW TEST:10.170 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:11:52.768: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e90ec909-e850-4783-82b0-19f831643670
STEP: Creating a pod to test consume secrets
Mar  1 00:11:52.805: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c" in namespace "projected-1643" to be "success or failure"
Mar  1 00:11:52.815: INFO: Pod "pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.424948ms
Mar  1 00:11:54.818: INFO: Pod "pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528085s
Mar  1 00:11:56.821: INFO: Pod "pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015516328s
STEP: Saw pod success
Mar  1 00:11:56.821: INFO: Pod "pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c" satisfied condition "success or failure"
Mar  1 00:11:56.823: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:11:56.844: INFO: Waiting for pod pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c to disappear
Mar  1 00:11:56.850: INFO: Pod pod-projected-secrets-ca692a20-9931-4b61-b64b-60f477bb8a7c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:11:56.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1643" for this suite.
Mar  1 00:12:02.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:12:02.948: INFO: namespace projected-1643 deletion completed in 6.094732212s

• [SLOW TEST:10.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:12:02.949: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-346f0908-a770-4151-bb58-9be194e107e8
STEP: Creating a pod to test consume secrets
Mar  1 00:12:02.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d" in namespace "projected-4004" to be "success or failure"
Mar  1 00:12:02.988: INFO: Pod "pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.73859ms
Mar  1 00:12:04.991: INFO: Pod "pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005776503s
Mar  1 00:12:06.995: INFO: Pod "pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010028854s
STEP: Saw pod success
Mar  1 00:12:06.995: INFO: Pod "pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d" satisfied condition "success or failure"
Mar  1 00:12:06.998: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:12:07.021: INFO: Waiting for pod pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d to disappear
Mar  1 00:12:07.023: INFO: Pod pod-projected-secrets-a95cbfb6-d08c-4518-b6aa-eddc71ecd83d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:12:07.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4004" for this suite.
Mar  1 00:12:13.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:12:13.119: INFO: namespace projected-4004 deletion completed in 6.091787971s

• [SLOW TEST:10.170 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:12:13.119: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:12:13.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 00:12:15.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618332, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618332, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618332, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618332, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:12:18.875: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:12:18.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6931" for this suite.
Mar  1 00:12:24.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:12:25.079: INFO: namespace webhook-6931 deletion completed in 6.109656234s
STEP: Destroying namespace "webhook-6931-markers" for this suite.
Mar  1 00:12:31.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:12:31.179: INFO: namespace webhook-6931-markers deletion completed in 6.09977833s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:12:31.187: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-1259f9e0-b9dd-47eb-9d77-5ee7da6fd170
STEP: Creating a pod to test consume configMaps
Mar  1 00:12:31.227: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef" in namespace "projected-1397" to be "success or failure"
Mar  1 00:12:31.229: INFO: Pod "pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850678ms
Mar  1 00:12:33.232: INFO: Pod "pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004937765s
Mar  1 00:12:35.237: INFO: Pod "pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009105088s
STEP: Saw pod success
Mar  1 00:12:35.237: INFO: Pod "pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef" satisfied condition "success or failure"
Mar  1 00:12:35.239: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:12:35.264: INFO: Waiting for pod pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef to disappear
Mar  1 00:12:35.267: INFO: Pod pod-projected-configmaps-22c49a42-022a-4b87-890a-53c1cc10b5ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:12:35.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1397" for this suite.
Mar  1 00:12:41.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:12:41.369: INFO: namespace projected-1397 deletion completed in 6.097601814s

• [SLOW TEST:10.182 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:12:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  1 00:12:45.934: INFO: Successfully updated pod "labelsupdate9fe68950-9b0f-45f4-8704-30e300594afa"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:12:47.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-955" for this suite.
Mar  1 00:12:59.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:13:00.066: INFO: namespace projected-955 deletion completed in 12.104044419s

• [SLOW TEST:18.696 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:13:00.070: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:13:00.121: INFO: Create a RollingUpdate DaemonSet
Mar  1 00:13:00.124: INFO: Check that daemon pods launch on every node of the cluster
Mar  1 00:13:00.128: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:00.132: INFO: Number of nodes with available pods: 0
Mar  1 00:13:00.132: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:13:01.137: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:01.139: INFO: Number of nodes with available pods: 0
Mar  1 00:13:01.139: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:13:02.137: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:02.140: INFO: Number of nodes with available pods: 0
Mar  1 00:13:02.140: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:13:03.137: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:03.139: INFO: Number of nodes with available pods: 2
Mar  1 00:13:03.140: INFO: Number of running nodes: 2, number of available pods: 2
Mar  1 00:13:03.140: INFO: Update the DaemonSet to trigger a rollout
Mar  1 00:13:03.144: INFO: Updating DaemonSet daemon-set
Mar  1 00:13:07.163: INFO: Roll back the DaemonSet before rollout is complete
Mar  1 00:13:07.168: INFO: Updating DaemonSet daemon-set
Mar  1 00:13:07.168: INFO: Make sure DaemonSet rollback is complete
Mar  1 00:13:07.175: INFO: Wrong image for pod: daemon-set-z72n9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 00:13:07.175: INFO: Pod daemon-set-z72n9 is not available
Mar  1 00:13:07.181: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:08.184: INFO: Wrong image for pod: daemon-set-z72n9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 00:13:08.184: INFO: Pod daemon-set-z72n9 is not available
Mar  1 00:13:08.188: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:09.183: INFO: Wrong image for pod: daemon-set-z72n9. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 00:13:09.183: INFO: Pod daemon-set-z72n9 is not available
Mar  1 00:13:09.186: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:13:10.184: INFO: Pod daemon-set-d87n4 is not available
Mar  1 00:13:10.187: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7266, will wait for the garbage collector to delete the pods
Mar  1 00:13:10.259: INFO: Deleting DaemonSet.extensions daemon-set took: 16.369157ms
Mar  1 00:13:10.760: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.531616ms
Mar  1 00:13:29.663: INFO: Number of nodes with available pods: 0
Mar  1 00:13:29.663: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 00:13:29.665: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7266/daemonsets","resourceVersion":"22627"},"items":null}

Mar  1 00:13:29.667: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7266/pods","resourceVersion":"22627"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:13:29.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7266" for this suite.
Mar  1 00:13:35.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:13:35.771: INFO: namespace daemonsets-7266 deletion completed in 6.089724509s

• [SLOW TEST:35.701 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:13:35.771: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:13:35.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006" in namespace "projected-8765" to be "success or failure"
Mar  1 00:13:35.813: INFO: Pod "downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006": Phase="Pending", Reason="", readiness=false. Elapsed: 5.645141ms
Mar  1 00:13:37.815: INFO: Pod "downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008317317s
Mar  1 00:13:39.819: INFO: Pod "downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011701779s
STEP: Saw pod success
Mar  1 00:13:39.819: INFO: Pod "downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006" satisfied condition "success or failure"
Mar  1 00:13:39.821: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006 container client-container: <nil>
STEP: delete the pod
Mar  1 00:13:39.843: INFO: Waiting for pod downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006 to disappear
Mar  1 00:13:39.845: INFO: Pod downwardapi-volume-5f1aa523-6125-417f-a003-ef753f1ab006 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:13:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8765" for this suite.
Mar  1 00:13:45.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:13:45.959: INFO: namespace projected-8765 deletion completed in 6.110455059s

• [SLOW TEST:10.188 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:13:45.960: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:13:45.987: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:13:50.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9504" for this suite.
Mar  1 00:14:34.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:14:34.261: INFO: namespace pods-9504 deletion completed in 44.096469759s

• [SLOW TEST:48.302 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:14:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:14:35.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 00:14:37.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:14:39.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618474, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:14:42.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:14:42.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2561" for this suite.
Mar  1 00:14:48.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:14:48.682: INFO: namespace webhook-2561 deletion completed in 6.108501965s
STEP: Destroying namespace "webhook-2561-markers" for this suite.
Mar  1 00:14:54.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:14:54.765: INFO: namespace webhook-2561-markers deletion completed in 6.082847452s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:14:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  1 00:14:54.810: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  1 00:15:03.852: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:15:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5594" for this suite.
Mar  1 00:15:09.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:15:09.956: INFO: namespace pods-5594 deletion completed in 6.098346243s

• [SLOW TEST:15.178 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:15:09.956: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 00:15:14.019: INFO: DNS probes using dns-test-7e10625a-14a9-4362-ad53-ddff9f164196 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 00:15:18.107: INFO: DNS probes using dns-test-5531f67a-0f52-4884-bb6b-fc7d5c885961 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4580.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4580.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 00:15:22.207: INFO: DNS probes using dns-test-166bce47-10ec-43bc-8083-97daac9f47b6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:15:22.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4580" for this suite.
Mar  1 00:15:28.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:15:28.375: INFO: namespace dns-4580 deletion completed in 6.125379203s

• [SLOW TEST:18.420 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:15:28.376: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 00:15:28.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7406'
Mar  1 00:15:28.534: INFO: stderr: ""
Mar  1 00:15:28.534: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  1 00:15:33.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pod e2e-test-httpd-pod --namespace=kubectl-7406 -o json'
Mar  1 00:15:33.700: INFO: stderr: ""
Mar  1 00:15:33.700: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.5.120/32\"\n        },\n        \"creationTimestamp\": \"2020-03-01T00:15:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7406\",\n        \"resourceVersion\": \"23160\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7406/pods/e2e-test-httpd-pod\",\n        \"uid\": \"5e1fa734-8213-40a3-9893-f4329598c190\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9f4g7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"alex-slot1-v3-vsp1-node-group-5b74768057\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9f4g7\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9f4g7\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T00:15:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T00:15:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T00:15:30Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T00:15:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9f6880165d9dcd005768c4410571c248357742d344a9b91378076d26cf8d1536\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-01T00:15:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.128.24\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.5.120\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.5.120\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-01T00:15:28Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  1 00:15:33.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 replace -f - --namespace=kubectl-7406'
Mar  1 00:15:33.993: INFO: stderr: ""
Mar  1 00:15:33.993: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Mar  1 00:15:33.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete pods e2e-test-httpd-pod --namespace=kubectl-7406'
Mar  1 00:15:35.754: INFO: stderr: ""
Mar  1 00:15:35.754: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:15:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7406" for this suite.
Mar  1 00:15:41.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:15:41.863: INFO: namespace kubectl-7406 deletion completed in 6.089408007s

• [SLOW TEST:13.487 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:15:41.863: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  1 00:15:41.893: INFO: Waiting up to 5m0s for pod "pod-62f689d9-df75-454c-b232-5712e62d4e1f" in namespace "emptydir-340" to be "success or failure"
Mar  1 00:15:41.897: INFO: Pod "pod-62f689d9-df75-454c-b232-5712e62d4e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414481ms
Mar  1 00:15:43.901: INFO: Pod "pod-62f689d9-df75-454c-b232-5712e62d4e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007357426s
Mar  1 00:15:45.904: INFO: Pod "pod-62f689d9-df75-454c-b232-5712e62d4e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010360333s
STEP: Saw pod success
Mar  1 00:15:45.904: INFO: Pod "pod-62f689d9-df75-454c-b232-5712e62d4e1f" satisfied condition "success or failure"
Mar  1 00:15:45.906: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-62f689d9-df75-454c-b232-5712e62d4e1f container test-container: <nil>
STEP: delete the pod
Mar  1 00:15:45.932: INFO: Waiting for pod pod-62f689d9-df75-454c-b232-5712e62d4e1f to disappear
Mar  1 00:15:45.934: INFO: Pod pod-62f689d9-df75-454c-b232-5712e62d4e1f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:15:45.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-340" for this suite.
Mar  1 00:15:51.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:15:52.039: INFO: namespace emptydir-340 deletion completed in 6.101203678s

• [SLOW TEST:10.176 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:15:52.040: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  1 00:15:52.077: INFO: Waiting up to 5m0s for pod "pod-d597c3b0-6c01-43aa-a00a-7499c34785a7" in namespace "emptydir-4314" to be "success or failure"
Mar  1 00:15:52.079: INFO: Pod "pod-d597c3b0-6c01-43aa-a00a-7499c34785a7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887268ms
Mar  1 00:15:54.082: INFO: Pod "pod-d597c3b0-6c01-43aa-a00a-7499c34785a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004650855s
Mar  1 00:15:56.084: INFO: Pod "pod-d597c3b0-6c01-43aa-a00a-7499c34785a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007222341s
STEP: Saw pod success
Mar  1 00:15:56.085: INFO: Pod "pod-d597c3b0-6c01-43aa-a00a-7499c34785a7" satisfied condition "success or failure"
Mar  1 00:15:56.087: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-d597c3b0-6c01-43aa-a00a-7499c34785a7 container test-container: <nil>
STEP: delete the pod
Mar  1 00:15:56.105: INFO: Waiting for pod pod-d597c3b0-6c01-43aa-a00a-7499c34785a7 to disappear
Mar  1 00:15:56.108: INFO: Pod pod-d597c3b0-6c01-43aa-a00a-7499c34785a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:15:56.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4314" for this suite.
Mar  1 00:16:02.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:16:02.184: INFO: namespace emptydir-4314 deletion completed in 6.072564208s

• [SLOW TEST:10.144 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:16:02.187: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  1 00:16:06.753: INFO: Successfully updated pod "labelsupdate588d1797-13e8-4e9a-b669-ecd78d2ca6ba"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:16:08.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9544" for this suite.
Mar  1 00:16:20.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:16:20.873: INFO: namespace downward-api-9544 deletion completed in 12.09207652s

• [SLOW TEST:18.685 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:16:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-266ae82c-4d82-4ea2-bd95-4b932c2b2479
STEP: Creating secret with name s-test-opt-upd-51458d39-a2e5-407a-9039-1db052ea12c1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-266ae82c-4d82-4ea2-bd95-4b932c2b2479
STEP: Updating secret s-test-opt-upd-51458d39-a2e5-407a-9039-1db052ea12c1
STEP: Creating secret with name s-test-opt-create-9c9f44f2-fabb-4e84-9b26-f7d99ad97357
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:16:29.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6639" for this suite.
Mar  1 00:16:41.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:16:41.190: INFO: namespace projected-6639 deletion completed in 12.11156278s

• [SLOW TEST:20.317 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:16:41.193: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0301 00:17:21.271851      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  1 00:17:21.272: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:17:21.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9124" for this suite.
Mar  1 00:17:27.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:17:27.375: INFO: namespace gc-9124 deletion completed in 6.098613647s

• [SLOW TEST:46.183 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:17:27.376: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a63aa27c-75bc-4b0b-8b62-26e37c1347b9
STEP: Creating a pod to test consume configMaps
Mar  1 00:17:27.409: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc" in namespace "configmap-4308" to be "success or failure"
Mar  1 00:17:27.421: INFO: Pod "pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.932187ms
Mar  1 00:17:29.424: INFO: Pod "pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014533346s
Mar  1 00:17:31.426: INFO: Pod "pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016986907s
STEP: Saw pod success
Mar  1 00:17:31.426: INFO: Pod "pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc" satisfied condition "success or failure"
Mar  1 00:17:31.429: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:17:31.448: INFO: Waiting for pod pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc to disappear
Mar  1 00:17:31.452: INFO: Pod pod-configmaps-cf3d2009-a073-4459-9a5a-c1aead0be8dc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:17:31.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4308" for this suite.
Mar  1 00:17:37.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:17:37.549: INFO: namespace configmap-4308 deletion completed in 6.091768227s

• [SLOW TEST:10.173 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:17:37.552: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:17:37.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788" in namespace "downward-api-5717" to be "success or failure"
Mar  1 00:17:37.594: INFO: Pod "downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927185ms
Mar  1 00:17:39.597: INFO: Pod "downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00932485s
Mar  1 00:17:41.603: INFO: Pod "downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015000745s
STEP: Saw pod success
Mar  1 00:17:41.603: INFO: Pod "downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788" satisfied condition "success or failure"
Mar  1 00:17:41.605: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788 container client-container: <nil>
STEP: delete the pod
Mar  1 00:17:41.641: INFO: Waiting for pod downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788 to disappear
Mar  1 00:17:41.644: INFO: Pod downwardapi-volume-a4bbcfdf-fabe-4749-8bfe-5d658cdd4788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:17:41.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5717" for this suite.
Mar  1 00:17:47.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:17:47.785: INFO: namespace downward-api-5717 deletion completed in 6.136663525s

• [SLOW TEST:10.234 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:17:47.788: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-e46241bb-7e1e-4b29-bd42-15f10a62b8f9
STEP: Creating a pod to test consume secrets
Mar  1 00:17:47.837: INFO: Waiting up to 5m0s for pod "pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540" in namespace "secrets-9923" to be "success or failure"
Mar  1 00:17:47.846: INFO: Pod "pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466296ms
Mar  1 00:17:49.850: INFO: Pod "pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012402941s
Mar  1 00:17:51.852: INFO: Pod "pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015264549s
STEP: Saw pod success
Mar  1 00:17:51.853: INFO: Pod "pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540" satisfied condition "success or failure"
Mar  1 00:17:51.855: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540 container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:17:51.879: INFO: Waiting for pod pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540 to disappear
Mar  1 00:17:51.881: INFO: Pod pod-secrets-1fa868b6-d5e3-4e04-8108-dbda63313540 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:17:51.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9923" for this suite.
Mar  1 00:17:57.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:17:57.971: INFO: namespace secrets-9923 deletion completed in 6.086722553s

• [SLOW TEST:10.184 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:17:57.973: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:17:58.908: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 00:18:00.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618678, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618678, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618678, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618677, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:18:03.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:18:03.942: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6322-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:18:05.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8049" for this suite.
Mar  1 00:18:11.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:18:11.149: INFO: namespace webhook-8049 deletion completed in 6.092238036s
STEP: Destroying namespace "webhook-8049-markers" for this suite.
Mar  1 00:18:17.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:18:17.233: INFO: namespace webhook-8049-markers deletion completed in 6.084036648s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.270 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:18:17.244: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:18:17.271: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  1 00:18:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-8241 create -f -'
Mar  1 00:18:23.056: INFO: stderr: ""
Mar  1 00:18:23.056: INFO: stdout: "e2e-test-crd-publish-openapi-9029-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 00:18:23.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-8241 delete e2e-test-crd-publish-openapi-9029-crds test-cr'
Mar  1 00:18:23.182: INFO: stderr: ""
Mar  1 00:18:23.182: INFO: stdout: "e2e-test-crd-publish-openapi-9029-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  1 00:18:23.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-8241 apply -f -'
Mar  1 00:18:23.495: INFO: stderr: ""
Mar  1 00:18:23.496: INFO: stdout: "e2e-test-crd-publish-openapi-9029-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 00:18:23.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-8241 delete e2e-test-crd-publish-openapi-9029-crds test-cr'
Mar  1 00:18:23.625: INFO: stderr: ""
Mar  1 00:18:23.625: INFO: stdout: "e2e-test-crd-publish-openapi-9029-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  1 00:18:23.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-9029-crds'
Mar  1 00:18:23.924: INFO: stderr: ""
Mar  1 00:18:23.925: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9029-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:18:28.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8241" for this suite.
Mar  1 00:18:34.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:18:35.003: INFO: namespace crd-publish-openapi-8241 deletion completed in 6.138264072s

• [SLOW TEST:17.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:18:35.003: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Mar  1 00:18:35.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 api-versions'
Mar  1 00:18:35.175: INFO: stderr: ""
Mar  1 00:18:35.175: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhelm.ccp.cisco.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:18:35.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1935" for this suite.
Mar  1 00:18:41.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:18:41.286: INFO: namespace kubectl-1935 deletion completed in 6.107142299s

• [SLOW TEST:6.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:18:41.286: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5fa8edef-d9ac-4a4c-b6c8-cdbad894f2ea
STEP: Creating a pod to test consume secrets
Mar  1 00:18:41.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7" in namespace "projected-7556" to be "success or failure"
Mar  1 00:18:41.344: INFO: Pod "pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.645834ms
Mar  1 00:18:43.349: INFO: Pod "pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015002496s
Mar  1 00:18:45.352: INFO: Pod "pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018706925s
STEP: Saw pod success
Mar  1 00:18:45.353: INFO: Pod "pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7" satisfied condition "success or failure"
Mar  1 00:18:45.356: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 00:18:45.380: INFO: Waiting for pod pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7 to disappear
Mar  1 00:18:45.382: INFO: Pod pod-projected-secrets-097633d8-778f-41fd-819b-9f00e380d1e7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:18:45.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7556" for this suite.
Mar  1 00:18:51.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:18:51.487: INFO: namespace projected-7556 deletion completed in 6.100902867s

• [SLOW TEST:10.201 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:18:51.488: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  1 00:18:51.521: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:18:56.512: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:19:15.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3761" for this suite.
Mar  1 00:19:21.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:19:21.805: INFO: namespace crd-publish-openapi-3761 deletion completed in 6.148874704s

• [SLOW TEST:30.317 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:19:21.807: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  1 00:19:21.846: INFO: Waiting up to 5m0s for pod "pod-8ffe862d-004d-4551-89d5-afa5ccd4514e" in namespace "emptydir-3495" to be "success or failure"
Mar  1 00:19:21.855: INFO: Pod "pod-8ffe862d-004d-4551-89d5-afa5ccd4514e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936858ms
Mar  1 00:19:23.859: INFO: Pod "pod-8ffe862d-004d-4551-89d5-afa5ccd4514e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012518742s
Mar  1 00:19:25.862: INFO: Pod "pod-8ffe862d-004d-4551-89d5-afa5ccd4514e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015964145s
STEP: Saw pod success
Mar  1 00:19:25.862: INFO: Pod "pod-8ffe862d-004d-4551-89d5-afa5ccd4514e" satisfied condition "success or failure"
Mar  1 00:19:25.867: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-8ffe862d-004d-4551-89d5-afa5ccd4514e container test-container: <nil>
STEP: delete the pod
Mar  1 00:19:25.896: INFO: Waiting for pod pod-8ffe862d-004d-4551-89d5-afa5ccd4514e to disappear
Mar  1 00:19:25.899: INFO: Pod pod-8ffe862d-004d-4551-89d5-afa5ccd4514e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:19:25.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3495" for this suite.
Mar  1 00:19:31.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:19:31.996: INFO: namespace emptydir-3495 deletion completed in 6.089567042s

• [SLOW TEST:10.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:19:31.998: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:19:36.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3697" for this suite.
Mar  1 00:19:44.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:19:44.175: INFO: namespace containers-3697 deletion completed in 8.097413981s

• [SLOW TEST:12.177 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:19:44.176: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:19:45.071: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 00:19:47.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618784, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618784, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618784, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618784, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:19:50.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:19:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9886" for this suite.
Mar  1 00:20:02.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:02.247: INFO: namespace webhook-9886 deletion completed in 12.097534298s
STEP: Destroying namespace "webhook-9886-markers" for this suite.
Mar  1 00:20:08.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:08.353: INFO: namespace webhook-9886-markers deletion completed in 6.10475938s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.189 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:20:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:20:08.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5" in namespace "downward-api-9105" to be "success or failure"
Mar  1 00:20:08.418: INFO: Pod "downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.565424ms
Mar  1 00:20:10.422: INFO: Pod "downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01042967s
Mar  1 00:20:12.425: INFO: Pod "downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013757395s
STEP: Saw pod success
Mar  1 00:20:12.426: INFO: Pod "downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5" satisfied condition "success or failure"
Mar  1 00:20:12.428: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5 container client-container: <nil>
STEP: delete the pod
Mar  1 00:20:12.447: INFO: Waiting for pod downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5 to disappear
Mar  1 00:20:12.450: INFO: Pod downwardapi-volume-91afb087-d1fc-4d09-979d-f08791d2c9b5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:20:12.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9105" for this suite.
Mar  1 00:20:18.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:18.546: INFO: namespace downward-api-9105 deletion completed in 6.091781177s

• [SLOW TEST:10.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:20:18.546: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:20:19.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 00:20:21.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618818, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618818, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618818, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718618818, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:20:24.082: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:20:24.085: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2929-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:20:24.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6086" for this suite.
Mar  1 00:20:30.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:30.965: INFO: namespace webhook-6086 deletion completed in 6.178430054s
STEP: Destroying namespace "webhook-6086-markers" for this suite.
Mar  1 00:20:36.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:37.050: INFO: namespace webhook-6086-markers deletion completed in 6.08476592s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:20:37.060: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:20:53.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6286" for this suite.
Mar  1 00:20:59.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:20:59.229: INFO: namespace resourcequota-6286 deletion completed in 6.104959648s

• [SLOW TEST:22.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:20:59.232: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:21:10.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8058" for this suite.
Mar  1 00:21:16.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:21:16.429: INFO: namespace resourcequota-8058 deletion completed in 6.09716957s

• [SLOW TEST:17.197 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:21:16.430: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  1 00:21:16.457: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:21:19.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9012" for this suite.
Mar  1 00:21:25.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:21:25.942: INFO: namespace init-container-9012 deletion completed in 6.105602076s

• [SLOW TEST:9.512 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:21:25.942: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 00:21:25.982: INFO: Waiting up to 5m0s for pod "pod-d9cd8a94-17c8-498f-878c-c5bc5d458757" in namespace "emptydir-647" to be "success or failure"
Mar  1 00:21:25.985: INFO: Pod "pod-d9cd8a94-17c8-498f-878c-c5bc5d458757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.745364ms
Mar  1 00:21:27.988: INFO: Pod "pod-d9cd8a94-17c8-498f-878c-c5bc5d458757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005550334s
Mar  1 00:21:29.993: INFO: Pod "pod-d9cd8a94-17c8-498f-878c-c5bc5d458757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010149825s
STEP: Saw pod success
Mar  1 00:21:29.993: INFO: Pod "pod-d9cd8a94-17c8-498f-878c-c5bc5d458757" satisfied condition "success or failure"
Mar  1 00:21:29.995: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-d9cd8a94-17c8-498f-878c-c5bc5d458757 container test-container: <nil>
STEP: delete the pod
Mar  1 00:21:30.014: INFO: Waiting for pod pod-d9cd8a94-17c8-498f-878c-c5bc5d458757 to disappear
Mar  1 00:21:30.018: INFO: Pod pod-d9cd8a94-17c8-498f-878c-c5bc5d458757 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:21:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-647" for this suite.
Mar  1 00:21:36.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:21:36.128: INFO: namespace emptydir-647 deletion completed in 6.107233368s

• [SLOW TEST:10.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:21:36.129: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:21:41.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5269" for this suite.
Mar  1 00:22:09.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:22:09.302: INFO: namespace replication-controller-5269 deletion completed in 28.091818337s

• [SLOW TEST:33.173 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:22:09.303: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:22:09.329: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:22:09.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6351" for this suite.
Mar  1 00:22:15.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:22:15.509: INFO: namespace custom-resource-definition-6351 deletion completed in 6.122111985s

• [SLOW TEST:6.206 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:22:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-1465d25e-b52e-4e5b-9db9-8f8a229d8094
STEP: Creating a pod to test consume configMaps
Mar  1 00:22:15.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e" in namespace "configmap-8480" to be "success or failure"
Mar  1 00:22:15.573: INFO: Pod "pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.534608ms
Mar  1 00:22:17.576: INFO: Pod "pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014623778s
Mar  1 00:22:19.579: INFO: Pod "pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017764921s
STEP: Saw pod success
Mar  1 00:22:19.579: INFO: Pod "pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e" satisfied condition "success or failure"
Mar  1 00:22:19.582: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:22:19.610: INFO: Waiting for pod pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e to disappear
Mar  1 00:22:19.611: INFO: Pod pod-configmaps-eca5c359-f24e-4338-870e-f2196f89c52e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:22:19.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8480" for this suite.
Mar  1 00:22:25.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:22:25.707: INFO: namespace configmap-8480 deletion completed in 6.089629273s

• [SLOW TEST:10.196 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:22:25.707: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:22:25.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504" in namespace "projected-5699" to be "success or failure"
Mar  1 00:22:25.754: INFO: Pod "downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985325ms
Mar  1 00:22:27.761: INFO: Pod "downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013027843s
Mar  1 00:22:29.765: INFO: Pod "downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016836122s
STEP: Saw pod success
Mar  1 00:22:29.765: INFO: Pod "downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504" satisfied condition "success or failure"
Mar  1 00:22:29.768: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504 container client-container: <nil>
STEP: delete the pod
Mar  1 00:22:29.795: INFO: Waiting for pod downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504 to disappear
Mar  1 00:22:29.797: INFO: Pod downwardapi-volume-4c054c0f-849c-4c55-988c-61109a9cf504 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:22:29.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5699" for this suite.
Mar  1 00:22:35.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:22:35.921: INFO: namespace projected-5699 deletion completed in 6.121232023s

• [SLOW TEST:10.214 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:22:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  1 00:22:40.481: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1983 pod-service-account-7c542e76-9bdb-4845-af9d-6338a46afbd0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  1 00:22:40.766: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1983 pod-service-account-7c542e76-9bdb-4845-af9d-6338a46afbd0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  1 00:22:41.071: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1983 pod-service-account-7c542e76-9bdb-4845-af9d-6338a46afbd0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:22:41.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1983" for this suite.
Mar  1 00:22:47.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:22:47.447: INFO: namespace svcaccounts-1983 deletion completed in 6.100698146s

• [SLOW TEST:11.526 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:22:47.448: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2962 to expose endpoints map[]
Mar  1 00:22:47.484: INFO: Get endpoints failed (2.27812ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  1 00:22:48.488: INFO: successfully validated that service endpoint-test2 in namespace services-2962 exposes endpoints map[] (1.006033124s elapsed)
STEP: Creating pod pod1 in namespace services-2962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2962 to expose endpoints map[pod1:[80]]
Mar  1 00:22:51.530: INFO: successfully validated that service endpoint-test2 in namespace services-2962 exposes endpoints map[pod1:[80]] (3.03196707s elapsed)
STEP: Creating pod pod2 in namespace services-2962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2962 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  1 00:22:54.572: INFO: successfully validated that service endpoint-test2 in namespace services-2962 exposes endpoints map[pod1:[80] pod2:[80]] (3.037624678s elapsed)
STEP: Deleting pod pod1 in namespace services-2962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2962 to expose endpoints map[pod2:[80]]
Mar  1 00:22:54.596: INFO: successfully validated that service endpoint-test2 in namespace services-2962 exposes endpoints map[pod2:[80]] (19.186792ms elapsed)
STEP: Deleting pod pod2 in namespace services-2962
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2962 to expose endpoints map[]
Mar  1 00:22:54.607: INFO: successfully validated that service endpoint-test2 in namespace services-2962 exposes endpoints map[] (1.479258ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:22:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2962" for this suite.
Mar  1 00:23:00.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:23:00.729: INFO: namespace services-2962 deletion completed in 6.088647097s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.282 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:23:00.733: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  1 00:23:05.290: INFO: Successfully updated pod "pod-update-activedeadlineseconds-46667c8a-6c10-487c-8472-ad39bd316cef"
Mar  1 00:23:05.291: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-46667c8a-6c10-487c-8472-ad39bd316cef" in namespace "pods-9959" to be "terminated due to deadline exceeded"
Mar  1 00:23:05.306: INFO: Pod "pod-update-activedeadlineseconds-46667c8a-6c10-487c-8472-ad39bd316cef": Phase="Running", Reason="", readiness=true. Elapsed: 15.724505ms
Mar  1 00:23:07.309: INFO: Pod "pod-update-activedeadlineseconds-46667c8a-6c10-487c-8472-ad39bd316cef": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.018655236s
Mar  1 00:23:07.309: INFO: Pod "pod-update-activedeadlineseconds-46667c8a-6c10-487c-8472-ad39bd316cef" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:23:07.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9959" for this suite.
Mar  1 00:23:13.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:23:13.417: INFO: namespace pods-9959 deletion completed in 6.10333199s

• [SLOW TEST:12.684 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:23:13.418: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-1648625d-bbcf-4dbd-ba1e-02d408ac07bb
STEP: Creating a pod to test consume configMaps
Mar  1 00:23:13.454: INFO: Waiting up to 5m0s for pod "pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d" in namespace "configmap-9970" to be "success or failure"
Mar  1 00:23:13.458: INFO: Pod "pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881793ms
Mar  1 00:23:15.462: INFO: Pod "pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007636215s
Mar  1 00:23:17.465: INFO: Pod "pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010584105s
STEP: Saw pod success
Mar  1 00:23:17.465: INFO: Pod "pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d" satisfied condition "success or failure"
Mar  1 00:23:17.466: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:23:17.490: INFO: Waiting for pod pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d to disappear
Mar  1 00:23:17.492: INFO: Pod pod-configmaps-fab67e35-8f76-4d54-9008-8908662a428d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:23:17.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9970" for this suite.
Mar  1 00:23:23.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:23:23.605: INFO: namespace configmap-9970 deletion completed in 6.109570518s

• [SLOW TEST:10.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:23:23.610: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-63b890ea-197c-48e1-acd6-fd54e08e0091
STEP: Creating configMap with name cm-test-opt-upd-92ba8736-6785-4034-bd56-99f542dd697d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-63b890ea-197c-48e1-acd6-fd54e08e0091
STEP: Updating configmap cm-test-opt-upd-92ba8736-6785-4034-bd56-99f542dd697d
STEP: Creating configMap with name cm-test-opt-create-dac45e02-fe94-4361-b605-3f62afb31161
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:23:31.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2707" for this suite.
Mar  1 00:23:43.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:23:43.892: INFO: namespace projected-2707 deletion completed in 12.090726846s

• [SLOW TEST:20.282 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:23:43.893: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  1 00:23:49.958: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:49.958: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.114: INFO: Exec stderr: ""
Mar  1 00:23:50.114: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.114: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.258: INFO: Exec stderr: ""
Mar  1 00:23:50.258: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.258: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.415: INFO: Exec stderr: ""
Mar  1 00:23:50.415: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.567: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  1 00:23:50.568: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.568: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.784: INFO: Exec stderr: ""
Mar  1 00:23:50.785: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:50.923: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  1 00:23:50.923: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:51.128: INFO: Exec stderr: ""
Mar  1 00:23:51.128: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:51.128: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:51.271: INFO: Exec stderr: ""
Mar  1 00:23:51.271: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:51.271: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:51.408: INFO: Exec stderr: ""
Mar  1 00:23:51.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8344 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 00:23:51.408: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:23:51.543: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:23:51.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8344" for this suite.
Mar  1 00:24:41.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:24:41.645: INFO: namespace e2e-kubelet-etc-hosts-8344 deletion completed in 50.096854606s

• [SLOW TEST:57.752 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:24:41.646: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Mar  1 00:24:41.684: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8161" to be "success or failure"
Mar  1 00:24:41.690: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840695ms
Mar  1 00:24:43.693: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008586983s
Mar  1 00:24:45.702: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018309563s
STEP: Saw pod success
Mar  1 00:24:45.702: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  1 00:24:45.705: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  1 00:24:45.727: INFO: Waiting for pod pod-host-path-test to disappear
Mar  1 00:24:45.735: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:24:45.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8161" for this suite.
Mar  1 00:24:51.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:24:51.855: INFO: namespace hostpath-8161 deletion completed in 6.117192134s

• [SLOW TEST:10.209 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:24:51.856: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5707
I0301 00:24:51.892761      21 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5707, replica count: 1
I0301 00:24:52.943484      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 00:24:53.943727      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 00:24:54.943923      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 00:24:55.058: INFO: Created: latency-svc-2cfh8
Mar  1 00:24:55.066: INFO: Got endpoints: latency-svc-2cfh8 [22.064239ms]
Mar  1 00:24:55.088: INFO: Created: latency-svc-9x6cc
Mar  1 00:24:55.097: INFO: Got endpoints: latency-svc-9x6cc [30.948288ms]
Mar  1 00:24:55.108: INFO: Created: latency-svc-f7f9l
Mar  1 00:24:55.116: INFO: Got endpoints: latency-svc-f7f9l [49.894134ms]
Mar  1 00:24:55.122: INFO: Created: latency-svc-5r9r9
Mar  1 00:24:55.127: INFO: Got endpoints: latency-svc-5r9r9 [60.704629ms]
Mar  1 00:24:55.144: INFO: Created: latency-svc-tvdxs
Mar  1 00:24:55.151: INFO: Got endpoints: latency-svc-tvdxs [84.213066ms]
Mar  1 00:24:55.156: INFO: Created: latency-svc-2d2sg
Mar  1 00:24:55.158: INFO: Got endpoints: latency-svc-2d2sg [90.968181ms]
Mar  1 00:24:55.171: INFO: Created: latency-svc-jg5jw
Mar  1 00:24:55.174: INFO: Got endpoints: latency-svc-jg5jw [106.868497ms]
Mar  1 00:24:55.186: INFO: Created: latency-svc-g6kmp
Mar  1 00:24:55.188: INFO: Got endpoints: latency-svc-g6kmp [120.651409ms]
Mar  1 00:24:55.203: INFO: Created: latency-svc-r2qlt
Mar  1 00:24:55.210: INFO: Got endpoints: latency-svc-r2qlt [142.040681ms]
Mar  1 00:24:55.226: INFO: Created: latency-svc-lc244
Mar  1 00:24:55.250: INFO: Got endpoints: latency-svc-lc244 [182.104665ms]
Mar  1 00:24:55.261: INFO: Created: latency-svc-5jzmr
Mar  1 00:24:55.261: INFO: Got endpoints: latency-svc-5jzmr [192.664642ms]
Mar  1 00:24:55.281: INFO: Created: latency-svc-4gxsq
Mar  1 00:24:55.296: INFO: Got endpoints: latency-svc-4gxsq [227.340565ms]
Mar  1 00:24:55.303: INFO: Created: latency-svc-nkxcl
Mar  1 00:24:55.308: INFO: Got endpoints: latency-svc-nkxcl [238.561981ms]
Mar  1 00:24:55.325: INFO: Created: latency-svc-gbmk6
Mar  1 00:24:55.333: INFO: Got endpoints: latency-svc-gbmk6 [263.296168ms]
Mar  1 00:24:55.342: INFO: Created: latency-svc-zq5zf
Mar  1 00:24:55.366: INFO: Got endpoints: latency-svc-zq5zf [296.485802ms]
Mar  1 00:24:55.367: INFO: Created: latency-svc-5qvl4
Mar  1 00:24:55.385: INFO: Got endpoints: latency-svc-5qvl4 [316.273582ms]
Mar  1 00:24:55.388: INFO: Created: latency-svc-ggrt4
Mar  1 00:24:55.402: INFO: Got endpoints: latency-svc-ggrt4 [304.418588ms]
Mar  1 00:24:55.405: INFO: Created: latency-svc-pp89h
Mar  1 00:24:55.407: INFO: Got endpoints: latency-svc-pp89h [290.487375ms]
Mar  1 00:24:55.423: INFO: Created: latency-svc-nmmsz
Mar  1 00:24:55.438: INFO: Got endpoints: latency-svc-nmmsz [310.380825ms]
Mar  1 00:24:55.439: INFO: Created: latency-svc-7dml6
Mar  1 00:24:55.449: INFO: Got endpoints: latency-svc-7dml6 [297.663798ms]
Mar  1 00:24:55.458: INFO: Created: latency-svc-tjch8
Mar  1 00:24:55.466: INFO: Got endpoints: latency-svc-tjch8 [307.467272ms]
Mar  1 00:24:55.483: INFO: Created: latency-svc-kptsc
Mar  1 00:24:55.485: INFO: Got endpoints: latency-svc-kptsc [310.737109ms]
Mar  1 00:24:55.507: INFO: Created: latency-svc-7rgpp
Mar  1 00:24:55.507: INFO: Got endpoints: latency-svc-7rgpp [318.37871ms]
Mar  1 00:24:55.516: INFO: Created: latency-svc-w7cr2
Mar  1 00:24:55.522: INFO: Got endpoints: latency-svc-w7cr2 [311.807802ms]
Mar  1 00:24:55.537: INFO: Created: latency-svc-mqzwq
Mar  1 00:24:55.550: INFO: Created: latency-svc-tf2nb
Mar  1 00:24:55.552: INFO: Got endpoints: latency-svc-mqzwq [301.214658ms]
Mar  1 00:24:55.561: INFO: Got endpoints: latency-svc-tf2nb [299.909037ms]
Mar  1 00:24:55.582: INFO: Created: latency-svc-xmbs9
Mar  1 00:24:55.608: INFO: Got endpoints: latency-svc-xmbs9 [311.695335ms]
Mar  1 00:24:55.608: INFO: Created: latency-svc-x9brw
Mar  1 00:24:55.608: INFO: Got endpoints: latency-svc-x9brw [299.888895ms]
Mar  1 00:24:55.635: INFO: Created: latency-svc-s92kl
Mar  1 00:24:55.635: INFO: Got endpoints: latency-svc-s92kl [302.365058ms]
Mar  1 00:24:55.663: INFO: Created: latency-svc-987fl
Mar  1 00:24:55.669: INFO: Got endpoints: latency-svc-987fl [302.554421ms]
Mar  1 00:24:55.672: INFO: Created: latency-svc-t9c6b
Mar  1 00:24:55.674: INFO: Got endpoints: latency-svc-t9c6b [289.061668ms]
Mar  1 00:24:55.690: INFO: Created: latency-svc-wrn65
Mar  1 00:24:55.693: INFO: Got endpoints: latency-svc-wrn65 [291.16469ms]
Mar  1 00:24:55.717: INFO: Created: latency-svc-8422d
Mar  1 00:24:55.719: INFO: Got endpoints: latency-svc-8422d [311.668919ms]
Mar  1 00:24:55.732: INFO: Created: latency-svc-6sw79
Mar  1 00:24:55.734: INFO: Got endpoints: latency-svc-6sw79 [295.603234ms]
Mar  1 00:24:55.762: INFO: Created: latency-svc-6sxlq
Mar  1 00:24:55.767: INFO: Got endpoints: latency-svc-6sxlq [317.910644ms]
Mar  1 00:24:55.786: INFO: Created: latency-svc-5rlvm
Mar  1 00:24:55.786: INFO: Got endpoints: latency-svc-5rlvm [320.028829ms]
Mar  1 00:24:55.798: INFO: Created: latency-svc-v6976
Mar  1 00:24:55.801: INFO: Got endpoints: latency-svc-v6976 [313.011775ms]
Mar  1 00:24:55.831: INFO: Created: latency-svc-sprrj
Mar  1 00:24:55.836: INFO: Got endpoints: latency-svc-sprrj [328.630005ms]
Mar  1 00:24:55.848: INFO: Created: latency-svc-rm7bd
Mar  1 00:24:55.850: INFO: Got endpoints: latency-svc-rm7bd [328.500142ms]
Mar  1 00:24:55.867: INFO: Created: latency-svc-8bfqp
Mar  1 00:24:55.874: INFO: Got endpoints: latency-svc-8bfqp [318.963113ms]
Mar  1 00:24:55.875: INFO: Created: latency-svc-qlmzd
Mar  1 00:24:55.886: INFO: Got endpoints: latency-svc-qlmzd [324.880384ms]
Mar  1 00:24:55.888: INFO: Created: latency-svc-w8txg
Mar  1 00:24:55.895: INFO: Got endpoints: latency-svc-w8txg [286.636171ms]
Mar  1 00:24:55.919: INFO: Created: latency-svc-sjsrf
Mar  1 00:24:55.922: INFO: Got endpoints: latency-svc-sjsrf [312.825387ms]
Mar  1 00:24:55.938: INFO: Created: latency-svc-cjnfl
Mar  1 00:24:55.939: INFO: Got endpoints: latency-svc-cjnfl [304.03006ms]
Mar  1 00:24:55.950: INFO: Created: latency-svc-bcmfq
Mar  1 00:24:55.958: INFO: Got endpoints: latency-svc-bcmfq [288.627332ms]
Mar  1 00:24:55.967: INFO: Created: latency-svc-pwgss
Mar  1 00:24:55.973: INFO: Got endpoints: latency-svc-pwgss [33.160621ms]
Mar  1 00:24:55.992: INFO: Created: latency-svc-s42kz
Mar  1 00:24:55.998: INFO: Got endpoints: latency-svc-s42kz [323.604011ms]
Mar  1 00:24:56.010: INFO: Created: latency-svc-kqbrq
Mar  1 00:24:56.029: INFO: Got endpoints: latency-svc-kqbrq [335.7538ms]
Mar  1 00:24:56.090: INFO: Created: latency-svc-vzg9r
Mar  1 00:24:56.125: INFO: Got endpoints: latency-svc-vzg9r [406.369536ms]
Mar  1 00:24:56.149: INFO: Created: latency-svc-lh8jg
Mar  1 00:24:56.165: INFO: Got endpoints: latency-svc-lh8jg [430.664218ms]
Mar  1 00:24:56.173: INFO: Created: latency-svc-knmzr
Mar  1 00:24:56.218: INFO: Got endpoints: latency-svc-knmzr [451.042556ms]
Mar  1 00:24:56.246: INFO: Created: latency-svc-56dbx
Mar  1 00:24:56.263: INFO: Got endpoints: latency-svc-56dbx [477.335344ms]
Mar  1 00:24:56.286: INFO: Created: latency-svc-plcd7
Mar  1 00:24:56.292: INFO: Got endpoints: latency-svc-plcd7 [490.846961ms]
Mar  1 00:24:56.312: INFO: Created: latency-svc-mjffh
Mar  1 00:24:56.319: INFO: Got endpoints: latency-svc-mjffh [482.685508ms]
Mar  1 00:24:56.336: INFO: Created: latency-svc-pgwns
Mar  1 00:24:56.338: INFO: Got endpoints: latency-svc-pgwns [487.364986ms]
Mar  1 00:24:56.360: INFO: Created: latency-svc-k2nbk
Mar  1 00:24:56.364: INFO: Got endpoints: latency-svc-k2nbk [489.40592ms]
Mar  1 00:24:56.395: INFO: Created: latency-svc-6264g
Mar  1 00:24:56.410: INFO: Created: latency-svc-4wxm5
Mar  1 00:24:56.422: INFO: Got endpoints: latency-svc-6264g [535.077398ms]
Mar  1 00:24:56.425: INFO: Created: latency-svc-c8dwl
Mar  1 00:24:56.452: INFO: Created: latency-svc-jx8mq
Mar  1 00:24:56.476: INFO: Got endpoints: latency-svc-4wxm5 [580.020737ms]
Mar  1 00:24:56.478: INFO: Created: latency-svc-jwr7w
Mar  1 00:24:56.495: INFO: Created: latency-svc-ffnff
Mar  1 00:24:56.510: INFO: Created: latency-svc-l4kt8
Mar  1 00:24:56.520: INFO: Got endpoints: latency-svc-c8dwl [598.298242ms]
Mar  1 00:24:56.535: INFO: Created: latency-svc-pbwxh
Mar  1 00:24:56.560: INFO: Created: latency-svc-966dd
Mar  1 00:24:56.567: INFO: Got endpoints: latency-svc-jx8mq [608.652784ms]
Mar  1 00:24:56.584: INFO: Created: latency-svc-z6j4f
Mar  1 00:24:56.599: INFO: Created: latency-svc-q8q99
Mar  1 00:24:56.616: INFO: Created: latency-svc-978np
Mar  1 00:24:56.618: INFO: Got endpoints: latency-svc-jwr7w [644.43461ms]
Mar  1 00:24:56.628: INFO: Created: latency-svc-g8zts
Mar  1 00:24:56.644: INFO: Created: latency-svc-76kvj
Mar  1 00:24:56.655: INFO: Created: latency-svc-sttjl
Mar  1 00:24:56.667: INFO: Got endpoints: latency-svc-ffnff [668.298183ms]
Mar  1 00:24:56.691: INFO: Created: latency-svc-6d28s
Mar  1 00:24:56.691: INFO: Created: latency-svc-t82d5
Mar  1 00:24:56.709: INFO: Created: latency-svc-f7x24
Mar  1 00:24:56.723: INFO: Created: latency-svc-48k8g
Mar  1 00:24:56.726: INFO: Got endpoints: latency-svc-l4kt8 [697.285993ms]
Mar  1 00:24:56.749: INFO: Created: latency-svc-fm8kp
Mar  1 00:24:56.762: INFO: Created: latency-svc-kgrtz
Mar  1 00:24:56.765: INFO: Got endpoints: latency-svc-pbwxh [639.980059ms]
Mar  1 00:24:56.786: INFO: Created: latency-svc-kpppg
Mar  1 00:24:56.795: INFO: Created: latency-svc-wctbz
Mar  1 00:24:56.816: INFO: Got endpoints: latency-svc-966dd [649.699107ms]
Mar  1 00:24:56.839: INFO: Created: latency-svc-7szzs
Mar  1 00:24:56.865: INFO: Got endpoints: latency-svc-z6j4f [646.430677ms]
Mar  1 00:24:56.912: INFO: Created: latency-svc-4qr8f
Mar  1 00:24:56.917: INFO: Got endpoints: latency-svc-q8q99 [653.136862ms]
Mar  1 00:24:56.939: INFO: Created: latency-svc-ksg7c
Mar  1 00:24:56.964: INFO: Got endpoints: latency-svc-978np [671.321046ms]
Mar  1 00:24:56.984: INFO: Created: latency-svc-w6vsp
Mar  1 00:24:57.014: INFO: Got endpoints: latency-svc-g8zts [694.765838ms]
Mar  1 00:24:57.033: INFO: Created: latency-svc-c2zbq
Mar  1 00:24:57.067: INFO: Got endpoints: latency-svc-76kvj [728.379172ms]
Mar  1 00:24:57.103: INFO: Created: latency-svc-2fb67
Mar  1 00:24:57.113: INFO: Got endpoints: latency-svc-sttjl [749.668972ms]
Mar  1 00:24:57.135: INFO: Created: latency-svc-6jk6f
Mar  1 00:24:57.164: INFO: Got endpoints: latency-svc-t82d5 [741.872473ms]
Mar  1 00:24:57.186: INFO: Created: latency-svc-wnv5n
Mar  1 00:24:57.224: INFO: Got endpoints: latency-svc-6d28s [748.517484ms]
Mar  1 00:24:57.242: INFO: Created: latency-svc-mds6c
Mar  1 00:24:57.264: INFO: Got endpoints: latency-svc-f7x24 [743.477355ms]
Mar  1 00:24:57.287: INFO: Created: latency-svc-bd4bp
Mar  1 00:24:57.316: INFO: Got endpoints: latency-svc-48k8g [749.122794ms]
Mar  1 00:24:57.340: INFO: Created: latency-svc-fg4hv
Mar  1 00:24:57.372: INFO: Got endpoints: latency-svc-fm8kp [753.661884ms]
Mar  1 00:24:57.392: INFO: Created: latency-svc-nrlkz
Mar  1 00:24:57.413: INFO: Got endpoints: latency-svc-kgrtz [744.305493ms]
Mar  1 00:24:57.446: INFO: Created: latency-svc-stv7l
Mar  1 00:24:57.464: INFO: Got endpoints: latency-svc-kpppg [737.994459ms]
Mar  1 00:24:57.486: INFO: Created: latency-svc-7xqnk
Mar  1 00:24:57.513: INFO: Got endpoints: latency-svc-wctbz [747.301231ms]
Mar  1 00:24:57.534: INFO: Created: latency-svc-4f58j
Mar  1 00:24:57.564: INFO: Got endpoints: latency-svc-7szzs [747.948948ms]
Mar  1 00:24:57.578: INFO: Created: latency-svc-4hfsp
Mar  1 00:24:57.618: INFO: Got endpoints: latency-svc-4qr8f [753.06359ms]
Mar  1 00:24:57.635: INFO: Created: latency-svc-dfdf9
Mar  1 00:24:57.664: INFO: Got endpoints: latency-svc-ksg7c [746.890176ms]
Mar  1 00:24:57.682: INFO: Created: latency-svc-tsgg2
Mar  1 00:24:57.721: INFO: Got endpoints: latency-svc-w6vsp [756.865844ms]
Mar  1 00:24:57.768: INFO: Got endpoints: latency-svc-c2zbq [754.341615ms]
Mar  1 00:24:57.769: INFO: Created: latency-svc-7c96t
Mar  1 00:24:57.785: INFO: Created: latency-svc-f7xmv
Mar  1 00:24:57.813: INFO: Got endpoints: latency-svc-2fb67 [746.712911ms]
Mar  1 00:24:57.835: INFO: Created: latency-svc-gffnl
Mar  1 00:24:57.865: INFO: Got endpoints: latency-svc-6jk6f [750.6785ms]
Mar  1 00:24:57.890: INFO: Created: latency-svc-mbgg8
Mar  1 00:24:57.918: INFO: Got endpoints: latency-svc-wnv5n [753.512734ms]
Mar  1 00:24:57.938: INFO: Created: latency-svc-2g9s7
Mar  1 00:24:57.966: INFO: Got endpoints: latency-svc-mds6c [741.725076ms]
Mar  1 00:24:57.987: INFO: Created: latency-svc-97l9z
Mar  1 00:24:58.018: INFO: Got endpoints: latency-svc-bd4bp [754.093276ms]
Mar  1 00:24:58.045: INFO: Created: latency-svc-8dpqg
Mar  1 00:24:58.069: INFO: Got endpoints: latency-svc-fg4hv [752.671016ms]
Mar  1 00:24:58.096: INFO: Created: latency-svc-ngwq5
Mar  1 00:24:58.115: INFO: Got endpoints: latency-svc-nrlkz [742.569246ms]
Mar  1 00:24:58.138: INFO: Created: latency-svc-mdbh6
Mar  1 00:24:58.164: INFO: Got endpoints: latency-svc-stv7l [751.560629ms]
Mar  1 00:24:58.187: INFO: Created: latency-svc-5fkv2
Mar  1 00:24:58.215: INFO: Got endpoints: latency-svc-7xqnk [750.179974ms]
Mar  1 00:24:58.237: INFO: Created: latency-svc-7wrhf
Mar  1 00:24:58.266: INFO: Got endpoints: latency-svc-4f58j [752.882532ms]
Mar  1 00:24:58.282: INFO: Created: latency-svc-d7kjz
Mar  1 00:24:58.314: INFO: Got endpoints: latency-svc-4hfsp [750.295446ms]
Mar  1 00:24:58.329: INFO: Created: latency-svc-k2dfd
Mar  1 00:24:58.364: INFO: Got endpoints: latency-svc-dfdf9 [745.490171ms]
Mar  1 00:24:58.389: INFO: Created: latency-svc-phbnv
Mar  1 00:24:58.416: INFO: Got endpoints: latency-svc-tsgg2 [751.282249ms]
Mar  1 00:24:58.438: INFO: Created: latency-svc-prw4s
Mar  1 00:24:58.464: INFO: Got endpoints: latency-svc-7c96t [742.742683ms]
Mar  1 00:24:58.483: INFO: Created: latency-svc-fm78g
Mar  1 00:24:58.514: INFO: Got endpoints: latency-svc-f7xmv [745.878433ms]
Mar  1 00:24:58.534: INFO: Created: latency-svc-vxdqc
Mar  1 00:24:58.585: INFO: Got endpoints: latency-svc-gffnl [771.072705ms]
Mar  1 00:24:58.604: INFO: Created: latency-svc-qrm7s
Mar  1 00:24:58.613: INFO: Got endpoints: latency-svc-mbgg8 [747.942551ms]
Mar  1 00:24:58.635: INFO: Created: latency-svc-kslsl
Mar  1 00:24:58.668: INFO: Got endpoints: latency-svc-2g9s7 [749.332035ms]
Mar  1 00:24:58.682: INFO: Created: latency-svc-vj99q
Mar  1 00:24:58.713: INFO: Got endpoints: latency-svc-97l9z [746.887678ms]
Mar  1 00:24:58.732: INFO: Created: latency-svc-dlp7m
Mar  1 00:24:58.767: INFO: Got endpoints: latency-svc-8dpqg [747.816827ms]
Mar  1 00:24:58.782: INFO: Created: latency-svc-xkbl4
Mar  1 00:24:58.815: INFO: Got endpoints: latency-svc-ngwq5 [745.124012ms]
Mar  1 00:24:58.831: INFO: Created: latency-svc-wq6bc
Mar  1 00:24:58.871: INFO: Got endpoints: latency-svc-mdbh6 [755.866388ms]
Mar  1 00:24:58.885: INFO: Created: latency-svc-f8kdd
Mar  1 00:24:58.917: INFO: Got endpoints: latency-svc-5fkv2 [752.500935ms]
Mar  1 00:24:58.935: INFO: Created: latency-svc-fgzs4
Mar  1 00:24:58.963: INFO: Got endpoints: latency-svc-7wrhf [746.86109ms]
Mar  1 00:24:58.982: INFO: Created: latency-svc-bzx4x
Mar  1 00:24:59.014: INFO: Got endpoints: latency-svc-d7kjz [747.708994ms]
Mar  1 00:24:59.030: INFO: Created: latency-svc-8q4wb
Mar  1 00:24:59.068: INFO: Got endpoints: latency-svc-k2dfd [754.035835ms]
Mar  1 00:24:59.088: INFO: Created: latency-svc-ws4n7
Mar  1 00:24:59.113: INFO: Got endpoints: latency-svc-phbnv [748.895934ms]
Mar  1 00:24:59.145: INFO: Created: latency-svc-z9xm5
Mar  1 00:24:59.166: INFO: Got endpoints: latency-svc-prw4s [749.603454ms]
Mar  1 00:24:59.191: INFO: Created: latency-svc-qsc4k
Mar  1 00:24:59.216: INFO: Got endpoints: latency-svc-fm78g [751.93676ms]
Mar  1 00:24:59.234: INFO: Created: latency-svc-tbxmb
Mar  1 00:24:59.265: INFO: Got endpoints: latency-svc-vxdqc [751.164999ms]
Mar  1 00:24:59.294: INFO: Created: latency-svc-tfrb4
Mar  1 00:24:59.316: INFO: Got endpoints: latency-svc-qrm7s [731.377512ms]
Mar  1 00:24:59.333: INFO: Created: latency-svc-f8j2q
Mar  1 00:24:59.365: INFO: Got endpoints: latency-svc-kslsl [750.992961ms]
Mar  1 00:24:59.384: INFO: Created: latency-svc-tgt7f
Mar  1 00:24:59.415: INFO: Got endpoints: latency-svc-vj99q [746.337604ms]
Mar  1 00:24:59.427: INFO: Created: latency-svc-8cpp9
Mar  1 00:24:59.464: INFO: Got endpoints: latency-svc-dlp7m [750.49103ms]
Mar  1 00:24:59.483: INFO: Created: latency-svc-2q97j
Mar  1 00:24:59.519: INFO: Got endpoints: latency-svc-xkbl4 [752.077233ms]
Mar  1 00:24:59.541: INFO: Created: latency-svc-8vcn8
Mar  1 00:24:59.568: INFO: Got endpoints: latency-svc-wq6bc [753.256808ms]
Mar  1 00:24:59.591: INFO: Created: latency-svc-9swlr
Mar  1 00:24:59.618: INFO: Got endpoints: latency-svc-f8kdd [746.925102ms]
Mar  1 00:24:59.636: INFO: Created: latency-svc-2s84l
Mar  1 00:24:59.672: INFO: Got endpoints: latency-svc-fgzs4 [754.974294ms]
Mar  1 00:24:59.697: INFO: Created: latency-svc-nq62q
Mar  1 00:24:59.725: INFO: Got endpoints: latency-svc-bzx4x [761.950701ms]
Mar  1 00:24:59.742: INFO: Created: latency-svc-jltmp
Mar  1 00:24:59.764: INFO: Got endpoints: latency-svc-8q4wb [749.953358ms]
Mar  1 00:24:59.781: INFO: Created: latency-svc-72l7d
Mar  1 00:24:59.814: INFO: Got endpoints: latency-svc-ws4n7 [745.695489ms]
Mar  1 00:24:59.841: INFO: Created: latency-svc-4ckhd
Mar  1 00:24:59.866: INFO: Got endpoints: latency-svc-z9xm5 [752.883304ms]
Mar  1 00:24:59.887: INFO: Created: latency-svc-vw98r
Mar  1 00:24:59.916: INFO: Got endpoints: latency-svc-qsc4k [750.846485ms]
Mar  1 00:24:59.933: INFO: Created: latency-svc-f2nvh
Mar  1 00:24:59.964: INFO: Got endpoints: latency-svc-tbxmb [747.699341ms]
Mar  1 00:24:59.986: INFO: Created: latency-svc-d7ltx
Mar  1 00:25:00.014: INFO: Got endpoints: latency-svc-tfrb4 [748.698416ms]
Mar  1 00:25:00.038: INFO: Created: latency-svc-pd6wm
Mar  1 00:25:00.063: INFO: Got endpoints: latency-svc-f8j2q [746.514106ms]
Mar  1 00:25:00.087: INFO: Created: latency-svc-bfcjg
Mar  1 00:25:00.115: INFO: Got endpoints: latency-svc-tgt7f [750.066973ms]
Mar  1 00:25:00.137: INFO: Created: latency-svc-9wpzz
Mar  1 00:25:00.170: INFO: Got endpoints: latency-svc-8cpp9 [754.721066ms]
Mar  1 00:25:00.192: INFO: Created: latency-svc-qmsg4
Mar  1 00:25:00.214: INFO: Got endpoints: latency-svc-2q97j [750.029559ms]
Mar  1 00:25:00.236: INFO: Created: latency-svc-cffnr
Mar  1 00:25:00.265: INFO: Got endpoints: latency-svc-8vcn8 [745.087091ms]
Mar  1 00:25:00.294: INFO: Created: latency-svc-2bgtb
Mar  1 00:25:00.313: INFO: Got endpoints: latency-svc-9swlr [744.043283ms]
Mar  1 00:25:00.330: INFO: Created: latency-svc-6rbhg
Mar  1 00:25:00.366: INFO: Got endpoints: latency-svc-2s84l [747.371547ms]
Mar  1 00:25:00.392: INFO: Created: latency-svc-k6wbp
Mar  1 00:25:00.413: INFO: Got endpoints: latency-svc-nq62q [741.427764ms]
Mar  1 00:25:00.440: INFO: Created: latency-svc-4rffr
Mar  1 00:25:00.465: INFO: Got endpoints: latency-svc-jltmp [739.96537ms]
Mar  1 00:25:00.485: INFO: Created: latency-svc-4t4p7
Mar  1 00:25:00.514: INFO: Got endpoints: latency-svc-72l7d [750.320078ms]
Mar  1 00:25:00.553: INFO: Created: latency-svc-k7d77
Mar  1 00:25:00.565: INFO: Got endpoints: latency-svc-4ckhd [750.559857ms]
Mar  1 00:25:00.592: INFO: Created: latency-svc-x88d2
Mar  1 00:25:00.616: INFO: Got endpoints: latency-svc-vw98r [749.351339ms]
Mar  1 00:25:00.640: INFO: Created: latency-svc-s7z8b
Mar  1 00:25:00.667: INFO: Got endpoints: latency-svc-f2nvh [749.208509ms]
Mar  1 00:25:00.693: INFO: Created: latency-svc-hd5gn
Mar  1 00:25:00.713: INFO: Got endpoints: latency-svc-d7ltx [748.421915ms]
Mar  1 00:25:00.736: INFO: Created: latency-svc-n92s2
Mar  1 00:25:00.765: INFO: Got endpoints: latency-svc-pd6wm [749.869305ms]
Mar  1 00:25:00.784: INFO: Created: latency-svc-vq4pm
Mar  1 00:25:00.826: INFO: Got endpoints: latency-svc-bfcjg [762.096223ms]
Mar  1 00:25:00.848: INFO: Created: latency-svc-tnclz
Mar  1 00:25:00.873: INFO: Got endpoints: latency-svc-9wpzz [757.281831ms]
Mar  1 00:25:00.904: INFO: Created: latency-svc-x58g5
Mar  1 00:25:00.916: INFO: Got endpoints: latency-svc-qmsg4 [745.079705ms]
Mar  1 00:25:00.955: INFO: Created: latency-svc-xmpwd
Mar  1 00:25:00.965: INFO: Got endpoints: latency-svc-cffnr [750.061099ms]
Mar  1 00:25:00.980: INFO: Created: latency-svc-7wb4l
Mar  1 00:25:01.014: INFO: Got endpoints: latency-svc-2bgtb [748.116958ms]
Mar  1 00:25:01.034: INFO: Created: latency-svc-wwh56
Mar  1 00:25:01.064: INFO: Got endpoints: latency-svc-6rbhg [751.039914ms]
Mar  1 00:25:01.085: INFO: Created: latency-svc-fm5vs
Mar  1 00:25:01.116: INFO: Got endpoints: latency-svc-k6wbp [750.271659ms]
Mar  1 00:25:01.133: INFO: Created: latency-svc-bt8t2
Mar  1 00:25:01.165: INFO: Got endpoints: latency-svc-4rffr [751.182461ms]
Mar  1 00:25:01.182: INFO: Created: latency-svc-9tqlh
Mar  1 00:25:01.214: INFO: Got endpoints: latency-svc-4t4p7 [748.219061ms]
Mar  1 00:25:01.235: INFO: Created: latency-svc-l57qb
Mar  1 00:25:01.279: INFO: Got endpoints: latency-svc-k7d77 [764.873803ms]
Mar  1 00:25:01.300: INFO: Created: latency-svc-cvsft
Mar  1 00:25:01.314: INFO: Got endpoints: latency-svc-x88d2 [747.436854ms]
Mar  1 00:25:01.335: INFO: Created: latency-svc-ml875
Mar  1 00:25:01.365: INFO: Got endpoints: latency-svc-s7z8b [748.746139ms]
Mar  1 00:25:01.387: INFO: Created: latency-svc-sd5jd
Mar  1 00:25:01.415: INFO: Got endpoints: latency-svc-hd5gn [747.847862ms]
Mar  1 00:25:01.443: INFO: Created: latency-svc-pqkdc
Mar  1 00:25:01.463: INFO: Got endpoints: latency-svc-n92s2 [749.808491ms]
Mar  1 00:25:01.484: INFO: Created: latency-svc-pnc2n
Mar  1 00:25:01.513: INFO: Got endpoints: latency-svc-vq4pm [748.146234ms]
Mar  1 00:25:01.536: INFO: Created: latency-svc-d44xv
Mar  1 00:25:01.567: INFO: Got endpoints: latency-svc-tnclz [741.129389ms]
Mar  1 00:25:01.583: INFO: Created: latency-svc-vn4zg
Mar  1 00:25:01.614: INFO: Got endpoints: latency-svc-x58g5 [741.721345ms]
Mar  1 00:25:01.656: INFO: Created: latency-svc-ts6bb
Mar  1 00:25:01.668: INFO: Got endpoints: latency-svc-xmpwd [751.651483ms]
Mar  1 00:25:01.695: INFO: Created: latency-svc-n6l99
Mar  1 00:25:01.716: INFO: Got endpoints: latency-svc-7wb4l [751.319222ms]
Mar  1 00:25:01.740: INFO: Created: latency-svc-jw2q5
Mar  1 00:25:01.767: INFO: Got endpoints: latency-svc-wwh56 [752.875715ms]
Mar  1 00:25:01.804: INFO: Created: latency-svc-lg7mj
Mar  1 00:25:01.827: INFO: Got endpoints: latency-svc-fm5vs [762.78845ms]
Mar  1 00:25:01.843: INFO: Created: latency-svc-t5sh6
Mar  1 00:25:01.864: INFO: Got endpoints: latency-svc-bt8t2 [748.130409ms]
Mar  1 00:25:01.992: INFO: Got endpoints: latency-svc-l57qb [778.104881ms]
Mar  1 00:25:01.996: INFO: Created: latency-svc-fw5w7
Mar  1 00:25:02.009: INFO: Got endpoints: latency-svc-9tqlh [844.508413ms]
Mar  1 00:25:02.070: INFO: Got endpoints: latency-svc-cvsft [790.405712ms]
Mar  1 00:25:02.095: INFO: Got endpoints: latency-svc-ml875 [780.699717ms]
Mar  1 00:25:02.110: INFO: Created: latency-svc-q9gk4
Mar  1 00:25:02.113: INFO: Got endpoints: latency-svc-sd5jd [747.528267ms]
Mar  1 00:25:02.130: INFO: Created: latency-svc-2hp4v
Mar  1 00:25:02.145: INFO: Created: latency-svc-l9nls
Mar  1 00:25:02.159: INFO: Created: latency-svc-97pvq
Mar  1 00:25:02.163: INFO: Got endpoints: latency-svc-pqkdc [748.114746ms]
Mar  1 00:25:02.184: INFO: Created: latency-svc-s6rcj
Mar  1 00:25:02.200: INFO: Created: latency-svc-9cdkw
Mar  1 00:25:02.220: INFO: Got endpoints: latency-svc-pnc2n [756.648179ms]
Mar  1 00:25:02.244: INFO: Created: latency-svc-4rznk
Mar  1 00:25:02.266: INFO: Got endpoints: latency-svc-d44xv [752.834042ms]
Mar  1 00:25:02.291: INFO: Created: latency-svc-gzg7m
Mar  1 00:25:02.318: INFO: Got endpoints: latency-svc-vn4zg [751.526822ms]
Mar  1 00:25:02.341: INFO: Created: latency-svc-c42pp
Mar  1 00:25:02.365: INFO: Got endpoints: latency-svc-ts6bb [750.102994ms]
Mar  1 00:25:02.386: INFO: Created: latency-svc-grq97
Mar  1 00:25:02.418: INFO: Got endpoints: latency-svc-n6l99 [749.504114ms]
Mar  1 00:25:02.446: INFO: Created: latency-svc-mw557
Mar  1 00:25:02.467: INFO: Got endpoints: latency-svc-jw2q5 [751.37817ms]
Mar  1 00:25:02.489: INFO: Created: latency-svc-6hjtn
Mar  1 00:25:02.517: INFO: Got endpoints: latency-svc-lg7mj [749.71662ms]
Mar  1 00:25:02.555: INFO: Created: latency-svc-vq9xn
Mar  1 00:25:02.562: INFO: Got endpoints: latency-svc-t5sh6 [734.9251ms]
Mar  1 00:25:02.585: INFO: Created: latency-svc-bmcq7
Mar  1 00:25:02.615: INFO: Got endpoints: latency-svc-fw5w7 [750.055385ms]
Mar  1 00:25:02.636: INFO: Created: latency-svc-nhj29
Mar  1 00:25:02.664: INFO: Got endpoints: latency-svc-q9gk4 [672.052617ms]
Mar  1 00:25:02.684: INFO: Created: latency-svc-97zxt
Mar  1 00:25:02.715: INFO: Got endpoints: latency-svc-2hp4v [705.279703ms]
Mar  1 00:25:02.740: INFO: Created: latency-svc-8blh9
Mar  1 00:25:02.764: INFO: Got endpoints: latency-svc-l9nls [693.966239ms]
Mar  1 00:25:02.780: INFO: Created: latency-svc-6s5p9
Mar  1 00:25:02.813: INFO: Got endpoints: latency-svc-97pvq [718.31895ms]
Mar  1 00:25:02.835: INFO: Created: latency-svc-8pkkm
Mar  1 00:25:02.867: INFO: Got endpoints: latency-svc-s6rcj [754.658995ms]
Mar  1 00:25:02.891: INFO: Created: latency-svc-rrrx2
Mar  1 00:25:02.915: INFO: Got endpoints: latency-svc-9cdkw [751.774237ms]
Mar  1 00:25:02.966: INFO: Got endpoints: latency-svc-4rznk [745.916586ms]
Mar  1 00:25:03.014: INFO: Got endpoints: latency-svc-gzg7m [747.518759ms]
Mar  1 00:25:03.065: INFO: Got endpoints: latency-svc-c42pp [745.752505ms]
Mar  1 00:25:03.114: INFO: Got endpoints: latency-svc-grq97 [749.420212ms]
Mar  1 00:25:03.165: INFO: Got endpoints: latency-svc-mw557 [746.658683ms]
Mar  1 00:25:03.217: INFO: Got endpoints: latency-svc-6hjtn [748.890308ms]
Mar  1 00:25:03.265: INFO: Got endpoints: latency-svc-vq9xn [747.346558ms]
Mar  1 00:25:03.316: INFO: Got endpoints: latency-svc-bmcq7 [753.511153ms]
Mar  1 00:25:03.365: INFO: Got endpoints: latency-svc-nhj29 [749.316586ms]
Mar  1 00:25:03.421: INFO: Got endpoints: latency-svc-97zxt [756.936461ms]
Mar  1 00:25:03.465: INFO: Got endpoints: latency-svc-8blh9 [750.08622ms]
Mar  1 00:25:03.514: INFO: Got endpoints: latency-svc-6s5p9 [749.777021ms]
Mar  1 00:25:03.565: INFO: Got endpoints: latency-svc-8pkkm [751.627326ms]
Mar  1 00:25:03.617: INFO: Got endpoints: latency-svc-rrrx2 [749.927338ms]
Mar  1 00:25:03.618: INFO: Latencies: [30.948288ms 33.160621ms 49.894134ms 60.704629ms 84.213066ms 90.968181ms 106.868497ms 120.651409ms 142.040681ms 182.104665ms 192.664642ms 227.340565ms 238.561981ms 263.296168ms 286.636171ms 288.627332ms 289.061668ms 290.487375ms 291.16469ms 295.603234ms 296.485802ms 297.663798ms 299.888895ms 299.909037ms 301.214658ms 302.365058ms 302.554421ms 304.03006ms 304.418588ms 307.467272ms 310.380825ms 310.737109ms 311.668919ms 311.695335ms 311.807802ms 312.825387ms 313.011775ms 316.273582ms 317.910644ms 318.37871ms 318.963113ms 320.028829ms 323.604011ms 324.880384ms 328.500142ms 328.630005ms 335.7538ms 406.369536ms 430.664218ms 451.042556ms 477.335344ms 482.685508ms 487.364986ms 489.40592ms 490.846961ms 535.077398ms 580.020737ms 598.298242ms 608.652784ms 639.980059ms 644.43461ms 646.430677ms 649.699107ms 653.136862ms 668.298183ms 671.321046ms 672.052617ms 693.966239ms 694.765838ms 697.285993ms 705.279703ms 718.31895ms 728.379172ms 731.377512ms 734.9251ms 737.994459ms 739.96537ms 741.129389ms 741.427764ms 741.721345ms 741.725076ms 741.872473ms 742.569246ms 742.742683ms 743.477355ms 744.043283ms 744.305493ms 745.079705ms 745.087091ms 745.124012ms 745.490171ms 745.695489ms 745.752505ms 745.878433ms 745.916586ms 746.337604ms 746.514106ms 746.658683ms 746.712911ms 746.86109ms 746.887678ms 746.890176ms 746.925102ms 747.301231ms 747.346558ms 747.371547ms 747.436854ms 747.518759ms 747.528267ms 747.699341ms 747.708994ms 747.816827ms 747.847862ms 747.942551ms 747.948948ms 748.114746ms 748.116958ms 748.130409ms 748.146234ms 748.219061ms 748.421915ms 748.517484ms 748.698416ms 748.746139ms 748.890308ms 748.895934ms 749.122794ms 749.208509ms 749.316586ms 749.332035ms 749.351339ms 749.420212ms 749.504114ms 749.603454ms 749.668972ms 749.71662ms 749.777021ms 749.808491ms 749.869305ms 749.927338ms 749.953358ms 750.029559ms 750.055385ms 750.061099ms 750.066973ms 750.08622ms 750.102994ms 750.179974ms 750.271659ms 750.295446ms 750.320078ms 750.49103ms 750.559857ms 750.6785ms 750.846485ms 750.992961ms 751.039914ms 751.164999ms 751.182461ms 751.282249ms 751.319222ms 751.37817ms 751.526822ms 751.560629ms 751.627326ms 751.651483ms 751.774237ms 751.93676ms 752.077233ms 752.500935ms 752.671016ms 752.834042ms 752.875715ms 752.882532ms 752.883304ms 753.06359ms 753.256808ms 753.511153ms 753.512734ms 753.661884ms 754.035835ms 754.093276ms 754.341615ms 754.658995ms 754.721066ms 754.974294ms 755.866388ms 756.648179ms 756.865844ms 756.936461ms 757.281831ms 761.950701ms 762.096223ms 762.78845ms 764.873803ms 771.072705ms 778.104881ms 780.699717ms 790.405712ms 844.508413ms]
Mar  1 00:25:03.618: INFO: 50 %ile: 746.887678ms
Mar  1 00:25:03.618: INFO: 90 %ile: 754.035835ms
Mar  1 00:25:03.619: INFO: 99 %ile: 790.405712ms
Mar  1 00:25:03.619: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:25:03.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5707" for this suite.
Mar  1 00:25:21.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:25:21.768: INFO: namespace svc-latency-5707 deletion completed in 18.138171102s

• [SLOW TEST:29.912 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:25:21.769: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6528
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar  1 00:25:21.817: INFO: Found 0 stateful pods, waiting for 3
Mar  1 00:25:31.821: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:25:31.821: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:25:31.821: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  1 00:25:31.849: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  1 00:25:41.884: INFO: Updating stateful set ss2
Mar  1 00:25:41.904: INFO: Waiting for Pod statefulset-6528/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  1 00:25:51.986: INFO: Found 2 stateful pods, waiting for 3
Mar  1 00:26:01.990: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:26:01.990: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 00:26:01.990: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  1 00:26:02.018: INFO: Updating stateful set ss2
Mar  1 00:26:02.034: INFO: Waiting for Pod statefulset-6528/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 00:26:12.061: INFO: Updating stateful set ss2
Mar  1 00:26:12.075: INFO: Waiting for StatefulSet statefulset-6528/ss2 to complete update
Mar  1 00:26:12.076: INFO: Waiting for Pod statefulset-6528/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 00:26:22.081: INFO: Deleting all statefulset in ns statefulset-6528
Mar  1 00:26:22.083: INFO: Scaling statefulset ss2 to 0
Mar  1 00:26:42.098: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 00:26:42.100: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:26:42.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6528" for this suite.
Mar  1 00:26:48.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:26:48.213: INFO: namespace statefulset-6528 deletion completed in 6.099492877s

• [SLOW TEST:86.444 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:26:48.214: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  1 00:26:48.246: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27238 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 00:26:48.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27238 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  1 00:26:58.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27257 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  1 00:26:58.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27257 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  1 00:27:08.260: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27278 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 00:27:08.260: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27278 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  1 00:27:18.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27297 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 00:27:18.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-a bb1bbc2c-369b-430f-aadd-227ce540dfce 27297 0 2020-03-01 00:26:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  1 00:27:28.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-b 2e55aef8-b6f6-43b3-b41a-6ce407137078 27319 0 2020-03-01 00:27:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 00:27:28.271: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-b 2e55aef8-b6f6-43b3-b41a-6ce407137078 27319 0 2020-03-01 00:27:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  1 00:27:38.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-b 2e55aef8-b6f6-43b3-b41a-6ce407137078 27339 0 2020-03-01 00:27:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 00:27:38.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2697 /api/v1/namespaces/watch-2697/configmaps/e2e-watch-test-configmap-b 2e55aef8-b6f6-43b3-b41a-6ce407137078 27339 0 2020-03-01 00:27:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:27:48.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2697" for this suite.
Mar  1 00:27:54.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:27:54.367: INFO: namespace watch-2697 deletion completed in 6.082533461s

• [SLOW TEST:66.153 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:27:54.371: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  1 00:27:54.401: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:27:58.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8196" for this suite.
Mar  1 00:28:10.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:28:10.647: INFO: namespace init-container-8196 deletion completed in 12.098789637s

• [SLOW TEST:16.276 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:28:10.649: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:28:10.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d" in namespace "projected-2580" to be "success or failure"
Mar  1 00:28:10.682: INFO: Pod "downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690596ms
Mar  1 00:28:12.686: INFO: Pod "downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008319849s
Mar  1 00:28:14.689: INFO: Pod "downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011612866s
STEP: Saw pod success
Mar  1 00:28:14.690: INFO: Pod "downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d" satisfied condition "success or failure"
Mar  1 00:28:14.692: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d container client-container: <nil>
STEP: delete the pod
Mar  1 00:28:14.724: INFO: Waiting for pod downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d to disappear
Mar  1 00:28:14.728: INFO: Pod downwardapi-volume-3f7e5983-6047-4ea3-b942-e4912492744d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:28:14.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2580" for this suite.
Mar  1 00:28:20.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:28:20.840: INFO: namespace projected-2580 deletion completed in 6.107261734s

• [SLOW TEST:10.191 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:28:20.843: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5300
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5300
I0301 00:28:20.924827      21 runners.go:184] Created replication controller with name: externalname-service, namespace: services-5300, replica count: 2
I0301 00:28:23.975892      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 00:28:23.976: INFO: Creating new exec pod
Mar  1 00:28:28.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-5300 execpodjclcl -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  1 00:28:29.834: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 00:28:29.834: INFO: stdout: ""
Mar  1 00:28:29.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-5300 execpodjclcl -- /bin/sh -x -c nc -zv -t -w 2 10.102.12.186 80'
Mar  1 00:28:30.146: INFO: stderr: "+ nc -zv -t -w 2 10.102.12.186 80\nConnection to 10.102.12.186 80 port [tcp/http] succeeded!\n"
Mar  1 00:28:30.146: INFO: stdout: ""
Mar  1 00:28:30.146: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:28:30.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5300" for this suite.
Mar  1 00:28:36.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:28:36.288: INFO: namespace services-5300 deletion completed in 6.108583974s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.445 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:28:36.293: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  1 00:28:36.573: INFO: Pod name wrapped-volume-race-8fd54445-129d-4ba5-a693-bf2fc874b36f: Found 3 pods out of 5
Mar  1 00:28:41.584: INFO: Pod name wrapped-volume-race-8fd54445-129d-4ba5-a693-bf2fc874b36f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8fd54445-129d-4ba5-a693-bf2fc874b36f in namespace emptydir-wrapper-4189, will wait for the garbage collector to delete the pods
Mar  1 00:28:53.671: INFO: Deleting ReplicationController wrapped-volume-race-8fd54445-129d-4ba5-a693-bf2fc874b36f took: 6.8689ms
Mar  1 00:28:54.272: INFO: Terminating ReplicationController wrapped-volume-race-8fd54445-129d-4ba5-a693-bf2fc874b36f pods took: 600.73373ms
STEP: Creating RC which spawns configmap-volume pods
Mar  1 00:29:40.094: INFO: Pod name wrapped-volume-race-e2f2a1cf-5846-40e0-beba-3dbaf49d9c5e: Found 0 pods out of 5
Mar  1 00:29:45.098: INFO: Pod name wrapped-volume-race-e2f2a1cf-5846-40e0-beba-3dbaf49d9c5e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e2f2a1cf-5846-40e0-beba-3dbaf49d9c5e in namespace emptydir-wrapper-4189, will wait for the garbage collector to delete the pods
Mar  1 00:29:57.190: INFO: Deleting ReplicationController wrapped-volume-race-e2f2a1cf-5846-40e0-beba-3dbaf49d9c5e took: 5.777007ms
Mar  1 00:29:57.791: INFO: Terminating ReplicationController wrapped-volume-race-e2f2a1cf-5846-40e0-beba-3dbaf49d9c5e pods took: 600.44048ms
STEP: Creating RC which spawns configmap-volume pods
Mar  1 00:30:39.711: INFO: Pod name wrapped-volume-race-ad9e7af0-ecaa-458d-950b-ccd7c42f1f36: Found 0 pods out of 5
Mar  1 00:30:44.716: INFO: Pod name wrapped-volume-race-ad9e7af0-ecaa-458d-950b-ccd7c42f1f36: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ad9e7af0-ecaa-458d-950b-ccd7c42f1f36 in namespace emptydir-wrapper-4189, will wait for the garbage collector to delete the pods
Mar  1 00:30:56.795: INFO: Deleting ReplicationController wrapped-volume-race-ad9e7af0-ecaa-458d-950b-ccd7c42f1f36 took: 6.451286ms
Mar  1 00:30:57.296: INFO: Terminating ReplicationController wrapped-volume-race-ad9e7af0-ecaa-458d-950b-ccd7c42f1f36 pods took: 500.457237ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:31:40.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4189" for this suite.
Mar  1 00:31:46.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:31:47.056: INFO: namespace emptydir-wrapper-4189 deletion completed in 6.098487887s

• [SLOW TEST:190.763 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:31:47.058: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:32:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2586" for this suite.
Mar  1 00:32:24.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:32:24.262: INFO: namespace namespaces-2586 deletion completed in 6.081373398s
STEP: Destroying namespace "nsdeletetest-9435" for this suite.
Mar  1 00:32:24.263: INFO: Namespace nsdeletetest-9435 was already deleted
STEP: Destroying namespace "nsdeletetest-3936" for this suite.
Mar  1 00:32:30.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:32:30.350: INFO: namespace nsdeletetest-3936 deletion completed in 6.086631535s

• [SLOW TEST:43.293 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:32:30.352: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:33:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1656" for this suite.
Mar  1 00:33:42.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:33:42.490: INFO: namespace container-probe-1656 deletion completed in 12.086311347s

• [SLOW TEST:72.138 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:33:42.491: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4cfa6814-30b1-4631-a5f7-cc526feb8df3
STEP: Creating a pod to test consume configMaps
Mar  1 00:33:42.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4" in namespace "projected-6505" to be "success or failure"
Mar  1 00:33:42.541: INFO: Pod "pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.559778ms
Mar  1 00:33:44.544: INFO: Pod "pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010655092s
Mar  1 00:33:46.547: INFO: Pod "pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013681466s
STEP: Saw pod success
Mar  1 00:33:46.547: INFO: Pod "pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4" satisfied condition "success or failure"
Mar  1 00:33:46.550: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:33:46.584: INFO: Waiting for pod pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4 to disappear
Mar  1 00:33:46.588: INFO: Pod pod-projected-configmaps-7bd6b5cb-1a8b-4ece-b76a-0e2150a78cf4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:33:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6505" for this suite.
Mar  1 00:33:52.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:33:52.688: INFO: namespace projected-6505 deletion completed in 6.095410789s

• [SLOW TEST:10.197 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:33:52.689: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar  1 00:33:52.720: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:34:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1872" for this suite.
Mar  1 00:34:27.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:34:27.197: INFO: namespace crd-publish-openapi-1872 deletion completed in 6.110166753s

• [SLOW TEST:34.508 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:34:27.199: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 00:34:30.252: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:34:30.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6352" for this suite.
Mar  1 00:34:36.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:34:36.365: INFO: namespace container-runtime-6352 deletion completed in 6.094652377s

• [SLOW TEST:9.166 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:34:36.366: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  1 00:34:36.399: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 00:34:36.422: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 00:34:36.427: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-14d9760cdc before test
Mar  1 00:34:36.446: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 00:34:36.446: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 00:34:36.446: INFO: nginx-ingress-default-backend-8465968b95-q4zrk from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Mar  1 00:34:36.446: INFO: metallb-controller-6c8c5fd7fd-vzjj8 from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container metallb-controller ready: true, restart count 0
Mar  1 00:34:36.446: INFO: kube-proxy-kwv7g from kube-system started at 2020-02-29 23:14:35 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 00:34:36.446: INFO: nvidia-device-plugin-daemonset-ptdg5 from kube-system started at 2020-02-29 23:14:59 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Mar  1 00:34:36.446: INFO: metallb-speaker-f4prv from ccp started at 2020-02-29 23:15:14 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.446: INFO: 	Container metallb-speaker ready: true, restart count 0
Mar  1 00:34:36.446: INFO: nginx-ingress-controller-7q5kc from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  1 00:34:36.447: INFO: calico-kube-controllers-67878b79d6-4qj8x from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 00:34:36.447: INFO: calico-node-mgknx from kube-system started at 2020-02-29 23:14:56 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 00:34:36.447: INFO: coredns-65f5bff896-v5nsg from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container coredns ready: true, restart count 0
Mar  1 00:34:36.447: INFO: cert-manager-7c4fdf69b7-tx4s5 from ccp started at 2020-02-29 23:50:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container cert-manager ready: true, restart count 0
Mar  1 00:34:36.447: INFO: ccp-helm-operator-5ff4f4944c-6fnxq from ccp started at 2020-02-29 23:50:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.447: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Mar  1 00:34:36.447: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-5b74768057 before test
Mar  1 00:34:36.454: INFO: sonobuoy-e2e-job-a8e644cf92284639 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:34:36.455: INFO: 	Container e2e ready: true, restart count 0
Mar  1 00:34:36.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 00:34:36.455: INFO: kube-proxy-n5h6f from kube-system started at 2020-02-29 23:16:04 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.455: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 00:34:36.455: INFO: calico-node-xvtvr from kube-system started at 2020-02-29 23:16:05 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.455: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 00:34:36.456: INFO: nvidia-device-plugin-daemonset-n7x6x from kube-system started at 2020-02-29 23:53:51 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.456: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Mar  1 00:34:36.456: INFO: nginx-ingress-controller-lzvzm from ccp started at 2020-02-29 23:54:01 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.456: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  1 00:34:36.456: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:34:36.456: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 00:34:36.456: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 00:34:36.456: INFO: metallb-speaker-hprcs from ccp started at 2020-02-29 23:53:48 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.456: INFO: 	Container metallb-speaker ready: true, restart count 0
Mar  1 00:34:36.456: INFO: sonobuoy from sonobuoy started at 2020-02-29 23:35:00 +0000 UTC (1 container statuses recorded)
Mar  1 00:34:36.457: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node alex-slot1-v3-vsp1-node-group-14d9760cdc
STEP: verifying the node has the label node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.511: INFO: Pod ccp-helm-operator-5ff4f4944c-6fnxq requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.512: INFO: Pod cert-manager-7c4fdf69b7-tx4s5 requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.512: INFO: Pod metallb-controller-6c8c5fd7fd-vzjj8 requesting resource cpu=100m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.512: INFO: Pod metallb-speaker-f4prv requesting resource cpu=100m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.512: INFO: Pod metallb-speaker-hprcs requesting resource cpu=100m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.513: INFO: Pod nginx-ingress-controller-7q5kc requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod nginx-ingress-controller-lzvzm requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.513: INFO: Pod nginx-ingress-default-backend-8465968b95-q4zrk requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod calico-kube-controllers-67878b79d6-4qj8x requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod calico-node-mgknx requesting resource cpu=250m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod calico-node-xvtvr requesting resource cpu=250m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.513: INFO: Pod coredns-65f5bff896-v5nsg requesting resource cpu=100m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod kube-proxy-kwv7g requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.513: INFO: Pod kube-proxy-n5h6f requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.513: INFO: Pod nvidia-device-plugin-daemonset-n7x6x requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.513: INFO: Pod nvidia-device-plugin-daemonset-ptdg5 requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.514: INFO: Pod sonobuoy requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.514: INFO: Pod sonobuoy-e2e-job-a8e644cf92284639 requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.514: INFO: Pod sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-5b74768057
Mar  1 00:34:36.514: INFO: Pod sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x requesting resource cpu=0m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
STEP: Starting Pods to consume most of the cluster CPU.
Mar  1 00:34:36.514: INFO: Creating a pod which consumes cpu=1015m on Node alex-slot1-v3-vsp1-node-group-14d9760cdc
Mar  1 00:34:36.525: INFO: Creating a pod which consumes cpu=1155m on Node alex-slot1-v3-vsp1-node-group-5b74768057
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84.15f806eb0e3684d0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1852/filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84 to alex-slot1-v3-vsp1-node-group-5b74768057]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84.15f806eb8903c7b8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84.15f806eb8e867991], Reason = [Created], Message = [Created container filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84.15f806eb9bcd7f44], Reason = [Started], Message = [Started container filler-pod-de713b15-fe3b-4056-8f7f-4736c40a9b84]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06.15f806eb0db37f81], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1852/filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06 to alex-slot1-v3-vsp1-node-group-14d9760cdc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06.15f806eb7aaabd94], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06.15f806eb80a7176c], Reason = [Created], Message = [Created container filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06.15f806eb8cb295cd], Reason = [Started], Message = [Started container filler-pod-e6bbc892-feaf-473d-9960-7b325ed1ff06]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f806ebfe42c83a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f806ebff69d860], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node alex-slot1-v3-vsp1-node-group-14d9760cdc
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node alex-slot1-v3-vsp1-node-group-5b74768057
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:34:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1852" for this suite.
Mar  1 00:34:47.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:34:47.791: INFO: namespace sched-pred-1852 deletion completed in 6.178814526s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.425 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:34:47.795: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar  1 00:34:47.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-7400'
Mar  1 00:34:48.213: INFO: stderr: ""
Mar  1 00:34:48.213: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 00:34:48.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:34:48.339: INFO: stderr: ""
Mar  1 00:34:48.339: INFO: stdout: "update-demo-nautilus-4lmcq update-demo-nautilus-cl2n4 "
Mar  1 00:34:48.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-4lmcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:48.438: INFO: stderr: ""
Mar  1 00:34:48.438: INFO: stdout: ""
Mar  1 00:34:48.438: INFO: update-demo-nautilus-4lmcq is created but not running
Mar  1 00:34:53.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:34:53.546: INFO: stderr: ""
Mar  1 00:34:53.546: INFO: stdout: "update-demo-nautilus-4lmcq update-demo-nautilus-cl2n4 "
Mar  1 00:34:53.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-4lmcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:53.670: INFO: stderr: ""
Mar  1 00:34:53.671: INFO: stdout: "true"
Mar  1 00:34:53.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-4lmcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:53.764: INFO: stderr: ""
Mar  1 00:34:53.765: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:34:53.765: INFO: validating pod update-demo-nautilus-4lmcq
Mar  1 00:34:53.769: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:34:53.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:34:53.769: INFO: update-demo-nautilus-4lmcq is verified up and running
Mar  1 00:34:53.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:53.872: INFO: stderr: ""
Mar  1 00:34:53.872: INFO: stdout: "true"
Mar  1 00:34:53.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:53.972: INFO: stderr: ""
Mar  1 00:34:53.972: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:34:53.972: INFO: validating pod update-demo-nautilus-cl2n4
Mar  1 00:34:53.977: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:34:53.977: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:34:53.977: INFO: update-demo-nautilus-cl2n4 is verified up and running
STEP: scaling down the replication controller
Mar  1 00:34:53.979: INFO: scanned /root for discovery docs: <nil>
Mar  1 00:34:53.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7400'
Mar  1 00:34:54.127: INFO: stderr: ""
Mar  1 00:34:54.128: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 00:34:54.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:34:54.274: INFO: stderr: ""
Mar  1 00:34:54.274: INFO: stdout: "update-demo-nautilus-4lmcq update-demo-nautilus-cl2n4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  1 00:34:59.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:34:59.396: INFO: stderr: ""
Mar  1 00:34:59.396: INFO: stdout: "update-demo-nautilus-cl2n4 "
Mar  1 00:34:59.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:59.509: INFO: stderr: ""
Mar  1 00:34:59.509: INFO: stdout: "true"
Mar  1 00:34:59.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:34:59.659: INFO: stderr: ""
Mar  1 00:34:59.659: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:34:59.659: INFO: validating pod update-demo-nautilus-cl2n4
Mar  1 00:34:59.663: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:34:59.663: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:34:59.663: INFO: update-demo-nautilus-cl2n4 is verified up and running
STEP: scaling up the replication controller
Mar  1 00:34:59.665: INFO: scanned /root for discovery docs: <nil>
Mar  1 00:34:59.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7400'
Mar  1 00:35:00.808: INFO: stderr: ""
Mar  1 00:35:00.808: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 00:35:00.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:35:01.010: INFO: stderr: ""
Mar  1 00:35:01.010: INFO: stdout: "update-demo-nautilus-cl2n4 update-demo-nautilus-gld65 "
Mar  1 00:35:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:01.191: INFO: stderr: ""
Mar  1 00:35:01.191: INFO: stdout: "true"
Mar  1 00:35:01.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:01.320: INFO: stderr: ""
Mar  1 00:35:01.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:35:01.320: INFO: validating pod update-demo-nautilus-cl2n4
Mar  1 00:35:01.325: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:35:01.325: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:35:01.325: INFO: update-demo-nautilus-cl2n4 is verified up and running
Mar  1 00:35:01.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gld65 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:01.432: INFO: stderr: ""
Mar  1 00:35:01.432: INFO: stdout: ""
Mar  1 00:35:01.432: INFO: update-demo-nautilus-gld65 is created but not running
Mar  1 00:35:06.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7400'
Mar  1 00:35:06.550: INFO: stderr: ""
Mar  1 00:35:06.550: INFO: stdout: "update-demo-nautilus-cl2n4 update-demo-nautilus-gld65 "
Mar  1 00:35:06.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:06.662: INFO: stderr: ""
Mar  1 00:35:06.662: INFO: stdout: "true"
Mar  1 00:35:06.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-cl2n4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:06.754: INFO: stderr: ""
Mar  1 00:35:06.754: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:35:06.754: INFO: validating pod update-demo-nautilus-cl2n4
Mar  1 00:35:06.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:35:06.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:35:06.757: INFO: update-demo-nautilus-cl2n4 is verified up and running
Mar  1 00:35:06.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gld65 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:06.860: INFO: stderr: ""
Mar  1 00:35:06.860: INFO: stdout: "true"
Mar  1 00:35:06.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods update-demo-nautilus-gld65 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7400'
Mar  1 00:35:06.979: INFO: stderr: ""
Mar  1 00:35:06.979: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 00:35:06.979: INFO: validating pod update-demo-nautilus-gld65
Mar  1 00:35:06.983: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 00:35:06.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 00:35:06.983: INFO: update-demo-nautilus-gld65 is verified up and running
STEP: using delete to clean up resources
Mar  1 00:35:06.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-7400'
Mar  1 00:35:07.081: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:35:07.081: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 00:35:07.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7400'
Mar  1 00:35:07.194: INFO: stderr: "No resources found in kubectl-7400 namespace.\n"
Mar  1 00:35:07.194: INFO: stdout: ""
Mar  1 00:35:07.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -l name=update-demo --namespace=kubectl-7400 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 00:35:07.315: INFO: stderr: ""
Mar  1 00:35:07.315: INFO: stdout: "update-demo-nautilus-cl2n4\nupdate-demo-nautilus-gld65\n"
Mar  1 00:35:07.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7400'
Mar  1 00:35:07.955: INFO: stderr: "No resources found in kubectl-7400 namespace.\n"
Mar  1 00:35:07.956: INFO: stdout: ""
Mar  1 00:35:07.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -l name=update-demo --namespace=kubectl-7400 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 00:35:08.085: INFO: stderr: ""
Mar  1 00:35:08.085: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:35:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7400" for this suite.
Mar  1 00:35:36.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:35:36.181: INFO: namespace kubectl-7400 deletion completed in 28.091715316s

• [SLOW TEST:48.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:35:36.182: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  1 00:35:36.222: INFO: Waiting up to 5m0s for pod "downward-api-860bacb2-4814-4d45-994c-07a256ed3da2" in namespace "downward-api-8536" to be "success or failure"
Mar  1 00:35:36.229: INFO: Pod "downward-api-860bacb2-4814-4d45-994c-07a256ed3da2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180436ms
Mar  1 00:35:38.233: INFO: Pod "downward-api-860bacb2-4814-4d45-994c-07a256ed3da2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010345544s
Mar  1 00:35:40.236: INFO: Pod "downward-api-860bacb2-4814-4d45-994c-07a256ed3da2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013519674s
STEP: Saw pod success
Mar  1 00:35:40.236: INFO: Pod "downward-api-860bacb2-4814-4d45-994c-07a256ed3da2" satisfied condition "success or failure"
Mar  1 00:35:40.238: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downward-api-860bacb2-4814-4d45-994c-07a256ed3da2 container dapi-container: <nil>
STEP: delete the pod
Mar  1 00:35:40.261: INFO: Waiting for pod downward-api-860bacb2-4814-4d45-994c-07a256ed3da2 to disappear
Mar  1 00:35:40.264: INFO: Pod downward-api-860bacb2-4814-4d45-994c-07a256ed3da2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:35:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8536" for this suite.
Mar  1 00:35:46.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:35:46.362: INFO: namespace downward-api-8536 deletion completed in 6.094673708s

• [SLOW TEST:10.180 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:35:46.363: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar  1 00:35:46.389: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Mar  1 00:35:47.335: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  1 00:35:49.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:35:51.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:35:53.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:35:55.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:35:57.397: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718619746, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:36:01.627: INFO: Waited 2.213116371s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:36:02.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5823" for this suite.
Mar  1 00:36:08.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:36:08.541: INFO: namespace aggregator-5823 deletion completed in 6.238597474s

• [SLOW TEST:22.179 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:36:08.549: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:36:08.610: INFO: (0) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 19.548678ms)
Mar  1 00:36:08.615: INFO: (1) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.400011ms)
Mar  1 00:36:08.619: INFO: (2) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.677188ms)
Mar  1 00:36:08.624: INFO: (3) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.859961ms)
Mar  1 00:36:08.628: INFO: (4) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.067378ms)
Mar  1 00:36:08.638: INFO: (5) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 9.460871ms)
Mar  1 00:36:08.642: INFO: (6) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.223018ms)
Mar  1 00:36:08.647: INFO: (7) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.439898ms)
Mar  1 00:36:08.652: INFO: (8) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.873818ms)
Mar  1 00:36:08.657: INFO: (9) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.317895ms)
Mar  1 00:36:08.661: INFO: (10) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.638355ms)
Mar  1 00:36:08.666: INFO: (11) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.164465ms)
Mar  1 00:36:08.672: INFO: (12) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.594833ms)
Mar  1 00:36:08.679: INFO: (13) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 6.635326ms)
Mar  1 00:36:08.684: INFO: (14) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.619665ms)
Mar  1 00:36:08.689: INFO: (15) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.249718ms)
Mar  1 00:36:08.694: INFO: (16) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.353961ms)
Mar  1 00:36:08.698: INFO: (17) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.59791ms)
Mar  1 00:36:08.702: INFO: (18) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.663275ms)
Mar  1 00:36:08.706: INFO: (19) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.489875ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:36:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1957" for this suite.
Mar  1 00:36:14.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:36:14.822: INFO: namespace proxy-1957 deletion completed in 6.113072498s

• [SLOW TEST:6.274 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:36:14.823: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar  1 00:36:14.877: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 00:36:14.897: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 00:36:14.901: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-14d9760cdc before test
Mar  1 00:36:14.914: INFO: nginx-ingress-default-backend-8465968b95-q4zrk from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Mar  1 00:36:14.914: INFO: metallb-controller-6c8c5fd7fd-vzjj8 from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container metallb-controller ready: true, restart count 0
Mar  1 00:36:14.914: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-lrf5x from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  1 00:36:14.914: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 00:36:14.914: INFO: nvidia-device-plugin-daemonset-ptdg5 from kube-system started at 2020-02-29 23:14:59 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Mar  1 00:36:14.914: INFO: metallb-speaker-f4prv from ccp started at 2020-02-29 23:15:14 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container metallb-speaker ready: true, restart count 0
Mar  1 00:36:14.914: INFO: nginx-ingress-controller-7q5kc from ccp started at 2020-02-29 23:15:10 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  1 00:36:14.914: INFO: calico-kube-controllers-67878b79d6-4qj8x from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 00:36:14.914: INFO: kube-proxy-kwv7g from kube-system started at 2020-02-29 23:14:35 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 00:36:14.914: INFO: coredns-65f5bff896-v5nsg from kube-system started at 2020-02-29 23:15:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container coredns ready: true, restart count 0
Mar  1 00:36:14.914: INFO: cert-manager-7c4fdf69b7-tx4s5 from ccp started at 2020-02-29 23:50:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container cert-manager ready: true, restart count 0
Mar  1 00:36:14.914: INFO: ccp-helm-operator-5ff4f4944c-6fnxq from ccp started at 2020-02-29 23:50:11 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container ccp-helm-operator ready: true, restart count 0
Mar  1 00:36:14.914: INFO: calico-node-mgknx from kube-system started at 2020-02-29 23:14:56 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.914: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 00:36:14.914: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v3-vsp1-node-group-5b74768057 before test
Mar  1 00:36:14.924: INFO: sonobuoy from sonobuoy started at 2020-02-29 23:35:00 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 00:36:14.930: INFO: sonobuoy-systemd-logs-daemon-set-b7ccd576a01845d1-hvrr4 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  1 00:36:14.930: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 00:36:14.930: INFO: metallb-speaker-hprcs from ccp started at 2020-02-29 23:53:48 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container metallb-speaker ready: true, restart count 0
Mar  1 00:36:14.930: INFO: nvidia-device-plugin-daemonset-n7x6x from kube-system started at 2020-02-29 23:53:51 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Mar  1 00:36:14.930: INFO: nginx-ingress-controller-lzvzm from ccp started at 2020-02-29 23:54:01 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar  1 00:36:14.930: INFO: sonobuoy-e2e-job-a8e644cf92284639 from sonobuoy started at 2020-02-29 23:35:06 +0000 UTC (2 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container e2e ready: true, restart count 0
Mar  1 00:36:14.930: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 00:36:14.930: INFO: kube-proxy-n5h6f from kube-system started at 2020-02-29 23:16:04 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 00:36:14.930: INFO: calico-node-xvtvr from kube-system started at 2020-02-29 23:16:05 +0000 UTC (1 container statuses recorded)
Mar  1 00:36:14.930: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1d63c433-eff4-4241-8894-25e5a1db9156 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1d63c433-eff4-4241-8894-25e5a1db9156 off the node alex-slot1-v3-vsp1-node-group-5b74768057
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1d63c433-eff4-4241-8894-25e5a1db9156
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:41:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7326" for this suite.
Mar  1 00:41:41.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:41:41.143: INFO: namespace sched-pred-7326 deletion completed in 18.094757314s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:326.320 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:41:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-97253976-429d-4214-8239-b7be4d3f7560
STEP: Creating a pod to test consume configMaps
Mar  1 00:41:41.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8" in namespace "configmap-8409" to be "success or failure"
Mar  1 00:41:41.209: INFO: Pod "pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.762065ms
Mar  1 00:41:43.213: INFO: Pod "pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007321211s
Mar  1 00:41:45.216: INFO: Pod "pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010407718s
STEP: Saw pod success
Mar  1 00:41:45.216: INFO: Pod "pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8" satisfied condition "success or failure"
Mar  1 00:41:45.218: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:41:45.266: INFO: Waiting for pod pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8 to disappear
Mar  1 00:41:45.271: INFO: Pod pod-configmaps-6f2556c5-8b58-4491-a22b-156d8dd3acf8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:41:45.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8409" for this suite.
Mar  1 00:41:51.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:41:51.388: INFO: namespace configmap-8409 deletion completed in 6.112941563s

• [SLOW TEST:10.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:41:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:41:51.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 version'
Mar  1 00:41:51.532: INFO: stderr: ""
Mar  1 00:41:51.532: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:23:11Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:13:49Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:41:51.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-554" for this suite.
Mar  1 00:41:57.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:41:57.649: INFO: namespace kubectl-554 deletion completed in 6.113223162s

• [SLOW TEST:6.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:41:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  1 00:41:57.708: INFO: Waiting up to 5m0s for pod "pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c" in namespace "emptydir-4623" to be "success or failure"
Mar  1 00:41:57.766: INFO: Pod "pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c": Phase="Pending", Reason="", readiness=false. Elapsed: 57.452103ms
Mar  1 00:41:59.769: INFO: Pod "pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060372344s
Mar  1 00:42:01.773: INFO: Pod "pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063952823s
STEP: Saw pod success
Mar  1 00:42:01.773: INFO: Pod "pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c" satisfied condition "success or failure"
Mar  1 00:42:01.776: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c container test-container: <nil>
STEP: delete the pod
Mar  1 00:42:01.807: INFO: Waiting for pod pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c to disappear
Mar  1 00:42:01.812: INFO: Pod pod-ea97f720-1149-4598-8bdc-cbb8e4eabe5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:42:01.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4623" for this suite.
Mar  1 00:42:07.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:42:07.924: INFO: namespace emptydir-4623 deletion completed in 6.106407266s

• [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:42:07.924: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar  1 00:42:12.492: INFO: Successfully updated pod "annotationupdatefa4db529-0621-4b9a-8fc9-1f10c665abfa"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:42:14.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8138" for this suite.
Mar  1 00:42:38.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:42:38.617: INFO: namespace projected-8138 deletion completed in 24.094550891s

• [SLOW TEST:30.692 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:42:38.617: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-rw4s
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 00:42:38.660: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rw4s" in namespace "subpath-6255" to be "success or failure"
Mar  1 00:42:38.663: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263857ms
Mar  1 00:42:40.666: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074638s
Mar  1 00:42:42.669: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.008832967s
Mar  1 00:42:44.672: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 6.012607006s
Mar  1 00:42:46.675: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 8.015563623s
Mar  1 00:42:48.679: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 10.018911093s
Mar  1 00:42:50.682: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.022165754s
Mar  1 00:42:52.688: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 14.028168811s
Mar  1 00:42:54.691: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 16.031179s
Mar  1 00:42:56.694: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 18.034370739s
Mar  1 00:42:58.698: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 20.037709585s
Mar  1 00:43:00.701: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Running", Reason="", readiness=true. Elapsed: 22.040772952s
Mar  1 00:43:02.704: INFO: Pod "pod-subpath-test-projected-rw4s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044093707s
STEP: Saw pod success
Mar  1 00:43:02.704: INFO: Pod "pod-subpath-test-projected-rw4s" satisfied condition "success or failure"
Mar  1 00:43:02.707: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-subpath-test-projected-rw4s container test-container-subpath-projected-rw4s: <nil>
STEP: delete the pod
Mar  1 00:43:02.733: INFO: Waiting for pod pod-subpath-test-projected-rw4s to disappear
Mar  1 00:43:02.735: INFO: Pod pod-subpath-test-projected-rw4s no longer exists
STEP: Deleting pod pod-subpath-test-projected-rw4s
Mar  1 00:43:02.736: INFO: Deleting pod "pod-subpath-test-projected-rw4s" in namespace "subpath-6255"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:43:02.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6255" for this suite.
Mar  1 00:43:08.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:43:08.837: INFO: namespace subpath-6255 deletion completed in 6.094416289s

• [SLOW TEST:30.221 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:43:08.839: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  1 00:43:08.890: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:08.895: INFO: Number of nodes with available pods: 0
Mar  1 00:43:08.895: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:43:09.900: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:09.903: INFO: Number of nodes with available pods: 0
Mar  1 00:43:09.903: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:43:10.901: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:10.904: INFO: Number of nodes with available pods: 0
Mar  1 00:43:10.904: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 00:43:11.900: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:11.903: INFO: Number of nodes with available pods: 2
Mar  1 00:43:11.903: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  1 00:43:11.926: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:11.932: INFO: Number of nodes with available pods: 1
Mar  1 00:43:11.932: INFO: Node alex-slot1-v3-vsp1-node-group-5b74768057 is running more than one daemon pod
Mar  1 00:43:12.959: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:12.962: INFO: Number of nodes with available pods: 1
Mar  1 00:43:12.962: INFO: Node alex-slot1-v3-vsp1-node-group-5b74768057 is running more than one daemon pod
Mar  1 00:43:13.938: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:13.942: INFO: Number of nodes with available pods: 1
Mar  1 00:43:13.942: INFO: Node alex-slot1-v3-vsp1-node-group-5b74768057 is running more than one daemon pod
Mar  1 00:43:14.939: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 00:43:14.943: INFO: Number of nodes with available pods: 2
Mar  1 00:43:14.943: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-673, will wait for the garbage collector to delete the pods
Mar  1 00:43:15.008: INFO: Deleting DaemonSet.extensions daemon-set took: 5.84892ms
Mar  1 00:43:15.509: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.567322ms
Mar  1 00:43:29.012: INFO: Number of nodes with available pods: 0
Mar  1 00:43:29.012: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 00:43:29.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-673/daemonsets","resourceVersion":"30784"},"items":null}

Mar  1 00:43:29.018: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-673/pods","resourceVersion":"30784"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:43:29.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-673" for this suite.
Mar  1 00:43:35.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:43:35.125: INFO: namespace daemonsets-673 deletion completed in 6.09375169s

• [SLOW TEST:26.286 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:43:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:43:46.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1875" for this suite.
Mar  1 00:43:52.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:43:52.265: INFO: namespace resourcequota-1875 deletion completed in 6.085060462s

• [SLOW TEST:17.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:43:52.269: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:43:56.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8050" for this suite.
Mar  1 00:44:40.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:44:40.422: INFO: namespace kubelet-test-8050 deletion completed in 44.091460871s

• [SLOW TEST:48.154 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:44:40.425: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-6543f700-f873-4184-a07d-29031e9ed53d in namespace container-probe-6296
Mar  1 00:44:44.520: INFO: Started pod busybox-6543f700-f873-4184-a07d-29031e9ed53d in namespace container-probe-6296
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 00:44:44.523: INFO: Initial restart count of pod busybox-6543f700-f873-4184-a07d-29031e9ed53d is 0
Mar  1 00:45:32.603: INFO: Restart count of pod container-probe-6296/busybox-6543f700-f873-4184-a07d-29031e9ed53d is now 1 (48.080542888s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:45:32.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6296" for this suite.
Mar  1 00:45:38.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:45:38.753: INFO: namespace container-probe-6296 deletion completed in 6.106630634s

• [SLOW TEST:58.328 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:45:38.755: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 00:45:38.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c" in namespace "projected-658" to be "success or failure"
Mar  1 00:45:38.808: INFO: Pod "downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652375ms
Mar  1 00:45:40.812: INFO: Pod "downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011490186s
Mar  1 00:45:42.814: INFO: Pod "downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01430366s
STEP: Saw pod success
Mar  1 00:45:42.814: INFO: Pod "downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c" satisfied condition "success or failure"
Mar  1 00:45:42.817: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c container client-container: <nil>
STEP: delete the pod
Mar  1 00:45:42.842: INFO: Waiting for pod downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c to disappear
Mar  1 00:45:42.846: INFO: Pod downwardapi-volume-f8eb1587-765d-4eb6-bd5e-d2ba9623113c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:45:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-658" for this suite.
Mar  1 00:45:48.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:45:48.971: INFO: namespace projected-658 deletion completed in 6.122443828s

• [SLOW TEST:10.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:45:48.971: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  1 00:45:49.071: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3006 /api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-resource-version 445b8e6d-91c0-48e5-9875-b2e64d67f934 31183 0 2020-03-01 00:45:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 00:45:49.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3006 /api/v1/namespaces/watch-3006/configmaps/e2e-watch-test-resource-version 445b8e6d-91c0-48e5-9875-b2e64d67f934 31184 0 2020-03-01 00:45:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:45:49.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3006" for this suite.
Mar  1 00:45:55.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:45:55.204: INFO: namespace watch-3006 deletion completed in 6.126235959s

• [SLOW TEST:6.233 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:45:55.210: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:45:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:45:55.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3903" for this suite.
Mar  1 00:46:01.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:46:01.890: INFO: namespace custom-resource-definition-3903 deletion completed in 6.089823287s

• [SLOW TEST:6.680 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:46:01.890: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:46:18.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-531" for this suite.
Mar  1 00:46:24.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:46:24.144: INFO: namespace resourcequota-531 deletion completed in 6.1109053s

• [SLOW TEST:22.254 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:46:24.148: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:46:24.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2090" for this suite.
Mar  1 00:46:30.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:46:30.322: INFO: namespace tables-2090 deletion completed in 6.089474889s

• [SLOW TEST:6.174 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:46:30.324: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:46:48.376: INFO: Container started at 2020-03-01 00:46:31 +0000 UTC, pod became ready at 2020-03-01 00:46:47 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:46:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4990" for this suite.
Mar  1 00:47:00.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:47:00.485: INFO: namespace container-probe-4990 deletion completed in 12.105251767s

• [SLOW TEST:30.161 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:47:00.486: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Mar  1 00:47:00.538: INFO: Waiting up to 5m0s for pod "client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa" in namespace "containers-9632" to be "success or failure"
Mar  1 00:47:00.543: INFO: Pod "client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887511ms
Mar  1 00:47:02.549: INFO: Pod "client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010431936s
Mar  1 00:47:04.558: INFO: Pod "client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019148594s
STEP: Saw pod success
Mar  1 00:47:04.558: INFO: Pod "client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa" satisfied condition "success or failure"
Mar  1 00:47:04.563: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa container test-container: <nil>
STEP: delete the pod
Mar  1 00:47:04.597: INFO: Waiting for pod client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa to disappear
Mar  1 00:47:04.600: INFO: Pod client-containers-23a03500-f165-4f04-bac1-5c0ee947aefa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:47:04.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9632" for this suite.
Mar  1 00:47:10.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:47:10.693: INFO: namespace containers-9632 deletion completed in 6.089467451s

• [SLOW TEST:10.207 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:47:10.694: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  1 00:47:10.727: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  1 00:47:30.747: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 00:47:35.676: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:47:55.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2521" for this suite.
Mar  1 00:48:01.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:48:01.508: INFO: namespace crd-publish-openapi-2521 deletion completed in 6.08620424s

• [SLOW TEST:50.814 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:48:01.509: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:48:01.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2605" for this suite.
Mar  1 00:48:13.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:48:13.681: INFO: namespace kubelet-test-2605 deletion completed in 12.112379946s

• [SLOW TEST:12.172 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:48:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-4ede49f9-e503-4fc9-888a-4b0831fad968
STEP: Creating a pod to test consume configMaps
Mar  1 00:48:13.726: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c" in namespace "projected-9112" to be "success or failure"
Mar  1 00:48:13.734: INFO: Pod "pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.469833ms
Mar  1 00:48:15.737: INFO: Pod "pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011275602s
Mar  1 00:48:17.740: INFO: Pod "pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014311735s
STEP: Saw pod success
Mar  1 00:48:17.740: INFO: Pod "pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c" satisfied condition "success or failure"
Mar  1 00:48:17.743: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:48:17.766: INFO: Waiting for pod pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c to disappear
Mar  1 00:48:17.768: INFO: Pod pod-projected-configmaps-3f35c800-2480-4625-8ae9-f3c1c572831c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:48:17.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9112" for this suite.
Mar  1 00:48:23.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:48:23.888: INFO: namespace projected-9112 deletion completed in 6.111002463s

• [SLOW TEST:10.205 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:48:23.889: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-l72w
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 00:48:23.942: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-l72w" in namespace "subpath-8582" to be "success or failure"
Mar  1 00:48:23.948: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289182ms
Mar  1 00:48:25.952: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009634908s
Mar  1 00:48:27.954: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 4.012205108s
Mar  1 00:48:29.957: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 6.015490014s
Mar  1 00:48:31.961: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 8.018857661s
Mar  1 00:48:33.965: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 10.023052488s
Mar  1 00:48:35.968: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 12.026061587s
Mar  1 00:48:37.971: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 14.029480134s
Mar  1 00:48:39.975: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 16.03346132s
Mar  1 00:48:41.979: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 18.036911416s
Mar  1 00:48:43.982: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 20.039859457s
Mar  1 00:48:45.985: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Running", Reason="", readiness=true. Elapsed: 22.042938888s
Mar  1 00:48:47.988: INFO: Pod "pod-subpath-test-secret-l72w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046331043s
STEP: Saw pod success
Mar  1 00:48:47.988: INFO: Pod "pod-subpath-test-secret-l72w" satisfied condition "success or failure"
Mar  1 00:48:47.990: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-subpath-test-secret-l72w container test-container-subpath-secret-l72w: <nil>
STEP: delete the pod
Mar  1 00:48:48.010: INFO: Waiting for pod pod-subpath-test-secret-l72w to disappear
Mar  1 00:48:48.012: INFO: Pod pod-subpath-test-secret-l72w no longer exists
STEP: Deleting pod pod-subpath-test-secret-l72w
Mar  1 00:48:48.012: INFO: Deleting pod "pod-subpath-test-secret-l72w" in namespace "subpath-8582"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:48:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8582" for this suite.
Mar  1 00:48:54.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:48:54.103: INFO: namespace subpath-8582 deletion completed in 6.087088952s

• [SLOW TEST:30.215 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:48:54.105: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6439.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6439.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6439.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6439.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6439.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6439.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 00:48:58.181: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.184: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.187: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.191: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.201: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.205: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.208: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.211: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6439.svc.cluster.local from pod dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6: the server could not find the requested resource (get pods dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6)
Mar  1 00:48:58.222: INFO: Lookups using dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6439.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6439.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6439.svc.cluster.local jessie_udp@dns-test-service-2.dns-6439.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6439.svc.cluster.local]

Mar  1 00:49:03.278: INFO: DNS probes using dns-6439/dns-test-a67bba11-3594-4c23-8dc9-1ff41e6bd0d6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:49:03.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6439" for this suite.
Mar  1 00:49:09.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:49:09.430: INFO: namespace dns-6439 deletion completed in 6.09025739s

• [SLOW TEST:15.326 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:49:09.431: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  1 00:49:09.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31865 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 00:49:09.479: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31866 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  1 00:49:09.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31867 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  1 00:49:19.503: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31887 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 00:49:19.503: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31888 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  1 00:49:19.503: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2987 /api/v1/namespaces/watch-2987/configmaps/e2e-watch-test-label-changed 68716da7-8fef-429a-9705-aa5a70c31158 31889 0 2020-03-01 00:49:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:49:19.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2987" for this suite.
Mar  1 00:49:25.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:49:25.627: INFO: namespace watch-2987 deletion completed in 6.117472221s

• [SLOW TEST:16.196 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:49:25.636: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:49:25.680: INFO: Waiting up to 5m0s for pod "busybox-user-65534-85b313a9-ebd8-4fcf-8fda-a677fc1a36f1" in namespace "security-context-test-6292" to be "success or failure"
Mar  1 00:49:25.683: INFO: Pod "busybox-user-65534-85b313a9-ebd8-4fcf-8fda-a677fc1a36f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19131ms
Mar  1 00:49:27.687: INFO: Pod "busybox-user-65534-85b313a9-ebd8-4fcf-8fda-a677fc1a36f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006444164s
Mar  1 00:49:29.691: INFO: Pod "busybox-user-65534-85b313a9-ebd8-4fcf-8fda-a677fc1a36f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010367065s
Mar  1 00:49:29.691: INFO: Pod "busybox-user-65534-85b313a9-ebd8-4fcf-8fda-a677fc1a36f1" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:49:29.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6292" for this suite.
Mar  1 00:49:35.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:49:35.789: INFO: namespace security-context-test-6292 deletion completed in 6.092363865s

• [SLOW TEST:10.153 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:49:35.791: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:49:41.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3627" for this suite.
Mar  1 00:49:47.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:49:48.020: INFO: namespace namespaces-3627 deletion completed in 6.101432606s
STEP: Destroying namespace "nsdeletetest-3158" for this suite.
Mar  1 00:49:48.025: INFO: Namespace nsdeletetest-3158 was already deleted
STEP: Destroying namespace "nsdeletetest-1687" for this suite.
Mar  1 00:49:54.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:49:54.128: INFO: namespace nsdeletetest-1687 deletion completed in 6.102910543s

• [SLOW TEST:18.338 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:49:54.129: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Mar  1 00:49:54.165: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-585087022 proxy --unix-socket=/tmp/kubectl-proxy-unix217286429/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:49:54.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2230" for this suite.
Mar  1 00:50:00.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:00.338: INFO: namespace kubectl-2230 deletion completed in 6.092051961s

• [SLOW TEST:6.208 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:50:00.341: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:50:01.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  1 00:50:03.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620600, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:50:06.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:50:06.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2965" for this suite.
Mar  1 00:50:12.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:12.346: INFO: namespace webhook-2965 deletion completed in 6.084545246s
STEP: Destroying namespace "webhook-2965-markers" for this suite.
Mar  1 00:50:18.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:18.434: INFO: namespace webhook-2965-markers deletion completed in 6.087711519s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.105 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:50:18.452: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  1 00:50:19.104: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  1 00:50:21.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620618, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620618, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620618, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718620618, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 00:50:24.133: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 00:50:24.136: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:50:25.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8155" for this suite.
Mar  1 00:50:31.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:31.358: INFO: namespace crd-webhook-8155 deletion completed in 6.090812096s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.916 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:50:31.369: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:50:36.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6611" for this suite.
Mar  1 00:50:42.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:42.656: INFO: namespace watch-6611 deletion completed in 6.178056424s

• [SLOW TEST:11.288 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:50:42.659: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-0983d8ff-4e93-486e-b9fa-c49d5d820ca1
STEP: Creating a pod to test consume configMaps
Mar  1 00:50:42.725: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364" in namespace "projected-4881" to be "success or failure"
Mar  1 00:50:42.735: INFO: Pod "pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364": Phase="Pending", Reason="", readiness=false. Elapsed: 7.977911ms
Mar  1 00:50:44.740: INFO: Pod "pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013040518s
Mar  1 00:50:46.743: INFO: Pod "pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016026213s
STEP: Saw pod success
Mar  1 00:50:46.743: INFO: Pod "pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364" satisfied condition "success or failure"
Mar  1 00:50:46.746: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:50:46.778: INFO: Waiting for pod pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364 to disappear
Mar  1 00:50:46.779: INFO: Pod pod-projected-configmaps-387bfe57-cb60-4a2e-b615-df9503307364 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:50:46.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4881" for this suite.
Mar  1 00:50:52.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:50:52.878: INFO: namespace projected-4881 deletion completed in 6.096034404s

• [SLOW TEST:10.219 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:50:52.880: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Mar  1 00:50:52.916: INFO: Waiting up to 5m0s for pod "var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc" in namespace "var-expansion-9609" to be "success or failure"
Mar  1 00:50:52.919: INFO: Pod "var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078216ms
Mar  1 00:50:54.923: INFO: Pod "var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00669458s
Mar  1 00:50:56.926: INFO: Pod "var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009738507s
STEP: Saw pod success
Mar  1 00:50:56.926: INFO: Pod "var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc" satisfied condition "success or failure"
Mar  1 00:50:56.928: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc container dapi-container: <nil>
STEP: delete the pod
Mar  1 00:50:56.943: INFO: Waiting for pod var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc to disappear
Mar  1 00:50:56.948: INFO: Pod var-expansion-26e08bd7-5e11-42ae-8417-3d13eafd17fc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:50:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9609" for this suite.
Mar  1 00:51:02.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:51:03.043: INFO: namespace var-expansion-9609 deletion completed in 6.091229725s

• [SLOW TEST:10.163 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:51:03.050: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  1 00:51:11.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 00:51:11.126: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 00:51:13.126: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 00:51:13.130: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 00:51:15.126: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 00:51:15.129: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:51:15.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2457" for this suite.
Mar  1 00:51:43.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:51:43.231: INFO: namespace container-lifecycle-hook-2457 deletion completed in 28.09775895s

• [SLOW TEST:40.181 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:51:43.231: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-396edc04-5bd4-4514-bb03-0a60f77b40f2
STEP: Creating a pod to test consume configMaps
Mar  1 00:51:43.279: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76" in namespace "projected-2792" to be "success or failure"
Mar  1 00:51:43.286: INFO: Pod "pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894036ms
Mar  1 00:51:45.290: INFO: Pod "pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011053046s
Mar  1 00:51:47.292: INFO: Pod "pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013830884s
STEP: Saw pod success
Mar  1 00:51:47.292: INFO: Pod "pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76" satisfied condition "success or failure"
Mar  1 00:51:47.294: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 00:51:47.314: INFO: Waiting for pod pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76 to disappear
Mar  1 00:51:47.335: INFO: Pod pod-projected-configmaps-70abcd82-53cc-49c9-b08a-64021159fe76 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:51:47.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2792" for this suite.
Mar  1 00:51:53.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:51:53.446: INFO: namespace projected-2792 deletion completed in 6.107586502s

• [SLOW TEST:10.215 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:51:53.448: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-4129
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4129 to expose endpoints map[]
Mar  1 00:51:53.499: INFO: successfully validated that service multi-endpoint-test in namespace services-4129 exposes endpoints map[] (5.191324ms elapsed)
STEP: Creating pod pod1 in namespace services-4129
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4129 to expose endpoints map[pod1:[100]]
Mar  1 00:51:56.555: INFO: successfully validated that service multi-endpoint-test in namespace services-4129 exposes endpoints map[pod1:[100]] (3.049398908s elapsed)
STEP: Creating pod pod2 in namespace services-4129
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4129 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  1 00:51:59.602: INFO: successfully validated that service multi-endpoint-test in namespace services-4129 exposes endpoints map[pod1:[100] pod2:[101]] (3.041815067s elapsed)
STEP: Deleting pod pod1 in namespace services-4129
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4129 to expose endpoints map[pod2:[101]]
Mar  1 00:52:00.639: INFO: successfully validated that service multi-endpoint-test in namespace services-4129 exposes endpoints map[pod2:[101]] (1.031598874s elapsed)
STEP: Deleting pod pod2 in namespace services-4129
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4129 to expose endpoints map[]
Mar  1 00:52:00.653: INFO: successfully validated that service multi-endpoint-test in namespace services-4129 exposes endpoints map[] (5.361965ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:52:00.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4129" for this suite.
Mar  1 00:52:12.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:52:12.780: INFO: namespace services-4129 deletion completed in 12.094309751s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.333 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:52:12.784: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Mar  1 00:52:12.814: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  1 00:52:12.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:13.594: INFO: stderr: ""
Mar  1 00:52:13.594: INFO: stdout: "service/redis-slave created\n"
Mar  1 00:52:13.594: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  1 00:52:13.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:13.897: INFO: stderr: ""
Mar  1 00:52:13.897: INFO: stdout: "service/redis-master created\n"
Mar  1 00:52:13.898: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  1 00:52:13.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:14.183: INFO: stderr: ""
Mar  1 00:52:14.183: INFO: stdout: "service/frontend created\n"
Mar  1 00:52:14.183: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  1 00:52:14.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:14.473: INFO: stderr: ""
Mar  1 00:52:14.473: INFO: stdout: "deployment.apps/frontend created\n"
Mar  1 00:52:14.473: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 00:52:14.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:14.750: INFO: stderr: ""
Mar  1 00:52:14.750: INFO: stdout: "deployment.apps/redis-master created\n"
Mar  1 00:52:14.750: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  1 00:52:14.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-6885'
Mar  1 00:52:15.143: INFO: stderr: ""
Mar  1 00:52:15.143: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar  1 00:52:15.143: INFO: Waiting for all frontend pods to be Running.
Mar  1 00:52:35.196: INFO: Waiting for frontend to serve content.
Mar  1 00:52:35.211: INFO: Trying to add a new entry to the guestbook.
Mar  1 00:52:35.223: INFO: Verifying that added entry can be retrieved.
Mar  1 00:52:35.243: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:52:40.259: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:52:45.276: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:52:50.295: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:52:55.310: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:53:00.324: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:53:05.339: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:53:10.357: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Mar  1 00:53:15.375: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Mar  1 00:53:20.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:20.543: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:20.543: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 00:53:20.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:20.705: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:20.705: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 00:53:20.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:20.861: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:20.861: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 00:53:20.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:21.015: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:21.015: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 00:53:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:21.117: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:21.117: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 00:53:21.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-6885'
Mar  1 00:53:21.233: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 00:53:21.233: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:53:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6885" for this suite.
Mar  1 00:53:33.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:53:33.328: INFO: namespace kubectl-6885 deletion completed in 12.091924926s

• [SLOW TEST:80.545 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:53:33.332: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar  1 00:53:33.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-5430'
Mar  1 00:53:33.654: INFO: stderr: ""
Mar  1 00:53:33.654: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  1 00:53:34.656: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:53:34.656: INFO: Found 0 / 1
Mar  1 00:53:35.657: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:53:35.657: INFO: Found 0 / 1
Mar  1 00:53:36.656: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:53:36.656: INFO: Found 1 / 1
Mar  1 00:53:36.656: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  1 00:53:36.658: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:53:36.658: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 00:53:36.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 patch pod redis-master-q8kz4 --namespace=kubectl-5430 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  1 00:53:36.807: INFO: stderr: ""
Mar  1 00:53:36.807: INFO: stdout: "pod/redis-master-q8kz4 patched\n"
STEP: checking annotations
Mar  1 00:53:36.810: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 00:53:36.810: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:53:36.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5430" for this suite.
Mar  1 00:54:04.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:54:04.916: INFO: namespace kubectl-5430 deletion completed in 28.100414388s

• [SLOW TEST:31.583 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:54:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Mar  1 00:54:05.544: INFO: created pod pod-service-account-defaultsa
Mar  1 00:54:05.545: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  1 00:54:05.555: INFO: created pod pod-service-account-mountsa
Mar  1 00:54:05.555: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  1 00:54:05.576: INFO: created pod pod-service-account-nomountsa
Mar  1 00:54:05.580: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  1 00:54:05.594: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  1 00:54:05.595: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  1 00:54:05.608: INFO: created pod pod-service-account-mountsa-mountspec
Mar  1 00:54:05.609: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  1 00:54:05.623: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  1 00:54:05.624: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  1 00:54:05.633: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  1 00:54:05.633: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  1 00:54:05.648: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  1 00:54:05.648: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  1 00:54:05.659: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  1 00:54:05.659: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:54:05.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5751" for this suite.
Mar  1 00:54:11.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:54:11.790: INFO: namespace svcaccounts-5751 deletion completed in 6.120413041s

• [SLOW TEST:6.873 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:54:11.792: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  1 00:54:15.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec pod-sharedvolume-0e8613f9-bca7-450d-8a2e-fdcd90ccdba7 -c busybox-main-container --namespace=emptydir-6438 -- cat /usr/share/volumeshare/shareddata.txt'
Mar  1 00:54:16.144: INFO: stderr: ""
Mar  1 00:54:16.144: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:54:16.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6438" for this suite.
Mar  1 00:54:22.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:54:22.234: INFO: namespace emptydir-6438 deletion completed in 6.085800352s

• [SLOW TEST:10.442 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:54:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  1 00:54:26.788: INFO: Successfully updated pod "adopt-release-pnx89"
STEP: Checking that the Job readopts the Pod
Mar  1 00:54:26.788: INFO: Waiting up to 15m0s for pod "adopt-release-pnx89" in namespace "job-8205" to be "adopted"
Mar  1 00:54:26.792: INFO: Pod "adopt-release-pnx89": Phase="Running", Reason="", readiness=true. Elapsed: 4.546441ms
Mar  1 00:54:28.795: INFO: Pod "adopt-release-pnx89": Phase="Running", Reason="", readiness=true. Elapsed: 2.007358608s
Mar  1 00:54:28.795: INFO: Pod "adopt-release-pnx89" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  1 00:54:29.316: INFO: Successfully updated pod "adopt-release-pnx89"
STEP: Checking that the Job releases the Pod
Mar  1 00:54:29.316: INFO: Waiting up to 15m0s for pod "adopt-release-pnx89" in namespace "job-8205" to be "released"
Mar  1 00:54:29.344: INFO: Pod "adopt-release-pnx89": Phase="Running", Reason="", readiness=true. Elapsed: 28.198686ms
Mar  1 00:54:29.344: INFO: Pod "adopt-release-pnx89" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:54:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8205" for this suite.
Mar  1 00:55:25.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:55:25.514: INFO: namespace job-8205 deletion completed in 56.130261302s

• [SLOW TEST:63.276 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:55:25.516: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 in namespace container-probe-1924
Mar  1 00:55:29.568: INFO: Started pod liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 in namespace container-probe-1924
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 00:55:29.570: INFO: Initial restart count of pod liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is 0
Mar  1 00:55:39.589: INFO: Restart count of pod container-probe-1924/liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is now 1 (10.018589762s elapsed)
Mar  1 00:55:59.624: INFO: Restart count of pod container-probe-1924/liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is now 2 (30.054254s elapsed)
Mar  1 00:56:19.670: INFO: Restart count of pod container-probe-1924/liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is now 3 (50.100157752s elapsed)
Mar  1 00:56:39.711: INFO: Restart count of pod container-probe-1924/liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is now 4 (1m10.141065413s elapsed)
Mar  1 00:57:51.830: INFO: Restart count of pod container-probe-1924/liveness-bf9c0b19-1f1e-409b-b01a-23d777739159 is now 5 (2m22.259825941s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:57:51.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1924" for this suite.
Mar  1 00:57:57.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:57:57.975: INFO: namespace container-probe-1924 deletion completed in 6.105420661s

• [SLOW TEST:152.459 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:57:57.978: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 00:58:02.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2955" for this suite.
Mar  1 00:58:08.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 00:58:08.182: INFO: namespace emptydir-wrapper-2955 deletion completed in 6.119187942s

• [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 00:58:08.182: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-7d2e8e97-3c27-4d65-a7dc-49484c72200f in namespace container-probe-8218
Mar  1 00:58:12.221: INFO: Started pod test-webserver-7d2e8e97-3c27-4d65-a7dc-49484c72200f in namespace container-probe-8218
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 00:58:12.224: INFO: Initial restart count of pod test-webserver-7d2e8e97-3c27-4d65-a7dc-49484c72200f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:02:12.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8218" for this suite.
Mar  1 01:02:18.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:02:18.747: INFO: namespace container-probe-8218 deletion completed in 6.08803727s

• [SLOW TEST:250.564 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:02:18.747: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  1 01:02:23.804: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:02:23.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4092" for this suite.
Mar  1 01:02:51.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:02:51.950: INFO: namespace replicaset-4092 deletion completed in 28.105901708s

• [SLOW TEST:33.204 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:02:51.952: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Mar  1 01:02:51.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 create -f - --namespace=kubectl-120'
Mar  1 01:02:52.789: INFO: stderr: ""
Mar  1 01:02:52.789: INFO: stdout: "pod/pause created\n"
Mar  1 01:02:52.789: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  1 01:02:52.789: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-120" to be "running and ready"
Mar  1 01:02:52.793: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.680563ms
Mar  1 01:02:54.799: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009473996s
Mar  1 01:02:54.799: INFO: Pod "pause" satisfied condition "running and ready"
Mar  1 01:02:54.799: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  1 01:02:54.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 label pods pause testing-label=testing-label-value --namespace=kubectl-120'
Mar  1 01:02:54.920: INFO: stderr: ""
Mar  1 01:02:54.923: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  1 01:02:54.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pod pause -L testing-label --namespace=kubectl-120'
Mar  1 01:02:55.029: INFO: stderr: ""
Mar  1 01:02:55.029: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  1 01:02:55.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 label pods pause testing-label- --namespace=kubectl-120'
Mar  1 01:02:55.153: INFO: stderr: ""
Mar  1 01:02:55.153: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  1 01:02:55.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pod pause -L testing-label --namespace=kubectl-120'
Mar  1 01:02:55.256: INFO: stderr: ""
Mar  1 01:02:55.256: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Mar  1 01:02:55.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete --grace-period=0 --force -f - --namespace=kubectl-120'
Mar  1 01:02:55.414: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 01:02:55.414: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  1 01:02:55.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get rc,svc -l name=pause --no-headers --namespace=kubectl-120'
Mar  1 01:02:55.590: INFO: stderr: "No resources found in kubectl-120 namespace.\n"
Mar  1 01:02:55.590: INFO: stdout: ""
Mar  1 01:02:55.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -l name=pause --namespace=kubectl-120 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 01:02:55.754: INFO: stderr: ""
Mar  1 01:02:55.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:02:55.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-120" for this suite.
Mar  1 01:03:01.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:01.849: INFO: namespace kubectl-120 deletion completed in 6.091040719s

• [SLOW TEST:9.898 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:03:01.850: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Mar  1 01:03:01.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 cluster-info'
Mar  1 01:03:02.000: INFO: stderr: ""
Mar  1 01:03:02.000: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:03:02.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7967" for this suite.
Mar  1 01:03:08.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:08.117: INFO: namespace kubectl-7967 deletion completed in 6.113328096s

• [SLOW TEST:6.267 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:03:08.121: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:03:08.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:03:10.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621387, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621387, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621387, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621387, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:03:13.820: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  1 01:03:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 attach --namespace=webhook-8500 to-be-attached-pod -i -c=container1'
Mar  1 01:03:17.999: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:03:18.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8500" for this suite.
Mar  1 01:03:30.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:30.106: INFO: namespace webhook-8500 deletion completed in 12.09662396s
STEP: Destroying namespace "webhook-8500-markers" for this suite.
Mar  1 01:03:36.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:36.213: INFO: namespace webhook-8500-markers deletion completed in 6.106384747s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.102 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:03:36.223: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:03:36.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f" in namespace "projected-9429" to be "success or failure"
Mar  1 01:03:36.268: INFO: Pod "downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482433ms
Mar  1 01:03:38.272: INFO: Pod "downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0083994s
Mar  1 01:03:40.275: INFO: Pod "downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01144779s
STEP: Saw pod success
Mar  1 01:03:40.275: INFO: Pod "downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f" satisfied condition "success or failure"
Mar  1 01:03:40.277: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f container client-container: <nil>
STEP: delete the pod
Mar  1 01:03:40.312: INFO: Waiting for pod downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f to disappear
Mar  1 01:03:40.315: INFO: Pod downwardapi-volume-ef79fb01-e469-430b-8f80-2e88408e3c2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:03:40.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9429" for this suite.
Mar  1 01:03:46.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:46.400: INFO: namespace projected-9429 deletion completed in 6.080645393s

• [SLOW TEST:10.177 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:03:46.403: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  1 01:03:46.456: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  1 01:03:51.459: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:03:51.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3400" for this suite.
Mar  1 01:03:57.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:03:57.733: INFO: namespace replication-controller-3400 deletion completed in 6.202656197s

• [SLOW TEST:11.330 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:03:57.735: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:03:57.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6" in namespace "downward-api-6072" to be "success or failure"
Mar  1 01:03:57.805: INFO: Pod "downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385002ms
Mar  1 01:03:59.809: INFO: Pod "downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012697116s
STEP: Saw pod success
Mar  1 01:03:59.809: INFO: Pod "downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6" satisfied condition "success or failure"
Mar  1 01:03:59.811: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6 container client-container: <nil>
STEP: delete the pod
Mar  1 01:03:59.831: INFO: Waiting for pod downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6 to disappear
Mar  1 01:03:59.837: INFO: Pod downwardapi-volume-152ccc0f-6f78-4365-a96f-c6d38d7c45f6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:03:59.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6072" for this suite.
Mar  1 01:04:05.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:04:05.950: INFO: namespace downward-api-6072 deletion completed in 6.108905183s

• [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:04:05.958: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-6cf81159-0a00-4112-bd40-155c40cb312d
STEP: Creating a pod to test consume configMaps
Mar  1 01:04:06.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03" in namespace "configmap-9244" to be "success or failure"
Mar  1 01:04:06.032: INFO: Pod "pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03": Phase="Pending", Reason="", readiness=false. Elapsed: 14.227725ms
Mar  1 01:04:08.037: INFO: Pod "pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019323724s
Mar  1 01:04:10.040: INFO: Pod "pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02242723s
STEP: Saw pod success
Mar  1 01:04:10.040: INFO: Pod "pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03" satisfied condition "success or failure"
Mar  1 01:04:10.046: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 01:04:10.075: INFO: Waiting for pod pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03 to disappear
Mar  1 01:04:10.078: INFO: Pod pod-configmaps-e756d372-b4f9-47e9-9dd5-a431a479dd03 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:04:10.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9244" for this suite.
Mar  1 01:04:16.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:04:16.173: INFO: namespace configmap-9244 deletion completed in 6.090746968s

• [SLOW TEST:10.216 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:04:16.176: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Mar  1 01:04:16.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-65 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  1 01:04:16.319: INFO: stderr: ""
Mar  1 01:04:16.319: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Mar  1 01:04:16.319: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  1 01:04:16.319: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-65" to be "running and ready, or succeeded"
Mar  1 01:04:16.325: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.331599ms
Mar  1 01:04:18.328: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008138634s
Mar  1 01:04:20.331: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.011323629s
Mar  1 01:04:20.331: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  1 01:04:20.331: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  1 01:04:20.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65'
Mar  1 01:04:20.479: INFO: stderr: ""
Mar  1 01:04:20.479: INFO: stdout: "I0301 01:04:17.708188       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/cmn2 432\nI0301 01:04:17.908615       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/vqng 522\nI0301 01:04:18.108376       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/kdr6 403\nI0301 01:04:18.308356       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gsdj 337\nI0301 01:04:18.508301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/jzl 492\nI0301 01:04:18.708331       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/md8l 531\nI0301 01:04:18.908448       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ns2 321\nI0301 01:04:19.108324       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wpf 548\nI0301 01:04:19.308337       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/lnsj 542\nI0301 01:04:19.508333       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/v4m 340\nI0301 01:04:19.708342       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/rrr 571\nI0301 01:04:19.908402       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/468d 586\nI0301 01:04:20.108393       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/87wm 559\nI0301 01:04:20.308341       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/lmg6 549\n"
STEP: limiting log lines
Mar  1 01:04:20.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65 --tail=1'
Mar  1 01:04:20.603: INFO: stderr: ""
Mar  1 01:04:20.603: INFO: stdout: "I0301 01:04:20.508362       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/4kw5 510\n"
STEP: limiting log bytes
Mar  1 01:04:20.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65 --limit-bytes=1'
Mar  1 01:04:20.748: INFO: stderr: ""
Mar  1 01:04:20.748: INFO: stdout: "I"
STEP: exposing timestamps
Mar  1 01:04:20.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65 --tail=1 --timestamps'
Mar  1 01:04:20.895: INFO: stderr: ""
Mar  1 01:04:20.895: INFO: stdout: "2020-03-01T01:04:20.7167082Z I0301 01:04:20.708319       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/ds87 344\n"
STEP: restricting to a time range
Mar  1 01:04:23.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65 --since=1s'
Mar  1 01:04:23.509: INFO: stderr: ""
Mar  1 01:04:23.509: INFO: stdout: "I0301 01:04:22.508401       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/k625 541\nI0301 01:04:22.708500       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/cqw 356\nI0301 01:04:22.909804       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/kj2r 502\nI0301 01:04:23.108344       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/ndpl 274\nI0301 01:04:23.308908       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/2wm 276\n"
Mar  1 01:04:23.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 logs logs-generator logs-generator --namespace=kubectl-65 --since=24h'
Mar  1 01:04:23.623: INFO: stderr: ""
Mar  1 01:04:23.623: INFO: stdout: "I0301 01:04:17.708188       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/cmn2 432\nI0301 01:04:17.908615       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/vqng 522\nI0301 01:04:18.108376       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/kdr6 403\nI0301 01:04:18.308356       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gsdj 337\nI0301 01:04:18.508301       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/jzl 492\nI0301 01:04:18.708331       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/md8l 531\nI0301 01:04:18.908448       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ns2 321\nI0301 01:04:19.108324       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/wpf 548\nI0301 01:04:19.308337       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/lnsj 542\nI0301 01:04:19.508333       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/v4m 340\nI0301 01:04:19.708342       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/rrr 571\nI0301 01:04:19.908402       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/468d 586\nI0301 01:04:20.108393       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/87wm 559\nI0301 01:04:20.308341       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/lmg6 549\nI0301 01:04:20.508362       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/4kw5 510\nI0301 01:04:20.708319       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/ds87 344\nI0301 01:04:20.908326       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/ctr 239\nI0301 01:04:21.108357       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/rkw 348\nI0301 01:04:21.308404       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/r5m9 383\nI0301 01:04:21.508360       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/ghx 391\nI0301 01:04:21.708347       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/krb 443\nI0301 01:04:21.908398       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/bkl 253\nI0301 01:04:22.108380       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/dkk 461\nI0301 01:04:22.308329       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/r29 483\nI0301 01:04:22.508401       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/k625 541\nI0301 01:04:22.708500       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/cqw 356\nI0301 01:04:22.909804       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/kj2r 502\nI0301 01:04:23.108344       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/ndpl 274\nI0301 01:04:23.308908       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/2wm 276\nI0301 01:04:23.509219       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/qzh 381\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Mar  1 01:04:23.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete pod logs-generator --namespace=kubectl-65'
Mar  1 01:04:25.526: INFO: stderr: ""
Mar  1 01:04:25.526: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:04:25.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-65" for this suite.
Mar  1 01:04:31.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:04:31.635: INFO: namespace kubectl-65 deletion completed in 6.104475458s

• [SLOW TEST:15.459 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:04:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-cb5597f9-be52-4a3d-b184-0992ad256846
STEP: Creating a pod to test consume configMaps
Mar  1 01:04:31.678: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923" in namespace "projected-1192" to be "success or failure"
Mar  1 01:04:31.680: INFO: Pod "pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169685ms
Mar  1 01:04:33.684: INFO: Pod "pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006022558s
Mar  1 01:04:35.688: INFO: Pod "pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009438345s
STEP: Saw pod success
Mar  1 01:04:35.688: INFO: Pod "pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923" satisfied condition "success or failure"
Mar  1 01:04:35.690: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 01:04:35.713: INFO: Waiting for pod pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923 to disappear
Mar  1 01:04:35.715: INFO: Pod pod-projected-configmaps-2ceca11c-6fbf-4230-b545-7f7dc77e6923 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:04:35.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1192" for this suite.
Mar  1 01:04:41.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:04:41.813: INFO: namespace projected-1192 deletion completed in 6.095264404s

• [SLOW TEST:10.177 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:04:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:04:41.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-338" for this suite.
Mar  1 01:04:47.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:04:47.974: INFO: namespace resourcequota-338 deletion completed in 6.094514787s

• [SLOW TEST:6.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:04:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:04:48.340: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  1 01:04:50.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621487, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621487, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621487, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621487, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:04:53.365: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:04:53.368: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:04:54.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9415" for this suite.
Mar  1 01:05:00.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:05:00.704: INFO: namespace crd-webhook-9415 deletion completed in 6.100774659s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.740 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:05:00.715: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:05:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:05:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8251" for this suite.
Mar  1 01:05:50.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:05:50.938: INFO: namespace pods-8251 deletion completed in 46.129860191s

• [SLOW TEST:50.223 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:05:50.938: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:05:50.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7" in namespace "downward-api-7848" to be "success or failure"
Mar  1 01:05:51.001: INFO: Pod "downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.071193ms
Mar  1 01:05:53.005: INFO: Pod "downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020035682s
Mar  1 01:05:55.008: INFO: Pod "downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022994665s
STEP: Saw pod success
Mar  1 01:05:55.008: INFO: Pod "downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7" satisfied condition "success or failure"
Mar  1 01:05:55.010: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7 container client-container: <nil>
STEP: delete the pod
Mar  1 01:05:55.038: INFO: Waiting for pod downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7 to disappear
Mar  1 01:05:55.040: INFO: Pod downwardapi-volume-7f507458-9978-44c0-b43b-f9af1ce673a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:05:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7848" for this suite.
Mar  1 01:06:01.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:06:01.153: INFO: namespace downward-api-7848 deletion completed in 6.103842567s

• [SLOW TEST:10.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:06:01.156: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Mar  1 01:06:01.211: INFO: Waiting up to 5m0s for pod "var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3" in namespace "var-expansion-4407" to be "success or failure"
Mar  1 01:06:01.226: INFO: Pod "var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.301557ms
Mar  1 01:06:03.230: INFO: Pod "var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018408498s
Mar  1 01:06:05.232: INFO: Pod "var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021062886s
STEP: Saw pod success
Mar  1 01:06:05.232: INFO: Pod "var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3" satisfied condition "success or failure"
Mar  1 01:06:05.234: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3 container dapi-container: <nil>
STEP: delete the pod
Mar  1 01:06:05.254: INFO: Waiting for pod var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3 to disappear
Mar  1 01:06:05.258: INFO: Pod var-expansion-c48bb21b-7d15-47e0-bec0-56c582ea4cc3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:06:05.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4407" for this suite.
Mar  1 01:06:11.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:06:11.347: INFO: namespace var-expansion-4407 deletion completed in 6.083707738s

• [SLOW TEST:10.191 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:06:11.347: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  1 01:06:11.389: INFO: Waiting up to 5m0s for pod "pod-9f672aeb-e95d-4352-a697-2f037185c08b" in namespace "emptydir-3801" to be "success or failure"
Mar  1 01:06:11.402: INFO: Pod "pod-9f672aeb-e95d-4352-a697-2f037185c08b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.855526ms
Mar  1 01:06:13.406: INFO: Pod "pod-9f672aeb-e95d-4352-a697-2f037185c08b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016639683s
Mar  1 01:06:15.409: INFO: Pod "pod-9f672aeb-e95d-4352-a697-2f037185c08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019714489s
STEP: Saw pod success
Mar  1 01:06:15.409: INFO: Pod "pod-9f672aeb-e95d-4352-a697-2f037185c08b" satisfied condition "success or failure"
Mar  1 01:06:15.411: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-9f672aeb-e95d-4352-a697-2f037185c08b container test-container: <nil>
STEP: delete the pod
Mar  1 01:06:15.437: INFO: Waiting for pod pod-9f672aeb-e95d-4352-a697-2f037185c08b to disappear
Mar  1 01:06:15.439: INFO: Pod pod-9f672aeb-e95d-4352-a697-2f037185c08b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:06:15.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3801" for this suite.
Mar  1 01:06:21.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:06:21.552: INFO: namespace emptydir-3801 deletion completed in 6.108456385s

• [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:06:21.553: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-00d1c0fe-5057-45d9-9f0a-912adc4f1427
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-00d1c0fe-5057-45d9-9f0a-912adc4f1427
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:07:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4226" for this suite.
Mar  1 01:07:52.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:07:52.121: INFO: namespace projected-4226 deletion completed in 12.091458456s

• [SLOW TEST:90.568 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:07:52.122: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:07:52.144: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:07:59.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7815" for this suite.
Mar  1 01:08:05.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:08:05.967: INFO: namespace custom-resource-definition-7815 deletion completed in 6.107769245s

• [SLOW TEST:13.845 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:08:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:08:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  1 01:08:10.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 create -f -'
Mar  1 01:08:12.044: INFO: stderr: ""
Mar  1 01:08:12.045: INFO: stdout: "e2e-test-crd-publish-openapi-1465-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 01:08:12.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 delete e2e-test-crd-publish-openapi-1465-crds test-foo'
Mar  1 01:08:12.169: INFO: stderr: ""
Mar  1 01:08:12.170: INFO: stdout: "e2e-test-crd-publish-openapi-1465-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  1 01:08:12.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 apply -f -'
Mar  1 01:08:12.427: INFO: stderr: ""
Mar  1 01:08:12.427: INFO: stdout: "e2e-test-crd-publish-openapi-1465-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 01:08:12.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 delete e2e-test-crd-publish-openapi-1465-crds test-foo'
Mar  1 01:08:12.554: INFO: stderr: ""
Mar  1 01:08:12.555: INFO: stdout: "e2e-test-crd-publish-openapi-1465-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  1 01:08:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 create -f -'
Mar  1 01:08:12.800: INFO: rc: 1
Mar  1 01:08:12.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 apply -f -'
Mar  1 01:08:13.078: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  1 01:08:13.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 create -f -'
Mar  1 01:08:13.331: INFO: rc: 1
Mar  1 01:08:13.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-903 apply -f -'
Mar  1 01:08:13.565: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  1 01:08:13.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1465-crds'
Mar  1 01:08:13.815: INFO: stderr: ""
Mar  1 01:08:13.815: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1465-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  1 01:08:13.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1465-crds.metadata'
Mar  1 01:08:14.074: INFO: stderr: ""
Mar  1 01:08:14.074: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1465-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  1 01:08:14.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1465-crds.spec'
Mar  1 01:08:14.341: INFO: stderr: ""
Mar  1 01:08:14.341: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1465-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  1 01:08:14.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1465-crds.spec.bars'
Mar  1 01:08:14.575: INFO: stderr: ""
Mar  1 01:08:14.575: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1465-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  1 01:08:14.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-1465-crds.spec.bars2'
Mar  1 01:08:14.820: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:08:19.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-903" for this suite.
Mar  1 01:08:25.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:08:25.880: INFO: namespace crd-publish-openapi-903 deletion completed in 6.148487162s

• [SLOW TEST:19.912 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:08:25.881: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:08:27.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:08:29.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621706, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621706, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621706, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:08:32.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:08:32.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9790" for this suite.
Mar  1 01:08:38.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:08:38.890: INFO: namespace webhook-9790 deletion completed in 6.091547871s
STEP: Destroying namespace "webhook-9790-markers" for this suite.
Mar  1 01:08:44.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:08:45.006: INFO: namespace webhook-9790-markers deletion completed in 6.115826074s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.134 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:08:45.015: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Mar  1 01:08:45.048: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-585087022 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:08:45.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8442" for this suite.
Mar  1 01:08:51.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:08:51.268: INFO: namespace kubectl-8442 deletion completed in 6.111608896s

• [SLOW TEST:6.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:08:51.269: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:08:52.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:08:54.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621731, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:08:57.606: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:09:09.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5661" for this suite.
Mar  1 01:09:15.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:09:15.820: INFO: namespace webhook-5661 deletion completed in 6.090502339s
STEP: Destroying namespace "webhook-5661-markers" for this suite.
Mar  1 01:09:21.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:09:21.915: INFO: namespace webhook-5661-markers deletion completed in 6.094122014s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:09:21.927: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  1 01:09:21.977: INFO: Waiting up to 5m0s for pod "pod-7ad882d4-4105-4647-ad1e-ac2957ea4607" in namespace "emptydir-3660" to be "success or failure"
Mar  1 01:09:21.986: INFO: Pod "pod-7ad882d4-4105-4647-ad1e-ac2957ea4607": Phase="Pending", Reason="", readiness=false. Elapsed: 9.741567ms
Mar  1 01:09:23.990: INFO: Pod "pod-7ad882d4-4105-4647-ad1e-ac2957ea4607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013309651s
Mar  1 01:09:25.993: INFO: Pod "pod-7ad882d4-4105-4647-ad1e-ac2957ea4607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01623632s
STEP: Saw pod success
Mar  1 01:09:25.993: INFO: Pod "pod-7ad882d4-4105-4647-ad1e-ac2957ea4607" satisfied condition "success or failure"
Mar  1 01:09:25.995: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-7ad882d4-4105-4647-ad1e-ac2957ea4607 container test-container: <nil>
STEP: delete the pod
Mar  1 01:09:26.025: INFO: Waiting for pod pod-7ad882d4-4105-4647-ad1e-ac2957ea4607 to disappear
Mar  1 01:09:26.026: INFO: Pod pod-7ad882d4-4105-4647-ad1e-ac2957ea4607 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:09:26.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3660" for this suite.
Mar  1 01:09:32.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:09:32.133: INFO: namespace emptydir-3660 deletion completed in 6.095736524s

• [SLOW TEST:10.206 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:09:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:09:33.584: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:09:35.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621772, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621772, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621772, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621772, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:09:38.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  1 01:09:38.647: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:09:38.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3055" for this suite.
Mar  1 01:09:44.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:09:44.743: INFO: namespace webhook-3055 deletion completed in 6.079877455s
STEP: Destroying namespace "webhook-3055-markers" for this suite.
Mar  1 01:09:50.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:09:50.843: INFO: namespace webhook-3055-markers deletion completed in 6.100139187s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:09:50.855: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:09:51.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:09:53.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621790, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621790, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621790, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621790, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:09:56.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:10:06.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9754" for this suite.
Mar  1 01:10:12.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:10:12.825: INFO: namespace webhook-9754 deletion completed in 6.092327842s
STEP: Destroying namespace "webhook-9754-markers" for this suite.
Mar  1 01:10:18.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:10:18.920: INFO: namespace webhook-9754-markers deletion completed in 6.094667777s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:28.086 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:10:18.941: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:10:19.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:10:21.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621819, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621819, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621819, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718621819, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:10:24.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:10:25.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3296" for this suite.
Mar  1 01:10:31.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:10:31.117: INFO: namespace webhook-3296 deletion completed in 6.083350698s
STEP: Destroying namespace "webhook-3296-markers" for this suite.
Mar  1 01:10:37.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:10:37.259: INFO: namespace webhook-3296-markers deletion completed in 6.14112569s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.328 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:10:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2620
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar  1 01:10:37.330: INFO: Found 0 stateful pods, waiting for 3
Mar  1 01:10:47.334: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 01:10:47.334: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 01:10:47.334: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 01:10:47.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-2620 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:10:47.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:10:47.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:10:47.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  1 01:10:57.623: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  1 01:11:07.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-2620 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:11:07.880: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:11:07.880: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:11:07.880: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Mar  1 01:11:27.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-2620 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:11:28.156: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:11:28.156: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:11:28.156: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 01:11:38.184: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  1 01:11:48.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-2620 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:11:48.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:11:48.454: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:11:48.454: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 01:11:58.472: INFO: Waiting for StatefulSet statefulset-2620/ss2 to complete update
Mar  1 01:11:58.472: INFO: Waiting for Pod statefulset-2620/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 01:11:58.472: INFO: Waiting for Pod statefulset-2620/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 01:11:58.472: INFO: Waiting for Pod statefulset-2620/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 01:12:08.479: INFO: Waiting for StatefulSet statefulset-2620/ss2 to complete update
Mar  1 01:12:08.479: INFO: Waiting for Pod statefulset-2620/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 01:12:08.479: INFO: Waiting for Pod statefulset-2620/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 01:12:18.478: INFO: Waiting for StatefulSet statefulset-2620/ss2 to complete update
Mar  1 01:12:18.479: INFO: Waiting for Pod statefulset-2620/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 01:12:28.479: INFO: Deleting all statefulset in ns statefulset-2620
Mar  1 01:12:28.481: INFO: Scaling statefulset ss2 to 0
Mar  1 01:13:08.495: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:13:08.497: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:13:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2620" for this suite.
Mar  1 01:13:14.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:13:14.618: INFO: namespace statefulset-2620 deletion completed in 6.098641223s

• [SLOW TEST:157.347 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:13:14.619: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:13:14.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9" in namespace "downward-api-5541" to be "success or failure"
Mar  1 01:13:14.672: INFO: Pod "downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.365026ms
Mar  1 01:13:16.676: INFO: Pod "downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012228778s
Mar  1 01:13:18.679: INFO: Pod "downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015078395s
STEP: Saw pod success
Mar  1 01:13:18.679: INFO: Pod "downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9" satisfied condition "success or failure"
Mar  1 01:13:18.681: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9 container client-container: <nil>
STEP: delete the pod
Mar  1 01:13:18.715: INFO: Waiting for pod downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9 to disappear
Mar  1 01:13:18.717: INFO: Pod downwardapi-volume-fc3ed3fd-8b47-40bd-9120-f927afb65aa9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:13:18.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5541" for this suite.
Mar  1 01:13:24.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:13:24.820: INFO: namespace downward-api-5541 deletion completed in 6.099752644s

• [SLOW TEST:10.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:13:24.820: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  1 01:13:24.852: INFO: Waiting up to 5m0s for pod "pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977" in namespace "emptydir-8555" to be "success or failure"
Mar  1 01:13:24.857: INFO: Pod "pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265934ms
Mar  1 01:13:26.864: INFO: Pod "pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01184334s
Mar  1 01:13:28.867: INFO: Pod "pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015046994s
STEP: Saw pod success
Mar  1 01:13:28.867: INFO: Pod "pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977" satisfied condition "success or failure"
Mar  1 01:13:28.869: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977 container test-container: <nil>
STEP: delete the pod
Mar  1 01:13:28.888: INFO: Waiting for pod pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977 to disappear
Mar  1 01:13:28.890: INFO: Pod pod-f1fad84c-4cb7-4dbb-bd3a-fafe9c152977 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:13:28.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8555" for this suite.
Mar  1 01:13:34.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:13:34.993: INFO: namespace emptydir-8555 deletion completed in 6.099234805s

• [SLOW TEST:10.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:13:34.993: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:13:35.034: INFO: Creating deployment "webserver-deployment"
Mar  1 01:13:35.038: INFO: Waiting for observed generation 1
Mar  1 01:13:37.053: INFO: Waiting for all required pods to come up
Mar  1 01:13:37.058: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  1 01:13:39.069: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  1 01:13:39.078: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  1 01:13:39.086: INFO: Updating deployment webserver-deployment
Mar  1 01:13:39.086: INFO: Waiting for observed generation 2
Mar  1 01:13:41.101: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  1 01:13:41.103: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  1 01:13:41.105: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 01:13:41.113: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  1 01:13:41.113: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  1 01:13:41.116: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 01:13:41.120: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  1 01:13:41.120: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  1 01:13:41.125: INFO: Updating deployment webserver-deployment
Mar  1 01:13:41.125: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  1 01:13:41.130: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  1 01:13:41.152: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  1 01:13:41.182: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8987 /apis/apps/v1/namespaces/deployment-8987/deployments/webserver-deployment 004a6caf-074c-4a85-80d6-3daad385b538 37320 3 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ca2028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-01 01:13:38 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-01 01:13:40 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  1 01:13:41.227: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8987 /apis/apps/v1/namespaces/deployment-8987/replicasets/webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 37304 3 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 004a6caf-074c-4a85-80d6-3daad385b538 0xc006cb4027 0xc006cb4028}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006cb4098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:13:41.227: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  1 01:13:41.227: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8987 /apis/apps/v1/namespaces/deployment-8987/replicasets/webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 37301 3 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 004a6caf-074c-4a85-80d6-3daad385b538 0xc004873f67 0xc004873f68}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004873fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:13:41.269: INFO: Pod "webserver-deployment-595b5b9587-45m54" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-45m54 webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-45m54 7dab6580-8e7c-4acf-a07f-04dedbcc8a95 37325 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2477 0xc003ca2478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.269: INFO: Pod "webserver-deployment-595b5b9587-49nx4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-49nx4 webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-49nx4 5a5ddea0-112c-4464-b902-fcbd0cbeeb52 37348 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2580 0xc003ca2581}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.270: INFO: Pod "webserver-deployment-595b5b9587-8hg6b" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8hg6b webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-8hg6b c9776819-da64-4210-985d-397349454acc 37185 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.5.250/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca26a0 0xc003ca26a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.250,StartTime:2020-03-01 01:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0eeeec92515257957d0e1336b528680ae5a591015510cfa14e4406546914fa23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.270: INFO: Pod "webserver-deployment-595b5b9587-8nbrn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8nbrn webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-8nbrn 227df43f-be34-4f13-8c19-1ed3faa0f646 37352 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2807 0xc003ca2808}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.270: INFO: Pod "webserver-deployment-595b5b9587-9z4lr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9z4lr webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-9z4lr 643f6209-8417-435e-ad0f-015fecbd87ce 37306 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2910 0xc003ca2911}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.271: INFO: Pod "webserver-deployment-595b5b9587-cbm4f" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cbm4f webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-cbm4f 278ebf55-7db1-41f2-8b97-823d8660860f 37219 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.5.2/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2a20 0xc003ca2a21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.2,StartTime:2020-03-01 01:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e7b4b3c627361674b723dacbc655932f80237d134774303d6ad0885dba710c26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.271: INFO: Pod "webserver-deployment-595b5b9587-cnggg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cnggg webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-cnggg eb9ed97c-4b52-432b-8661-24c0a735fbda 37350 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2b80 0xc003ca2b81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.271: INFO: Pod "webserver-deployment-595b5b9587-k8kqf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k8kqf webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-k8kqf 04e900d5-0bc3-4c55-8f02-053c656d4658 37213 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.4.106/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2c90 0xc003ca2c91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.106,StartTime:2020-03-01 01:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a0ef07ff64f09c04a766c25aa56ec5fbb97fd079d6702a0d190ad4b7e4c52519,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.272: INFO: Pod "webserver-deployment-595b5b9587-mdj4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mdj4b webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-mdj4b 78c2445b-dfcc-49d1-bb95-61159284f88d 37317 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2df7 0xc003ca2df8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.273: INFO: Pod "webserver-deployment-595b5b9587-mtj6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mtj6q webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-mtj6q 88499673-9c28-4e18-8cd7-4ead7a017941 37314 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca2f00 0xc003ca2f01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.273: INFO: Pod "webserver-deployment-595b5b9587-n2hsq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n2hsq webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-n2hsq d40bc9f7-55a4-484a-9d78-be5ce968b43b 37231 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.5.253/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3010 0xc003ca3011}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.253,StartTime:2020-03-01 01:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://976824fec5b4c55260886e1b4110943afc500beb3e81e2f2ccddc08500ce61be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.274: INFO: Pod "webserver-deployment-595b5b9587-n2mfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n2mfs webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-n2mfs ba072de8-9dd7-454d-a1a2-70717e504879 37340 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3177 0xc003ca3178}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.274: INFO: Pod "webserver-deployment-595b5b9587-p2blp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p2blp webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-p2blp 4de5eacd-430c-4321-a656-c8adf38927e2 37216 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.4.103/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3290 0xc003ca3291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.103,StartTime:2020-03-01 01:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ff6913f313bfdbabee4abdda103bea99cfd3614ad88db98e13e3df1ddec5c270,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.274: INFO: Pod "webserver-deployment-595b5b9587-s9z9m" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-s9z9m webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-s9z9m 49f86ac9-09cc-4a12-95a6-f949f94ded31 37222 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.5.251/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3407 0xc003ca3408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.251,StartTime:2020-03-01 01:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://17c572d9c49871228b9830eaecb95a2bda2d6335b656412fe328a5c4ceab1d1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.275: INFO: Pod "webserver-deployment-595b5b9587-sjfcp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sjfcp webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-sjfcp dfd5b4ac-8e3f-4e22-affe-9ca60e22540b 37210 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.4.104/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3587 0xc003ca3588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.104,StartTime:2020-03-01 01:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fea002d7daec85eeb5387cbf0a263da68871a6039b9030f362e66ddce388fee1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.275: INFO: Pod "webserver-deployment-595b5b9587-t8ckn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t8ckn webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-t8ckn 7808f56a-f9d8-4254-87c5-884e3868512b 37207 0 2020-03-01 01:13:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.4.105/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3707 0xc003ca3708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.4.105,StartTime:2020-03-01 01:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:13:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3f375b422c067412fa8d2c62a048245ec45ad5d86f73a755dc6150ca04a62bb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.4.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.276: INFO: Pod "webserver-deployment-595b5b9587-vk9vb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vk9vb webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-vk9vb 19e8d2d2-1319-4b0a-a1b2-ea9b56add5f0 37324 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3877 0xc003ca3878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.276: INFO: Pod "webserver-deployment-595b5b9587-wq7v7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wq7v7 webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-wq7v7 05384f81-489a-4e86-b6d7-3cbe0cf50bdd 37322 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3980 0xc003ca3981}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.277: INFO: Pod "webserver-deployment-595b5b9587-xjmkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xjmkq webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-xjmkq 7b9f55bc-5256-4069-aa03-1ddec5cf8c62 37332 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3a80 0xc003ca3a81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.277: INFO: Pod "webserver-deployment-595b5b9587-z6hx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z6hx6 webserver-deployment-595b5b9587- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-595b5b9587-z6hx6 65f4fa68-8bc4-4682-9194-075cda66d8d2 37339 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a33cbdc6-8c7e-4f41-b45d-7289d384793a 0xc003ca3b80 0xc003ca3b81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.277: INFO: Pod "webserver-deployment-c7997dcc8-4hhjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4hhjw webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-4hhjw d579b61b-5878-4710-9a27-f01680165312 37338 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc003ca3c80 0xc003ca3c81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.277: INFO: Pod "webserver-deployment-c7997dcc8-5ktw8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5ktw8 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-5ktw8 95ac41be-095e-4003-ab01-3a260baa232a 37341 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc003ca3d77 0xc003ca3d78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.278: INFO: Pod "webserver-deployment-c7997dcc8-5xjv5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5xjv5 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-5xjv5 0a85fa38-5a06-45f1-82c2-42e52fb758b4 37298 0 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.5.4/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc003ca3e87 0xc003ca3e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:,StartTime:2020-03-01 01:13:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.278: INFO: Pod "webserver-deployment-c7997dcc8-8cq4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8cq4r webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-8cq4r 444814a4-260d-42f1-8b13-185132b8da06 37292 0 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.4.107/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4007 0xc0068b4008}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2020-03-01 01:13:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.278: INFO: Pod "webserver-deployment-c7997dcc8-b4dfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b4dfp webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-b4dfp 9aba20ec-dba8-4322-8c12-7f8e9cb4cc52 37347 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4177 0xc0068b4178}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.278: INFO: Pod "webserver-deployment-c7997dcc8-cwvjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cwvjw webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-cwvjw 70ee901d-1074-46da-9918-7f70240e3846 37346 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4277 0xc0068b4278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.278: INFO: Pod "webserver-deployment-c7997dcc8-d5tt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d5tt4 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-d5tt4 355af1e7-cf0e-4a72-924c-aff4d1235273 37321 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4390 0xc0068b4391}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.279: INFO: Pod "webserver-deployment-c7997dcc8-ftgk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ftgk8 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-ftgk8 1fd4219b-11cc-43da-bb4c-7808ec73f678 37294 0 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.4.108/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b44b0 0xc0068b44b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-14d9760cdc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2020-03-01 01:13:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.279: INFO: Pod "webserver-deployment-c7997dcc8-hvt25" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hvt25 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-hvt25 cb7760bc-55b5-4dba-b85a-13549c65a9b0 37342 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4617 0xc0068b4618}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.279: INFO: Pod "webserver-deployment-c7997dcc8-krd8m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-krd8m webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-krd8m 8e9ed55f-e07a-4066-ab3b-8bb6d0d6466b 37296 0 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.5.3/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4727 0xc0068b4728}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:,StartTime:2020-03-01 01:13:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.279: INFO: Pod "webserver-deployment-c7997dcc8-x249d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x249d webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-x249d bb95243f-5541-4ebc-9efe-aee3249bc7a6 37308 0 2020-03-01 01:13:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.5.5/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b48a7 0xc0068b48a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:,StartTime:2020-03-01 01:13:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 01:13:41.279: INFO: Pod "webserver-deployment-c7997dcc8-zhbk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zhbk8 webserver-deployment-c7997dcc8- deployment-8987 /api/v1/namespaces/deployment-8987/pods/webserver-deployment-c7997dcc8-zhbk8 ec125417-d39e-4ddd-86d8-5092598d2081 37344 0 2020-03-01 01:13:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d6c5947f-e381-4d66-9c13-13f378f578f5 0xc0068b4a17 0xc0068b4a18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cm7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cm7xd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cm7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:13:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:13:41.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8987" for this suite.
Mar  1 01:13:49.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:13:49.469: INFO: namespace deployment-8987 deletion completed in 8.167910499s

• [SLOW TEST:14.476 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:13:49.470: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 01:13:49.511: INFO: Waiting up to 5m0s for pod "pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac" in namespace "emptydir-3596" to be "success or failure"
Mar  1 01:13:49.516: INFO: Pod "pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.550159ms
Mar  1 01:13:51.519: INFO: Pod "pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008461444s
Mar  1 01:13:53.523: INFO: Pod "pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011948376s
STEP: Saw pod success
Mar  1 01:13:53.523: INFO: Pod "pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac" satisfied condition "success or failure"
Mar  1 01:13:53.525: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac container test-container: <nil>
STEP: delete the pod
Mar  1 01:13:53.545: INFO: Waiting for pod pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac to disappear
Mar  1 01:13:53.559: INFO: Pod pod-5739a9e4-a3f7-4f3a-8df7-64a3ac21b7ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:13:53.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3596" for this suite.
Mar  1 01:13:59.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:13:59.674: INFO: namespace emptydir-3596 deletion completed in 6.109459607s

• [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:13:59.675: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:13:59.713: INFO: Creating deployment "test-recreate-deployment"
Mar  1 01:13:59.720: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  1 01:13:59.735: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  1 01:14:01.740: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  1 01:14:01.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622038, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622038, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622038, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622038, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:14:03.745: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  1 01:14:03.752: INFO: Updating deployment test-recreate-deployment
Mar  1 01:14:03.752: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  1 01:14:03.861: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6565 /apis/apps/v1/namespaces/deployment-6565/deployments/test-recreate-deployment 623eff7b-3a24-487c-90e6-073ad0821e80 37725 2 2020-03-01 01:13:58 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002868e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-01 01:14:02 +0000 UTC,LastTransitionTime:2020-03-01 01:14:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-01 01:14:02 +0000 UTC,LastTransitionTime:2020-03-01 01:13:58 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 01:14:03.865: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6565 /apis/apps/v1/namespaces/deployment-6565/replicasets/test-recreate-deployment-5f94c574ff 36e9de3d-6f08-478b-bdd6-e52e5ac9e67b 37723 1 2020-03-01 01:14:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 623eff7b-3a24-487c-90e6-073ad0821e80 0xc002869407 0xc002869408}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028694e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:14:03.865: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  1 01:14:03.865: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-6565 /apis/apps/v1/namespaces/deployment-6565/replicasets/test-recreate-deployment-68fc85c7bb 8b026e66-6a90-45f4-973e-728d4df77f23 37712 2 2020-03-01 01:13:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 623eff7b-3a24-487c-90e6-073ad0821e80 0xc002869577 0xc002869578}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028697b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:14:03.869: INFO: Pod "test-recreate-deployment-5f94c574ff-hr62f" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-hr62f test-recreate-deployment-5f94c574ff- deployment-6565 /api/v1/namespaces/deployment-6565/pods/test-recreate-deployment-5f94c574ff-hr62f e9821323-74c2-4cb5-94e1-d3d5821467c4 37724 0 2020-03-01 01:14:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 36e9de3d-6f08-478b-bdd6-e52e5ac9e67b 0xc00271dbe7 0xc00271dbe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8hwsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8hwsl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8hwsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:14:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:14:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:14:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:14:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:,StartTime:2020-03-01 01:14:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:14:03.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6565" for this suite.
Mar  1 01:14:09.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:14:09.977: INFO: namespace deployment-6565 deletion completed in 6.104489433s

• [SLOW TEST:10.302 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:14:09.983: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:14:10.029: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  1 01:14:10.043: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:10.055: INFO: Number of nodes with available pods: 0
Mar  1 01:14:10.055: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 01:14:11.060: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:11.064: INFO: Number of nodes with available pods: 0
Mar  1 01:14:11.064: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 01:14:12.061: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:12.065: INFO: Number of nodes with available pods: 0
Mar  1 01:14:12.065: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 01:14:13.060: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:13.063: INFO: Number of nodes with available pods: 2
Mar  1 01:14:13.063: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  1 01:14:13.103: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:13.103: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:13.113: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:14.116: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:14.116: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:14.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:15.116: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:15.116: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:15.121: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:16.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:16.117: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:16.117: INFO: Pod daemon-set-xccl4 is not available
Mar  1 01:14:16.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:17.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:17.117: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:17.117: INFO: Pod daemon-set-xccl4 is not available
Mar  1 01:14:17.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:18.116: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:18.117: INFO: Wrong image for pod: daemon-set-xccl4. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:18.117: INFO: Pod daemon-set-xccl4 is not available
Mar  1 01:14:18.121: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:19.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:19.117: INFO: Pod daemon-set-wfhsk is not available
Mar  1 01:14:19.121: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:20.140: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:20.141: INFO: Pod daemon-set-wfhsk is not available
Mar  1 01:14:20.146: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:21.119: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:21.119: INFO: Pod daemon-set-wfhsk is not available
Mar  1 01:14:21.123: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:22.116: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:22.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:23.119: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:23.119: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:23.123: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:24.118: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:24.118: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:24.128: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:25.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:25.117: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:25.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:26.118: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:26.118: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:26.124: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:27.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:27.117: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:27.122: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:28.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:28.117: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:28.120: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:29.117: INFO: Wrong image for pod: daemon-set-dm8kx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 01:14:29.117: INFO: Pod daemon-set-dm8kx is not available
Mar  1 01:14:29.122: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:30.117: INFO: Pod daemon-set-h2xvw is not available
Mar  1 01:14:30.122: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  1 01:14:30.127: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:30.130: INFO: Number of nodes with available pods: 1
Mar  1 01:14:30.130: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 01:14:31.136: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:31.139: INFO: Number of nodes with available pods: 1
Mar  1 01:14:31.139: INFO: Node alex-slot1-v3-vsp1-node-group-14d9760cdc is running more than one daemon pod
Mar  1 01:14:32.135: INFO: DaemonSet pods can't tolerate node alex-slot1-v3-vsp1-master-gro-22271c3398 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 01:14:32.139: INFO: Number of nodes with available pods: 2
Mar  1 01:14:32.139: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3374, will wait for the garbage collector to delete the pods
Mar  1 01:14:32.215: INFO: Deleting DaemonSet.extensions daemon-set took: 11.981152ms
Mar  1 01:14:32.715: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.234906ms
Mar  1 01:14:36.019: INFO: Number of nodes with available pods: 0
Mar  1 01:14:36.019: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 01:14:36.022: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3374/daemonsets","resourceVersion":"37900"},"items":null}

Mar  1 01:14:36.025: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3374/pods","resourceVersion":"37900"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:14:36.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3374" for this suite.
Mar  1 01:14:42.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:14:42.138: INFO: namespace daemonsets-3374 deletion completed in 6.100221138s

• [SLOW TEST:32.156 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:14:42.143: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  1 01:14:42.745: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0301 01:14:42.745440      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:14:42.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7148" for this suite.
Mar  1 01:14:48.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:14:48.841: INFO: namespace gc-7148 deletion completed in 6.091655052s

• [SLOW TEST:6.699 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:14:48.845: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:14:48.884: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb" in namespace "security-context-test-6652" to be "success or failure"
Mar  1 01:14:48.891: INFO: Pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092043ms
Mar  1 01:14:50.894: INFO: Pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00924852s
Mar  1 01:14:52.897: INFO: Pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012335999s
Mar  1 01:14:52.897: INFO: Pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb" satisfied condition "success or failure"
Mar  1 01:14:52.907: INFO: Got logs for pod "busybox-privileged-false-ec765e71-6641-4ca7-8598-a5dda09822cb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:14:52.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6652" for this suite.
Mar  1 01:14:58.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:14:58.991: INFO: namespace security-context-test-6652 deletion completed in 6.078009506s

• [SLOW TEST:10.147 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:14:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-3a3613f5-7fe1-496f-947f-83ad38df7161
STEP: Creating secret with name s-test-opt-upd-926bf549-871d-4609-b025-c8a02dd37b5e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3a3613f5-7fe1-496f-947f-83ad38df7161
STEP: Updating secret s-test-opt-upd-926bf549-871d-4609-b025-c8a02dd37b5e
STEP: Creating secret with name s-test-opt-create-2893ac6e-4269-44da-b081-5964a0756c96
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:15:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2773" for this suite.
Mar  1 01:15:21.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:15:21.229: INFO: namespace secrets-2773 deletion completed in 14.093751286s

• [SLOW TEST:22.237 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:15:21.236: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:15:21.307: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9ce8b614-4f72-4962-b866-fbf50f371e14", Controller:(*bool)(0xc003169f16), BlockOwnerDeletion:(*bool)(0xc003169f17)}}
Mar  1 01:15:21.313: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b6c5f8d7-623f-4783-9117-fe41ebeba59c", Controller:(*bool)(0xc0048d40b6), BlockOwnerDeletion:(*bool)(0xc0048d40b7)}}
Mar  1 01:15:21.321: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0414624f-9829-4837-aaec-6c826bac15b6", Controller:(*bool)(0xc0048d4256), BlockOwnerDeletion:(*bool)(0xc0048d4257)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:15:26.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1685" for this suite.
Mar  1 01:15:32.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:15:32.438: INFO: namespace gc-1685 deletion completed in 6.09215785s

• [SLOW TEST:11.202 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:15:32.441: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-lsvj
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 01:15:32.490: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lsvj" in namespace "subpath-381" to be "success or failure"
Mar  1 01:15:32.493: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30184ms
Mar  1 01:15:34.499: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008928069s
Mar  1 01:15:36.503: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 4.012776137s
Mar  1 01:15:38.506: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 6.016002596s
Mar  1 01:15:40.509: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 8.01904669s
Mar  1 01:15:42.512: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 10.022130972s
Mar  1 01:15:44.515: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 12.025305842s
Mar  1 01:15:46.519: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 14.029706636s
Mar  1 01:15:48.522: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 16.032403374s
Mar  1 01:15:50.526: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 18.036065804s
Mar  1 01:15:52.529: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Running", Reason="", readiness=true. Elapsed: 20.038932924s
Mar  1 01:15:54.533: INFO: Pod "pod-subpath-test-downwardapi-lsvj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04273702s
STEP: Saw pod success
Mar  1 01:15:54.533: INFO: Pod "pod-subpath-test-downwardapi-lsvj" satisfied condition "success or failure"
Mar  1 01:15:54.535: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-subpath-test-downwardapi-lsvj container test-container-subpath-downwardapi-lsvj: <nil>
STEP: delete the pod
Mar  1 01:15:54.560: INFO: Waiting for pod pod-subpath-test-downwardapi-lsvj to disappear
Mar  1 01:15:54.562: INFO: Pod pod-subpath-test-downwardapi-lsvj no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lsvj
Mar  1 01:15:54.562: INFO: Deleting pod "pod-subpath-test-downwardapi-lsvj" in namespace "subpath-381"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:15:54.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-381" for this suite.
Mar  1 01:16:00.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:16:00.654: INFO: namespace subpath-381 deletion completed in 6.087819799s

• [SLOW TEST:28.213 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:16:00.654: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:16:07.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-300" for this suite.
Mar  1 01:16:13.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:16:13.807: INFO: namespace resourcequota-300 deletion completed in 6.103219452s

• [SLOW TEST:13.153 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:16:13.807: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6348
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-6348
Mar  1 01:16:13.854: INFO: Found 0 stateful pods, waiting for 1
Mar  1 01:16:23.857: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 01:16:23.875: INFO: Deleting all statefulset in ns statefulset-6348
Mar  1 01:16:23.883: INFO: Scaling statefulset ss to 0
Mar  1 01:16:43.896: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:16:43.898: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:16:43.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6348" for this suite.
Mar  1 01:16:49.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:16:50.016: INFO: namespace statefulset-6348 deletion completed in 6.094662249s

• [SLOW TEST:36.209 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:16:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-616.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-616.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-616.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-616.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-616.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 01:16:54.113: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-616/dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c: the server could not find the requested resource (get pods dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c)
Mar  1 01:16:54.117: INFO: Unable to read jessie_udp@PodARecord from pod dns-616/dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c: the server could not find the requested resource (get pods dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c)
Mar  1 01:16:54.120: INFO: Unable to read jessie_tcp@PodARecord from pod dns-616/dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c: the server could not find the requested resource (get pods dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c)
Mar  1 01:16:54.120: INFO: Lookups using dns-616/dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  1 01:16:59.146: INFO: DNS probes using dns-616/dns-test-b5dce2de-fc1b-402e-8f92-7c6e878a9b9c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:16:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-616" for this suite.
Mar  1 01:17:05.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:17:05.295: INFO: namespace dns-616 deletion completed in 6.104377203s

• [SLOW TEST:15.277 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:17:05.297: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-701fa185-5ff2-42a4-8c1c-98496acae73c
STEP: Creating a pod to test consume secrets
Mar  1 01:17:05.336: INFO: Waiting up to 5m0s for pod "pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4" in namespace "secrets-826" to be "success or failure"
Mar  1 01:17:05.345: INFO: Pod "pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.856384ms
Mar  1 01:17:07.350: INFO: Pod "pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014094034s
Mar  1 01:17:09.354: INFO: Pod "pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01786237s
STEP: Saw pod success
Mar  1 01:17:09.354: INFO: Pod "pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4" satisfied condition "success or failure"
Mar  1 01:17:09.356: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4 container secret-env-test: <nil>
STEP: delete the pod
Mar  1 01:17:09.379: INFO: Waiting for pod pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4 to disappear
Mar  1 01:17:09.383: INFO: Pod pod-secrets-ac7f855a-0a95-4a77-b054-b58579d954c4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:17:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-826" for this suite.
Mar  1 01:17:15.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:17:15.506: INFO: namespace secrets-826 deletion completed in 6.119841203s

• [SLOW TEST:10.209 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:17:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:17:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9942" for this suite.
Mar  1 01:17:32.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:17:32.684: INFO: namespace resourcequota-9942 deletion completed in 6.089395392s

• [SLOW TEST:17.177 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:17:32.684: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:17:32.721: INFO: Creating ReplicaSet my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca
Mar  1 01:17:32.733: INFO: Pod name my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca: Found 0 pods out of 1
Mar  1 01:17:37.736: INFO: Pod name my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca: Found 1 pods out of 1
Mar  1 01:17:37.736: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca" is running
Mar  1 01:17:37.741: INFO: Pod "my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca-gvw4h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 01:17:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 01:17:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 01:17:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 01:17:31 +0000 UTC Reason: Message:}])
Mar  1 01:17:37.741: INFO: Trying to dial the pod
Mar  1 01:17:42.750: INFO: Controller my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca: Got expected result from replica 1 [my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca-gvw4h]: "my-hostname-basic-15655982-143d-4cb6-a3fe-e5d3b378f4ca-gvw4h", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:17:42.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3178" for this suite.
Mar  1 01:17:48.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:17:48.847: INFO: namespace replicaset-3178 deletion completed in 6.092530601s

• [SLOW TEST:16.163 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:17:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  1 01:17:50.288: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 01:17:52.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622269, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622269, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622269, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622269, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  1 01:17:55.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:17:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3947-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:17:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8506" for this suite.
Mar  1 01:18:01.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:18:02.084: INFO: namespace webhook-8506 deletion completed in 6.106550228s
STEP: Destroying namespace "webhook-8506-markers" for this suite.
Mar  1 01:18:08.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:18:08.203: INFO: namespace webhook-8506-markers deletion completed in 6.11865049s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.368 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:18:08.219: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5433
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  1 01:18:08.262: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  1 01:18:30.355: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.5.32:8080/dial?request=hostName&protocol=http&host=192.168.4.122&port=8080&tries=1'] Namespace:pod-network-test-5433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 01:18:30.355: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 01:18:30.512: INFO: Waiting for endpoints: map[]
Mar  1 01:18:30.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.5.32:8080/dial?request=hostName&protocol=http&host=192.168.5.31&port=8080&tries=1'] Namespace:pod-network-test-5433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 01:18:30.516: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
Mar  1 01:18:30.676: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:18:30.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5433" for this suite.
Mar  1 01:18:42.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:18:42.787: INFO: namespace pod-network-test-5433 deletion completed in 12.107044571s

• [SLOW TEST:34.569 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:18:42.787: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:18:42.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a" in namespace "projected-5329" to be "success or failure"
Mar  1 01:18:42.834: INFO: Pod "downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.53854ms
Mar  1 01:18:44.838: INFO: Pod "downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011106081s
Mar  1 01:18:46.841: INFO: Pod "downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014375662s
STEP: Saw pod success
Mar  1 01:18:46.841: INFO: Pod "downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a" satisfied condition "success or failure"
Mar  1 01:18:46.843: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a container client-container: <nil>
STEP: delete the pod
Mar  1 01:18:46.870: INFO: Waiting for pod downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a to disappear
Mar  1 01:18:46.872: INFO: Pod downwardapi-volume-13b12224-064a-4c6c-ae1d-b9274856988a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:18:46.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5329" for this suite.
Mar  1 01:18:52.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:18:52.992: INFO: namespace projected-5329 deletion completed in 6.116320215s

• [SLOW TEST:10.205 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:18:52.993: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1033
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1033
STEP: Creating statefulset with conflicting port in namespace statefulset-1033
STEP: Waiting until pod test-pod will start running in namespace statefulset-1033
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1033
Mar  1 01:18:57.074: INFO: Observed stateful pod in namespace: statefulset-1033, name: ss-0, uid: b869b776-3d91-4bc4-a959-98663ce19dff, status phase: Pending. Waiting for statefulset controller to delete.
Mar  1 01:18:57.256: INFO: Observed stateful pod in namespace: statefulset-1033, name: ss-0, uid: b869b776-3d91-4bc4-a959-98663ce19dff, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 01:18:57.259: INFO: Observed stateful pod in namespace: statefulset-1033, name: ss-0, uid: b869b776-3d91-4bc4-a959-98663ce19dff, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 01:18:57.265: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1033
STEP: Removing pod with conflicting port in namespace statefulset-1033
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1033 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 01:19:01.295: INFO: Deleting all statefulset in ns statefulset-1033
Mar  1 01:19:01.298: INFO: Scaling statefulset ss to 0
Mar  1 01:19:11.318: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:19:11.320: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:19:11.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1033" for this suite.
Mar  1 01:19:17.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:19:17.440: INFO: namespace statefulset-1033 deletion completed in 6.103928698s

• [SLOW TEST:24.447 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:19:17.441: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 01:19:17.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1109'
Mar  1 01:19:18.092: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 01:19:18.092: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar  1 01:19:18.109: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  1 01:19:18.123: INFO: scanned /root for discovery docs: <nil>
Mar  1 01:19:18.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1109'
Mar  1 01:19:33.969: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  1 01:19:33.969: INFO: stdout: "Created e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22\nScaling up e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar  1 01:19:33.969: INFO: stdout: "Created e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22\nScaling up e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar  1 01:19:33.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-1109'
Mar  1 01:19:34.081: INFO: stderr: ""
Mar  1 01:19:34.081: INFO: stdout: "e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22-mt8m2 "
Mar  1 01:19:34.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22-mt8m2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Mar  1 01:19:34.185: INFO: stderr: ""
Mar  1 01:19:34.185: INFO: stdout: "true"
Mar  1 01:19:34.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 get pods e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22-mt8m2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Mar  1 01:19:34.293: INFO: stderr: ""
Mar  1 01:19:34.293: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar  1 01:19:34.293: INFO: e2e-test-httpd-rc-d81d54f36adffacbda522c378c57dd22-mt8m2 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Mar  1 01:19:34.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 delete rc e2e-test-httpd-rc --namespace=kubectl-1109'
Mar  1 01:19:34.399: INFO: stderr: ""
Mar  1 01:19:34.399: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:19:34.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1109" for this suite.
Mar  1 01:19:40.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:19:40.539: INFO: namespace kubectl-1109 deletion completed in 6.126124273s

• [SLOW TEST:23.098 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:19:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar  1 01:19:40.582: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:19:44.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3641" for this suite.
Mar  1 01:19:50.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:19:50.843: INFO: namespace init-container-3641 deletion completed in 6.119080877s

• [SLOW TEST:10.300 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:19:50.843: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:19:50.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2720" for this suite.
Mar  1 01:19:56.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:19:57.014: INFO: namespace services-2720 deletion completed in 6.125882571s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.171 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:19:57.016: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:20:13.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6156" for this suite.
Mar  1 01:20:19.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:20:19.255: INFO: namespace resourcequota-6156 deletion completed in 6.095575825s

• [SLOW TEST:22.239 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:20:19.256: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Mar  1 01:20:19.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=kubectl-9464 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  1 01:20:21.736: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  1 01:20:21.737: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:20:23.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9464" for this suite.
Mar  1 01:20:29.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:20:29.841: INFO: namespace kubectl-9464 deletion completed in 6.092347985s

• [SLOW TEST:10.586 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:20:29.846: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7740
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7740
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7740
Mar  1 01:20:29.894: INFO: Found 0 stateful pods, waiting for 1
Mar  1 01:20:39.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  1 01:20:39.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:20:40.141: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:20:40.141: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:20:40.141: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 01:20:40.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 01:20:50.147: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 01:20:50.147: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:20:50.155: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999772s
Mar  1 01:20:51.159: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998037142s
Mar  1 01:20:52.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9947118s
Mar  1 01:20:53.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991089051s
Mar  1 01:20:54.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985757733s
Mar  1 01:20:55.174: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982207735s
Mar  1 01:20:56.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979511374s
Mar  1 01:20:57.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97587946s
Mar  1 01:20:58.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.972659923s
Mar  1 01:20:59.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.546207ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7740
Mar  1 01:21:00.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:21:00.467: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:21:00.467: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:21:00.467: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 01:21:00.470: INFO: Found 1 stateful pods, waiting for 3
Mar  1 01:21:10.474: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 01:21:10.474: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 01:21:10.474: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  1 01:21:10.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:21:10.757: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:21:10.757: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:21:10.757: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 01:21:10.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:21:11.071: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:21:11.071: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:21:11.071: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 01:21:11.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 01:21:11.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 01:21:11.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 01:21:11.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 01:21:11.360: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:21:11.363: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  1 01:21:21.370: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 01:21:21.370: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 01:21:21.370: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 01:21:21.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999658s
Mar  1 01:21:22.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997012037s
Mar  1 01:21:23.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993166063s
Mar  1 01:21:24.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989934663s
Mar  1 01:21:25.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985610326s
Mar  1 01:21:26.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981591672s
Mar  1 01:21:27.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976430739s
Mar  1 01:21:28.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972267259s
Mar  1 01:21:29.410: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968341823s
Mar  1 01:21:30.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.809556ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7740
Mar  1 01:21:31.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:21:31.667: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:21:31.667: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:21:31.667: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 01:21:31.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:21:31.947: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:21:31.947: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:21:31.947: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 01:21:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=statefulset-7740 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 01:21:32.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 01:21:32.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 01:21:32.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 01:21:32.194: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar  1 01:22:02.206: INFO: Deleting all statefulset in ns statefulset-7740
Mar  1 01:22:02.211: INFO: Scaling statefulset ss to 0
Mar  1 01:22:02.218: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 01:22:02.221: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:22:02.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7740" for this suite.
Mar  1 01:22:08.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:22:08.382: INFO: namespace statefulset-7740 deletion completed in 6.123779123s

• [SLOW TEST:98.536 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:22:08.383: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-220a211c-305c-424e-bd97-ed3d1d7d2de0
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:22:08.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7153" for this suite.
Mar  1 01:22:14.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:22:14.528: INFO: namespace secrets-7153 deletion completed in 6.099460548s

• [SLOW TEST:6.146 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:22:14.532: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar  1 01:22:14.571: INFO: Waiting up to 5m0s for pod "downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d" in namespace "downward-api-5096" to be "success or failure"
Mar  1 01:22:14.579: INFO: Pod "downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.676862ms
Mar  1 01:22:16.582: INFO: Pod "downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010641029s
Mar  1 01:22:18.585: INFO: Pod "downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013765287s
STEP: Saw pod success
Mar  1 01:22:18.585: INFO: Pod "downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d" satisfied condition "success or failure"
Mar  1 01:22:18.587: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d container dapi-container: <nil>
STEP: delete the pod
Mar  1 01:22:18.620: INFO: Waiting for pod downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d to disappear
Mar  1 01:22:18.623: INFO: Pod downward-api-3557a77b-e6ab-4ec6-8208-dfc2d725cb0d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:22:18.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5096" for this suite.
Mar  1 01:22:24.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:22:24.772: INFO: namespace downward-api-5096 deletion completed in 6.146057712s

• [SLOW TEST:10.241 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:22:24.774: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:22:24.830: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  1 01:22:28.842: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  1 01:22:30.845: INFO: Creating deployment "test-rollover-deployment"
Mar  1 01:22:30.854: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  1 01:22:32.863: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  1 01:22:32.868: INFO: Ensure that both replica sets have 1 created replica
Mar  1 01:22:32.872: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  1 01:22:32.879: INFO: Updating deployment test-rollover-deployment
Mar  1 01:22:32.879: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  1 01:22:34.893: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  1 01:22:34.900: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  1 01:22:34.905: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:34.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622552, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:36.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:36.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:38.917: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:38.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:40.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:40.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:42.911: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:42.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:44.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:44.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:46.914: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:46.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:48.912: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:48.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:50.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:50.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:52.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:52.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:54.913: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 01:22:54.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622554, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718622550, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 01:22:56.912: INFO: 
Mar  1 01:22:56.912: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar  1 01:22:56.918: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8782 /apis/apps/v1/namespaces/deployment-8782/deployments/test-rollover-deployment c28c75d7-5dd2-43b1-a106-c8fa298ee2ce 40110 2 2020-03-01 01:22:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001b72ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-01 01:22:30 +0000 UTC,LastTransitionTime:2020-03-01 01:22:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-03-01 01:22:54 +0000 UTC,LastTransitionTime:2020-03-01 01:22:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 01:22:56.922: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-8782 /apis/apps/v1/namespaces/deployment-8782/replicasets/test-rollover-deployment-7d7dc6548c f94e807e-0adc-4299-bbef-f01e3bdd39c4 40100 2 2020-03-01 01:22:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c28c75d7-5dd2-43b1-a106-c8fa298ee2ce 0xc001b737f7 0xc001b737f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001b73868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:22:56.923: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  1 01:22:56.923: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8782 /apis/apps/v1/namespaces/deployment-8782/replicasets/test-rollover-controller 64062995-4bc7-4562-8a18-da451bd95287 40109 2 2020-03-01 01:22:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c28c75d7-5dd2-43b1-a106-c8fa298ee2ce 0xc001b73627 0xc001b73628}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001b73768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:22:56.923: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8782 /apis/apps/v1/namespaces/deployment-8782/replicasets/test-rollover-deployment-f6c94f66c 3d0308b2-dacf-4ede-a9ed-30d6a0ec9927 40050 2 2020-03-01 01:22:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c28c75d7-5dd2-43b1-a106-c8fa298ee2ce 0xc001b738d0 0xc001b738d1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001b73948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 01:22:56.926: INFO: Pod "test-rollover-deployment-7d7dc6548c-8xzr9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-8xzr9 test-rollover-deployment-7d7dc6548c- deployment-8782 /api/v1/namespaces/deployment-8782/pods/test-rollover-deployment-7d7dc6548c-8xzr9 c1d2c5ad-91d3-4311-a6d6-f16815415985 40061 0 2020-03-01 01:22:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:192.168.5.45/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c f94e807e-0adc-4299-bbef-f01e3bdd39c4 0xc0048d41b7 0xc0048d41b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jjps7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jjps7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jjps7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v3-vsp1-node-group-5b74768057,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:22:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:22:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:22:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-01 01:22:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.128.24,PodIP:192.168.5.45,StartTime:2020-03-01 01:22:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-01 01:22:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://3b0051bc19a88db6a7fa0bd8a9e05d11860dccaea67322cdf765cf8f18d5764f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.5.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:22:56.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8782" for this suite.
Mar  1 01:23:02.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:23:03.020: INFO: namespace deployment-8782 deletion completed in 6.088534283s

• [SLOW TEST:38.247 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:23:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:23:03.052: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  1 01:23:07.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-1028 create -f -'
Mar  1 01:23:08.774: INFO: stderr: ""
Mar  1 01:23:08.774: INFO: stdout: "e2e-test-crd-publish-openapi-2982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 01:23:08.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-1028 delete e2e-test-crd-publish-openapi-2982-crds test-cr'
Mar  1 01:23:08.887: INFO: stderr: ""
Mar  1 01:23:08.887: INFO: stdout: "e2e-test-crd-publish-openapi-2982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  1 01:23:08.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-1028 apply -f -'
Mar  1 01:23:09.187: INFO: stderr: ""
Mar  1 01:23:09.187: INFO: stdout: "e2e-test-crd-publish-openapi-2982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 01:23:09.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 --namespace=crd-publish-openapi-1028 delete e2e-test-crd-publish-openapi-2982-crds test-cr'
Mar  1 01:23:09.290: INFO: stderr: ""
Mar  1 01:23:09.290: INFO: stdout: "e2e-test-crd-publish-openapi-2982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  1 01:23:09.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 explain e2e-test-crd-publish-openapi-2982-crds'
Mar  1 01:23:09.573: INFO: stderr: ""
Mar  1 01:23:09.573: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2982-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:23:15.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1028" for this suite.
Mar  1 01:23:21.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:23:21.210: INFO: namespace crd-publish-openapi-1028 deletion completed in 6.135618507s

• [SLOW TEST:18.190 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:23:21.213: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-3294
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3294
STEP: Deleting pre-stop pod
Mar  1 01:23:34.295: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:23:34.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3294" for this suite.
Mar  1 01:24:18.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:24:18.413: INFO: namespace prestop-3294 deletion completed in 44.096491562s

• [SLOW TEST:57.200 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:24:18.414: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-6bbc9051-284c-41b2-8505-8f3457e5d6a7
STEP: Creating a pod to test consume configMaps
Mar  1 01:24:18.460: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967" in namespace "projected-8482" to be "success or failure"
Mar  1 01:24:18.475: INFO: Pod "pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967": Phase="Pending", Reason="", readiness=false. Elapsed: 11.642728ms
Mar  1 01:24:20.479: INFO: Pod "pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015352144s
Mar  1 01:24:22.482: INFO: Pod "pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01847111s
STEP: Saw pod success
Mar  1 01:24:22.482: INFO: Pod "pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967" satisfied condition "success or failure"
Mar  1 01:24:22.485: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 01:24:22.523: INFO: Waiting for pod pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967 to disappear
Mar  1 01:24:22.526: INFO: Pod pod-projected-configmaps-25823265-6168-43e8-98c8-e18d5c269967 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:24:22.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8482" for this suite.
Mar  1 01:24:28.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:24:28.640: INFO: namespace projected-8482 deletion completed in 6.107029195s

• [SLOW TEST:10.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:24:28.640: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-4eeb3c2c-43fc-4130-8f48-b0cd28a978b3 in namespace container-probe-5496
Mar  1 01:24:32.682: INFO: Started pod busybox-4eeb3c2c-43fc-4130-8f48-b0cd28a978b3 in namespace container-probe-5496
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 01:24:32.684: INFO: Initial restart count of pod busybox-4eeb3c2c-43fc-4130-8f48-b0cd28a978b3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:28:33.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5496" for this suite.
Mar  1 01:28:39.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:28:39.251: INFO: namespace container-probe-5496 deletion completed in 6.127489516s

• [SLOW TEST:250.611 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:28:39.252: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:28:39.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4" in namespace "downward-api-8525" to be "success or failure"
Mar  1 01:28:39.294: INFO: Pod "downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115041ms
Mar  1 01:28:41.298: INFO: Pod "downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010486821s
Mar  1 01:28:43.303: INFO: Pod "downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015355615s
STEP: Saw pod success
Mar  1 01:28:43.303: INFO: Pod "downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4" satisfied condition "success or failure"
Mar  1 01:28:43.305: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4 container client-container: <nil>
STEP: delete the pod
Mar  1 01:28:43.337: INFO: Waiting for pod downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4 to disappear
Mar  1 01:28:43.340: INFO: Pod downwardapi-volume-8a76ceab-9b42-45d5-8340-62f998f36ae4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:28:43.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8525" for this suite.
Mar  1 01:28:49.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:28:49.428: INFO: namespace downward-api-8525 deletion completed in 6.084858696s

• [SLOW TEST:10.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:28:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Mar  1 01:28:53.476: INFO: Pod pod-hostip-e12b0f45-b3f3-4814-81f6-49f2dcc3ac3a has hostIP: 10.10.128.24
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:28:53.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2918" for this suite.
Mar  1 01:29:21.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:29:21.568: INFO: namespace pods-2918 deletion completed in 28.087381936s

• [SLOW TEST:32.139 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:29:21.568: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Mar  1 01:29:21.613: INFO: Waiting up to 5m0s for pod "var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e" in namespace "var-expansion-350" to be "success or failure"
Mar  1 01:29:21.618: INFO: Pod "var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.736496ms
Mar  1 01:29:23.622: INFO: Pod "var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009133414s
Mar  1 01:29:25.625: INFO: Pod "var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012062619s
STEP: Saw pod success
Mar  1 01:29:25.625: INFO: Pod "var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e" satisfied condition "success or failure"
Mar  1 01:29:25.627: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e container dapi-container: <nil>
STEP: delete the pod
Mar  1 01:29:25.646: INFO: Waiting for pod var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e to disappear
Mar  1 01:29:25.651: INFO: Pod var-expansion-16b02b4e-44a3-4eb1-9ffa-c8d72b0e028e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:29:25.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-350" for this suite.
Mar  1 01:29:31.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:29:31.756: INFO: namespace var-expansion-350 deletion completed in 6.101054858s

• [SLOW TEST:10.189 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:29:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-d08c8f0e-ecc3-470b-a0b4-e5fcfb0e28c6
STEP: Creating a pod to test consume configMaps
Mar  1 01:29:31.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd" in namespace "configmap-3333" to be "success or failure"
Mar  1 01:29:31.805: INFO: Pod "pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.71989ms
Mar  1 01:29:33.808: INFO: Pod "pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00934835s
Mar  1 01:29:35.812: INFO: Pod "pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012880925s
STEP: Saw pod success
Mar  1 01:29:35.812: INFO: Pod "pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd" satisfied condition "success or failure"
Mar  1 01:29:35.815: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 01:29:35.835: INFO: Waiting for pod pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd to disappear
Mar  1 01:29:35.837: INFO: Pod pod-configmaps-8c554cf1-8e8e-495a-b9c9-dcab22a419bd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:29:35.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3333" for this suite.
Mar  1 01:29:41.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:29:41.943: INFO: namespace configmap-3333 deletion completed in 6.102545277s

• [SLOW TEST:10.186 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:29:41.944: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-8nqn
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 01:29:41.985: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8nqn" in namespace "subpath-6976" to be "success or failure"
Mar  1 01:29:41.993: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279234ms
Mar  1 01:29:43.996: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010614296s
Mar  1 01:29:45.999: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 4.013684668s
Mar  1 01:29:48.002: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 6.016407418s
Mar  1 01:29:50.005: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 8.019488985s
Mar  1 01:29:52.007: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 10.021867476s
Mar  1 01:29:54.011: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 12.025055338s
Mar  1 01:29:56.014: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 14.028379794s
Mar  1 01:29:58.017: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 16.031248946s
Mar  1 01:30:00.020: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 18.034356802s
Mar  1 01:30:02.024: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Running", Reason="", readiness=true. Elapsed: 20.038668175s
Mar  1 01:30:04.027: INFO: Pod "pod-subpath-test-configmap-8nqn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041706277s
STEP: Saw pod success
Mar  1 01:30:04.027: INFO: Pod "pod-subpath-test-configmap-8nqn" satisfied condition "success or failure"
Mar  1 01:30:04.030: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-subpath-test-configmap-8nqn container test-container-subpath-configmap-8nqn: <nil>
STEP: delete the pod
Mar  1 01:30:04.047: INFO: Waiting for pod pod-subpath-test-configmap-8nqn to disappear
Mar  1 01:30:04.051: INFO: Pod pod-subpath-test-configmap-8nqn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8nqn
Mar  1 01:30:04.051: INFO: Deleting pod "pod-subpath-test-configmap-8nqn" in namespace "subpath-6976"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:30:04.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6976" for this suite.
Mar  1 01:30:10.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:30:10.145: INFO: namespace subpath-6976 deletion completed in 6.089815544s

• [SLOW TEST:28.202 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:30:10.146: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-db9785e5-5390-4b0b-a9f0-c065da34656b
STEP: Creating a pod to test consume secrets
Mar  1 01:30:10.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042" in namespace "projected-2058" to be "success or failure"
Mar  1 01:30:10.200: INFO: Pod "pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645397ms
Mar  1 01:30:12.203: INFO: Pod "pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011929382s
Mar  1 01:30:14.206: INFO: Pod "pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015213956s
STEP: Saw pod success
Mar  1 01:30:14.207: INFO: Pod "pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042" satisfied condition "success or failure"
Mar  1 01:30:14.209: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 01:30:14.227: INFO: Waiting for pod pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042 to disappear
Mar  1 01:30:14.230: INFO: Pod pod-projected-secrets-79258991-6eb4-48e8-ac88-dc3e3bf08042 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:30:14.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2058" for this suite.
Mar  1 01:30:20.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:30:20.317: INFO: namespace projected-2058 deletion completed in 6.082383273s

• [SLOW TEST:10.172 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:30:20.321: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6260
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6260
STEP: creating replication controller externalsvc in namespace services-6260
I0301 01:30:20.395708      21 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6260, replica count: 2
I0301 01:30:23.446226      21 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  1 01:30:23.471: INFO: Creating new exec pod
Mar  1 01:30:27.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-6260 execpodbg4m5 -- /bin/sh -x -c nslookup nodeport-service'
Mar  1 01:30:27.798: INFO: stderr: "+ nslookup nodeport-service\n"
Mar  1 01:30:27.798: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6260.svc.cluster.local\tcanonical name = externalsvc.services-6260.svc.cluster.local.\nName:\texternalsvc.services-6260.svc.cluster.local\nAddress: 10.99.245.80\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6260, will wait for the garbage collector to delete the pods
Mar  1 01:30:27.856: INFO: Deleting ReplicationController externalsvc took: 4.775696ms
Mar  1 01:30:28.356: INFO: Terminating ReplicationController externalsvc pods took: 500.382155ms
Mar  1 01:30:39.682: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:30:39.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6260" for this suite.
Mar  1 01:30:45.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:30:45.828: INFO: namespace services-6260 deletion completed in 6.1190454s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.508 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:30:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar  1 01:30:45.876: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5" in namespace "downward-api-7302" to be "success or failure"
Mar  1 01:30:45.880: INFO: Pod "downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803246ms
Mar  1 01:30:47.884: INFO: Pod "downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007066417s
Mar  1 01:30:49.887: INFO: Pod "downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010717175s
STEP: Saw pod success
Mar  1 01:30:49.887: INFO: Pod "downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5" satisfied condition "success or failure"
Mar  1 01:30:49.890: INFO: Trying to get logs from node alex-slot1-v3-vsp1-node-group-5b74768057 pod downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5 container client-container: <nil>
STEP: delete the pod
Mar  1 01:30:49.919: INFO: Waiting for pod downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5 to disappear
Mar  1 01:30:49.922: INFO: Pod downwardapi-volume-fc0391f3-33ce-4283-9f08-3220c51518a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:30:49.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7302" for this suite.
Mar  1 01:30:55.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:30:56.025: INFO: namespace downward-api-7302 deletion completed in 6.099918421s

• [SLOW TEST:10.195 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:30:56.028: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-3242
STEP: creating replication controller nodeport-test in namespace services-3242
I0301 01:30:56.101164      21 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-3242, replica count: 2
I0301 01:30:59.151845      21 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 01:30:59.152: INFO: Creating new exec pod
Mar  1 01:31:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  1 01:31:04.422: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  1 01:31:04.422: INFO: stdout: ""
Mar  1 01:31:04.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 10.109.144.143 80'
Mar  1 01:31:04.674: INFO: stderr: "+ nc -zv -t -w 2 10.109.144.143 80\nConnection to 10.109.144.143 80 port [tcp/http] succeeded!\n"
Mar  1 01:31:04.674: INFO: stdout: ""
Mar  1 01:31:04.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.22 32020'
Mar  1 01:31:04.990: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.22 32020\nConnection to 10.10.128.22 32020 port [tcp/32020] succeeded!\n"
Mar  1 01:31:04.990: INFO: stdout: ""
Mar  1 01:31:04.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.24 32020'
Mar  1 01:31:05.286: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.24 32020\nConnection to 10.10.128.24 32020 port [tcp/32020] succeeded!\n"
Mar  1 01:31:05.286: INFO: stdout: ""
Mar  1 01:31:05.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.22 32020'
Mar  1 01:31:05.597: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.22 32020\nConnection to 10.10.128.22 32020 port [tcp/32020] succeeded!\n"
Mar  1 01:31:05.597: INFO: stdout: ""
Mar  1 01:31:05.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-585087022 exec --namespace=services-3242 execpodfc7nj -- /bin/sh -x -c nc -zv -t -w 2 10.10.128.24 32020'
Mar  1 01:31:05.880: INFO: stderr: "+ nc -zv -t -w 2 10.10.128.24 32020\nConnection to 10.10.128.24 32020 port [tcp/32020] succeeded!\n"
Mar  1 01:31:05.881: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:31:05.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3242" for this suite.
Mar  1 01:31:11.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:31:11.999: INFO: namespace services-3242 deletion completed in 6.105284578s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.971 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:31:12.000: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar  1 01:31:12.055: INFO: (0) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 16.958595ms)
Mar  1 01:31:12.062: INFO: (1) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 6.236383ms)
Mar  1 01:31:12.067: INFO: (2) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.672399ms)
Mar  1 01:31:12.070: INFO: (3) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.139419ms)
Mar  1 01:31:12.073: INFO: (4) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.960983ms)
Mar  1 01:31:12.076: INFO: (5) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.329119ms)
Mar  1 01:31:12.080: INFO: (6) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.504466ms)
Mar  1 01:31:12.083: INFO: (7) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.994891ms)
Mar  1 01:31:12.087: INFO: (8) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.692559ms)
Mar  1 01:31:12.090: INFO: (9) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.32235ms)
Mar  1 01:31:12.093: INFO: (10) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.467402ms)
Mar  1 01:31:12.096: INFO: (11) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.852844ms)
Mar  1 01:31:12.099: INFO: (12) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.884194ms)
Mar  1 01:31:12.101: INFO: (13) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.614664ms)
Mar  1 01:31:12.105: INFO: (14) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.382795ms)
Mar  1 01:31:12.107: INFO: (15) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.415044ms)
Mar  1 01:31:12.110: INFO: (16) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.116676ms)
Mar  1 01:31:12.113: INFO: (17) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.98895ms)
Mar  1 01:31:12.117: INFO: (18) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.220021ms)
Mar  1 01:31:12.120: INFO: (19) /api/v1/nodes/alex-slot1-v3-vsp1-node-group-14d9760cdc/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.213436ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:31:12.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7135" for this suite.
Mar  1 01:31:18.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:31:18.230: INFO: namespace proxy-7135 deletion completed in 6.105622102s

• [SLOW TEST:6.231 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:31:18.234: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-0ed4926c-2764-4623-8567-be7da138436c in namespace container-probe-7756
Mar  1 01:31:20.274: INFO: Started pod liveness-0ed4926c-2764-4623-8567-be7da138436c in namespace container-probe-7756
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 01:31:20.281: INFO: Initial restart count of pod liveness-0ed4926c-2764-4623-8567-be7da138436c is 0
Mar  1 01:31:38.315: INFO: Restart count of pod container-probe-7756/liveness-0ed4926c-2764-4623-8567-be7da138436c is now 1 (18.033323698s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:31:38.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7756" for this suite.
Mar  1 01:31:44.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:31:44.448: INFO: namespace container-probe-7756 deletion completed in 6.086640844s

• [SLOW TEST:26.214 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:31:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vl452 in namespace proxy-3376
I0301 01:31:44.507398      21 runners.go:184] Created replication controller with name: proxy-service-vl452, namespace: proxy-3376, replica count: 1
I0301 01:31:45.557685      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 01:31:46.557965      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 01:31:47.558241      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:48.558518      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:49.558753      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:50.559151      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:51.559385      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:52.559590      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:53.559803      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 01:31:54.560333      21 runners.go:184] proxy-service-vl452 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 01:31:54.563: INFO: setup took 10.079164465s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  1 01:31:54.578: INFO: (0) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 14.63631ms)
Mar  1 01:31:54.578: INFO: (0) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 14.894523ms)
Mar  1 01:31:54.578: INFO: (0) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 15.081003ms)
Mar  1 01:31:54.578: INFO: (0) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 15.029151ms)
Mar  1 01:31:54.578: INFO: (0) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 15.011056ms)
Mar  1 01:31:54.584: INFO: (0) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 20.867988ms)
Mar  1 01:31:54.584: INFO: (0) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 21.069521ms)
Mar  1 01:31:54.584: INFO: (0) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 21.112769ms)
Mar  1 01:31:54.585: INFO: (0) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 21.154022ms)
Mar  1 01:31:54.585: INFO: (0) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 21.528709ms)
Mar  1 01:31:54.585: INFO: (0) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 21.888022ms)
Mar  1 01:31:54.585: INFO: (0) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 22.057454ms)
Mar  1 01:31:54.586: INFO: (0) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 22.798397ms)
Mar  1 01:31:54.586: INFO: (0) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 23.112036ms)
Mar  1 01:31:54.587: INFO: (0) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 24.075085ms)
Mar  1 01:31:54.591: INFO: (0) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 28.047035ms)
Mar  1 01:31:54.600: INFO: (1) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 7.157046ms)
Mar  1 01:31:54.601: INFO: (1) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 7.446675ms)
Mar  1 01:31:54.601: INFO: (1) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 10.034926ms)
Mar  1 01:31:54.602: INFO: (1) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 8.756884ms)
Mar  1 01:31:54.602: INFO: (1) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 9.806544ms)
Mar  1 01:31:54.603: INFO: (1) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 10.982626ms)
Mar  1 01:31:54.603: INFO: (1) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 10.292978ms)
Mar  1 01:31:54.603: INFO: (1) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 11.518057ms)
Mar  1 01:31:54.603: INFO: (1) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 10.563886ms)
Mar  1 01:31:54.603: INFO: (1) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 10.372486ms)
Mar  1 01:31:54.609: INFO: (1) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 15.308377ms)
Mar  1 01:31:54.609: INFO: (1) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 16.792887ms)
Mar  1 01:31:54.609: INFO: (1) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 17.881536ms)
Mar  1 01:31:54.611: INFO: (1) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 18.422732ms)
Mar  1 01:31:54.612: INFO: (1) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 19.468305ms)
Mar  1 01:31:54.612: INFO: (1) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 19.337729ms)
Mar  1 01:31:54.619: INFO: (2) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 7.11553ms)
Mar  1 01:31:54.620: INFO: (2) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 7.072053ms)
Mar  1 01:31:54.620: INFO: (2) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 7.603287ms)
Mar  1 01:31:54.620: INFO: (2) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 7.787754ms)
Mar  1 01:31:54.620: INFO: (2) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 7.260531ms)
Mar  1 01:31:54.622: INFO: (2) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 9.660427ms)
Mar  1 01:31:54.623: INFO: (2) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 10.222209ms)
Mar  1 01:31:54.623: INFO: (2) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 10.653328ms)
Mar  1 01:31:54.624: INFO: (2) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 11.055424ms)
Mar  1 01:31:54.624: INFO: (2) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 11.559053ms)
Mar  1 01:31:54.625: INFO: (2) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 11.752059ms)
Mar  1 01:31:54.625: INFO: (2) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 11.7261ms)
Mar  1 01:31:54.626: INFO: (2) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 13.746159ms)
Mar  1 01:31:54.626: INFO: (2) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 13.024329ms)
Mar  1 01:31:54.627: INFO: (2) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 13.355195ms)
Mar  1 01:31:54.627: INFO: (2) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 14.290105ms)
Mar  1 01:31:54.633: INFO: (3) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 5.246043ms)
Mar  1 01:31:54.634: INFO: (3) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 6.048699ms)
Mar  1 01:31:54.634: INFO: (3) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 5.96641ms)
Mar  1 01:31:54.634: INFO: (3) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 5.843723ms)
Mar  1 01:31:54.634: INFO: (3) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 6.367206ms)
Mar  1 01:31:54.635: INFO: (3) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 7.505118ms)
Mar  1 01:31:54.638: INFO: (3) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 8.738352ms)
Mar  1 01:31:54.638: INFO: (3) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 9.695981ms)
Mar  1 01:31:54.639: INFO: (3) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 9.563061ms)
Mar  1 01:31:54.639: INFO: (3) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 10.318574ms)
Mar  1 01:31:54.640: INFO: (3) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 11.709669ms)
Mar  1 01:31:54.640: INFO: (3) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 10.532514ms)
Mar  1 01:31:54.640: INFO: (3) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 12.169401ms)
Mar  1 01:31:54.640: INFO: (3) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 11.327688ms)
Mar  1 01:31:54.640: INFO: (3) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 12.145455ms)
Mar  1 01:31:54.641: INFO: (3) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 11.648323ms)
Mar  1 01:31:54.652: INFO: (4) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 10.526669ms)
Mar  1 01:31:54.654: INFO: (4) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 11.990814ms)
Mar  1 01:31:54.654: INFO: (4) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 10.854981ms)
Mar  1 01:31:54.655: INFO: (4) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 12.462174ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 16.294664ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 15.67638ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 17.22356ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 17.25175ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 16.192197ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 17.203019ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 16.765844ms)
Mar  1 01:31:54.659: INFO: (4) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 16.784305ms)
Mar  1 01:31:54.661: INFO: (4) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 19.022778ms)
Mar  1 01:31:54.661: INFO: (4) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 18.34551ms)
Mar  1 01:31:54.662: INFO: (4) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 19.886363ms)
Mar  1 01:31:54.662: INFO: (4) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 19.65645ms)
Mar  1 01:31:54.669: INFO: (5) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 7.001912ms)
Mar  1 01:31:54.669: INFO: (5) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 7.012994ms)
Mar  1 01:31:54.669: INFO: (5) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 6.96917ms)
Mar  1 01:31:54.669: INFO: (5) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 6.910713ms)
Mar  1 01:31:54.671: INFO: (5) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 8.720186ms)
Mar  1 01:31:54.671: INFO: (5) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 8.946231ms)
Mar  1 01:31:54.672: INFO: (5) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 9.556241ms)
Mar  1 01:31:54.672: INFO: (5) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 9.801263ms)
Mar  1 01:31:54.673: INFO: (5) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 10.480553ms)
Mar  1 01:31:54.673: INFO: (5) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 10.858157ms)
Mar  1 01:31:54.674: INFO: (5) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 11.07597ms)
Mar  1 01:31:54.674: INFO: (5) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 11.462773ms)
Mar  1 01:31:54.674: INFO: (5) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 12.237251ms)
Mar  1 01:31:54.675: INFO: (5) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 12.403358ms)
Mar  1 01:31:54.675: INFO: (5) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 12.744696ms)
Mar  1 01:31:54.675: INFO: (5) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 12.504008ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 12.846218ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 13.056635ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 13.040224ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 13.792633ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 14.455448ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 14.669091ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 14.733636ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 13.626961ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 14.41005ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 13.316002ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 14.329586ms)
Mar  1 01:31:54.690: INFO: (6) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 13.228883ms)
Mar  1 01:31:54.691: INFO: (6) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 14.626554ms)
Mar  1 01:31:54.691: INFO: (6) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 15.533685ms)
Mar  1 01:31:54.691: INFO: (6) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 15.225799ms)
Mar  1 01:31:54.692: INFO: (6) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 14.834241ms)
Mar  1 01:31:54.700: INFO: (7) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 7.857849ms)
Mar  1 01:31:54.701: INFO: (7) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 9.073227ms)
Mar  1 01:31:54.701: INFO: (7) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 8.931604ms)
Mar  1 01:31:54.701: INFO: (7) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 9.261539ms)
Mar  1 01:31:54.701: INFO: (7) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 9.56878ms)
Mar  1 01:31:54.701: INFO: (7) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 9.288046ms)
Mar  1 01:31:54.706: INFO: (7) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 13.69235ms)
Mar  1 01:31:54.706: INFO: (7) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 14.13899ms)
Mar  1 01:31:54.707: INFO: (7) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 15.413964ms)
Mar  1 01:31:54.707: INFO: (7) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 14.930817ms)
Mar  1 01:31:54.708: INFO: (7) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 15.445634ms)
Mar  1 01:31:54.708: INFO: (7) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 15.524781ms)
Mar  1 01:31:54.708: INFO: (7) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 15.468287ms)
Mar  1 01:31:54.708: INFO: (7) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 16.274081ms)
Mar  1 01:31:54.709: INFO: (7) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 16.45214ms)
Mar  1 01:31:54.710: INFO: (7) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 17.296001ms)
Mar  1 01:31:54.716: INFO: (8) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 5.288704ms)
Mar  1 01:31:54.717: INFO: (8) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 6.928555ms)
Mar  1 01:31:54.717: INFO: (8) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 6.906817ms)
Mar  1 01:31:54.717: INFO: (8) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 6.305673ms)
Mar  1 01:31:54.718: INFO: (8) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 7.468249ms)
Mar  1 01:31:54.718: INFO: (8) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 7.805636ms)
Mar  1 01:31:54.719: INFO: (8) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 7.817095ms)
Mar  1 01:31:54.720: INFO: (8) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 9.077327ms)
Mar  1 01:31:54.720: INFO: (8) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 9.422151ms)
Mar  1 01:31:54.721: INFO: (8) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 10.698381ms)
Mar  1 01:31:54.721: INFO: (8) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 10.337826ms)
Mar  1 01:31:54.721: INFO: (8) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 10.985949ms)
Mar  1 01:31:54.722: INFO: (8) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 11.151556ms)
Mar  1 01:31:54.722: INFO: (8) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 11.633714ms)
Mar  1 01:31:54.722: INFO: (8) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 12.268372ms)
Mar  1 01:31:54.722: INFO: (8) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 10.88211ms)
Mar  1 01:31:54.728: INFO: (9) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 5.15906ms)
Mar  1 01:31:54.728: INFO: (9) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 5.088738ms)
Mar  1 01:31:54.729: INFO: (9) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 6.262578ms)
Mar  1 01:31:54.729: INFO: (9) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 6.089258ms)
Mar  1 01:31:54.731: INFO: (9) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 7.326559ms)
Mar  1 01:31:54.731: INFO: (9) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 7.180267ms)
Mar  1 01:31:54.732: INFO: (9) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 7.762602ms)
Mar  1 01:31:54.732: INFO: (9) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 8.719119ms)
Mar  1 01:31:54.733: INFO: (9) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 8.958305ms)
Mar  1 01:31:54.733: INFO: (9) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 10.372809ms)
Mar  1 01:31:54.733: INFO: (9) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 9.397291ms)
Mar  1 01:31:54.734: INFO: (9) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 10.322892ms)
Mar  1 01:31:54.734: INFO: (9) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 9.458242ms)
Mar  1 01:31:54.735: INFO: (9) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 12.744426ms)
Mar  1 01:31:54.735: INFO: (9) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 11.495112ms)
Mar  1 01:31:54.735: INFO: (9) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 11.66897ms)
Mar  1 01:31:54.740: INFO: (10) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 4.000367ms)
Mar  1 01:31:54.741: INFO: (10) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 4.751389ms)
Mar  1 01:31:54.741: INFO: (10) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 5.165729ms)
Mar  1 01:31:54.741: INFO: (10) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 5.549619ms)
Mar  1 01:31:54.742: INFO: (10) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 4.935735ms)
Mar  1 01:31:54.742: INFO: (10) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 5.727566ms)
Mar  1 01:31:54.742: INFO: (10) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 5.686173ms)
Mar  1 01:31:54.745: INFO: (10) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 8.315915ms)
Mar  1 01:31:54.746: INFO: (10) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 9.269996ms)
Mar  1 01:31:54.746: INFO: (10) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 9.104995ms)
Mar  1 01:31:54.746: INFO: (10) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 10.354794ms)
Mar  1 01:31:54.747: INFO: (10) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 10.043863ms)
Mar  1 01:31:54.747: INFO: (10) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 10.657004ms)
Mar  1 01:31:54.748: INFO: (10) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 11.433203ms)
Mar  1 01:31:54.748: INFO: (10) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 11.377194ms)
Mar  1 01:31:54.749: INFO: (10) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 12.354481ms)
Mar  1 01:31:54.754: INFO: (11) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 4.638242ms)
Mar  1 01:31:54.755: INFO: (11) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 5.114328ms)
Mar  1 01:31:54.755: INFO: (11) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 5.316579ms)
Mar  1 01:31:54.755: INFO: (11) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 5.844674ms)
Mar  1 01:31:54.757: INFO: (11) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 6.964808ms)
Mar  1 01:31:54.757: INFO: (11) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 7.408418ms)
Mar  1 01:31:54.757: INFO: (11) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 7.598659ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 9.287939ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 9.806195ms)
Mar  1 01:31:54.760: INFO: (11) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 9.719093ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 9.567517ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 9.717508ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 9.504247ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 10.064287ms)
Mar  1 01:31:54.759: INFO: (11) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 10.124915ms)
Mar  1 01:31:54.760: INFO: (11) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 10.083877ms)
Mar  1 01:31:54.768: INFO: (12) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 7.56555ms)
Mar  1 01:31:54.769: INFO: (12) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 8.440357ms)
Mar  1 01:31:54.769: INFO: (12) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 8.904848ms)
Mar  1 01:31:54.770: INFO: (12) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 9.183131ms)
Mar  1 01:31:54.770: INFO: (12) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 9.910339ms)
Mar  1 01:31:54.770: INFO: (12) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 10.173656ms)
Mar  1 01:31:54.771: INFO: (12) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 10.154212ms)
Mar  1 01:31:54.771: INFO: (12) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 10.351987ms)
Mar  1 01:31:54.771: INFO: (12) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 10.693692ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 12.995257ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 13.098565ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 13.086504ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 13.221868ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 13.511448ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 13.333126ms)
Mar  1 01:31:54.774: INFO: (12) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 13.553715ms)
Mar  1 01:31:54.779: INFO: (13) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 4.218509ms)
Mar  1 01:31:54.780: INFO: (13) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 4.608801ms)
Mar  1 01:31:54.781: INFO: (13) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 5.502468ms)
Mar  1 01:31:54.782: INFO: (13) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 5.882298ms)
Mar  1 01:31:54.783: INFO: (13) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 7.825169ms)
Mar  1 01:31:54.784: INFO: (13) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 8.965681ms)
Mar  1 01:31:54.784: INFO: (13) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 9.443866ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 9.154928ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 9.331463ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 9.267689ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 9.354955ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 9.816076ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 9.822465ms)
Mar  1 01:31:54.785: INFO: (13) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 9.776322ms)
Mar  1 01:31:54.786: INFO: (13) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 10.44962ms)
Mar  1 01:31:54.786: INFO: (13) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 10.216901ms)
Mar  1 01:31:54.794: INFO: (14) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 7.586687ms)
Mar  1 01:31:54.795: INFO: (14) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 8.120553ms)
Mar  1 01:31:54.796: INFO: (14) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 8.734455ms)
Mar  1 01:31:54.796: INFO: (14) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 8.733422ms)
Mar  1 01:31:54.797: INFO: (14) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 10.656625ms)
Mar  1 01:31:54.800: INFO: (14) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 12.897206ms)
Mar  1 01:31:54.800: INFO: (14) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 13.114895ms)
Mar  1 01:31:54.801: INFO: (14) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 12.64763ms)
Mar  1 01:31:54.801: INFO: (14) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 13.107538ms)
Mar  1 01:31:54.802: INFO: (14) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 13.405543ms)
Mar  1 01:31:54.802: INFO: (14) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 14.350182ms)
Mar  1 01:31:54.802: INFO: (14) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 14.197018ms)
Mar  1 01:31:54.802: INFO: (14) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 15.518663ms)
Mar  1 01:31:54.802: INFO: (14) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 15.847469ms)
Mar  1 01:31:54.803: INFO: (14) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 15.010234ms)
Mar  1 01:31:54.803: INFO: (14) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 16.368214ms)
Mar  1 01:31:54.809: INFO: (15) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 5.861443ms)
Mar  1 01:31:54.809: INFO: (15) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 6.259268ms)
Mar  1 01:31:54.810: INFO: (15) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 6.445046ms)
Mar  1 01:31:54.810: INFO: (15) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 6.682949ms)
Mar  1 01:31:54.810: INFO: (15) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 6.260394ms)
Mar  1 01:31:54.810: INFO: (15) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 7.089437ms)
Mar  1 01:31:54.811: INFO: (15) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 6.785125ms)
Mar  1 01:31:54.811: INFO: (15) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 7.110555ms)
Mar  1 01:31:54.811: INFO: (15) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 7.366901ms)
Mar  1 01:31:54.811: INFO: (15) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 7.704503ms)
Mar  1 01:31:54.812: INFO: (15) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 8.245382ms)
Mar  1 01:31:54.812: INFO: (15) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 8.531421ms)
Mar  1 01:31:54.813: INFO: (15) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 8.93085ms)
Mar  1 01:31:54.813: INFO: (15) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 8.992463ms)
Mar  1 01:31:54.813: INFO: (15) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 9.42369ms)
Mar  1 01:31:54.813: INFO: (15) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 9.532321ms)
Mar  1 01:31:54.818: INFO: (16) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 5.191683ms)
Mar  1 01:31:54.819: INFO: (16) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 5.721462ms)
Mar  1 01:31:54.819: INFO: (16) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 5.763272ms)
Mar  1 01:31:54.821: INFO: (16) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 7.384606ms)
Mar  1 01:31:54.822: INFO: (16) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 8.666135ms)
Mar  1 01:31:54.822: INFO: (16) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 8.724667ms)
Mar  1 01:31:54.823: INFO: (16) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 9.125645ms)
Mar  1 01:31:54.826: INFO: (16) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 12.647961ms)
Mar  1 01:31:54.826: INFO: (16) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 12.692077ms)
Mar  1 01:31:54.826: INFO: (16) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 12.799334ms)
Mar  1 01:31:54.826: INFO: (16) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 12.928499ms)
Mar  1 01:31:54.827: INFO: (16) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 13.194571ms)
Mar  1 01:31:54.827: INFO: (16) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 13.254447ms)
Mar  1 01:31:54.827: INFO: (16) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 13.252822ms)
Mar  1 01:31:54.827: INFO: (16) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 13.388958ms)
Mar  1 01:31:54.827: INFO: (16) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 13.798959ms)
Mar  1 01:31:54.833: INFO: (17) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 5.012651ms)
Mar  1 01:31:54.837: INFO: (17) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 8.074126ms)
Mar  1 01:31:54.838: INFO: (17) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 9.792326ms)
Mar  1 01:31:54.839: INFO: (17) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 10.388541ms)
Mar  1 01:31:54.840: INFO: (17) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 11.758377ms)
Mar  1 01:31:54.841: INFO: (17) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 12.807314ms)
Mar  1 01:31:54.842: INFO: (17) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 12.922055ms)
Mar  1 01:31:54.842: INFO: (17) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 13.10037ms)
Mar  1 01:31:54.842: INFO: (17) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 13.978565ms)
Mar  1 01:31:54.842: INFO: (17) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 14.597456ms)
Mar  1 01:31:54.843: INFO: (17) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 15.584224ms)
Mar  1 01:31:54.845: INFO: (17) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 16.341823ms)
Mar  1 01:31:54.845: INFO: (17) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 17.101446ms)
Mar  1 01:31:54.845: INFO: (17) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 17.042644ms)
Mar  1 01:31:54.845: INFO: (17) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 16.818268ms)
Mar  1 01:31:54.846: INFO: (17) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 17.882748ms)
Mar  1 01:31:54.854: INFO: (18) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 7.442977ms)
Mar  1 01:31:54.860: INFO: (18) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 13.667993ms)
Mar  1 01:31:54.861: INFO: (18) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 14.995755ms)
Mar  1 01:31:54.861: INFO: (18) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 14.426397ms)
Mar  1 01:31:54.862: INFO: (18) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 15.499381ms)
Mar  1 01:31:54.862: INFO: (18) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 15.427723ms)
Mar  1 01:31:54.862: INFO: (18) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 15.363009ms)
Mar  1 01:31:54.862: INFO: (18) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 15.748224ms)
Mar  1 01:31:54.863: INFO: (18) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 15.716485ms)
Mar  1 01:31:54.864: INFO: (18) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 17.062493ms)
Mar  1 01:31:54.866: INFO: (18) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 19.387052ms)
Mar  1 01:31:54.867: INFO: (18) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 20.323226ms)
Mar  1 01:31:54.868: INFO: (18) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 20.74388ms)
Mar  1 01:31:54.868: INFO: (18) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 20.945279ms)
Mar  1 01:31:54.868: INFO: (18) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 21.459194ms)
Mar  1 01:31:54.870: INFO: (18) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 23.309829ms)
Mar  1 01:31:54.881: INFO: (19) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:162/proxy/: bar (200; 9.363785ms)
Mar  1 01:31:54.881: INFO: (19) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:443/proxy/tlsrewritem... (200; 9.665796ms)
Mar  1 01:31:54.881: INFO: (19) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:160/proxy/: foo (200; 10.119832ms)
Mar  1 01:31:54.881: INFO: (19) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:462/proxy/: tls qux (200; 9.593075ms)
Mar  1 01:31:54.882: INFO: (19) /api/v1/namespaces/proxy-3376/pods/https:proxy-service-vl452-c552k:460/proxy/: tls baz (200; 10.575623ms)
Mar  1 01:31:54.883: INFO: (19) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname2/proxy/: bar (200; 12.389513ms)
Mar  1 01:31:54.884: INFO: (19) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname2/proxy/: bar (200; 13.451163ms)
Mar  1 01:31:54.885: INFO: (19) /api/v1/namespaces/proxy-3376/services/http:proxy-service-vl452:portname1/proxy/: foo (200; 14.022952ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname1/proxy/: tls baz (200; 14.695335ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/services/proxy-service-vl452:portname1/proxy/: foo (200; 14.062676ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/services/https:proxy-service-vl452:tlsportname2/proxy/: tls qux (200; 15.437676ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:162/proxy/: bar (200; 15.582667ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/http:proxy-service-vl452-c552k:1080/proxy/rewriteme">... (200; 15.243787ms)
Mar  1 01:31:54.886: INFO: (19) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:160/proxy/: foo (200; 15.795974ms)
Mar  1 01:31:54.887: INFO: (19) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k/proxy/rewriteme">test</a> (200; 16.408862ms)
Mar  1 01:31:54.887: INFO: (19) /api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/: <a href="/api/v1/namespaces/proxy-3376/pods/proxy-service-vl452-c552k:1080/proxy/rewriteme">test<... (200; 16.96243ms)
STEP: deleting ReplicationController proxy-service-vl452 in namespace proxy-3376, will wait for the garbage collector to delete the pods
Mar  1 01:31:54.948: INFO: Deleting ReplicationController proxy-service-vl452 took: 5.970894ms
Mar  1 01:31:55.448: INFO: Terminating ReplicationController proxy-service-vl452 pods took: 500.484183ms
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:32:08.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3376" for this suite.
Mar  1 01:32:15.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:32:15.114: INFO: namespace proxy-3376 deletion completed in 6.116720882s

• [SLOW TEST:30.665 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:32:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-f0dfb1b0-8099-4fe4-b7b0-67bd638e1bbe
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f0dfb1b0-8099-4fe4-b7b0-67bd638e1bbe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:33:39.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9917" for this suite.
Mar  1 01:33:51.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:33:51.715: INFO: namespace configmap-9917 deletion completed in 12.092466251s

• [SLOW TEST:96.601 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:33:51.717: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar  1 01:33:51.748: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:34:18.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3223" for this suite.
Mar  1 01:34:24.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:34:24.685: INFO: namespace crd-publish-openapi-3223 deletion completed in 6.11977946s

• [SLOW TEST:32.968 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar  1 01:34:24.687: INFO: >>> kubeConfig: /tmp/kubeconfig-585087022
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 01:34:28.803: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar  1 01:34:28.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-263" for this suite.
Mar  1 01:34:34.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 01:34:34.972: INFO: namespace container-runtime-263 deletion completed in 6.151858109s

• [SLOW TEST:10.286 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMar  1 01:34:34.973: INFO: Running AfterSuite actions on all nodes
Mar  1 01:34:34.973: INFO: Running AfterSuite actions on node 1
Mar  1 01:34:34.973: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 7119.968 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 1h58m42.386713354s
Test Suite Passed
