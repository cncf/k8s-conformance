**************************************************************************
bootstrap.py is deprecated!
test-infra oncall does not support any job still using bootstrap.py.
Please migrate your job to podutils!
https://github.com/kubernetes/test-infra/blob/master/prow/pod-utilities.md
**************************************************************************
Args: --job=ci-kubernetes-gce-conformance-latest-1-20 --service-account=/etc/service-account/service-account.json --upload=gs://kubernetes-jenkins/logs --timeout=220 --bare --scenario=kubernetes_e2e -- --extract=ci/latest-1.20 --extract-ci-bucket=k8s-release-dev --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
Bootstrap ci-kubernetes-gce-conformance-latest-1-20...
Builder: d63549e4-76c0-11eb-aa32-3a9397e74bc0
Image: gcr.io/k8s-testimages/kubekins-e2e:v20210223-952586a143-1.20
Gubernator results at https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-20/1364619498342060032
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [prow-build@k8s-infra-prow-build.iam.gserviceaccount.com]
process 34 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 47 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using prow-build@k8s-infra-prow-build.iam.gserviceaccount.com
Root: /workspace
cd to /workspace
Configure environment...
Call:  git show -s --format=format:%ct HEAD
fatal: not a git repository (or any of the parent directories): .git
process 60 exited with code 128 after 0.0m
Unable to print commit date for HEAD
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [prow-build@k8s-infra-prow-build.iam.gserviceaccount.com]
process 61 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 74 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using prow-build@k8s-infra-prow-build.iam.gserviceaccount.com
Start 1364619498342060032 at ...
Call:  gsutil -q -h Content-Type:application/json cp /tmp/gsutil__pJKCy gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-20/1364619498342060032/started.json
process 87 exited with code 0 after 0.0m
Call:  /workspace/./test-infra/jenkins/../scenarios/kubernetes_e2e.py --extract=ci/latest-1.20 --extract-ci-bucket=k8s-release-dev --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
starts with local mode
Environment:
ARTIFACTS=/workspace/_artifacts
AWS_SSH_PRIVATE_KEY_FILE=/root/.ssh/kube_aws_rsa
AWS_SSH_PUBLIC_KEY_FILE=/root/.ssh/kube_aws_rsa.pub
BAZEL_REMOTE_CACHE_ENABLED=false
BAZEL_VERSION=3.4.1
BOOTSTRAP_MIGRATION=yes
BOSKOS_METRICS_PORT=tcp://10.35.242.179:9090
BOSKOS_METRICS_PORT_9090_TCP=tcp://10.35.242.179:9090
BOSKOS_METRICS_PORT_9090_TCP_ADDR=10.35.242.179
BOSKOS_METRICS_PORT_9090_TCP_PORT=9090
BOSKOS_METRICS_PORT_9090_TCP_PROTO=tcp
BOSKOS_METRICS_SERVICE_HOST=10.35.242.179
BOSKOS_METRICS_SERVICE_PORT=9090
BOSKOS_METRICS_SERVICE_PORT_METRICS=9090
BOSKOS_PORT=tcp://10.35.241.148:80
BOSKOS_PORT_80_TCP=tcp://10.35.241.148:80
BOSKOS_PORT_80_TCP_ADDR=10.35.241.148
BOSKOS_PORT_80_TCP_PORT=80
BOSKOS_PORT_80_TCP_PROTO=tcp
BOSKOS_SERVICE_HOST=10.35.241.148
BOSKOS_SERVICE_PORT=80
BOSKOS_SERVICE_PORT_DEFAULT=80
BUILD_ID=1364619498342060032
BUILD_NUMBER=1364619498342060032
CI=true
CLOUDSDK_COMPONENT_MANAGER_DISABLE_UPDATE_CHECK=true
CLOUDSDK_CONFIG=/workspace/.config/gcloud
CLOUDSDK_CORE_DISABLE_PROMPTS=1
CLOUDSDK_EXPERIMENTAL_FAST_COMPONENT_UPDATE=false
DOCKER_IN_DOCKER_ENABLED=false
DOCKER_IN_DOCKER_IPV6_ENABLED=false
E2E_GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GCS_ARTIFACTS_DIR=gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-20/1364619498342060032/artifacts
GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GOOGLE_APPLICATION_CREDENTIALS_DEPRECATED=Migrate to workload identity, contact sig-testing
GOPATH=/go
GOPROXY=https://proxy.golang.org
GO_TARBALL=go1.15.5.linux-amd64.tar.gz
HOME=/workspace
HOSTNAME=d63549e4-76c0-11eb-aa32-3a9397e74bc0
IMAGE=gcr.io/k8s-testimages/kubekins-e2e:v20210223-952586a143-1.20
INSTANCE_PREFIX=bootstrap-e2e
JENKINS_GCE_SSH_PRIVATE_KEY_FILE=/workspace/.ssh/google_compute_engine
JENKINS_GCE_SSH_PUBLIC_KEY_FILE=/workspace/.ssh/google_compute_engine.pub
JOB_NAME=ci-kubernetes-gce-conformance-latest-1-20
JOB_SPEC={"type":"periodic","job":"ci-kubernetes-gce-conformance-latest-1-20","buildid":"1364619498342060032","prowjobid":"d63549e4-76c0-11eb-aa32-3a9397e74bc0"}
JOB_TYPE=periodic
KUBERNETES_PORT=tcp://10.35.240.1:443
KUBERNETES_PORT_443_TCP=tcp://10.35.240.1:443
KUBERNETES_PORT_443_TCP_ADDR=10.35.240.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_HOST=10.35.240.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBETEST_IN_DOCKER=true
KUBETEST_MANUAL_DUMP=y
KUBE_AWS_INSTANCE_PREFIX=bootstrap-e2e
KUBE_GCE_INSTANCE_PREFIX=bootstrap-e2e
LOG_DUMP_SCRIPT_PATH=/workspace/log-dump.sh
NODE_NAME=d63549e4-76c0-11eb-aa32-3a9397e74bc0
PATH=/go/bin:/go/bin:/usr/local/go/bin:/google-cloud-sdk/bin:/workspace:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PROW_JOB_ID=d63549e4-76c0-11eb-aa32-3a9397e74bc0
PWD=/workspace
SHLVL=2
SOURCE_DATE_EPOCH=
TERM=xterm
USER=prow
WORKSPACE=/workspace
_=./test-infra/jenkins/bootstrap.py
Run: ('kubetest', '--dump=/workspace/_artifacts', '--gcp-service-account=/etc/service-account/service-account.json', '--up', '--down', '--test', '--provider=gce', '--cluster=bootstrap-e2e', '--gcp-network=bootstrap-e2e', '--extract=ci/latest-1.20', '--extract-ci-bucket=k8s-release-dev', '--gcp-master-image=gci', '--gcp-node-image=gci', '--gcp-zone=us-west1-b', '--test_args=--ginkgo.focus=\\[Conformance\\]', '--timeout=200m')
2021/02/24 16:54:23 Warning: Couldn't find directory src/sigs.k8s.io/cloud-provider-azure under any of GOPATH /go, defaulting to /go/src/k8s.io/cloud-provider-azure
2021/02/24 16:54:23 extract_k8s.go:112: Matched extraction strategy: ^ci/(.+)$
2021/02/24 16:54:23 main.go:331: Limiting testing to 3h20m0s
2021/02/24 16:54:23 process.go:153: Running: gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [prow-build@k8s-infra-prow-build.iam.gserviceaccount.com]
2021/02/24 16:54:24 process.go:155: Step 'gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json' finished in 885.269657ms
2021/02/24 16:54:24 main.go:730: --gcp-project is missing, trying to fetch a project from boskos.
(for local runs please set --gcp-project to your dev project)
2021/02/24 16:54:24 main.go:742: provider gce, will acquire project type gce-project from boskos
2021/02/24 16:54:24 process.go:153: Running: gcloud config set project k8s-infra-e2e-boskos-014
Updated property [core/project].
WARNING: You do not appear to have access to project [k8s-infra-e2e-boskos-014] or it does not exist.
2021/02/24 16:54:25 process.go:155: Step 'gcloud config set project k8s-infra-e2e-boskos-014' finished in 869.171824ms
2021/02/24 16:54:25 main.go:781: Checking existing of GCP ssh keys...
2021/02/24 16:54:25 main.go:791: Checking presence of public key in k8s-infra-e2e-boskos-014
2021/02/24 16:54:25 process.go:153: Running: gcloud compute --project=k8s-infra-e2e-boskos-014 project-info describe
2021/02/24 16:54:26 process.go:155: Step 'gcloud compute --project=k8s-infra-e2e-boskos-014 project-info describe' finished in 1.445985398s
2021/02/24 16:54:26 extract_k8s.go:143: rm kubernetes
2021/02/24 16:54:26 extract_k8s.go:295: U=https://storage.googleapis.com/k8s-release-dev/ci R=v1.20.5-rc.0.10+165e5664b0e65d get-kube.sh
2021/02/24 16:54:26 process.go:153: Running: ./get-kube.sh
Downloading kubernetes release v1.20.5-rc.0.10+165e5664b0e65d
  from https://storage.googleapis.com/k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes.tar.gz
  to /workspace/kubernetes.tar.gz
Copying gs://k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes.tar.gz...
/ [0 files][    0.0 B/504.3 KiB]                                                / [1 files][504.3 KiB/504.3 KiB]                                                
Operation completed over 1 objects/504.3 KiB.                                    
Unpacking kubernetes release v1.20.5-rc.0.10+165e5664b0e65d
Kubernetes release: v1.20.5-rc.0.10+165e5664b0e65d
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)  (to override, set KUBERNETES_CLIENT_OS and/or KUBERNETES_CLIENT_ARCH)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d
Will download and extract kubernetes-test tarball(s) from https://storage.googleapis.com/k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d
Copying gs://k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes-server-linux-amd64.tar.gz...
/ [0 files][    0.0 B/302.4 MiB]                                                ==> NOTE: You are downloading one or more large file(s), which would
run significantly faster if you enabled sliced object downloads. This
feature is enabled by default but requires that compiled crcmod be
installed (see "gsutil help crcmod").

-- [0 files][ 64.4 MiB/302.4 MiB]                                                \|| [0 files][109.6 MiB/302.4 MiB]                                                /-- [0 files][162.9 MiB/302.4 MiB]                                                \|| [0 files][246.7 MiB/302.4 MiB]                                                // [1 files][302.4 MiB/302.4 MiB]                                                -
Operation completed over 1 objects/302.4 MiB.                                    

md5sum(kubernetes-server-linux-amd64.tar.gz)=f85c69f15afde8d0b9640421c8d7f714
sha512sum(kubernetes-server-linux-amd64.tar.gz)=b5dbbf188ae503ff0188bea5d77cae1ced2cc0695ae7fa26d103d0f7c5bbfeb1abc01948a0baf48affd3212b99bc23398d59c83dff9cd8e57312dfce7581baa8

Copying gs://k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes-client-linux-amd64.tar.gz...
/ [0 files][    0.0 B/ 11.4 MiB]                                                -- [1 files][ 11.4 MiB/ 11.4 MiB]                                                
Operation completed over 1 objects/11.4 MiB.                                     

md5sum(kubernetes-client-linux-amd64.tar.gz)=1087705fea5d9a89fd795fd8a25acb33
sha512sum(kubernetes-client-linux-amd64.tar.gz)=6395257b1f4f467e8f10827485aac8dda6f558fcb0c31de17c8aa50defb0fe6f1e9f1d419f131040891b5488410295b2497d1d3f560fb704b34b74a10ecc5680

Extracting /workspace/kubernetes/client/kubernetes-client-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
Add '/workspace/kubernetes/client/bin' to your PATH to use newly-installed binaries.
Copying gs://k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes-test-portable.tar.gz...
/ [0 files][    0.0 B/224.7 KiB]                                                / [1 files][224.7 KiB/224.7 KiB]                                                
Operation completed over 1 objects/224.7 KiB.                                    

md5sum(kubernetes-test-portable.tar.gz)=c879448abed79830979dced16f1587e3
sha512sum(kubernetes-test-portable.tar.gz)=1f95d988b540c9ccfd0714da18d3ec41a09626c651d955d72cf1143013ff79b712a096de28f83d99e9e2fa19335c3d283da28d117b465e6487d44b1efc53834d

Extracting kubernetes-test-portable.tar.gz into /workspace/kubernetes
Copying gs://k8s-release-dev/ci/v1.20.5-rc.0.10+165e5664b0e65d/kubernetes-test-linux-amd64.tar.gz...
/ [0 files][    0.0 B/196.7 MiB]                                                ==> NOTE: You are downloading one or more large file(s), which would
run significantly faster if you enabled sliced object downloads. This
feature is enabled by default but requires that compiled crcmod be
installed (see "gsutil help crcmod").

-- [0 files][ 16.2 MiB/196.7 MiB]                                                \\ [0 files][ 44.9 MiB/196.7 MiB]                                                |// [0 files][ 96.2 MiB/196.7 MiB]                                                -\\ [0 files][138.2 MiB/196.7 MiB]                                                || [0 files][162.4 MiB/196.7 MiB]                                                /-- [1 files][196.7 MiB/196.7 MiB]                                                \
Operation completed over 1 objects/196.7 MiB.                                    

md5sum(kubernetes-test-linux-amd64.tar.gz)=228dd0ac494571174aa2f221ff56bf6f
sha512sum(kubernetes-test-linux-amd64.tar.gz)=ae6f74e02435b31b6ac6b1abaa05d1ab5708c07bd0e9fe97b19742cbc6e6ab0d0dd83304b3966bf684dc4f16935b7fbaf78c96af0478fcb3ed66020e93524c6e

Extracting /workspace/kubernetes/test/kubernetes-test-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
2021/02/24 16:55:04 process.go:155: Step './get-kube.sh' finished in 37.744142725s
2021/02/24 16:55:04 process.go:153: Running: ./hack/e2e-internal/e2e-down.sh
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
Shutting down test cluster in background.
Bringing down cluster using provider: gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling kube-down
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
WARNING: The following filter keys were not present in any resource : name, zone
WARNING: The following filter keys were not present in any resource : name, zone
INSTANCE_GROUPS=
NODE_NAMES=
Bringing down cluster
WARNING: The following filter keys were not present in any resource : name
WARNING: The following filter keys were not present in any resource : name, zone
WARNING: The following filter keys were not present in any resource : name
WARNING: The following filter keys were not present in any resource : name
WARNING: The following filter keys were not present in any resource : name, zone
Deleting firewall rules remaining in network bootstrap-e2e: 
W0224 16:55:54.545816    1905 loader.go:223] Config not found: /workspace/.kube/config
W0224 16:55:54.783190    1957 loader.go:223] Config not found: /workspace/.kube/config
W0224 16:55:54.783542    1957 loader.go:223] Config not found: /workspace/.kube/config
Property "clusters.k8s-infra-e2e-boskos-014_bootstrap-e2e" unset.
W0224 16:55:55.002951    2009 loader.go:223] Config not found: /workspace/.kube/config
W0224 16:55:55.003246    2009 loader.go:223] Config not found: /workspace/.kube/config
Property "users.k8s-infra-e2e-boskos-014_bootstrap-e2e" unset.
W0224 16:55:55.242575    2062 loader.go:223] Config not found: /workspace/.kube/config
W0224 16:55:55.242916    2062 loader.go:223] Config not found: /workspace/.kube/config
Property "users.k8s-infra-e2e-boskos-014_bootstrap-e2e-basic-auth" unset.
W0224 16:55:55.491058    2115 loader.go:223] Config not found: /workspace/.kube/config
W0224 16:55:55.491310    2115 loader.go:223] Config not found: /workspace/.kube/config
2021/02/24 16:55:55 process.go:155: Step './hack/e2e-internal/e2e-down.sh' finished in 50.985495628s
2021/02/24 16:55:55 process.go:153: Running: ./hack/e2e-internal/e2e-up.sh
Property "contexts.k8s-infra-e2e-boskos-014_bootstrap-e2e" unset.
Cleared config for k8s-infra-e2e-boskos-014_bootstrap-e2e from /workspace/.kube/config
Done
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
... Starting cluster in us-west1-b using provider gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling verify-release-tars
... calling kube-up
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
+++ Staging tars to Google Storage: gs://kubernetes-staging-c7ca109202/bootstrap-e2e-devel
+++ kubernetes-server-linux-amd64.tar.gz uploaded (sha512 = b5dbbf188ae503ff0188bea5d77cae1ced2cc0695ae7fa26d103d0f7c5bbfeb1abc01948a0baf48affd3212b99bc23398d59c83dff9cd8e57312dfce7581baa8)
+++ kubernetes-manifests.tar.gz uploaded (sha512 = 9d4cd8fec37c8d4354f12d39cea495886c92a1e1a4fa4691be509cc2c0f710467765334f2dcf2b6ee4c8dba2b37394328a2cdc5d073f03002ff3630e51b646fb)
Creating new auto network: bootstrap-e2e
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/networks/bootstrap-e2e].
NAME           SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
bootstrap-e2e  AUTO         REGIONAL

Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:

$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp:22,tcp:3389,icmp

IP aliases are disabled.
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-default-internal-master].
NAME                                   NETWORK        DIRECTION  PRIORITY  ALLOW                                       DENY  DISABLED
bootstrap-e2e-default-internal-master  bootstrap-e2e  INGRESS    1000      tcp:1-2379,tcp:2382-65535,udp:1-65535,icmp        False
done.
Found subnet for region us-west1 in network bootstrap-e2e: bootstrap-e2e
Starting master and configuring firewalls
Configuring firewall for apiserver konnectivity server
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-default-internal-node].
NAME                                 NETWORK        DIRECTION  PRIORITY  ALLOW                         DENY  DISABLED
bootstrap-e2e-default-internal-node  bootstrap-e2e  INGRESS    1000      tcp:1-65535,udp:1-65535,icmp        False
done.
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/zones/us-west1-b/disks/bootstrap-e2e-master-pd].
NAME                     ZONE        SIZE_GB  TYPE    STATUS
bootstrap-e2e-master-pd  us-west1-b  20       pd-ssd  READY

New disks are unformatted. You must format and mount a disk before it
can be used. You can find instructions on how to do this at:

https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting

Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-default-ssh].
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW   DENY  DISABLED
bootstrap-e2e-default-ssh  bootstrap-e2e  INGRESS    1000      tcp:22        False
done.
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-master-https].
NAME                        NETWORK        DIRECTION  PRIORITY  ALLOW    DENY  DISABLED
bootstrap-e2e-master-https  bootstrap-e2e  INGRESS    1000      tcp:443        False
done.
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/regions/us-west1/addresses/bootstrap-e2e-master-ip].
Generating certs for alternate-names: IP:34.83.69.28,IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local,DNS:bootstrap-e2e-master
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-master-etcd].
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW              DENY  DISABLED
bootstrap-e2e-master-etcd  bootstrap-e2e  INGRESS    1000      tcp:2380,tcp:2381        False
done.
2021/02/24 16:57:25 [INFO] generating a new CA key and certificate from CSR
2021/02/24 16:57:25 [INFO] generate received request
2021/02/24 16:57:25 [INFO] received CSR
2021/02/24 16:57:25 [INFO] generating key: ecdsa-256
2021/02/24 16:57:25 [INFO] encoded CSR
2021/02/24 16:57:25 [INFO] signed certificate with serial number 463589555642594501055117336161766620677152587441
2021/02/24 16:57:25 [INFO] generate received request
2021/02/24 16:57:25 [INFO] received CSR
2021/02/24 16:57:25 [INFO] generating key: ecdsa-256
2021/02/24 16:57:26 [INFO] encoded CSR
2021/02/24 16:57:26 [INFO] signed certificate with serial number 605362617523546257298676196407857561657265567942
2021/02/24 16:57:26 [INFO] generate received request
2021/02/24 16:57:26 [INFO] received CSR
2021/02/24 16:57:26 [INFO] generating key: ecdsa-256
2021/02/24 16:57:26 [INFO] encoded CSR
2021/02/24 16:57:26 [INFO] signed certificate with serial number 155020433011509390460662037851554061129588819998
2021/02/24 16:57:26 [INFO] generate received request
2021/02/24 16:57:26 [INFO] received CSR
2021/02/24 16:57:26 [INFO] generating key: ecdsa-256
2021/02/24 16:57:26 [INFO] encoded CSR
2021/02/24 16:57:26 [INFO] signed certificate with serial number 220316210672519282416885072134674400773908320463
2021/02/24 16:57:26 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
Generate peer certificates...
Generate server certificates...
Generate client certificates...
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-minion-all].
NAME                      NETWORK        DIRECTION  PRIORITY  ALLOW                     DENY  DISABLED
bootstrap-e2e-minion-all  bootstrap-e2e  INGRESS    1000      tcp,udp,icmp,esp,ah,sctp        False
done.
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/zones/us-west1-b/instances/bootstrap-e2e-master].
WARNING: Some requests generated warnings:
 - Disk size: '20 GB' is larger than image size: '10 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/cos-cloud/global/images/cos-85-13310-1041-9' is deprecated. A suggested replacement is 'projects/cos-cloud/global/images/cos-85-13310-1041-14'.

NAME                  ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
bootstrap-e2e-master  us-west1-b  n1-standard-1               10.138.0.2   34.83.69.28  RUNNING
Creating nodes.
/workspace/kubernetes/cluster/../cluster/../cluster/gce/util.sh: line 1543: WINDOWS_CONTAINER_RUNTIME_ENDPOINT: unbound variable
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-minion-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/instanceTemplates/bootstrap-e2e-minion-template].
NAME                           MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-minion-template  n1-standard-2               2021-02-24T08:57:46.470-08:00
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-windows-node-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/instanceTemplates/bootstrap-e2e-windows-node-template].
NAME                                 MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-windows-node-template  n1-standard-2               2021-02-24T08:57:51.385-08:00
Created [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/zones/us-west1-b/instanceGroupManagers/bootstrap-e2e-minion-group].
NAME                        LOCATION    SCOPE  BASE_INSTANCE_NAME          SIZE  TARGET_SIZE  INSTANCE_TEMPLATE              AUTOSCALED
bootstrap-e2e-minion-group  us-west1-b  zone   bootstrap-e2e-minion-group  0     3            bootstrap-e2e-minion-template  no
Waiting for group to become stable, current operations: creating: 3
Group is stable
INSTANCE_GROUPS=bootstrap-e2e-minion-group
NODE_NAMES=bootstrap-e2e-minion-group-2qjz bootstrap-e2e-minion-group-9mrc bootstrap-e2e-minion-group-bthl
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Waiting up to 300 seconds for cluster initialization.

  This will continually check to see if the API for kubernetes is reachable.
  This may time out if there was some uncaught error during start up.

Using master: bootstrap-e2e-master (external IP: 34.83.69.28; internal IP: (not set))
........................................Kubernetes cluster created.
Cluster "k8s-infra-e2e-boskos-014_bootstrap-e2e" set.
User "k8s-infra-e2e-boskos-014_bootstrap-e2e" set.
Context "k8s-infra-e2e-boskos-014_bootstrap-e2e" created.
Switched to context "k8s-infra-e2e-boskos-014_bootstrap-e2e".
User "k8s-infra-e2e-boskos-014_bootstrap-e2e-basic-auth" set.
Wrote config for k8s-infra-e2e-boskos-014_bootstrap-e2e to /workspace/.kube/config
Updated [https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/zones/us-west1-b/instances/bootstrap-e2e-master].

Kubernetes cluster is running.  The master is running at:

  https://34.83.69.28

The user name and password to use is located in /workspace/.kube/config.

Validating gce cluster, MULTIZONE=
... calling validate-cluster
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
No resources found
Waiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.
No resources found
Waiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.
Waiting for 4 ready nodes. 0 ready nodes, 1 registered. Retrying.
Waiting for 4 ready nodes. 2 ready nodes, 4 registered. Retrying.
Found 4 node(s).
NAME                              STATUS                     ROLES    AGE   VERSION
bootstrap-e2e-master              Ready,SchedulingDisabled   <none>   34s   v1.20.5-rc.0.10+165e5664b0e65d
bootstrap-e2e-minion-group-2qjz   Ready                      <none>   24s   v1.20.5-rc.0.10+165e5664b0e65d
bootstrap-e2e-minion-group-9mrc   Ready                      <none>   24s   v1.20.5-rc.0.10+165e5664b0e65d
bootstrap-e2e-minion-group-bthl   Ready                      <none>   24s   v1.20.5-rc.0.10+165e5664b0e65d
Warning: v1 ComponentStatus is deprecated in v1.19+
Validate output:
Warning: v1 ComponentStatus is deprecated in v1.19+
Done, listing cluster services:

NAME                 STATUS    MESSAGE             ERROR
etcd-1               Healthy   {"health":"true"}   
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
Cluster validation succeeded
Kubernetes control plane is running at https://34.83.69.28
GLBCDefaultBackend is running at https://34.83.69.28/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
CoreDNS is running at https://34.83.69.28/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://34.83.69.28/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

NAME                           NETWORK        DIRECTION  PRIORITY  ALLOW            DENY  DISABLED
bootstrap-e2e-minion-http-alt  bootstrap-e2e  INGRESS    1000      tcp:80,tcp:8080        False
allowed:
- IPProtocol: tcp
  ports:
  - '80'
- IPProtocol: tcp
  ports:
  - '8080'
creationTimestamp: '2021-02-24T09:01:19.408-08:00'
description: ''
direction: INGRESS
disabled: false
id: '8649828571270559920'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-http-alt
network: https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-minion-http-alt
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
NAME                            NETWORK        DIRECTION  PRIORITY  ALLOW                            DENY  DISABLED
bootstrap-e2e-minion-nodeports  bootstrap-e2e  INGRESS    1000      tcp:30000-32767,udp:30000-32767        False
allowed:
- IPProtocol: tcp
  ports:
  - 30000-32767
- IPProtocol: udp
  ports:
  - 30000-32767
creationTimestamp: '2021-02-24T09:01:27.312-08:00'
description: ''
direction: INGRESS
disabled: false
id: '5758129079167170696'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-nodeports
network: https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/k8s-infra-e2e-boskos-014/global/firewalls/bootstrap-e2e-minion-nodeports
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
2021/02/24 17:01:34 process.go:155: Step './hack/e2e-internal/e2e-up.sh' finished in 5m38.591321928s
2021/02/24 17:01:34 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2021/02/24 17:01:34 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 496.294438ms
2021/02/24 17:01:34 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false get nodes -oyaml
2021/02/24 17:01:35 process.go:155: Step './cluster/kubectl.sh --match-server-version=false get nodes -oyaml' finished in 483.469413ms
2021/02/24 17:01:35 process.go:153: Running: ./hack/e2e-internal/e2e-status.sh
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
Client Version: version.Info{Major:"1", Minor:"20+", GitVersion:"v1.20.5-rc.0.10+165e5664b0e65d", GitCommit:"165e5664b0e65d2e0f8fbcfec16aa95f2bc3cb19", GitTreeState:"clean", BuildDate:"2021-02-21T07:33:42Z", GoVersion:"go1.15.8", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"20+", GitVersion:"v1.20.5-rc.0.10+165e5664b0e65d", GitCommit:"165e5664b0e65d2e0f8fbcfec16aa95f2bc3cb19", GitTreeState:"clean", BuildDate:"2021-02-21T07:33:42Z", GoVersion:"go1.15.8", Compiler:"gc", Platform:"linux/amd64"}
2021/02/24 17:01:35 process.go:155: Step './hack/e2e-internal/e2e-status.sh' finished in 599.292889ms
2021/02/24 17:01:35 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2021/02/24 17:01:36 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 419.677759ms
2021/02/24 17:01:36 process.go:153: Running: ./hack/ginkgo-e2e.sh --ginkgo.focus=\[Conformance\] --report-dir=/workspace/_artifacts --disable-log-dump=true
Setting up for KUBERNETES_PROVIDER="gce".
Project: k8s-infra-e2e-boskos-014
Network Project: k8s-infra-e2e-boskos-014
Zone: us-west1-b
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Using master: bootstrap-e2e-master (external IP: 34.83.69.28; internal IP: (not set))
Feb 24 17:01:39.886: INFO: Fetching cloud provider for "gce"
I0224 17:01:39.886083   10144 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0224 17:01:39.887406   10144 gce.go:909] Using DefaultTokenSource &oauth2.reuseTokenSource{new:jwt.jwtSource{ctx:(*context.emptyCtx)(0xc00005a0d0), conf:(*jwt.Config)(0xc001b69400)}, mu:sync.Mutex{state:0, sema:0x0}, t:(*oauth2.Token)(nil)}
W0224 17:01:39.996085   10144 gce.go:477] No network name or URL specified.
I0224 17:01:39.996245   10144 e2e.go:129] Starting e2e run "16c591ba-52c4-4277-81b9-61c35ae6a6e4" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1614186097 - Will randomize all specs
Will run 311 of 5667 specs

Feb 24 17:01:46.537: INFO: cluster-master-image: cos-85-13310-1041-9
Feb 24 17:01:46.537: INFO: cluster-node-image: cos-85-13310-1041-9
Feb 24 17:01:46.538: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:01:46.542: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 24 17:01:46.759: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 24 17:01:46.960: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:46.960: INFO: The status of Pod metrics-server-v0.3.6-8b98f98c9-pvx78 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:46.960: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 24 17:01:46.960: INFO: expected 6 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:46.960: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:46.960: INFO: l7-default-backend-6f8dd4f4d5-c7mgv    bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:46.960: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78  bootstrap-e2e-minion-group-9mrc  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  }]
Feb 24 17:01:46.960: INFO: 
Feb 24 17:01:49.148: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:49.148: INFO: The status of Pod metrics-server-v0.3.6-8b98f98c9-pvx78 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:49.148: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Feb 24 17:01:49.148: INFO: expected 6 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:49.148: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:49.148: INFO: l7-default-backend-6f8dd4f4d5-c7mgv    bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:49.148: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78  bootstrap-e2e-minion-group-9mrc  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  }]
Feb 24 17:01:49.148: INFO: 
Feb 24 17:01:51.169: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:51.169: INFO: The status of Pod metrics-server-v0.3.6-8b98f98c9-pvx78 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:51.169: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Feb 24 17:01:51.169: INFO: expected 6 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:51.169: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:51.169: INFO: l7-default-backend-6f8dd4f4d5-c7mgv    bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:51.169: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78  bootstrap-e2e-minion-group-9mrc  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:37 +0000 UTC  }]
Feb 24 17:01:51.169: INFO: 
Feb 24 17:01:53.137: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:53.137: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Feb 24 17:01:53.137: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:53.137: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:53.137: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:53.137: INFO: 
Feb 24 17:01:55.135: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:55.135: INFO: 18 / 19 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Feb 24 17:01:55.135: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:55.135: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:55.135: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:55.135: INFO: 
Feb 24 17:01:57.137: INFO: The status of Pod kube-controller-manager-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:57.137: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:57.137: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Feb 24 17:01:57.137: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:57.137: INFO: POD                                           NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:57.137: INFO: kube-controller-manager-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 24 17:01:57.137: INFO: l7-default-backend-6f8dd4f4d5-c7mgv           bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:57.137: INFO: 
Feb 24 17:01:59.139: INFO: The status of Pod kube-controller-manager-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:59.139: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:01:59.139: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (12 seconds elapsed)
Feb 24 17:01:59.139: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:01:59.139: INFO: POD                                           NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:01:59.139: INFO: kube-controller-manager-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 24 17:01:59.139: INFO: l7-default-backend-6f8dd4f4d5-c7mgv           bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:01:59.139: INFO: 
Feb 24 17:02:01.151: INFO: The status of Pod kube-controller-manager-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:01.151: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:01.151: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
Feb 24 17:02:01.151: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:01.151: INFO: POD                                           NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:01.151: INFO: kube-controller-manager-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 24 17:02:01.151: INFO: l7-default-backend-6f8dd4f4d5-c7mgv           bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:01.151: INFO: 
Feb 24 17:02:03.145: INFO: The status of Pod kube-controller-manager-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:03.145: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:03.145: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
Feb 24 17:02:03.145: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:03.145: INFO: POD                                           NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:03.145: INFO: kube-controller-manager-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 24 17:02:03.145: INFO: l7-default-backend-6f8dd4f4d5-c7mgv           bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:03.145: INFO: 
Feb 24 17:02:05.177: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:05.177: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
Feb 24 17:02:05.177: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:05.177: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:05.177: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:05.177: INFO: 
Feb 24 17:02:07.138: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:07.138: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
Feb 24 17:02:07.138: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:07.138: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:07.138: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:07.138: INFO: 
Feb 24 17:02:09.138: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:09.138: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
Feb 24 17:02:09.138: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:09.138: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:09.138: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:09.138: INFO: 
Feb 24 17:02:11.136: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:11.136: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (24 seconds elapsed)
Feb 24 17:02:11.136: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:11.136: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:11.136: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:11.136: INFO: 
Feb 24 17:02:13.159: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:13.159: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (26 seconds elapsed)
Feb 24 17:02:13.159: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:13.159: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:13.159: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:13.159: INFO: 
Feb 24 17:02:15.138: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:15.138: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (28 seconds elapsed)
Feb 24 17:02:15.138: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:15.138: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:15.138: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:15.139: INFO: 
Feb 24 17:02:17.137: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:17.137: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (30 seconds elapsed)
Feb 24 17:02:17.137: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:17.137: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:17.137: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:17.137: INFO: 
Feb 24 17:02:19.137: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:19.137: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (32 seconds elapsed)
Feb 24 17:02:19.137: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:19.137: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:19.137: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:19.137: INFO: 
Feb 24 17:02:21.135: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:21.135: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (34 seconds elapsed)
Feb 24 17:02:21.135: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:21.135: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:21.135: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:21.135: INFO: 
Feb 24 17:02:23.164: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:23.164: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (36 seconds elapsed)
Feb 24 17:02:23.164: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:23.164: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:23.164: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:23.164: INFO: 
Feb 24 17:02:25.146: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:25.146: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (38 seconds elapsed)
Feb 24 17:02:25.146: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:25.146: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:25.146: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:25.146: INFO: 
Feb 24 17:02:27.172: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:27.172: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (40 seconds elapsed)
Feb 24 17:02:27.172: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:27.172: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:27.172: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:27.172: INFO: 
Feb 24 17:02:29.194: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:29.194: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (42 seconds elapsed)
Feb 24 17:02:29.194: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:29.194: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:29.194: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:29.194: INFO: 
Feb 24 17:02:31.157: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:31.157: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (44 seconds elapsed)
Feb 24 17:02:31.157: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:31.157: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:31.157: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:31.157: INFO: 
Feb 24 17:02:33.145: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:33.145: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (46 seconds elapsed)
Feb 24 17:02:33.145: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:33.145: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:33.145: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:33.145: INFO: 
Feb 24 17:02:35.135: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:35.135: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (48 seconds elapsed)
Feb 24 17:02:35.135: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:35.135: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:35.135: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:35.135: INFO: 
Feb 24 17:02:37.236: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:37.236: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (50 seconds elapsed)
Feb 24 17:02:37.236: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:37.236: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:37.236: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:37.236: INFO: 
Feb 24 17:02:39.139: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:39.139: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (52 seconds elapsed)
Feb 24 17:02:39.139: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:39.139: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:39.139: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:39.139: INFO: 
Feb 24 17:02:41.138: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:41.139: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (54 seconds elapsed)
Feb 24 17:02:41.139: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:41.139: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:41.139: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:41.139: INFO: 
Feb 24 17:02:43.142: INFO: The status of Pod l7-default-backend-6f8dd4f4d5-c7mgv is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 17:02:43.142: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (56 seconds elapsed)
Feb 24 17:02:43.142: INFO: expected 5 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 24 17:02:43.142: INFO: POD                                  NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:02:43.142: INFO: l7-default-backend-6f8dd4f4d5-c7mgv  bootstrap-e2e-minion-group-bthl  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC ContainersNotReady containers with unready status: [default-http-backend]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:01:13 +0000 UTC  }]
Feb 24 17:02:43.142: INFO: 
Feb 24 17:02:45.136: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (58 seconds elapsed)
Feb 24 17:02:45.136: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Feb 24 17:02:45.136: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 24 17:02:45.194: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Feb 24 17:02:45.194: INFO: e2e test version: v1.20.5-rc.0.10+165e5664b0e65d
Feb 24 17:02:45.244: INFO: kube-apiserver version: v1.20.5-rc.0.10+165e5664b0e65d
Feb 24 17:02:45.244: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:02:45.298: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:02:45.298: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
Feb 24 17:02:45.506: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5177
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5177
I0224 17:02:45.835666   10144 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5177, replica count: 2
I0224 17:02:48.936175   10144 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 17:02:51.936412   10144 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:02:51.936: INFO: Creating new exec pod
Feb 24 17:02:59.144: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5177 exec execpodfwlv8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 24 17:02:59.805: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 17:02:59.805: INFO: stdout: ""
Feb 24 17:02:59.806: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5177 exec execpodfwlv8 -- /bin/sh -x -c nc -zv -t -w 2 10.0.213.8 80'
Feb 24 17:03:00.476: INFO: stderr: "+ nc -zv -t -w 2 10.0.213.8 80\nConnection to 10.0.213.8 80 port [tcp/http] succeeded!\n"
Feb 24 17:03:00.476: INFO: stdout: ""
Feb 24 17:03:00.476: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:03:00.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5177" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":1,"skipped":6,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:03:00.676: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:03:00.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac" in namespace "projected-7955" to be "Succeeded or Failed"
Feb 24 17:03:01.044: INFO: Pod "downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 53.300233ms
Feb 24 17:03:03.098: INFO: Pod "downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107004842s
STEP: Saw pod success
Feb 24 17:03:03.098: INFO: Pod "downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac" satisfied condition "Succeeded or Failed"
Feb 24 17:03:03.153: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac container client-container: <nil>
STEP: delete the pod
Feb 24 17:03:03.295: INFO: Waiting for pod downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac to disappear
Feb 24 17:03:03.346: INFO: Pod downwardapi-volume-3f2a1d42-ea67-4528-9241-a4d43d98d2ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:03:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7955" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":33,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:03:03.466: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Feb 24 17:05:04.567: INFO: Successfully updated pod "var-expansion-aaae6484-d1e6-4243-8574-4cf10e96c6f9"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb 24 17:05:06.673: INFO: Deleting pod "var-expansion-aaae6484-d1e6-4243-8574-4cf10e96c6f9" in namespace "var-expansion-4641"
Feb 24 17:05:06.728: INFO: Wait up to 5m0s for pod "var-expansion-aaae6484-d1e6-4243-8574-4cf10e96c6f9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:05:38.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4641" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":3,"skipped":37,"failed":0}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:05:38.947: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 24 17:05:43.660: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:43.660: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:44.054: INFO: Exec stderr: ""
Feb 24 17:05:44.054: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:44.054: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:44.445: INFO: Exec stderr: ""
Feb 24 17:05:44.446: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:44.446: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:44.807: INFO: Exec stderr: ""
Feb 24 17:05:44.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:44.807: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:45.181: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 24 17:05:45.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:45.181: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:45.541: INFO: Exec stderr: ""
Feb 24 17:05:45.541: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:45.541: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:45.907: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 24 17:05:45.907: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:45.907: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:46.313: INFO: Exec stderr: ""
Feb 24 17:05:46.313: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:46.313: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:46.697: INFO: Exec stderr: ""
Feb 24 17:05:46.697: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:46.697: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:47.086: INFO: Exec stderr: ""
Feb 24 17:05:47.086: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-761 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:05:47.086: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:05:47.445: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:05:47.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-761" for this suite.
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":42,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:05:47.557: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb 24 17:05:47.873: INFO: Waiting up to 5m0s for pod "downward-api-637f43e6-937c-4f33-ac93-64e3806a2595" in namespace "downward-api-9816" to be "Succeeded or Failed"
Feb 24 17:05:47.924: INFO: Pod "downward-api-637f43e6-937c-4f33-ac93-64e3806a2595": Phase="Pending", Reason="", readiness=false. Elapsed: 51.195675ms
Feb 24 17:05:49.976: INFO: Pod "downward-api-637f43e6-937c-4f33-ac93-64e3806a2595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.102765178s
STEP: Saw pod success
Feb 24 17:05:49.976: INFO: Pod "downward-api-637f43e6-937c-4f33-ac93-64e3806a2595" satisfied condition "Succeeded or Failed"
Feb 24 17:05:50.027: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod downward-api-637f43e6-937c-4f33-ac93-64e3806a2595 container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:05:50.160: INFO: Waiting for pod downward-api-637f43e6-937c-4f33-ac93-64e3806a2595 to disappear
Feb 24 17:05:50.211: INFO: Pod downward-api-637f43e6-937c-4f33-ac93-64e3806a2595 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:05:50.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9816" for this suite.
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":43,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:05:50.322: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb 24 17:05:50.579: INFO: PodSpec: initContainers in spec.initContainers
Feb 24 17:06:38.793: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6cd4d16c-d851-4419-9eff-f40199f027e1", GenerateName:"", Namespace:"init-container-7726", SelfLink:"", UID:"9548b31c-e448-45a9-9bf2-b70dc6c7c01a", ResourceVersion:"1637", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63749783150, loc:(*time.Location)(0x7977e40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"579916042"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001f48060), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001f48080)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001f480a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001f480c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-jdzvf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020c6100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jdzvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jdzvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jdzvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f8a0c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"bootstrap-e2e-minion-group-9mrc", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003326000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f8a140)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f8a170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f8a178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f8a17c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0008d40b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783150, loc:(*time.Location)(0x7977e40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783150, loc:(*time.Location)(0x7977e40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783150, loc:(*time.Location)(0x7977e40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783150, loc:(*time.Location)(0x7977e40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.0.3", PodIP:"10.64.2.7", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.64.2.7"}}, StartTime:(*v1.Time)(0xc001f480e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033260e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003326150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://ce887ddcca1cd04df2b18a11eb5dda8c6a67cb7b7872a313dda2a4ec4d461da2", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f48120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f48100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc001f8a1ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:06:38.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7726" for this suite.
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":6,"skipped":53,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:06:38.908: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 24 17:06:39.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3129  7b08ec25-4784-4035-8e94-a9039d2a301e 1648 0 2021-02-24 17:06:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-24 17:06:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:06:39.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3129  7b08ec25-4784-4035-8e94-a9039d2a301e 1649 0 2021-02-24 17:06:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-24 17:06:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:06:39.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3129" for this suite.
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":7,"skipped":71,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:06:39.653: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7658
STEP: creating service affinity-nodeport-transition in namespace services-7658
STEP: creating replication controller affinity-nodeport-transition in namespace services-7658
I0224 17:06:40.052433   10144 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7658, replica count: 3
I0224 17:06:43.152897   10144 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:06:43.316: INFO: Creating new exec pod
Feb 24 17:06:46.581: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Feb 24 17:06:48.236: INFO: rc: 1
Feb 24 17:06:48.236: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 24 17:06:49.236: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Feb 24 17:06:50.923: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 24 17:06:50.924: INFO: stdout: ""
Feb 24 17:06:50.924: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 10.0.57.85 80'
Feb 24 17:06:51.578: INFO: stderr: "+ nc -zv -t -w 2 10.0.57.85 80\nConnection to 10.0.57.85 80 port [tcp/http] succeeded!\n"
Feb 24 17:06:51.578: INFO: stdout: ""
Feb 24 17:06:51.578: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.4 31422'
Feb 24 17:06:52.204: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.4 31422\nConnection to 10.138.0.4 31422 port [tcp/31422] succeeded!\n"
Feb 24 17:06:52.204: INFO: stdout: ""
Feb 24 17:06:52.204: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.5 31422'
Feb 24 17:06:52.930: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.5 31422\nConnection to 10.138.0.5 31422 port [tcp/31422] succeeded!\n"
Feb 24 17:06:52.930: INFO: stdout: ""
Feb 24 17:06:52.930: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 104.198.10.242 31422'
Feb 24 17:06:53.646: INFO: stderr: "+ nc -zv -t -w 2 104.198.10.242 31422\nConnection to 104.198.10.242 31422 port [tcp/31422] succeeded!\n"
Feb 24 17:06:53.646: INFO: stdout: ""
Feb 24 17:06:53.646: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c nc -zv -t -w 2 34.105.126.161 31422'
Feb 24 17:06:54.364: INFO: stderr: "+ nc -zv -t -w 2 34.105.126.161 31422\nConnection to 34.105.126.161 31422 port [tcp/31422] succeeded!\n"
Feb 24 17:06:54.364: INFO: stdout: ""
Feb 24 17:06:54.471: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.0.4:31422/ ; done'
Feb 24 17:06:55.214: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n"
Feb 24 17:06:55.214: INFO: stdout: "\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw"
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:06:55.214: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:07:25.215: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.0.4:31422/ ; done'
Feb 24 17:07:25.988: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n"
Feb 24 17:07:25.988: INFO: stdout: "\naffinity-nodeport-transition-pzpfz\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-pzpfz\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-pzpfz\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-pzpfz\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-nrwsw\naffinity-nodeport-transition-pzpfz\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-pzpfz"
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-nrwsw
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:25.988: INFO: Received response from host: affinity-nodeport-transition-pzpfz
Feb 24 17:07:26.098: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-7658 exec execpod-affinitywpmw7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.0.4:31422/ ; done'
Feb 24 17:07:26.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31422/\n"
Feb 24 17:07:26.846: INFO: stdout: "\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5\naffinity-nodeport-transition-tbdx5"
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Received response from host: affinity-nodeport-transition-tbdx5
Feb 24 17:07:26.846: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7658, will wait for the garbage collector to delete the pods
Feb 24 17:07:27.119: INFO: Deleting ReplicationController affinity-nodeport-transition took: 53.863651ms
Feb 24 17:07:27.919: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.330364ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:42.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7658" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":8,"skipped":136,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:42.524: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:07:44.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:07:46.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783263, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:07:49.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:49.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2101" for this suite.
STEP: Destroying namespace "webhook-2101-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":9,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:49.824: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 24 17:07:50.149: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7880  33580f62-ea89-4f21-b446-5bb0777eab3f 1928 0 2021-02-24 17:07:50 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-02-24 17:07:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l5lc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l5lc5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l5lc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:07:50.201: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 24 17:07:52.253: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 24 17:07:52.253: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7880 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:07:52.253: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Verifying customized DNS server is configured on pod...
Feb 24 17:07:52.649: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7880 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:07:52.649: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:07:53.080: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7880" for this suite.
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":10,"skipped":193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:53.251: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0224 17:07:54.020359   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 17:07:56.281: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1763" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":11,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:56.394: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 24 17:07:58.179: INFO: starting watch
STEP: patching
STEP: updating
Feb 24 17:07:58.363: INFO: waiting for watch events with expected annotations
Feb 24 17:07:58.364: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:58.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7678" for this suite.
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":12,"skipped":274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:59.070: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 24 17:07:59.439: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:07:59.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1840" for this suite.
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":13,"skipped":318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:07:59.709: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:08:00.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e" in namespace "downward-api-7622" to be "Succeeded or Failed"
Feb 24 17:08:00.081: INFO: Pod "downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e": Phase="Pending", Reason="", readiness=false. Elapsed: 51.135097ms
Feb 24 17:08:02.148: INFO: Pod "downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117408575s
STEP: Saw pod success
Feb 24 17:08:02.148: INFO: Pod "downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e" satisfied condition "Succeeded or Failed"
Feb 24 17:08:02.199: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e container client-container: <nil>
STEP: delete the pod
Feb 24 17:08:02.323: INFO: Waiting for pod downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e to disappear
Feb 24 17:08:02.375: INFO: Pod downwardapi-volume-322ddd27-0b29-4e38-915c-4c90ac41384e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:08:02.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7622" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":377,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:08:02.486: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-23dccc45-a076-4037-94b5-18d774493cf6
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:08:02.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6005" for this suite.
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":15,"skipped":384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:08:02.908: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:08:16.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1407" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":16,"skipped":456,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:08:16.888: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:08:19.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783298, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783298, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783299, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749783298, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:08:22.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 24 17:08:22.408: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:08:22.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8683" for this suite.
STEP: Destroying namespace "webhook-8683-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":17,"skipped":463,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:08:23.185: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb 24 17:08:23.443: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 17:08:23.552: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 17:08:23.604: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-2qjz before test
Feb 24 17:08:23.667: INFO: coredns-6954c77b9b-dk8mk from kube-system started at 2021-02-24 17:01:15 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.667: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:08:23.667: INFO: kube-proxy-bootstrap-e2e-minion-group-2qjz from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.667: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:08:23.667: INFO: metadata-proxy-v0.1-mjnrs from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:08:23.667: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:08:23.667: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:08:23.667: INFO: sample-webhook-deployment-6bd9446d55-69td4 from webhook-8683 started at 2021-02-24 17:08:18 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.667: INFO: 	Container sample-webhook ready: true, restart count 0
Feb 24 17:08:23.667: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-9mrc before test
Feb 24 17:08:23.727: INFO: coredns-6954c77b9b-9rzld from kube-system started at 2021-02-24 17:01:20 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.727: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:08:23.727: INFO: kube-proxy-bootstrap-e2e-minion-group-9mrc from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.727: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:08:23.727: INFO: metadata-proxy-v0.1-rgzg9 from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:08:23.727: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:08:23.727: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:08:23.727: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78 from kube-system started at 2021-02-24 17:01:37 +0000 UTC (2 container statuses recorded)
Feb 24 17:08:23.727: INFO: 	Container metrics-server ready: true, restart count 0
Feb 24 17:08:23.727: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 24 17:08:23.727: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-bthl before test
Feb 24 17:08:23.782: INFO: kube-dns-autoscaler-76478fcf46-gsgnb from kube-system started at 2021-02-24 17:01:17 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.782: INFO: 	Container autoscaler ready: true, restart count 0
Feb 24 17:08:23.782: INFO: kube-proxy-bootstrap-e2e-minion-group-bthl from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.782: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:08:23.782: INFO: l7-default-backend-6f8dd4f4d5-c7mgv from kube-system started at 2021-02-24 17:01:13 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.782: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 24 17:08:23.782: INFO: metadata-proxy-v0.1-whmsq from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:08:23.782: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:08:23.782: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:08:23.782: INFO: volume-snapshot-controller-0 from kube-system started at 2021-02-24 17:01:18 +0000 UTC (1 container statuses recorded)
Feb 24 17:08:23.782: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-904000cc-5f0b-41eb-8ad4-8654f29a38f4 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.138.0.4 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.138.0.4 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb 24 17:08:34.910: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:34.910: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321
Feb 24 17:08:35.343: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.0.4:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:35.343: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321 UDP
Feb 24 17:08:35.748: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.0.4 54321] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:35.748: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb 24 17:08:41.151: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:41.151: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321
Feb 24 17:08:41.565: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.0.4:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:41.565: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321 UDP
Feb 24 17:08:41.933: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.0.4 54321] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:41.933: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb 24 17:08:47.289: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:47.289: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321
Feb 24 17:08:47.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.0.4:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:47.655: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321 UDP
Feb 24 17:08:48.019: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.0.4 54321] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:48.019: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb 24 17:08:53.403: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:53.403: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321
Feb 24 17:08:53.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.0.4:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:53.809: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321 UDP
Feb 24 17:08:54.176: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.0.4 54321] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:54.176: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb 24 17:08:59.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.138.0.4 http://127.0.0.1:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:59.549: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321
Feb 24 17:08:59.909: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.138.0.4:54321/hostname] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:08:59.909: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.138.0.4, port: 54321 UDP
Feb 24 17:09:00.289: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.138.0.4 54321] Namespace:sched-pred-3038 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:09:00.289: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: removing the label kubernetes.io/e2e-904000cc-5f0b-41eb-8ad4-8654f29a38f4 off the node bootstrap-e2e-minion-group-2qjz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-904000cc-5f0b-41eb-8ad4-8654f29a38f4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:09:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3038" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":18,"skipped":469,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:09:05.948: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-01eae1cf-872a-4e69-b5d1-b81d6e9b2288
STEP: Creating a pod to test consume configMaps
Feb 24 17:09:06.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6" in namespace "projected-3740" to be "Succeeded or Failed"
Feb 24 17:09:06.366: INFO: Pod "pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6": Phase="Pending", Reason="", readiness=false. Elapsed: 51.293029ms
Feb 24 17:09:08.418: INFO: Pod "pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.102984573s
STEP: Saw pod success
Feb 24 17:09:08.418: INFO: Pod "pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6" satisfied condition "Succeeded or Failed"
Feb 24 17:09:08.470: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:09:08.615: INFO: Waiting for pod pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6 to disappear
Feb 24 17:09:08.666: INFO: Pod pod-projected-configmaps-d3506dae-e18f-4864-9f5d-f540a44fc2a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:09:08.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3740" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":484,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:09:08.775: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 24 17:09:13.510: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:13.561: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 17:09:15.561: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:15.614: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 17:09:17.561: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:17.615: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 17:09:19.561: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:19.613: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 17:09:21.561: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:21.615: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 17:09:23.561: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 17:09:23.613: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:09:23.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2915" for this suite.
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":485,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:09:23.780: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:09:24.052: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 24 17:09:28.940: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1765 --namespace=crd-publish-openapi-1765 create -f -'
Feb 24 17:09:30.105: INFO: stderr: ""
Feb 24 17:09:30.105: INFO: stdout: "e2e-test-crd-publish-openapi-2741-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 17:09:30.105: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1765 --namespace=crd-publish-openapi-1765 delete e2e-test-crd-publish-openapi-2741-crds test-cr'
Feb 24 17:09:30.504: INFO: stderr: ""
Feb 24 17:09:30.504: INFO: stdout: "e2e-test-crd-publish-openapi-2741-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 24 17:09:30.504: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1765 --namespace=crd-publish-openapi-1765 apply -f -'
Feb 24 17:09:31.218: INFO: stderr: ""
Feb 24 17:09:31.218: INFO: stdout: "e2e-test-crd-publish-openapi-2741-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 17:09:31.218: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1765 --namespace=crd-publish-openapi-1765 delete e2e-test-crd-publish-openapi-2741-crds test-cr'
Feb 24 17:09:31.592: INFO: stderr: ""
Feb 24 17:09:31.592: INFO: stdout: "e2e-test-crd-publish-openapi-2741-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 24 17:09:31.592: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1765 explain e2e-test-crd-publish-openapi-2741-crds'
Feb 24 17:09:32.090: INFO: stderr: ""
Feb 24 17:09:32.090: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2741-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:09:36.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1765" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":21,"skipped":499,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:09:36.921: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:09:37.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301" in namespace "downward-api-858" to be "Succeeded or Failed"
Feb 24 17:09:37.331: INFO: Pod "downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301": Phase="Pending", Reason="", readiness=false. Elapsed: 57.424032ms
Feb 24 17:09:39.383: INFO: Pod "downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.109241365s
STEP: Saw pod success
Feb 24 17:09:39.383: INFO: Pod "downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301" satisfied condition "Succeeded or Failed"
Feb 24 17:09:39.434: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301 container client-container: <nil>
STEP: delete the pod
Feb 24 17:09:39.568: INFO: Waiting for pod downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301 to disappear
Feb 24 17:09:39.619: INFO: Pod downwardapi-volume-16e7b73c-ad76-428b-9d73-ea57070aa301 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:09:39.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-858" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":504,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:09:39.728: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 24 17:09:39.986: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:09:44.787: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:10:04.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6604" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":23,"skipped":510,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:10:04.320: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4aaacaf6-b9af-44ea-b2f3-fd55a031cf54 in namespace container-probe-968
Feb 24 17:10:06.745: INFO: Started pod liveness-4aaacaf6-b9af-44ea-b2f3-fd55a031cf54 in namespace container-probe-968
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 17:10:06.797: INFO: Initial restart count of pod liveness-4aaacaf6-b9af-44ea-b2f3-fd55a031cf54 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:07.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-968" for this suite.
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:07.435: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:14:07.713: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:16.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2014" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":25,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:17.088: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 24 17:14:21.945: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 17:14:22.002: INFO: Pod pod-with-poststart-http-hook still exists
Feb 24 17:14:24.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 17:14:24.060: INFO: Pod pod-with-poststart-http-hook still exists
Feb 24 17:14:26.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 17:14:26.058: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:26.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1477" for this suite.
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:26.182: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-9c627f7a-c77d-4f62-a937-113202f30c74
STEP: Creating a pod to test consume secrets
Feb 24 17:14:26.802: INFO: Waiting up to 5m0s for pod "pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be" in namespace "secrets-7956" to be "Succeeded or Failed"
Feb 24 17:14:26.857: INFO: Pod "pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be": Phase="Pending", Reason="", readiness=false. Elapsed: 54.927545ms
Feb 24 17:14:28.913: INFO: Pod "pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110648816s
STEP: Saw pod success
Feb 24 17:14:28.913: INFO: Pod "pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be" satisfied condition "Succeeded or Failed"
Feb 24 17:14:28.968: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:14:29.110: INFO: Waiting for pod pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be to disappear
Feb 24 17:14:29.165: INFO: Pod pod-secrets-ef81aebb-e1ff-4ee3-993c-7f39e583a3be no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:29.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7956" for this suite.
STEP: Destroying namespace "secret-namespace-2226" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":605,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:29.349: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8045
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 24 17:14:29.626: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 17:14:29.979: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 17:14:32.035: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:34.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:36.037: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:38.035: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:40.037: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:42.038: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:44.035: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:46.035: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:48.037: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:14:50.035: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 24 17:14:50.149: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 24 17:14:50.262: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 24 17:14:52.711: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 17:14:52.711: INFO: Going to poll 10.64.1.21 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:14:52.766: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.1.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8045 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:14:52.766: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:14:54.131: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 17:14:54.131: INFO: Going to poll 10.64.2.11 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:14:54.186: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.2.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8045 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:14:54.186: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:14:55.570: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 17:14:55.571: INFO: Going to poll 10.64.3.12 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:14:55.626: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.3.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8045 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:14:55.626: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:14:56.981: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:56.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8045" for this suite.
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:57.105: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-4e64fd13-e842-4df6-b384-1d2ea51e853e
STEP: Creating a pod to test consume secrets
Feb 24 17:14:57.499: INFO: Waiting up to 5m0s for pod "pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d" in namespace "secrets-3067" to be "Succeeded or Failed"
Feb 24 17:14:57.555: INFO: Pod "pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d": Phase="Pending", Reason="", readiness=false. Elapsed: 55.398872ms
Feb 24 17:14:59.610: INFO: Pod "pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110861127s
STEP: Saw pod success
Feb 24 17:14:59.610: INFO: Pod "pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d" satisfied condition "Succeeded or Failed"
Feb 24 17:14:59.665: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d container secret-env-test: <nil>
STEP: delete the pod
Feb 24 17:14:59.811: INFO: Waiting for pod pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d to disappear
Feb 24 17:14:59.866: INFO: Pod pod-secrets-e8e9da1d-7656-4c43-8758-3f058c7a169d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:14:59.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3067" for this suite.
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:14:59.987: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5222
STEP: creating service affinity-clusterip-transition in namespace services-5222
STEP: creating replication controller affinity-clusterip-transition in namespace services-5222
I0224 17:15:00.383949   10144 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5222, replica count: 3
I0224 17:15:03.484720   10144 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 17:15:06.485080   10144 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:15:06.596: INFO: Creating new exec pod
Feb 24 17:15:09.823: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5222 exec execpod-affinitykfjjt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Feb 24 17:15:10.506: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 24 17:15:10.506: INFO: stdout: ""
Feb 24 17:15:10.506: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5222 exec execpod-affinitykfjjt -- /bin/sh -x -c nc -zv -t -w 2 10.0.14.165 80'
Feb 24 17:15:11.119: INFO: stderr: "+ nc -zv -t -w 2 10.0.14.165 80\nConnection to 10.0.14.165 80 port [tcp/http] succeeded!\n"
Feb 24 17:15:11.119: INFO: stdout: ""
Feb 24 17:15:11.234: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5222 exec execpod-affinitykfjjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.165:80/ ; done'
Feb 24 17:15:11.942: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n"
Feb 24 17:15:11.942: INFO: stdout: "\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl"
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:11.942: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:41.943: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5222 exec execpod-affinitykfjjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.165:80/ ; done'
Feb 24 17:15:42.653: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n"
Feb 24 17:15:42.653: INFO: stdout: "\naffinity-clusterip-transition-48n4l\naffinity-clusterip-transition-48n4l\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-48n4l\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-hrtnl\naffinity-clusterip-transition-48n4l\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-48n4l\naffinity-clusterip-transition-bqtrs"
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-48n4l
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-48n4l
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-48n4l
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-hrtnl
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-48n4l
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-48n4l
Feb 24 17:15:42.653: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:42.760: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-5222 exec execpod-affinitykfjjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.165:80/ ; done'
Feb 24 17:15:43.426: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.165:80/\n"
Feb 24 17:15:43.426: INFO: stdout: "\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs\naffinity-clusterip-transition-bqtrs"
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Received response from host: affinity-clusterip-transition-bqtrs
Feb 24 17:15:43.426: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5222, will wait for the garbage collector to delete the pods
Feb 24 17:15:43.708: INFO: Deleting ReplicationController affinity-clusterip-transition took: 53.841723ms
Feb 24 17:15:44.508: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 800.26988ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:15:52.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5222" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":30,"skipped":691,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:15:52.593: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb 24 17:15:55.071: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8634 PodName:var-expansion-b2641194-f643-4a76-9f5c-33f80b5f9603 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:15:55.071: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: test for file in mounted path
Feb 24 17:15:55.480: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8634 PodName:var-expansion-b2641194-f643-4a76-9f5c-33f80b5f9603 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:15:55.480: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: updating the annotation value
Feb 24 17:15:56.481: INFO: Successfully updated pod "var-expansion-b2641194-f643-4a76-9f5c-33f80b5f9603"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb 24 17:15:56.532: INFO: Deleting pod "var-expansion-b2641194-f643-4a76-9f5c-33f80b5f9603" in namespace "var-expansion-8634"
Feb 24 17:15:56.586: INFO: Wait up to 5m0s for pod "var-expansion-b2641194-f643-4a76-9f5c-33f80b5f9603" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:16:32.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8634" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":31,"skipped":699,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:16:32.804: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:16:33.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8684" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":32,"skipped":699,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:16:33.523: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Feb 24 17:16:34.008: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.008: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.008: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.008: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.010: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.010: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.010: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:34.010: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 17:16:35.026: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 17:16:35.026: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 17:16:35.713: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Feb 24 17:16:35.832: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 0
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:35.882: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 2
Feb 24 17:16:35.883: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
STEP: listing Deployments
Feb 24 17:16:35.935: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Feb 24 17:16:36.049: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Feb 24 17:16:36.182: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.182: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.184: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.185: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.185: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.251: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 17:16:36.322: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Feb 24 17:16:37.991: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.991: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.991: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.991: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.992: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.992: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
Feb 24 17:16:37.995: INFO: observed Deployment test-deployment in namespace deployment-9476 with ReadyReplicas 1
STEP: deleting the Deployment
Feb 24 17:16:38.105: INFO: observed event type MODIFIED
Feb 24 17:16:38.105: INFO: observed event type MODIFIED
Feb 24 17:16:38.105: INFO: observed event type MODIFIED
Feb 24 17:16:38.106: INFO: observed event type MODIFIED
Feb 24 17:16:38.108: INFO: observed event type MODIFIED
Feb 24 17:16:38.108: INFO: observed event type MODIFIED
Feb 24 17:16:38.109: INFO: observed event type MODIFIED
Feb 24 17:16:38.109: INFO: observed event type MODIFIED
Feb 24 17:16:38.109: INFO: observed event type MODIFIED
Feb 24 17:16:38.109: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 17:16:38.163: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 24 17:16:38.216: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9476  62892f68-94fe-4f03-9a8e-b7871f46ae7c 3747 3 2021-02-24 17:16:36 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 9af36557-73f1-4ae3-9659-249af03aba75 0xc002064be7 0xc002064be8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9af36557-73f1-4ae3-9659-249af03aba75\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002064ce0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Feb 24 17:16:38.269: INFO: pod: "test-deployment-768947d6f5-7l2r5":
&Pod{ObjectMeta:{test-deployment-768947d6f5-7l2r5 test-deployment-768947d6f5- deployment-9476  6816b8d0-e1de-49fb-b2a4-556569adba85 3736 0 2021-02-24 17:16:36 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 62892f68-94fe-4f03-9a8e-b7871f46ae7c 0xc002065357 0xc002065358}] []  [{kube-controller-manager Update v1 2021-02-24 17:16:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62892f68-94fe-4f03-9a8e-b7871f46ae7c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:16:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9g87s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9g87s,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9g87s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.28,StartTime:2021-02-24 17:16:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:16:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6266b6a9e8ed310259f63c45e268f3b7b9714f30650bacaac76c2e8a985d2a77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 24 17:16:38.269: INFO: pod: "test-deployment-768947d6f5-qjws4":
&Pod{ObjectMeta:{test-deployment-768947d6f5-qjws4 test-deployment-768947d6f5- deployment-9476  6d6df1c7-a7e8-46a2-bd8d-09121681a4e5 3749 0 2021-02-24 17:16:37 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 62892f68-94fe-4f03-9a8e-b7871f46ae7c 0xc002065527 0xc002065528}] []  [{kube-controller-manager Update v1 2021-02-24 17:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62892f68-94fe-4f03-9a8e-b7871f46ae7c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:16:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9g87s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9g87s,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9g87s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:16:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 24 17:16:38.269: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9476  32d113c6-eb30-4867-ae91-8ee6d8fd272b 3750 4 2021-02-24 17:16:35 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 9af36557-73f1-4ae3-9659-249af03aba75 0xc002064d47 0xc002064d48}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:16:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9af36557-73f1-4ae3-9659-249af03aba75\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002064dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 24 17:16:38.321: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9476  da8575bc-2d5f-46c2-befa-377c91bc4512 3708 2 2021-02-24 17:16:33 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 9af36557-73f1-4ae3-9659-249af03aba75 0xc002064f37 0xc002064f38}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:16:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9af36557-73f1-4ae3-9659-249af03aba75\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002064fe0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Feb 24 17:16:38.374: INFO: pod: "test-deployment-8b6954bfb-gk7hs":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-gk7hs test-deployment-8b6954bfb- deployment-9476  9b6fb447-d30a-4000-a4c6-5cf27e19ec0e 3680 0 2021-02-24 17:16:33 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8b6954bfb da8575bc-2d5f-46c2-befa-377c91bc4512 0xc000b50af7 0xc000b50af8}] []  [{kube-controller-manager Update v1 2021-02-24 17:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da8575bc-2d5f-46c2-befa-377c91bc4512\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:16:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9g87s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9g87s,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9g87s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:16:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.14,StartTime:2021-02-24 17:16:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:16:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://0bc5a8ae8dc6b03102cdc6373ab76c060ebd8ff3279e9c57020c0edb196eae2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:16:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9476" for this suite.
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":33,"skipped":709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:16:38.491: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-8833/configmap-test-5a923d8f-b838-4b86-be90-bc737d82c013
STEP: Creating a pod to test consume configMaps
Feb 24 17:16:38.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333" in namespace "configmap-8833" to be "Succeeded or Failed"
Feb 24 17:16:38.939: INFO: Pod "pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333": Phase="Pending", Reason="", readiness=false. Elapsed: 51.407299ms
Feb 24 17:16:40.992: INFO: Pod "pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103984915s
STEP: Saw pod success
Feb 24 17:16:40.992: INFO: Pod "pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333" satisfied condition "Succeeded or Failed"
Feb 24 17:16:41.043: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333 container env-test: <nil>
STEP: delete the pod
Feb 24 17:16:41.179: INFO: Waiting for pod pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333 to disappear
Feb 24 17:16:41.230: INFO: Pod pod-configmaps-9b3be979-da4e-4136-be9d-7bb84d90e333 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:16:41.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8833" for this suite.
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":759,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:16:41.345: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:16:41.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4878" for this suite.
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":35,"skipped":775,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:16:41.827: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-wh8q
STEP: Creating a pod to test atomic-volume-subpath
Feb 24 17:16:42.246: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wh8q" in namespace "subpath-351" to be "Succeeded or Failed"
Feb 24 17:16:42.299: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Pending", Reason="", readiness=false. Elapsed: 53.114087ms
Feb 24 17:16:44.352: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.105761537s
Feb 24 17:16:46.407: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 4.160777656s
Feb 24 17:16:48.461: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 6.214650097s
Feb 24 17:16:50.515: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 8.268314629s
Feb 24 17:16:52.568: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 10.321721108s
Feb 24 17:16:54.621: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 12.374966068s
Feb 24 17:16:56.675: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 14.428535481s
Feb 24 17:16:58.734: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 16.487728781s
Feb 24 17:17:00.788: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 18.541145455s
Feb 24 17:17:02.841: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Running", Reason="", readiness=true. Elapsed: 20.594756488s
Feb 24 17:17:04.895: INFO: Pod "pod-subpath-test-projected-wh8q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.648337502s
STEP: Saw pod success
Feb 24 17:17:04.895: INFO: Pod "pod-subpath-test-projected-wh8q" satisfied condition "Succeeded or Failed"
Feb 24 17:17:04.948: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-subpath-test-projected-wh8q container test-container-subpath-projected-wh8q: <nil>
STEP: delete the pod
Feb 24 17:17:05.073: INFO: Waiting for pod pod-subpath-test-projected-wh8q to disappear
Feb 24 17:17:05.124: INFO: Pod pod-subpath-test-projected-wh8q no longer exists
STEP: Deleting pod pod-subpath-test-projected-wh8q
Feb 24 17:17:05.124: INFO: Deleting pod "pod-subpath-test-projected-wh8q" in namespace "subpath-351"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:05.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-351" for this suite.
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":36,"skipped":796,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:05.289: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Feb 24 17:17:05.571: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2810 api-versions'
Feb 24 17:17:05.887: INFO: stderr: ""
Feb 24 17:17:05.887: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:05.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2810" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":37,"skipped":808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:05.999: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:17:06.261: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:08.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5171" for this suite.
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":38,"skipped":839,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:08.763: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:17:11.180: INFO: Deleting pod "var-expansion-18bb74f5-f7d7-496a-82db-cfe90fbe69c1" in namespace "var-expansion-6819"
Feb 24 17:17:11.233: INFO: Wait up to 5m0s for pod "var-expansion-18bb74f5-f7d7-496a-82db-cfe90fbe69c1" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:13.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6819" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":39,"skipped":839,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:13.452: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 24 17:17:13.775: INFO: Waiting up to 5m0s for pod "pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f" in namespace "emptydir-3802" to be "Succeeded or Failed"
Feb 24 17:17:13.865: INFO: Pod "pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 90.751562ms
Feb 24 17:17:15.917: INFO: Pod "pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.14250097s
Feb 24 17:17:17.969: INFO: Pod "pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.194158725s
STEP: Saw pod success
Feb 24 17:17:17.969: INFO: Pod "pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f" satisfied condition "Succeeded or Failed"
Feb 24 17:17:18.020: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f container test-container: <nil>
STEP: delete the pod
Feb 24 17:17:18.139: INFO: Waiting for pod pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f to disappear
Feb 24 17:17:18.190: INFO: Pod pod-2e999235-961c-4d90-90aa-a61b4bb1bd7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:18.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3802" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":839,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:18.303: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:17:18.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de" in namespace "projected-5425" to be "Succeeded or Failed"
Feb 24 17:17:18.675: INFO: Pod "downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de": Phase="Pending", Reason="", readiness=false. Elapsed: 52.305731ms
Feb 24 17:17:20.727: INFO: Pod "downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105239653s
STEP: Saw pod success
Feb 24 17:17:20.727: INFO: Pod "downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de" satisfied condition "Succeeded or Failed"
Feb 24 17:17:20.780: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de container client-container: <nil>
STEP: delete the pod
Feb 24 17:17:20.904: INFO: Waiting for pod downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de to disappear
Feb 24 17:17:20.955: INFO: Pod downwardapi-volume-e278f5a2-cfce-4702-875f-940b43f5f6de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5425" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:21.069: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 24 17:17:21.381: INFO: Waiting up to 5m0s for pod "pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c" in namespace "emptydir-5152" to be "Succeeded or Failed"
Feb 24 17:17:21.433: INFO: Pod "pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c": Phase="Pending", Reason="", readiness=false. Elapsed: 51.490052ms
Feb 24 17:17:23.486: INFO: Pod "pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10447271s
STEP: Saw pod success
Feb 24 17:17:23.486: INFO: Pod "pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c" satisfied condition "Succeeded or Failed"
Feb 24 17:17:23.537: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c container test-container: <nil>
STEP: delete the pod
Feb 24 17:17:23.659: INFO: Waiting for pod pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c to disappear
Feb 24 17:17:23.712: INFO: Pod pod-251d2538-754f-4aa4-9544-6fec8fdd5e6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:23.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5152" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":878,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:23.825: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb 24 17:17:24.081: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 17:17:24.229: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 17:17:24.281: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-2qjz before test
Feb 24 17:17:24.343: INFO: coredns-6954c77b9b-dk8mk from kube-system started at 2021-02-24 17:01:15 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.343: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:17:24.343: INFO: kube-proxy-bootstrap-e2e-minion-group-2qjz from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.343: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:17:24.343: INFO: metadata-proxy-v0.1-mjnrs from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:17:24.343: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:17:24.343: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:17:24.343: INFO: pod-logs-websocket-91c564ca-6526-49cc-b067-1349d787cbbd from pods-5171 started at 2021-02-24 17:17:06 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.343: INFO: 	Container main ready: true, restart count 0
Feb 24 17:17:24.343: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-9mrc before test
Feb 24 17:17:24.403: INFO: coredns-6954c77b9b-9rzld from kube-system started at 2021-02-24 17:01:20 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.403: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:17:24.403: INFO: kube-proxy-bootstrap-e2e-minion-group-9mrc from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.403: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:17:24.403: INFO: metadata-proxy-v0.1-rgzg9 from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:17:24.403: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:17:24.403: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:17:24.403: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78 from kube-system started at 2021-02-24 17:01:37 +0000 UTC (2 container statuses recorded)
Feb 24 17:17:24.403: INFO: 	Container metrics-server ready: true, restart count 0
Feb 24 17:17:24.403: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 24 17:17:24.403: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-bthl before test
Feb 24 17:17:24.463: INFO: kube-dns-autoscaler-76478fcf46-gsgnb from kube-system started at 2021-02-24 17:01:17 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.463: INFO: 	Container autoscaler ready: true, restart count 0
Feb 24 17:17:24.463: INFO: kube-proxy-bootstrap-e2e-minion-group-bthl from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.463: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:17:24.463: INFO: l7-default-backend-6f8dd4f4d5-c7mgv from kube-system started at 2021-02-24 17:01:13 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.463: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 24 17:17:24.463: INFO: metadata-proxy-v0.1-whmsq from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:17:24.463: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:17:24.463: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:17:24.463: INFO: volume-snapshot-controller-0 from kube-system started at 2021-02-24 17:01:18 +0000 UTC (1 container statuses recorded)
Feb 24 17:17:24.463: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1666be9098dd86d7], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:25.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9043" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":43,"skipped":881,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:25.892: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:17:26.160: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:26.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7168" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":44,"skipped":891,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:26.729: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 24 17:17:26.987: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:17:31.442: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:17:51.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-603" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":45,"skipped":896,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:17:51.487: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-zgl8
STEP: Creating a pod to test atomic-volume-subpath
Feb 24 17:17:51.913: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zgl8" in namespace "subpath-1283" to be "Succeeded or Failed"
Feb 24 17:17:51.970: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Pending", Reason="", readiness=false. Elapsed: 57.480267ms
Feb 24 17:17:54.024: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.111316142s
Feb 24 17:17:56.078: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.165337117s
Feb 24 17:17:58.131: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.218781336s
Feb 24 17:18:00.185: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.271965718s
Feb 24 17:18:02.238: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.325644545s
Feb 24 17:18:04.293: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.380762591s
Feb 24 17:18:06.347: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.4341416s
Feb 24 17:18:08.401: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.488210536s
Feb 24 17:18:10.454: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.541431874s
Feb 24 17:18:12.507: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.594367989s
Feb 24 17:18:14.559: INFO: Pod "pod-subpath-test-configmap-zgl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.646287055s
STEP: Saw pod success
Feb 24 17:18:14.559: INFO: Pod "pod-subpath-test-configmap-zgl8" satisfied condition "Succeeded or Failed"
Feb 24 17:18:14.612: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-subpath-test-configmap-zgl8 container test-container-subpath-configmap-zgl8: <nil>
STEP: delete the pod
Feb 24 17:18:14.728: INFO: Waiting for pod pod-subpath-test-configmap-zgl8 to disappear
Feb 24 17:18:14.780: INFO: Pod pod-subpath-test-configmap-zgl8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zgl8
Feb 24 17:18:14.780: INFO: Deleting pod "pod-subpath-test-configmap-zgl8" in namespace "subpath-1283"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1283" for this suite.
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":46,"skipped":904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:14.945: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:17.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4137" for this suite.
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":957,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:17.633: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 24 17:18:20.153: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:20.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9070" for this suite.
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":959,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:20.380: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Feb 24 17:18:21.403: INFO: created pod pod-service-account-defaultsa
Feb 24 17:18:21.403: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 24 17:18:21.456: INFO: created pod pod-service-account-mountsa
Feb 24 17:18:21.456: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 24 17:18:21.510: INFO: created pod pod-service-account-nomountsa
Feb 24 17:18:21.510: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 24 17:18:21.562: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 24 17:18:21.563: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 24 17:18:21.615: INFO: created pod pod-service-account-mountsa-mountspec
Feb 24 17:18:21.615: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 24 17:18:21.668: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 24 17:18:21.668: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 24 17:18:21.727: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 24 17:18:21.727: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 24 17:18:21.781: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 24 17:18:21.781: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 24 17:18:21.834: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 24 17:18:21.834: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:21.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4242" for this suite.
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":49,"skipped":967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:21.948: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 24 17:18:24.570: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-334" for this suite.
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":50,"skipped":1013,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:24.854: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:27.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7191" for this suite.
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":1024,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:27.490: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Feb 24 17:18:27.908: INFO: observed Pod pod-test in namespace pods-6788 in phase Pending conditions []
Feb 24 17:18:27.908: INFO: observed Pod pod-test in namespace pods-6788 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:18:27 +0000 UTC  }]
Feb 24 17:18:27.910: INFO: observed Pod pod-test in namespace pods-6788 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:18:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:18:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:18:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:18:27 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Feb 24 17:18:29.375: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Feb 24 17:18:29.648: INFO: observed event type ADDED
Feb 24 17:18:29.648: INFO: observed event type MODIFIED
Feb 24 17:18:29.648: INFO: observed event type MODIFIED
Feb 24 17:18:29.648: INFO: observed event type MODIFIED
Feb 24 17:18:29.650: INFO: observed event type MODIFIED
Feb 24 17:18:29.650: INFO: observed event type MODIFIED
Feb 24 17:18:29.652: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:29.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6788" for this suite.
{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":52,"skipped":1038,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:29.764: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-be80e18a-ac6a-4e41-8cf9-e4fddd535be7
STEP: Creating secret with name secret-projected-all-test-volume-f89c7ce1-b864-4161-bcce-f278d864b575
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 24 17:18:30.192: INFO: Waiting up to 5m0s for pod "projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d" in namespace "projected-4635" to be "Succeeded or Failed"
Feb 24 17:18:30.243: INFO: Pod "projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d": Phase="Pending", Reason="", readiness=false. Elapsed: 51.341118ms
Feb 24 17:18:32.305: INFO: Pod "projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.112779744s
STEP: Saw pod success
Feb 24 17:18:32.305: INFO: Pod "projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d" satisfied condition "Succeeded or Failed"
Feb 24 17:18:32.359: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 24 17:18:32.482: INFO: Waiting for pod projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d to disappear
Feb 24 17:18:32.536: INFO: Pod projected-volume-c3c026a0-0dfd-43c0-b71d-9a129642a94d no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:32.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4635" for this suite.
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":53,"skipped":1057,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:32.658: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f8e4fdde-7097-4237-bc9b-dbe24bd3f347
STEP: Creating a pod to test consume secrets
Feb 24 17:18:33.062: INFO: Waiting up to 5m0s for pod "pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2" in namespace "secrets-7385" to be "Succeeded or Failed"
Feb 24 17:18:33.114: INFO: Pod "pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2": Phase="Pending", Reason="", readiness=false. Elapsed: 51.187844ms
Feb 24 17:18:35.166: INFO: Pod "pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103354185s
STEP: Saw pod success
Feb 24 17:18:35.166: INFO: Pod "pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2" satisfied condition "Succeeded or Failed"
Feb 24 17:18:35.220: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2 container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:18:35.361: INFO: Waiting for pod pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2 to disappear
Feb 24 17:18:35.412: INFO: Pod pod-secrets-94f8ef25-7bb9-4a6e-886e-e4b67421b1e2 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:35.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7385" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":1074,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:35.526: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Feb 24 17:18:35.869: INFO: created test-event-1
Feb 24 17:18:35.921: INFO: created test-event-2
Feb 24 17:18:35.972: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb 24 17:18:36.024: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb 24 17:18:36.084: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:36.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4083" for this suite.
{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":55,"skipped":1075,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:36.332: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1086.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1086.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 17:18:47.241: INFO: DNS probes using dns-1086/dns-test-c6e4c269-21b9-4f7f-b4ec-29dfc248ca16 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:18:47.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1086" for this suite.
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":56,"skipped":1089,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:18:47.418: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:18:47.998: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 24 17:18:48.126: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:48.180: INFO: Number of nodes with available pods: 0
Feb 24 17:18:48.180: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 17:18:49.242: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:49.296: INFO: Number of nodes with available pods: 1
Feb 24 17:18:49.296: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 17:18:50.241: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:50.295: INFO: Number of nodes with available pods: 3
Feb 24 17:18:50.295: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 24 17:18:50.672: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:50.672: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:50.672: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:50.733: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:51.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:51.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:51.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:51.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:52.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:52.787: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:52.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:52.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:52.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:53.786: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:53.786: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:53.786: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:53.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:53.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:54.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:54.787: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:54.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:54.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:54.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:55.786: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:55.786: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:55.786: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:55.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:55.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:56.786: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:56.786: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:56.786: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:56.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:56.845: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:57.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:57.788: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:57.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:57.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:57.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:58.788: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:58.788: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:58.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:58.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:58.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:18:59.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:59.787: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:18:59.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:59.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:18:59.844: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:00.788: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:00.788: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:19:00.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:00.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:00.855: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:01.787: INFO: Wrong image for pod: daemon-set-kr4mh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:01.787: INFO: Pod daemon-set-kr4mh is not available
Feb 24 17:19:01.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:01.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:01.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:02.786: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:02.786: INFO: Pod daemon-set-n8vl4 is not available
Feb 24 17:19:02.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:02.846: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:03.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:03.787: INFO: Pod daemon-set-n8vl4 is not available
Feb 24 17:19:03.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:03.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:04.786: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:04.786: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:04.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:04.846: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:05.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:05.787: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:05.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:05.850: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:06.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:06.788: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:06.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:06.846: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:07.790: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:07.790: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:07.790: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:07.852: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:08.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:08.787: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:08.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:08.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:09.787: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:09.787: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:09.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:09.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:10.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:10.788: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:10.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:10.857: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:11.788: INFO: Wrong image for pod: daemon-set-lcbcz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:11.788: INFO: Pod daemon-set-lcbcz is not available
Feb 24 17:19:11.788: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:11.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:12.786: INFO: Pod daemon-set-86pw6 is not available
Feb 24 17:19:12.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:12.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:13.787: INFO: Pod daemon-set-86pw6 is not available
Feb 24 17:19:13.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:13.849: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:14.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:14.846: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:15.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:15.787: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:15.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:16.805: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:16.805: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:16.865: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:17.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:17.786: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:17.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:18.786: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:18.786: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:18.845: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:19.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:19.787: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:19.847: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:20.790: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:20.790: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:20.850: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:21.787: INFO: Wrong image for pod: daemon-set-vlk2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb 24 17:19:21.787: INFO: Pod daemon-set-vlk2j is not available
Feb 24 17:19:21.848: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:22.787: INFO: Pod daemon-set-qdjgz is not available
Feb 24 17:19:22.849: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 24 17:19:22.909: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:22.963: INFO: Number of nodes with available pods: 2
Feb 24 17:19:22.963: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 17:19:24.025: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:19:24.078: INFO: Number of nodes with available pods: 3
Feb 24 17:19:24.078: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8005, will wait for the garbage collector to delete the pods
Feb 24 17:19:24.543: INFO: Deleting DaemonSet.extensions daemon-set took: 53.812612ms
Feb 24 17:19:25.343: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.246431ms
Feb 24 17:19:32.395: INFO: Number of nodes with available pods: 0
Feb 24 17:19:32.395: INFO: Number of running nodes: 0, number of available pods: 0
Feb 24 17:19:32.446: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4737"},"items":null}

Feb 24 17:19:32.497: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4737"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:32.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8005" for this suite.
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":57,"skipped":1102,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:32.827: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb 24 17:19:33.083: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 17:19:33.201: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 17:19:33.253: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-2qjz before test
Feb 24 17:19:33.309: INFO: coredns-6954c77b9b-dk8mk from kube-system started at 2021-02-24 17:01:15 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.309: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:19:33.309: INFO: kube-proxy-bootstrap-e2e-minion-group-2qjz from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.309: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:19:33.309: INFO: metadata-proxy-v0.1-mjnrs from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:19:33.309: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:19:33.309: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:19:33.309: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-9mrc before test
Feb 24 17:19:33.371: INFO: coredns-6954c77b9b-9rzld from kube-system started at 2021-02-24 17:01:20 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.371: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:19:33.371: INFO: kube-proxy-bootstrap-e2e-minion-group-9mrc from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.371: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:19:33.371: INFO: metadata-proxy-v0.1-rgzg9 from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:19:33.371: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:19:33.371: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:19:33.371: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78 from kube-system started at 2021-02-24 17:01:37 +0000 UTC (2 container statuses recorded)
Feb 24 17:19:33.371: INFO: 	Container metrics-server ready: true, restart count 0
Feb 24 17:19:33.371: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 24 17:19:33.371: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-bthl before test
Feb 24 17:19:33.429: INFO: kube-dns-autoscaler-76478fcf46-gsgnb from kube-system started at 2021-02-24 17:01:17 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.429: INFO: 	Container autoscaler ready: true, restart count 0
Feb 24 17:19:33.429: INFO: kube-proxy-bootstrap-e2e-minion-group-bthl from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.429: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:19:33.429: INFO: l7-default-backend-6f8dd4f4d5-c7mgv from kube-system started at 2021-02-24 17:01:13 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.429: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 24 17:19:33.429: INFO: metadata-proxy-v0.1-whmsq from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:19:33.429: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:19:33.429: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:19:33.429: INFO: volume-snapshot-controller-0 from kube-system started at 2021-02-24 17:01:18 +0000 UTC (1 container statuses recorded)
Feb 24 17:19:33.429: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d8af365d-f339-4a6b-ba0a-279752b0194b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d8af365d-f339-4a6b-ba0a-279752b0194b off the node bootstrap-e2e-minion-group-2qjz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d8af365d-f339-4a6b-ba0a-279752b0194b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:38.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7027" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":58,"skipped":1104,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:38.341: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 24 17:19:41.383: INFO: Successfully updated pod "adopt-release-fsb6b"
STEP: Checking that the Job readopts the Pod
Feb 24 17:19:41.383: INFO: Waiting up to 15m0s for pod "adopt-release-fsb6b" in namespace "job-3362" to be "adopted"
Feb 24 17:19:41.436: INFO: Pod "adopt-release-fsb6b": Phase="Running", Reason="", readiness=true. Elapsed: 52.76253ms
Feb 24 17:19:41.436: INFO: Pod "adopt-release-fsb6b" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 24 17:19:42.050: INFO: Successfully updated pod "adopt-release-fsb6b"
STEP: Checking that the Job releases the Pod
Feb 24 17:19:42.050: INFO: Waiting up to 15m0s for pod "adopt-release-fsb6b" in namespace "job-3362" to be "released"
Feb 24 17:19:42.102: INFO: Pod "adopt-release-fsb6b": Phase="Running", Reason="", readiness=true. Elapsed: 51.385092ms
Feb 24 17:19:42.102: INFO: Pod "adopt-release-fsb6b" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:42.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3362" for this suite.
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":59,"skipped":1114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:42.215: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:43.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6781" for this suite.
{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":60,"skipped":1157,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:43.441: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ee10f916-b49a-461c-b0d8-9b216bc9981d
STEP: Creating a pod to test consume secrets
Feb 24 17:19:43.834: INFO: Waiting up to 5m0s for pod "pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a" in namespace "secrets-5517" to be "Succeeded or Failed"
Feb 24 17:19:43.895: INFO: Pod "pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 60.200775ms
Feb 24 17:19:45.946: INFO: Pod "pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11189253s
STEP: Saw pod success
Feb 24 17:19:45.946: INFO: Pod "pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a" satisfied condition "Succeeded or Failed"
Feb 24 17:19:45.997: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:19:46.123: INFO: Waiting for pod pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a to disappear
Feb 24 17:19:46.187: INFO: Pod pod-secrets-9d0b7071-3d9b-4e5c-945c-3c4b5c575f3a no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:46.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5517" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:46.300: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:48.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-817" for this suite.
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1220,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:48.896: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-51e33b7a-d091-4949-b160-4ac7fe4f8244
STEP: Creating a pod to test consume secrets
Feb 24 17:19:49.262: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54" in namespace "projected-7910" to be "Succeeded or Failed"
Feb 24 17:19:49.313: INFO: Pod "pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54": Phase="Pending", Reason="", readiness=false. Elapsed: 51.106666ms
Feb 24 17:19:51.367: INFO: Pod "pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105161435s
STEP: Saw pod success
Feb 24 17:19:51.367: INFO: Pod "pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54" satisfied condition "Succeeded or Failed"
Feb 24 17:19:51.421: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:19:51.549: INFO: Waiting for pod pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54 to disappear
Feb 24 17:19:51.600: INFO: Pod pod-projected-secrets-46fb9aef-8b20-4940-99bb-e6267c0eca54 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:19:51.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7910" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1229,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:19:51.712: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6610
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6610
STEP: creating replication controller externalsvc in namespace services-6610
I0224 17:19:52.145012   10144 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6610, replica count: 2
I0224 17:19:55.245433   10144 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 24 17:19:55.414: INFO: Creating new exec pod
Feb 24 17:19:57.572: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6610 exec execpodb28cz -- /bin/sh -x -c nslookup nodeport-service.services-6610.svc.cluster.local'
Feb 24 17:19:58.698: INFO: stderr: "+ nslookup nodeport-service.services-6610.svc.cluster.local\n"
Feb 24 17:19:58.698: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-6610.svc.cluster.local\tcanonical name = externalsvc.services-6610.svc.cluster.local.\nName:\texternalsvc.services-6610.svc.cluster.local\nAddress: 10.0.142.221\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6610, will wait for the garbage collector to delete the pods
Feb 24 17:19:58.904: INFO: Deleting ReplicationController externalsvc took: 54.24868ms
Feb 24 17:19:59.004: INFO: Terminating ReplicationController externalsvc pods took: 100.278673ms
Feb 24 17:20:12.296: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:20:12.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6610" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":64,"skipped":1244,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:20:12.496: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2877
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2877
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2877
Feb 24 17:20:12.977: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 24 17:20:23.032: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 24 17:20:23.085: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:20:23.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:20:23.737: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:20:23.737: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:20:23.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 17:20:33.845: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:20:33.845: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:20:34.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999442s
Feb 24 17:20:35.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.94831415s
Feb 24 17:20:36.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.894770616s
Feb 24 17:20:37.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.841802185s
Feb 24 17:20:38.298: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.758498009s
Feb 24 17:20:39.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.704985442s
Feb 24 17:20:40.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.651966825s
Feb 24 17:20:41.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.59860127s
Feb 24 17:20:42.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.541635194s
Feb 24 17:20:43.568: INFO: Verifying statefulset ss doesn't scale past 1 for another 487.83486ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2877
Feb 24 17:20:44.621: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:20:45.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 17:20:45.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:20:45.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:20:45.319: INFO: Found 1 stateful pods, waiting for 3
Feb 24 17:20:55.374: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:20:55.374: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:20:55.374: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 24 17:20:55.485: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:20:56.092: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:20:56.092: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:20:56.092: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:20:56.092: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:20:56.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:20:56.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:20:56.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:20:56.720: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:20:57.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:20:57.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:20:57.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:20:57.406: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:20:57.457: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 24 17:21:07.564: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:21:07.564: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:21:07.564: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:21:07.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999524s
Feb 24 17:21:08.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.941990319s
Feb 24 17:21:09.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.888243333s
Feb 24 17:21:10.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.834543601s
Feb 24 17:21:11.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.782111075s
Feb 24 17:21:12.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.726385566s
Feb 24 17:21:14.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.67095942s
Feb 24 17:21:15.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.615168431s
Feb 24 17:21:16.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.561121605s
Feb 24 17:21:17.218: INFO: Verifying statefulset ss doesn't scale past 3 for another 505.6051ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2877
Feb 24 17:21:18.274: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:18.972: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 17:21:18.972: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:21:18.972: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:21:18.972: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:19.660: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 17:21:19.660: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:21:19.660: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:21:19.660: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:20.101: INFO: rc: 1
Feb 24 17:21:20.101: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb 24 17:21:30.102: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:30.585: INFO: rc: 1
Feb 24 17:21:30.585: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb 24 17:21:40.585: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:40.898: INFO: rc: 1
Feb 24 17:21:40.898: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:21:50.898: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:21:51.216: INFO: rc: 1
Feb 24 17:21:51.216: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:01.216: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:01.559: INFO: rc: 1
Feb 24 17:22:01.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:11.560: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:11.851: INFO: rc: 1
Feb 24 17:22:11.851: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:21.852: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:22.140: INFO: rc: 1
Feb 24 17:22:22.140: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:32.141: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:32.449: INFO: rc: 1
Feb 24 17:22:32.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:42.449: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:42.747: INFO: rc: 1
Feb 24 17:22:42.748: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:22:52.748: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:22:53.043: INFO: rc: 1
Feb 24 17:22:53.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:03.044: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:03.343: INFO: rc: 1
Feb 24 17:23:03.343: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:13.343: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:13.678: INFO: rc: 1
Feb 24 17:23:13.678: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:23.679: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:23.962: INFO: rc: 1
Feb 24 17:23:23.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:33.962: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:34.242: INFO: rc: 1
Feb 24 17:23:34.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:44.243: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:44.568: INFO: rc: 1
Feb 24 17:23:44.569: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:23:54.569: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:23:54.885: INFO: rc: 1
Feb 24 17:23:54.885: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:04.885: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:05.185: INFO: rc: 1
Feb 24 17:24:05.185: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:15.186: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:15.529: INFO: rc: 1
Feb 24 17:24:15.530: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:25.530: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:25.844: INFO: rc: 1
Feb 24 17:24:25.844: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:35.844: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:36.151: INFO: rc: 1
Feb 24 17:24:36.151: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:46.152: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:46.464: INFO: rc: 1
Feb 24 17:24:46.464: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:24:56.465: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:24:56.762: INFO: rc: 1
Feb 24 17:24:56.762: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:06.763: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:07.054: INFO: rc: 1
Feb 24 17:25:07.055: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:17.055: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:17.397: INFO: rc: 1
Feb 24 17:25:17.397: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:27.397: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:27.732: INFO: rc: 1
Feb 24 17:25:27.732: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:37.732: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:38.081: INFO: rc: 1
Feb 24 17:25:38.081: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:48.082: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:48.399: INFO: rc: 1
Feb 24 17:25:48.399: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:25:58.399: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:25:58.707: INFO: rc: 1
Feb 24 17:25:58.707: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:26:08.708: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:26:09.042: INFO: rc: 1
Feb 24 17:26:09.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:26:19.043: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:26:19.364: INFO: rc: 1
Feb 24 17:26:19.364: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 24 17:26:29.364: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-2877 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:26:29.734: INFO: rc: 1
Feb 24 17:26:29.734: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Feb 24 17:26:29.734: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 17:26:29.997: INFO: Deleting all statefulset in ns statefulset-2877
Feb 24 17:26:30.048: INFO: Scaling statefulset ss to 0
Feb 24 17:26:30.204: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:26:30.255: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:26:30.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2877" for this suite.

 [SLOW TEST:378.085 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":65,"skipped":1263,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:26:30.582: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-252ce0bb-5d4c-4250-b3fb-95750067d099
STEP: Creating secret with name s-test-opt-upd-29df0c75-656c-44d5-b0d1-ea07fb06d67d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-252ce0bb-5d4c-4250-b3fb-95750067d099
STEP: Updating secret s-test-opt-upd-29df0c75-656c-44d5-b0d1-ea07fb06d67d
STEP: Creating secret with name s-test-opt-create-11f01cb6-5a9b-447d-99c8-23531a5f2edb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:26:35.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2566" for this suite.
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:26:35.932: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-7384
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7384
STEP: Deleting pre-stop pod
Feb 24 17:26:45.664: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:26:45.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7384" for this suite.
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":67,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:26:45.899: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:26:46.229: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce" in namespace "downward-api-7125" to be "Succeeded or Failed"
Feb 24 17:26:46.281: INFO: Pod "downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce": Phase="Pending", Reason="", readiness=false. Elapsed: 52.588941ms
Feb 24 17:26:48.333: INFO: Pod "downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104142587s
STEP: Saw pod success
Feb 24 17:26:48.333: INFO: Pod "downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce" satisfied condition "Succeeded or Failed"
Feb 24 17:26:48.384: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce container client-container: <nil>
STEP: delete the pod
Feb 24 17:26:48.499: INFO: Waiting for pod downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce to disappear
Feb 24 17:26:48.550: INFO: Pod downwardapi-volume-b23cab91-17e1-4ef7-ab00-687dd126dbce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:26:48.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7125" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":1337,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:26:48.661: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 24 17:26:48.979: INFO: Waiting up to 5m0s for pod "pod-14bac749-6a67-4774-9b19-256a6d84e4f6" in namespace "emptydir-5791" to be "Succeeded or Failed"
Feb 24 17:26:49.030: INFO: Pod "pod-14bac749-6a67-4774-9b19-256a6d84e4f6": Phase="Pending", Reason="", readiness=false. Elapsed: 51.037986ms
Feb 24 17:26:51.082: INFO: Pod "pod-14bac749-6a67-4774-9b19-256a6d84e4f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103539971s
STEP: Saw pod success
Feb 24 17:26:51.082: INFO: Pod "pod-14bac749-6a67-4774-9b19-256a6d84e4f6" satisfied condition "Succeeded or Failed"
Feb 24 17:26:51.134: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-14bac749-6a67-4774-9b19-256a6d84e4f6 container test-container: <nil>
STEP: delete the pod
Feb 24 17:26:51.247: INFO: Waiting for pod pod-14bac749-6a67-4774-9b19-256a6d84e4f6 to disappear
Feb 24 17:26:51.298: INFO: Pod pod-14bac749-6a67-4774-9b19-256a6d84e4f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:26:51.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5791" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1351,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:26:51.410: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-bdx4
STEP: Creating a pod to test atomic-volume-subpath
Feb 24 17:26:51.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bdx4" in namespace "subpath-4913" to be "Succeeded or Failed"
Feb 24 17:26:51.888: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.902771ms
Feb 24 17:26:53.944: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 2.10931249s
Feb 24 17:26:56.016: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 4.18108169s
Feb 24 17:26:58.069: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 6.234045557s
Feb 24 17:27:00.122: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 8.2872569s
Feb 24 17:27:02.174: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 10.339092032s
Feb 24 17:27:04.228: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 12.392563126s
Feb 24 17:27:06.282: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 14.447118225s
Feb 24 17:27:08.336: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 16.500687256s
Feb 24 17:27:10.388: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 18.553355659s
Feb 24 17:27:12.442: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Running", Reason="", readiness=true. Elapsed: 20.606837241s
Feb 24 17:27:14.495: INFO: Pod "pod-subpath-test-secret-bdx4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.660056794s
STEP: Saw pod success
Feb 24 17:27:14.495: INFO: Pod "pod-subpath-test-secret-bdx4" satisfied condition "Succeeded or Failed"
Feb 24 17:27:14.547: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-subpath-test-secret-bdx4 container test-container-subpath-secret-bdx4: <nil>
STEP: delete the pod
Feb 24 17:27:14.662: INFO: Waiting for pod pod-subpath-test-secret-bdx4 to disappear
Feb 24 17:27:14.713: INFO: Pod pod-subpath-test-secret-bdx4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-bdx4
Feb 24 17:27:14.713: INFO: Deleting pod "pod-subpath-test-secret-bdx4" in namespace "subpath-4913"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:14.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4913" for this suite.
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":70,"skipped":1357,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:14.879: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 24 17:27:19.668: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:19.724: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:21.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:21.777: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:23.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:23.778: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:25.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:25.778: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:27.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:27.778: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:29.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:29.776: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:31.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:31.776: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 17:27:33.725: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 17:27:33.776: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:33.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7198" for this suite.
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1376,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:33.897: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Feb 24 17:27:34.156: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2983 proxy --unix-socket=/tmp/kubectl-proxy-unix901257391/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:34.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2983" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":72,"skipped":1394,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:34.440: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:27:36.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784456, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784456, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784456, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784456, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:27:39.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:27:39.379: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-902-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:40.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6854" for this suite.
STEP: Destroying namespace "webhook-6854-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":73,"skipped":1403,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:40.711: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Feb 24 17:27:41.021: INFO: Major version: 1
STEP: Confirm minor version
Feb 24 17:27:41.021: INFO: cleanMinorVersion: 20
Feb 24 17:27:41.021: INFO: Minor version: 20+
[AfterEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:41.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8657" for this suite.
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":74,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:41.131: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:27:41.390: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 create -f -'
Feb 24 17:27:42.293: INFO: stderr: ""
Feb 24 17:27:42.293: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 24 17:27:42.293: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 create -f -'
Feb 24 17:27:42.850: INFO: stderr: ""
Feb 24 17:27:42.850: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 24 17:27:43.902: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 17:27:43.902: INFO: Found 0 / 1
Feb 24 17:27:44.908: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 17:27:44.908: INFO: Found 1 / 1
Feb 24 17:27:44.908: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 17:27:44.959: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 17:27:44.960: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 17:27:44.960: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 describe pod agnhost-primary-vb4nj'
Feb 24 17:27:45.435: INFO: stderr: ""
Feb 24 17:27:45.435: INFO: stdout: "Name:         agnhost-primary-vb4nj\nNamespace:    kubectl-9709\nPriority:     0\nNode:         bootstrap-e2e-minion-group-2qjz/10.138.0.4\nStart Time:   Wed, 24 Feb 2021 17:27:42 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.64.1.50\nIPs:\n  IP:           10.64.1.50\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://30e786eba9b58d0180d82520a2459f129355c99ceda71b923f8d28f5e195449f\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 24 Feb 2021 17:27:44 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-9h4lp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-9h4lp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-9h4lp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    3s    default-scheduler  Successfully assigned kubectl-9709/agnhost-primary-vb4nj to bootstrap-e2e-minion-group-2qjz\n  Warning  FailedMount  2s    kubelet            MountVolume.SetUp failed for volume \"default-token-9h4lp\" : failed to sync secret cache: timed out waiting for the condition\n  Normal   Pulled       1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal   Created      1s    kubelet            Created container agnhost-primary\n  Normal   Started      1s    kubelet            Started container agnhost-primary\n"
Feb 24 17:27:45.435: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 describe rc agnhost-primary'
Feb 24 17:27:46.005: INFO: stderr: ""
Feb 24 17:27:46.005: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9709\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-vb4nj\n"
Feb 24 17:27:46.005: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 describe service agnhost-primary'
Feb 24 17:27:46.481: INFO: stderr: ""
Feb 24 17:27:46.481: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9709\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.0.149.111\nIPs:               10.0.149.111\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.64.1.50:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 24 17:27:46.541: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 describe node bootstrap-e2e-master'
Feb 24 17:27:47.177: INFO: stderr: ""
Feb 24 17:27:47.178: INFO: stdout: "Name:               bootstrap-e2e-master\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-1\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/metadata-proxy-ready=true\n                    failure-domain.beta.kubernetes.io/region=us-west1\n                    failure-domain.beta.kubernetes.io/zone=us-west1-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=bootstrap-e2e-master\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=n1-standard-1\n                    topology.kubernetes.io/region=us-west1\n                    topology.kubernetes.io/zone=us-west1-b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 24 Feb 2021 17:00:42 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\n                    node.kubernetes.io/unschedulable:NoSchedule\nUnschedulable:      true\nLease:\n  HolderIdentity:  bootstrap-e2e-master\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 24 Feb 2021 17:27:38 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 24 Feb 2021 17:00:59 +0000   Wed, 24 Feb 2021 17:00:59 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 24 Feb 2021 17:26:58 +0000   Wed, 24 Feb 2021 17:00:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 24 Feb 2021 17:26:58 +0000   Wed, 24 Feb 2021 17:00:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 24 Feb 2021 17:26:58 +0000   Wed, 24 Feb 2021 17:00:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 24 Feb 2021 17:26:58 +0000   Wed, 24 Feb 2021 17:00:53 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.138.0.2\n  ExternalIP:   34.83.69.28\n  InternalDNS:  bootstrap-e2e-master.c.k8s-infra-e2e-boskos-014.internal\n  Hostname:     bootstrap-e2e-master.c.k8s-infra-e2e-boskos-014.internal\nCapacity:\n  attachable-volumes-gce-pd:  127\n  cpu:                        1\n  ephemeral-storage:          16293736Ki\n  hugepages-2Mi:              0\n  memory:                     3776224Ki\n  pods:                       110\nAllocatable:\n  attachable-volumes-gce-pd:  127\n  cpu:                        1\n  ephemeral-storage:          15016307073\n  hugepages-2Mi:              0\n  memory:                     3520224Ki\n  pods:                       110\nSystem Info:\n  Machine ID:                 114ded2427802f64834ef3b1f72530c1\n  System UUID:                114ded24-2780-2f64-834e-f3b1f72530c1\n  Boot ID:                    23e05a6c-aab8-49ba-a2e3-ceb9a95c2abd\n  Kernel Version:             5.4.49+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.1\n  Kubelet Version:            v1.20.5-rc.0.10+165e5664b0e65d\n  Kube-Proxy Version:         v1.20.5-rc.0.10+165e5664b0e65d\nPodCIDR:                      10.64.0.0/24\nPodCIDRs:                     10.64.0.0/24\nProviderID:                   gce://k8s-infra-e2e-boskos-014/us-west1-b/bootstrap-e2e-master\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                            ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-server-bootstrap-e2e-master                200m (20%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 etcd-server-events-bootstrap-e2e-master         100m (10%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-addon-manager-bootstrap-e2e-master         5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         27m\n  kube-system                 kube-apiserver-bootstrap-e2e-master             250m (25%)    0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-controller-manager-bootstrap-e2e-master    200m (20%)    0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                 kube-scheduler-bootstrap-e2e-master             75m (7%)      0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 l7-lb-controller-bootstrap-e2e-master           10m (1%)      0 (0%)      50Mi (1%)        0 (0%)         26m\n  kube-system                 metadata-proxy-v0.1-tts24                       32m (3%)      32m (3%)    45Mi (1%)        45Mi (1%)      27m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        872m (87%)  32m (3%)\n  memory                     145Mi (4%)  45Mi (1%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  hugepages-2Mi              0 (0%)      0 (0%)\n  attachable-volumes-gce-pd  0           0\nEvents:                      <none>\n"
Feb 24 17:27:47.178: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9709 describe namespace kubectl-9709'
Feb 24 17:27:47.671: INFO: stderr: ""
Feb 24 17:27:47.671: INFO: stdout: "Name:         kubectl-9709\nLabels:       e2e-framework=kubectl\n              e2e-run=16c591ba-52c4-4277-81b9-61c35ae6a6e4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:47.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9709" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":75,"skipped":1430,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:47.785: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb 24 17:27:48.042: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9941" for this suite.
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":76,"skipped":1430,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:54.380: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:27:54.637: INFO: Creating deployment "test-recreate-deployment"
Feb 24 17:27:54.690: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 24 17:27:54.808: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 24 17:27:54.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784474, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784474, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784474, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784474, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:27:56.911: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 24 17:27:57.018: INFO: Updating deployment test-recreate-deployment
Feb 24 17:27:57.018: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 17:27:57.216: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8920  b76c8158-7e4c-494f-998e-184a41289757 6428 2 2021-02-24 17:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-24 17:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 17:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0087f8948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-24 17:27:57 +0000 UTC,LastTransitionTime:2021-02-24 17:27:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-02-24 17:27:57 +0000 UTC,LastTransitionTime:2021-02-24 17:27:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 24 17:27:57.267: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8920  06cc106a-a1ee-4e4c-bb21-49c6992127ba 6425 1 2021-02-24 17:27:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b76c8158-7e4c-494f-998e-184a41289757 0xc00202d0f0 0xc00202d0f1}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b76c8158-7e4c-494f-998e-184a41289757\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00202d188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:27:57.267: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 24 17:27:57.267: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-8920  e2de81be-5730-48a9-9e67-4e1e3eddf4d5 6419 2 2021-02-24 17:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b76c8158-7e4c-494f-998e-184a41289757 0xc00202cfd7 0xc00202cfd8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b76c8158-7e4c-494f-998e-184a41289757\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00202d088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:27:57.324: INFO: Pod "test-recreate-deployment-f79dd4667-cxhxx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-cxhxx test-recreate-deployment-f79dd4667- deployment-8920  b708bbc3-8a19-44c3-b5b3-4da2c7611bef 6426 0 2021-02-24 17:27:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 06cc106a-a1ee-4e4c-bb21-49c6992127ba 0xc0088cb290 0xc0088cb291}] []  [{kube-controller-manager Update v1 2021-02-24 17:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06cc106a-a1ee-4e4c-bb21-49c6992127ba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:27:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ldk9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ldk9n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ldk9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:27:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:27:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:27:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:27:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:27:57.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8920" for this suite.
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":77,"skipped":1430,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:27:57.449: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7836
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Feb 24 17:27:57.868: INFO: Found 1 stateful pods, waiting for 3
Feb 24 17:28:07.923: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:28:07.923: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:28:07.923: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 24 17:28:08.200: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 24 17:28:08.422: INFO: Updating stateful set ss2
Feb 24 17:28:08.538: INFO: Waiting for Pod statefulset-7836/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 24 17:28:18.926: INFO: Found 2 stateful pods, waiting for 3
Feb 24 17:28:28.982: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:28:28.982: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:28:28.982: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 24 17:28:29.204: INFO: Updating stateful set ss2
Feb 24 17:28:29.314: INFO: Waiting for Pod statefulset-7836/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 24 17:28:39.421: INFO: Waiting for Pod statefulset-7836/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 24 17:28:49.542: INFO: Updating stateful set ss2
Feb 24 17:28:49.652: INFO: Waiting for StatefulSet statefulset-7836/ss2 to complete update
Feb 24 17:28:49.652: INFO: Waiting for Pod statefulset-7836/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 24 17:28:59.759: INFO: Waiting for StatefulSet statefulset-7836/ss2 to complete update
Feb 24 17:28:59.759: INFO: Waiting for Pod statefulset-7836/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 17:29:09.760: INFO: Deleting all statefulset in ns statefulset-7836
Feb 24 17:29:09.811: INFO: Scaling statefulset ss2 to 0
Feb 24 17:29:40.024: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:29:40.077: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:29:40.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7836" for this suite.
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":78,"skipped":1446,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:29:40.350: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:29:51.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1877" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":79,"skipped":1457,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:29:52.105: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 24 17:29:52.817: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:29:52.871: INFO: Number of nodes with available pods: 0
Feb 24 17:29:52.871: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 17:29:53.931: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:29:53.986: INFO: Number of nodes with available pods: 0
Feb 24 17:29:53.986: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 17:29:54.929: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:29:54.983: INFO: Number of nodes with available pods: 3
Feb 24 17:29:54.983: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 24 17:29:55.258: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:29:55.311: INFO: Number of nodes with available pods: 2
Feb 24 17:29:55.311: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 17:29:56.371: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 17:29:56.424: INFO: Number of nodes with available pods: 3
Feb 24 17:29:56.424: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1331, will wait for the garbage collector to delete the pods
Feb 24 17:29:56.735: INFO: Deleting DaemonSet.extensions daemon-set took: 56.720034ms
Feb 24 17:29:56.835: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.246577ms
Feb 24 17:30:00.287: INFO: Number of nodes with available pods: 0
Feb 24 17:30:00.287: INFO: Number of running nodes: 0, number of available pods: 0
Feb 24 17:30:00.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6991"},"items":null}

Feb 24 17:30:00.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6991"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:00.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1331" for this suite.
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":80,"skipped":1463,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:00.723: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:30:02.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:30:04.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784602, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:30:07.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:30:07.724: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9546" for this suite.
STEP: Destroying namespace "webhook-9546-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":81,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:10.440: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-9af62851-ae3f-4ac9-9ae7-254034c55551
STEP: Creating a pod to test consume configMaps
Feb 24 17:30:11.080: INFO: Waiting up to 5m0s for pod "pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45" in namespace "configmap-6593" to be "Succeeded or Failed"
Feb 24 17:30:11.236: INFO: Pod "pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45": Phase="Pending", Reason="", readiness=false. Elapsed: 155.470066ms
Feb 24 17:30:13.290: INFO: Pod "pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.209569088s
STEP: Saw pod success
Feb 24 17:30:13.290: INFO: Pod "pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45" satisfied condition "Succeeded or Failed"
Feb 24 17:30:13.343: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:30:13.471: INFO: Waiting for pod pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45 to disappear
Feb 24 17:30:13.522: INFO: Pod pod-configmaps-4dc26246-884c-40d8-bb76-f595bb022f45 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:13.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6593" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1491,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:13.636: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:30:15.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784614, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784614, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784614, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784614, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:30:18.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:18.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6140" for this suite.
STEP: Destroying namespace "webhook-6140-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":83,"skipped":1495,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:19.147: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb 24 17:30:19.462: INFO: Waiting up to 5m0s for pod "downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a" in namespace "downward-api-6040" to be "Succeeded or Failed"
Feb 24 17:30:19.515: INFO: Pod "downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a": Phase="Pending", Reason="", readiness=false. Elapsed: 52.332387ms
Feb 24 17:30:21.568: INFO: Pod "downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105833434s
STEP: Saw pod success
Feb 24 17:30:21.568: INFO: Pod "downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a" satisfied condition "Succeeded or Failed"
Feb 24 17:30:21.622: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:30:21.737: INFO: Waiting for pod downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a to disappear
Feb 24 17:30:21.788: INFO: Pod downward-api-2a2769a7-3259-4993-ba16-9e937d117f9a no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6040" for this suite.
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:21.901: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 24 17:30:22.214: INFO: Waiting up to 5m0s for pod "pod-437a92ff-448f-4baa-b718-f9836820077f" in namespace "emptydir-7141" to be "Succeeded or Failed"
Feb 24 17:30:22.265: INFO: Pod "pod-437a92ff-448f-4baa-b718-f9836820077f": Phase="Pending", Reason="", readiness=false. Elapsed: 51.284301ms
Feb 24 17:30:24.317: INFO: Pod "pod-437a92ff-448f-4baa-b718-f9836820077f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103152568s
STEP: Saw pod success
Feb 24 17:30:24.317: INFO: Pod "pod-437a92ff-448f-4baa-b718-f9836820077f" satisfied condition "Succeeded or Failed"
Feb 24 17:30:24.371: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-437a92ff-448f-4baa-b718-f9836820077f container test-container: <nil>
STEP: delete the pod
Feb 24 17:30:24.496: INFO: Waiting for pod pod-437a92ff-448f-4baa-b718-f9836820077f to disappear
Feb 24 17:30:24.547: INFO: Pod pod-437a92ff-448f-4baa-b718-f9836820077f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:24.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7141" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1520,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:24.662: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1877
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 24 17:30:24.920: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 17:30:25.298: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 17:30:27.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:29.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:31.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:33.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:35.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:37.350: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:39.411: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 17:30:41.351: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 24 17:30:41.457: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 24 17:30:43.510: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 24 17:30:43.615: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 24 17:30:46.039: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 17:30:46.039: INFO: Going to poll 10.64.1.60 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:30:46.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.1.60:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:30:46.091: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:30:46.489: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 17:30:46.489: INFO: Going to poll 10.64.2.34 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:30:46.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.2.34:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:30:46.540: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:30:46.924: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 17:30:46.924: INFO: Going to poll 10.64.3.45 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb 24 17:30:46.976: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.3.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 17:30:46.976: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 17:30:47.333: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1877" for this suite.
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1531,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:47.450: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:30:48.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:30:50.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749784648, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:30:54.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:30:55.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9875" for this suite.
STEP: Destroying namespace "webhook-9875-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":87,"skipped":1541,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:30:55.556: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb 24 17:30:55.812: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 17:30:55.929: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 17:30:55.980: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-2qjz before test
Feb 24 17:30:56.035: INFO: coredns-6954c77b9b-dk8mk from kube-system started at 2021-02-24 17:01:15 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.035: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:30:56.035: INFO: kube-proxy-bootstrap-e2e-minion-group-2qjz from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.035: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:30:56.035: INFO: metadata-proxy-v0.1-mjnrs from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:30:56.035: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:30:56.035: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:30:56.035: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-9mrc before test
Feb 24 17:30:56.094: INFO: coredns-6954c77b9b-9rzld from kube-system started at 2021-02-24 17:01:20 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.094: INFO: 	Container coredns ready: true, restart count 0
Feb 24 17:30:56.094: INFO: kube-proxy-bootstrap-e2e-minion-group-9mrc from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.094: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:30:56.094: INFO: metadata-proxy-v0.1-rgzg9 from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:30:56.094: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:30:56.094: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:30:56.094: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78 from kube-system started at 2021-02-24 17:01:37 +0000 UTC (2 container statuses recorded)
Feb 24 17:30:56.094: INFO: 	Container metrics-server ready: true, restart count 0
Feb 24 17:30:56.094: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 24 17:30:56.094: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-bthl before test
Feb 24 17:30:56.151: INFO: kube-dns-autoscaler-76478fcf46-gsgnb from kube-system started at 2021-02-24 17:01:17 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.151: INFO: 	Container autoscaler ready: true, restart count 0
Feb 24 17:30:56.151: INFO: kube-proxy-bootstrap-e2e-minion-group-bthl from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.151: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 17:30:56.151: INFO: l7-default-backend-6f8dd4f4d5-c7mgv from kube-system started at 2021-02-24 17:01:13 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.151: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 24 17:30:56.151: INFO: metadata-proxy-v0.1-whmsq from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 17:30:56.151: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 17:30:56.151: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 17:30:56.151: INFO: volume-snapshot-controller-0 from kube-system started at 2021-02-24 17:01:18 +0000 UTC (1 container statuses recorded)
Feb 24 17:30:56.151: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2b429c48-8e80-4ea0-8f73-11abc1be9100 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.138.0.4 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-2b429c48-8e80-4ea0-8f73-11abc1be9100 off the node bootstrap-e2e-minion-group-2qjz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b429c48-8e80-4ea0-8f73-11abc1be9100
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:01.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5007" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

 [SLOW TEST:305.742 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":88,"skipped":1544,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:01.298: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-2295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2295 to expose endpoints map[]
Feb 24 17:36:01.773: INFO: successfully validated that service multi-endpoint-test in namespace services-2295 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2295 to expose endpoints map[pod1:[100]]
Feb 24 17:36:04.089: INFO: successfully validated that service multi-endpoint-test in namespace services-2295 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2295 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 24 17:36:06.452: INFO: successfully validated that service multi-endpoint-test in namespace services-2295 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2295 to expose endpoints map[pod2:[101]]
Feb 24 17:36:06.761: INFO: successfully validated that service multi-endpoint-test in namespace services-2295 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2295
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2295 to expose endpoints map[]
Feb 24 17:36:06.977: INFO: successfully validated that service multi-endpoint-test in namespace services-2295 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:07.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2295" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":89,"skipped":1560,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:07.182: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 24 17:36:07.465: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4343 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Feb 24 17:36:08.154: INFO: stderr: ""
Feb 24 17:36:08.154: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Feb 24 17:36:08.205: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4343 delete pods e2e-test-httpd-pod'
Feb 24 17:36:11.127: INFO: stderr: ""
Feb 24 17:36:11.127: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:11.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4343" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":90,"skipped":1574,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:11.241: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 24 17:36:11.970: INFO: starting watch
STEP: patching
STEP: updating
Feb 24 17:36:12.128: INFO: waiting for watch events with expected annotations
Feb 24 17:36:12.128: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:12.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6116" for this suite.
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":91,"skipped":1586,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:12.516: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 24 17:36:17.686: INFO: Successfully updated pod "pod-update-5c10618c-796c-4076-905d-d2ee585fca60"
STEP: verifying the updated pod is in kubernetes
Feb 24 17:36:17.792: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:17.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5438" for this suite.
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1599,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:17.904: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:36:18.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b" in namespace "downward-api-1662" to be "Succeeded or Failed"
Feb 24 17:36:18.268: INFO: Pod "downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.596372ms
Feb 24 17:36:20.320: INFO: Pod "downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103386856s
STEP: Saw pod success
Feb 24 17:36:20.320: INFO: Pod "downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b" satisfied condition "Succeeded or Failed"
Feb 24 17:36:20.372: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b container client-container: <nil>
STEP: delete the pod
Feb 24 17:36:20.505: INFO: Waiting for pod downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b to disappear
Feb 24 17:36:20.556: INFO: Pod downwardapi-volume-a7f7bf08-89db-4ea8-a80d-82cea709b24b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:20.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1662" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:20.668: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:36:20.925: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3688
I0224 17:36:20.988262   10144 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3688, replica count: 1
I0224 17:36:22.088825   10144 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 17:36:23.089126   10144 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:36:23.248: INFO: Created: latency-svc-zf2gz
Feb 24 17:36:23.258: INFO: Got endpoints: latency-svc-zf2gz [69.185512ms]
Feb 24 17:36:23.321: INFO: Created: latency-svc-mfrd6
Feb 24 17:36:23.337: INFO: Got endpoints: latency-svc-mfrd6 [78.32348ms]
Feb 24 17:36:23.337: INFO: Created: latency-svc-7pcdl
Feb 24 17:36:23.346: INFO: Got endpoints: latency-svc-7pcdl [87.238091ms]
Feb 24 17:36:23.349: INFO: Created: latency-svc-q6prv
Feb 24 17:36:23.369: INFO: Got endpoints: latency-svc-q6prv [111.126876ms]
Feb 24 17:36:23.374: INFO: Created: latency-svc-l2tkx
Feb 24 17:36:23.380: INFO: Got endpoints: latency-svc-l2tkx [121.612989ms]
Feb 24 17:36:23.387: INFO: Created: latency-svc-mrd72
Feb 24 17:36:23.397: INFO: Created: latency-svc-dfngp
Feb 24 17:36:23.403: INFO: Got endpoints: latency-svc-mrd72 [144.29791ms]
Feb 24 17:36:23.410: INFO: Created: latency-svc-9sbfk
Feb 24 17:36:23.412: INFO: Got endpoints: latency-svc-dfngp [153.530839ms]
Feb 24 17:36:23.422: INFO: Created: latency-svc-cx92v
Feb 24 17:36:23.429: INFO: Got endpoints: latency-svc-9sbfk [170.877376ms]
Feb 24 17:36:23.440: INFO: Got endpoints: latency-svc-cx92v [181.725227ms]
Feb 24 17:36:23.444: INFO: Created: latency-svc-src79
Feb 24 17:36:23.458: INFO: Got endpoints: latency-svc-src79 [199.555437ms]
Feb 24 17:36:23.462: INFO: Created: latency-svc-c75qd
Feb 24 17:36:23.470: INFO: Got endpoints: latency-svc-c75qd [211.640229ms]
Feb 24 17:36:23.473: INFO: Created: latency-svc-2jthb
Feb 24 17:36:23.493: INFO: Created: latency-svc-7ztsl
Feb 24 17:36:23.493: INFO: Got endpoints: latency-svc-2jthb [234.401608ms]
Feb 24 17:36:23.503: INFO: Got endpoints: latency-svc-7ztsl [244.124434ms]
Feb 24 17:36:23.509: INFO: Created: latency-svc-zllzj
Feb 24 17:36:23.523: INFO: Got endpoints: latency-svc-zllzj [263.821983ms]
Feb 24 17:36:23.523: INFO: Created: latency-svc-jnbm6
Feb 24 17:36:23.536: INFO: Got endpoints: latency-svc-jnbm6 [276.779306ms]
Feb 24 17:36:23.541: INFO: Created: latency-svc-jfzb7
Feb 24 17:36:23.547: INFO: Got endpoints: latency-svc-jfzb7 [288.638052ms]
Feb 24 17:36:23.551: INFO: Created: latency-svc-fbjww
Feb 24 17:36:23.559: INFO: Got endpoints: latency-svc-fbjww [222.455391ms]
Feb 24 17:36:23.571: INFO: Created: latency-svc-kzb86
Feb 24 17:36:23.575: INFO: Got endpoints: latency-svc-kzb86 [229.309089ms]
Feb 24 17:36:23.583: INFO: Created: latency-svc-f6twh
Feb 24 17:36:23.631: INFO: Got endpoints: latency-svc-f6twh [261.492876ms]
Feb 24 17:36:23.661: INFO: Created: latency-svc-9jbwk
Feb 24 17:36:23.701: INFO: Created: latency-svc-bzn44
Feb 24 17:36:23.709: INFO: Got endpoints: latency-svc-9jbwk [329.055807ms]
Feb 24 17:36:23.742: INFO: Created: latency-svc-ws2fz
Feb 24 17:36:23.743: INFO: Got endpoints: latency-svc-bzn44 [339.991029ms]
Feb 24 17:36:23.757: INFO: Got endpoints: latency-svc-ws2fz [345.133256ms]
Feb 24 17:36:23.761: INFO: Created: latency-svc-hgpqd
Feb 24 17:36:23.788: INFO: Created: latency-svc-v2nz2
Feb 24 17:36:23.804: INFO: Got endpoints: latency-svc-hgpqd [374.820458ms]
Feb 24 17:36:23.862: INFO: Got endpoints: latency-svc-v2nz2 [421.530022ms]
Feb 24 17:36:23.862: INFO: Created: latency-svc-nqh59
Feb 24 17:36:23.944: INFO: Created: latency-svc-g4dnd
Feb 24 17:36:23.944: INFO: Got endpoints: latency-svc-nqh59 [485.990925ms]
Feb 24 17:36:23.960: INFO: Got endpoints: latency-svc-g4dnd [202.147836ms]
Feb 24 17:36:23.960: INFO: Created: latency-svc-nbpfl
Feb 24 17:36:23.982: INFO: Got endpoints: latency-svc-nbpfl [511.580022ms]
Feb 24 17:36:23.983: INFO: Created: latency-svc-z2ckp
Feb 24 17:36:24.007: INFO: Got endpoints: latency-svc-z2ckp [514.140053ms]
Feb 24 17:36:24.013: INFO: Created: latency-svc-sbxgt
Feb 24 17:36:24.045: INFO: Created: latency-svc-54m2m
Feb 24 17:36:24.049: INFO: Got endpoints: latency-svc-sbxgt [545.815753ms]
Feb 24 17:36:24.068: INFO: Created: latency-svc-4sv6j
Feb 24 17:36:24.070: INFO: Got endpoints: latency-svc-54m2m [547.313996ms]
Feb 24 17:36:24.090: INFO: Created: latency-svc-jgqpv
Feb 24 17:36:24.093: INFO: Got endpoints: latency-svc-4sv6j [557.6727ms]
Feb 24 17:36:24.110: INFO: Got endpoints: latency-svc-jgqpv [562.900249ms]
Feb 24 17:36:24.118: INFO: Created: latency-svc-g5db7
Feb 24 17:36:24.128: INFO: Created: latency-svc-tsttb
Feb 24 17:36:24.135: INFO: Got endpoints: latency-svc-tsttb [575.900819ms]
Feb 24 17:36:24.139: INFO: Got endpoints: latency-svc-g5db7 [563.69632ms]
Feb 24 17:36:24.144: INFO: Created: latency-svc-cdmqw
Feb 24 17:36:24.156: INFO: Got endpoints: latency-svc-cdmqw [524.491017ms]
Feb 24 17:36:24.159: INFO: Created: latency-svc-4wq4l
Feb 24 17:36:24.169: INFO: Got endpoints: latency-svc-4wq4l [459.203688ms]
Feb 24 17:36:24.173: INFO: Created: latency-svc-hjkjv
Feb 24 17:36:24.183: INFO: Got endpoints: latency-svc-hjkjv [439.615522ms]
Feb 24 17:36:24.190: INFO: Created: latency-svc-hxktq
Feb 24 17:36:24.197: INFO: Got endpoints: latency-svc-hxktq [392.376694ms]
Feb 24 17:36:24.202: INFO: Created: latency-svc-99b4c
Feb 24 17:36:24.214: INFO: Got endpoints: latency-svc-99b4c [351.612765ms]
Feb 24 17:36:24.219: INFO: Created: latency-svc-2d7c7
Feb 24 17:36:24.227: INFO: Got endpoints: latency-svc-2d7c7 [283.192527ms]
Feb 24 17:36:24.233: INFO: Created: latency-svc-hbtz4
Feb 24 17:36:24.243: INFO: Got endpoints: latency-svc-hbtz4 [282.838969ms]
Feb 24 17:36:24.246: INFO: Created: latency-svc-pxj52
Feb 24 17:36:24.256: INFO: Got endpoints: latency-svc-pxj52 [273.691564ms]
Feb 24 17:36:24.264: INFO: Created: latency-svc-jw2tt
Feb 24 17:36:24.278: INFO: Got endpoints: latency-svc-jw2tt [270.661475ms]
Feb 24 17:36:24.283: INFO: Created: latency-svc-jj65f
Feb 24 17:36:24.288: INFO: Got endpoints: latency-svc-jj65f [239.438356ms]
Feb 24 17:36:24.297: INFO: Created: latency-svc-cclwl
Feb 24 17:36:24.309: INFO: Got endpoints: latency-svc-cclwl [238.496007ms]
Feb 24 17:36:24.315: INFO: Created: latency-svc-6rb8r
Feb 24 17:36:24.319: INFO: Got endpoints: latency-svc-6rb8r [225.901547ms]
Feb 24 17:36:24.323: INFO: Created: latency-svc-7p66c
Feb 24 17:36:24.336: INFO: Got endpoints: latency-svc-7p66c [225.962357ms]
Feb 24 17:36:24.344: INFO: Created: latency-svc-9j6kf
Feb 24 17:36:24.349: INFO: Got endpoints: latency-svc-9j6kf [214.133575ms]
Feb 24 17:36:24.359: INFO: Created: latency-svc-t5zfb
Feb 24 17:36:24.371: INFO: Got endpoints: latency-svc-t5zfb [231.723404ms]
Feb 24 17:36:24.380: INFO: Created: latency-svc-8qhbh
Feb 24 17:36:24.392: INFO: Got endpoints: latency-svc-8qhbh [236.745736ms]
Feb 24 17:36:24.397: INFO: Created: latency-svc-dslpf
Feb 24 17:36:24.408: INFO: Got endpoints: latency-svc-dslpf [239.431956ms]
Feb 24 17:36:24.412: INFO: Created: latency-svc-7jppn
Feb 24 17:36:24.424: INFO: Got endpoints: latency-svc-7jppn [240.835557ms]
Feb 24 17:36:24.429: INFO: Created: latency-svc-rxgxc
Feb 24 17:36:24.438: INFO: Got endpoints: latency-svc-rxgxc [241.103085ms]
Feb 24 17:36:24.446: INFO: Created: latency-svc-99fdn
Feb 24 17:36:24.455: INFO: Created: latency-svc-mvrmx
Feb 24 17:36:24.488: INFO: Created: latency-svc-x6ztn
Feb 24 17:36:24.497: INFO: Got endpoints: latency-svc-99fdn [283.063288ms]
Feb 24 17:36:24.510: INFO: Created: latency-svc-qgdlp
Feb 24 17:36:24.526: INFO: Created: latency-svc-mwljc
Feb 24 17:36:24.532: INFO: Got endpoints: latency-svc-mvrmx [304.531904ms]
Feb 24 17:36:24.538: INFO: Created: latency-svc-mwrzs
Feb 24 17:36:24.546: INFO: Created: latency-svc-f5fsm
Feb 24 17:36:24.557: INFO: Created: latency-svc-gbmcf
Feb 24 17:36:24.566: INFO: Created: latency-svc-gs84c
Feb 24 17:36:24.579: INFO: Created: latency-svc-dh5br
Feb 24 17:36:24.580: INFO: Got endpoints: latency-svc-x6ztn [337.239911ms]
Feb 24 17:36:24.591: INFO: Created: latency-svc-sld7b
Feb 24 17:36:24.601: INFO: Created: latency-svc-85wdt
Feb 24 17:36:24.606: INFO: Created: latency-svc-dfzpw
Feb 24 17:36:24.614: INFO: Created: latency-svc-9t98v
Feb 24 17:36:24.630: INFO: Created: latency-svc-49h4p
Feb 24 17:36:24.635: INFO: Got endpoints: latency-svc-qgdlp [379.413744ms]
Feb 24 17:36:24.641: INFO: Created: latency-svc-4bnsw
Feb 24 17:36:24.650: INFO: Created: latency-svc-fhcvz
Feb 24 17:36:24.661: INFO: Created: latency-svc-fjkqh
Feb 24 17:36:24.676: INFO: Got endpoints: latency-svc-mwljc [397.638959ms]
Feb 24 17:36:24.694: INFO: Created: latency-svc-mzqdn
Feb 24 17:36:24.727: INFO: Got endpoints: latency-svc-mwrzs [438.878388ms]
Feb 24 17:36:24.741: INFO: Created: latency-svc-gmvhh
Feb 24 17:36:24.782: INFO: Got endpoints: latency-svc-f5fsm [473.538148ms]
Feb 24 17:36:24.797: INFO: Created: latency-svc-8n5jh
Feb 24 17:36:24.826: INFO: Got endpoints: latency-svc-gbmcf [506.438611ms]
Feb 24 17:36:24.840: INFO: Created: latency-svc-rh7vw
Feb 24 17:36:24.876: INFO: Got endpoints: latency-svc-gs84c [540.058237ms]
Feb 24 17:36:24.888: INFO: Created: latency-svc-lljnw
Feb 24 17:36:24.932: INFO: Got endpoints: latency-svc-dh5br [582.255833ms]
Feb 24 17:36:24.938: INFO: Created: latency-svc-mnrl7
Feb 24 17:36:24.979: INFO: Got endpoints: latency-svc-sld7b [607.88088ms]
Feb 24 17:36:24.990: INFO: Created: latency-svc-4kjwh
Feb 24 17:36:25.029: INFO: Got endpoints: latency-svc-85wdt [636.768013ms]
Feb 24 17:36:25.039: INFO: Created: latency-svc-ttf4p
Feb 24 17:36:25.079: INFO: Got endpoints: latency-svc-dfzpw [670.266611ms]
Feb 24 17:36:25.089: INFO: Created: latency-svc-vm6kq
Feb 24 17:36:25.126: INFO: Got endpoints: latency-svc-9t98v [701.991456ms]
Feb 24 17:36:25.138: INFO: Created: latency-svc-95749
Feb 24 17:36:25.175: INFO: Got endpoints: latency-svc-49h4p [737.005229ms]
Feb 24 17:36:25.189: INFO: Created: latency-svc-q5prv
Feb 24 17:36:25.228: INFO: Got endpoints: latency-svc-4bnsw [731.400488ms]
Feb 24 17:36:25.242: INFO: Created: latency-svc-x5qxr
Feb 24 17:36:25.280: INFO: Got endpoints: latency-svc-fhcvz [748.227066ms]
Feb 24 17:36:25.289: INFO: Created: latency-svc-p6xss
Feb 24 17:36:25.329: INFO: Got endpoints: latency-svc-fjkqh [749.405827ms]
Feb 24 17:36:25.340: INFO: Created: latency-svc-ctzvm
Feb 24 17:36:25.379: INFO: Got endpoints: latency-svc-mzqdn [743.538651ms]
Feb 24 17:36:25.388: INFO: Created: latency-svc-lzxhc
Feb 24 17:36:25.427: INFO: Got endpoints: latency-svc-gmvhh [751.517957ms]
Feb 24 17:36:25.439: INFO: Created: latency-svc-rg8tb
Feb 24 17:36:25.482: INFO: Got endpoints: latency-svc-8n5jh [754.9893ms]
Feb 24 17:36:25.509: INFO: Created: latency-svc-7b72p
Feb 24 17:36:25.528: INFO: Got endpoints: latency-svc-rh7vw [745.465104ms]
Feb 24 17:36:25.540: INFO: Created: latency-svc-fxsb5
Feb 24 17:36:25.576: INFO: Got endpoints: latency-svc-lljnw [750.320062ms]
Feb 24 17:36:25.587: INFO: Created: latency-svc-dmvxs
Feb 24 17:36:25.625: INFO: Got endpoints: latency-svc-mnrl7 [748.792736ms]
Feb 24 17:36:25.639: INFO: Created: latency-svc-sr77l
Feb 24 17:36:25.681: INFO: Got endpoints: latency-svc-4kjwh [749.416787ms]
Feb 24 17:36:25.709: INFO: Created: latency-svc-rxbpk
Feb 24 17:36:25.730: INFO: Got endpoints: latency-svc-ttf4p [750.807765ms]
Feb 24 17:36:25.745: INFO: Created: latency-svc-2hwhq
Feb 24 17:36:25.780: INFO: Got endpoints: latency-svc-vm6kq [750.513761ms]
Feb 24 17:36:25.797: INFO: Created: latency-svc-tlvwf
Feb 24 17:36:25.831: INFO: Got endpoints: latency-svc-95749 [752.27559ms]
Feb 24 17:36:25.842: INFO: Created: latency-svc-vwxf5
Feb 24 17:36:25.877: INFO: Got endpoints: latency-svc-q5prv [750.861525ms]
Feb 24 17:36:25.892: INFO: Created: latency-svc-cbzks
Feb 24 17:36:25.933: INFO: Got endpoints: latency-svc-x5qxr [757.859183ms]
Feb 24 17:36:25.942: INFO: Created: latency-svc-wtdsd
Feb 24 17:36:25.979: INFO: Got endpoints: latency-svc-p6xss [750.716533ms]
Feb 24 17:36:26.000: INFO: Created: latency-svc-tspng
Feb 24 17:36:26.028: INFO: Got endpoints: latency-svc-ctzvm [747.23855ms]
Feb 24 17:36:26.045: INFO: Created: latency-svc-75qzp
Feb 24 17:36:26.080: INFO: Got endpoints: latency-svc-lzxhc [750.830435ms]
Feb 24 17:36:26.087: INFO: Created: latency-svc-rtxsh
Feb 24 17:36:26.127: INFO: Got endpoints: latency-svc-rg8tb [748.071646ms]
Feb 24 17:36:26.140: INFO: Created: latency-svc-whc2w
Feb 24 17:36:26.181: INFO: Got endpoints: latency-svc-7b72p [754.072805ms]
Feb 24 17:36:26.189: INFO: Created: latency-svc-zhvpz
Feb 24 17:36:26.226: INFO: Got endpoints: latency-svc-fxsb5 [743.351847ms]
Feb 24 17:36:26.239: INFO: Created: latency-svc-9xbt5
Feb 24 17:36:26.281: INFO: Got endpoints: latency-svc-dmvxs [753.234766ms]
Feb 24 17:36:26.289: INFO: Created: latency-svc-r7gsr
Feb 24 17:36:26.327: INFO: Got endpoints: latency-svc-sr77l [751.042333ms]
Feb 24 17:36:26.339: INFO: Created: latency-svc-l6dn4
Feb 24 17:36:26.379: INFO: Got endpoints: latency-svc-rxbpk [754.247959ms]
Feb 24 17:36:26.389: INFO: Created: latency-svc-5jcdb
Feb 24 17:36:26.431: INFO: Got endpoints: latency-svc-2hwhq [750.251582ms]
Feb 24 17:36:26.441: INFO: Created: latency-svc-v98hx
Feb 24 17:36:26.478: INFO: Got endpoints: latency-svc-tlvwf [747.938339ms]
Feb 24 17:36:26.489: INFO: Created: latency-svc-v7p8q
Feb 24 17:36:26.525: INFO: Got endpoints: latency-svc-vwxf5 [745.478713ms]
Feb 24 17:36:26.560: INFO: Created: latency-svc-bjx42
Feb 24 17:36:26.576: INFO: Got endpoints: latency-svc-cbzks [744.517894ms]
Feb 24 17:36:26.587: INFO: Created: latency-svc-z2bqh
Feb 24 17:36:26.625: INFO: Got endpoints: latency-svc-wtdsd [748.487112ms]
Feb 24 17:36:26.640: INFO: Created: latency-svc-hdh2b
Feb 24 17:36:26.679: INFO: Got endpoints: latency-svc-tspng [746.078531ms]
Feb 24 17:36:26.689: INFO: Created: latency-svc-rgzdr
Feb 24 17:36:26.727: INFO: Got endpoints: latency-svc-75qzp [748.03717ms]
Feb 24 17:36:26.738: INFO: Created: latency-svc-7ht6m
Feb 24 17:36:26.784: INFO: Got endpoints: latency-svc-rtxsh [755.834461ms]
Feb 24 17:36:26.803: INFO: Created: latency-svc-2qhtt
Feb 24 17:36:26.832: INFO: Got endpoints: latency-svc-whc2w [751.207817ms]
Feb 24 17:36:26.846: INFO: Created: latency-svc-lnbnz
Feb 24 17:36:26.880: INFO: Got endpoints: latency-svc-zhvpz [752.478448ms]
Feb 24 17:36:26.900: INFO: Created: latency-svc-jm967
Feb 24 17:36:26.926: INFO: Got endpoints: latency-svc-9xbt5 [744.165153ms]
Feb 24 17:36:26.938: INFO: Created: latency-svc-qxfl5
Feb 24 17:36:26.976: INFO: Got endpoints: latency-svc-r7gsr [750.499801ms]
Feb 24 17:36:26.986: INFO: Created: latency-svc-7sfvl
Feb 24 17:36:27.027: INFO: Got endpoints: latency-svc-l6dn4 [746.36234ms]
Feb 24 17:36:27.038: INFO: Created: latency-svc-9kfxh
Feb 24 17:36:27.081: INFO: Got endpoints: latency-svc-5jcdb [753.599958ms]
Feb 24 17:36:27.087: INFO: Created: latency-svc-xzjgw
Feb 24 17:36:27.125: INFO: Got endpoints: latency-svc-v98hx [746.03554ms]
Feb 24 17:36:27.139: INFO: Created: latency-svc-lgmkx
Feb 24 17:36:27.175: INFO: Got endpoints: latency-svc-v7p8q [743.964129ms]
Feb 24 17:36:27.187: INFO: Created: latency-svc-df27r
Feb 24 17:36:27.226: INFO: Got endpoints: latency-svc-bjx42 [748.282478ms]
Feb 24 17:36:27.238: INFO: Created: latency-svc-rc28s
Feb 24 17:36:27.280: INFO: Got endpoints: latency-svc-z2bqh [755.139141ms]
Feb 24 17:36:27.289: INFO: Created: latency-svc-6lcjp
Feb 24 17:36:27.325: INFO: Got endpoints: latency-svc-hdh2b [749.476403ms]
Feb 24 17:36:27.338: INFO: Created: latency-svc-4ktwr
Feb 24 17:36:27.378: INFO: Got endpoints: latency-svc-rgzdr [752.55095ms]
Feb 24 17:36:27.387: INFO: Created: latency-svc-xtmgh
Feb 24 17:36:27.426: INFO: Got endpoints: latency-svc-7ht6m [746.52377ms]
Feb 24 17:36:27.437: INFO: Created: latency-svc-57xpg
Feb 24 17:36:27.476: INFO: Got endpoints: latency-svc-2qhtt [748.57885ms]
Feb 24 17:36:27.488: INFO: Created: latency-svc-tws8t
Feb 24 17:36:27.530: INFO: Got endpoints: latency-svc-lnbnz [746.088691ms]
Feb 24 17:36:27.539: INFO: Created: latency-svc-cmjwk
Feb 24 17:36:27.575: INFO: Got endpoints: latency-svc-jm967 [743.808366ms]
Feb 24 17:36:27.587: INFO: Created: latency-svc-mkpzc
Feb 24 17:36:27.626: INFO: Got endpoints: latency-svc-qxfl5 [746.214612ms]
Feb 24 17:36:27.638: INFO: Created: latency-svc-gm6ql
Feb 24 17:36:27.677: INFO: Got endpoints: latency-svc-7sfvl [751.078721ms]
Feb 24 17:36:27.686: INFO: Created: latency-svc-5w9z6
Feb 24 17:36:27.734: INFO: Got endpoints: latency-svc-9kfxh [757.768667ms]
Feb 24 17:36:27.756: INFO: Created: latency-svc-xqvk6
Feb 24 17:36:27.780: INFO: Got endpoints: latency-svc-xzjgw [752.987246ms]
Feb 24 17:36:27.803: INFO: Created: latency-svc-qb4mt
Feb 24 17:36:27.840: INFO: Got endpoints: latency-svc-lgmkx [758.930147ms]
Feb 24 17:36:27.880: INFO: Created: latency-svc-k4rlg
Feb 24 17:36:27.884: INFO: Got endpoints: latency-svc-df27r [758.815709ms]
Feb 24 17:36:27.980: INFO: Created: latency-svc-nw8f2
Feb 24 17:36:27.980: INFO: Got endpoints: latency-svc-rc28s [804.59751ms]
Feb 24 17:36:28.000: INFO: Got endpoints: latency-svc-6lcjp [773.590983ms]
Feb 24 17:36:28.000: INFO: Created: latency-svc-gq5wl
Feb 24 17:36:28.029: INFO: Got endpoints: latency-svc-4ktwr [748.530874ms]
Feb 24 17:36:28.085: INFO: Created: latency-svc-llp4d
Feb 24 17:36:28.087: INFO: Got endpoints: latency-svc-xtmgh [762.270693ms]
Feb 24 17:36:28.096: INFO: Created: latency-svc-2qskh
Feb 24 17:36:28.106: INFO: Created: latency-svc-gf9lq
Feb 24 17:36:28.126: INFO: Got endpoints: latency-svc-57xpg [747.847499ms]
Feb 24 17:36:28.146: INFO: Created: latency-svc-94bvb
Feb 24 17:36:28.176: INFO: Got endpoints: latency-svc-tws8t [750.122375ms]
Feb 24 17:36:28.187: INFO: Created: latency-svc-mhdvt
Feb 24 17:36:28.236: INFO: Got endpoints: latency-svc-cmjwk [760.584523ms]
Feb 24 17:36:28.247: INFO: Created: latency-svc-rx6hc
Feb 24 17:36:28.282: INFO: Got endpoints: latency-svc-mkpzc [752.449163ms]
Feb 24 17:36:28.301: INFO: Created: latency-svc-c7l8n
Feb 24 17:36:28.330: INFO: Got endpoints: latency-svc-gm6ql [754.505895ms]
Feb 24 17:36:28.345: INFO: Created: latency-svc-pqkpt
Feb 24 17:36:28.380: INFO: Got endpoints: latency-svc-5w9z6 [753.695654ms]
Feb 24 17:36:28.391: INFO: Created: latency-svc-f9tzp
Feb 24 17:36:28.427: INFO: Got endpoints: latency-svc-xqvk6 [750.255171ms]
Feb 24 17:36:28.444: INFO: Created: latency-svc-v6msx
Feb 24 17:36:28.478: INFO: Got endpoints: latency-svc-qb4mt [744.121493ms]
Feb 24 17:36:28.491: INFO: Created: latency-svc-f7ck2
Feb 24 17:36:28.527: INFO: Got endpoints: latency-svc-k4rlg [746.122083ms]
Feb 24 17:36:28.541: INFO: Created: latency-svc-vg5m8
Feb 24 17:36:28.577: INFO: Got endpoints: latency-svc-nw8f2 [736.67502ms]
Feb 24 17:36:28.588: INFO: Created: latency-svc-qhk92
Feb 24 17:36:28.628: INFO: Got endpoints: latency-svc-gq5wl [743.544979ms]
Feb 24 17:36:28.654: INFO: Created: latency-svc-pm9k2
Feb 24 17:36:28.693: INFO: Created: latency-svc-czmnw
Feb 24 17:36:28.694: INFO: Got endpoints: latency-svc-llp4d [714.069697ms]
Feb 24 17:36:28.726: INFO: Got endpoints: latency-svc-2qskh [726.803927ms]
Feb 24 17:36:28.753: INFO: Created: latency-svc-vb7cl
Feb 24 17:36:28.778: INFO: Got endpoints: latency-svc-gf9lq [749.252171ms]
Feb 24 17:36:28.789: INFO: Created: latency-svc-2tgvc
Feb 24 17:36:28.826: INFO: Got endpoints: latency-svc-94bvb [738.729244ms]
Feb 24 17:36:28.851: INFO: Created: latency-svc-wh9k4
Feb 24 17:36:28.875: INFO: Got endpoints: latency-svc-mhdvt [749.606896ms]
Feb 24 17:36:28.887: INFO: Created: latency-svc-ckd6t
Feb 24 17:36:28.926: INFO: Got endpoints: latency-svc-rx6hc [749.920277ms]
Feb 24 17:36:28.938: INFO: Created: latency-svc-wf2k2
Feb 24 17:36:28.977: INFO: Got endpoints: latency-svc-c7l8n [740.709804ms]
Feb 24 17:36:28.987: INFO: Created: latency-svc-wvkqt
Feb 24 17:36:29.026: INFO: Got endpoints: latency-svc-pqkpt [743.625748ms]
Feb 24 17:36:29.038: INFO: Created: latency-svc-cvpcl
Feb 24 17:36:29.077: INFO: Got endpoints: latency-svc-f9tzp [746.500147ms]
Feb 24 17:36:29.088: INFO: Created: latency-svc-dbpsk
Feb 24 17:36:29.126: INFO: Got endpoints: latency-svc-v6msx [746.282146ms]
Feb 24 17:36:29.139: INFO: Created: latency-svc-88gnn
Feb 24 17:36:29.176: INFO: Got endpoints: latency-svc-f7ck2 [748.412716ms]
Feb 24 17:36:29.188: INFO: Created: latency-svc-m2pjv
Feb 24 17:36:29.226: INFO: Got endpoints: latency-svc-vg5m8 [747.668032ms]
Feb 24 17:36:29.237: INFO: Created: latency-svc-k5zsp
Feb 24 17:36:29.281: INFO: Got endpoints: latency-svc-qhk92 [753.884526ms]
Feb 24 17:36:29.288: INFO: Created: latency-svc-xqstr
Feb 24 17:36:29.326: INFO: Got endpoints: latency-svc-pm9k2 [749.166792ms]
Feb 24 17:36:29.338: INFO: Created: latency-svc-qhcct
Feb 24 17:36:29.380: INFO: Got endpoints: latency-svc-czmnw [752.275632ms]
Feb 24 17:36:29.387: INFO: Created: latency-svc-7t9rb
Feb 24 17:36:29.429: INFO: Got endpoints: latency-svc-vb7cl [734.346255ms]
Feb 24 17:36:29.438: INFO: Created: latency-svc-2nc7t
Feb 24 17:36:29.477: INFO: Got endpoints: latency-svc-2tgvc [750.523909ms]
Feb 24 17:36:29.487: INFO: Created: latency-svc-r5drq
Feb 24 17:36:29.527: INFO: Got endpoints: latency-svc-wh9k4 [748.621861ms]
Feb 24 17:36:29.538: INFO: Created: latency-svc-ppxcf
Feb 24 17:36:29.576: INFO: Got endpoints: latency-svc-ckd6t [749.81139ms]
Feb 24 17:36:29.588: INFO: Created: latency-svc-h6tsn
Feb 24 17:36:29.627: INFO: Got endpoints: latency-svc-wf2k2 [751.09502ms]
Feb 24 17:36:29.639: INFO: Created: latency-svc-mh68w
Feb 24 17:36:29.676: INFO: Got endpoints: latency-svc-wvkqt [750.323741ms]
Feb 24 17:36:29.687: INFO: Created: latency-svc-lbhxc
Feb 24 17:36:29.733: INFO: Got endpoints: latency-svc-cvpcl [756.15303ms]
Feb 24 17:36:29.740: INFO: Created: latency-svc-znwgg
Feb 24 17:36:29.779: INFO: Got endpoints: latency-svc-dbpsk [753.27681ms]
Feb 24 17:36:29.791: INFO: Created: latency-svc-kpn6n
Feb 24 17:36:29.826: INFO: Got endpoints: latency-svc-88gnn [749.035627ms]
Feb 24 17:36:29.838: INFO: Created: latency-svc-qhwgt
Feb 24 17:36:29.878: INFO: Got endpoints: latency-svc-m2pjv [751.704435ms]
Feb 24 17:36:29.889: INFO: Created: latency-svc-sxtqw
Feb 24 17:36:29.925: INFO: Got endpoints: latency-svc-k5zsp [749.633336ms]
Feb 24 17:36:29.938: INFO: Created: latency-svc-g7cwn
Feb 24 17:36:29.978: INFO: Got endpoints: latency-svc-xqstr [752.411959ms]
Feb 24 17:36:29.987: INFO: Created: latency-svc-ttnjj
Feb 24 17:36:30.029: INFO: Got endpoints: latency-svc-qhcct [748.443245ms]
Feb 24 17:36:30.039: INFO: Created: latency-svc-d8hxf
Feb 24 17:36:30.077: INFO: Got endpoints: latency-svc-7t9rb [751.13808ms]
Feb 24 17:36:30.092: INFO: Created: latency-svc-nhgs7
Feb 24 17:36:30.128: INFO: Got endpoints: latency-svc-2nc7t [747.617218ms]
Feb 24 17:36:30.138: INFO: Created: latency-svc-fqvdj
Feb 24 17:36:30.176: INFO: Got endpoints: latency-svc-r5drq [746.881394ms]
Feb 24 17:36:30.188: INFO: Created: latency-svc-sbm9w
Feb 24 17:36:30.227: INFO: Got endpoints: latency-svc-ppxcf [749.872156ms]
Feb 24 17:36:30.251: INFO: Created: latency-svc-rtmnk
Feb 24 17:36:30.280: INFO: Got endpoints: latency-svc-h6tsn [753.094697ms]
Feb 24 17:36:30.288: INFO: Created: latency-svc-x4jp7
Feb 24 17:36:30.326: INFO: Got endpoints: latency-svc-mh68w [749.907561ms]
Feb 24 17:36:30.338: INFO: Created: latency-svc-lltxs
Feb 24 17:36:30.375: INFO: Got endpoints: latency-svc-lbhxc [748.419824ms]
Feb 24 17:36:30.387: INFO: Created: latency-svc-wf6ff
Feb 24 17:36:30.428: INFO: Got endpoints: latency-svc-znwgg [751.291259ms]
Feb 24 17:36:30.438: INFO: Created: latency-svc-dpx9x
Feb 24 17:36:30.477: INFO: Got endpoints: latency-svc-kpn6n [743.246849ms]
Feb 24 17:36:30.487: INFO: Created: latency-svc-sgr8f
Feb 24 17:36:30.527: INFO: Got endpoints: latency-svc-qhwgt [748.090124ms]
Feb 24 17:36:30.541: INFO: Created: latency-svc-sfql9
Feb 24 17:36:30.576: INFO: Got endpoints: latency-svc-sxtqw [750.514633ms]
Feb 24 17:36:30.588: INFO: Created: latency-svc-fg5ld
Feb 24 17:36:30.626: INFO: Got endpoints: latency-svc-g7cwn [748.054355ms]
Feb 24 17:36:30.638: INFO: Created: latency-svc-dbc8f
Feb 24 17:36:30.677: INFO: Got endpoints: latency-svc-ttnjj [751.226896ms]
Feb 24 17:36:30.688: INFO: Created: latency-svc-nsbxm
Feb 24 17:36:30.726: INFO: Got endpoints: latency-svc-d8hxf [747.460058ms]
Feb 24 17:36:30.736: INFO: Created: latency-svc-rf75j
Feb 24 17:36:30.779: INFO: Got endpoints: latency-svc-nhgs7 [750.240331ms]
Feb 24 17:36:30.789: INFO: Created: latency-svc-v8pc9
Feb 24 17:36:30.825: INFO: Got endpoints: latency-svc-fqvdj [747.928974ms]
Feb 24 17:36:30.838: INFO: Created: latency-svc-8b77t
Feb 24 17:36:30.879: INFO: Got endpoints: latency-svc-sbm9w [750.53202ms]
Feb 24 17:36:30.887: INFO: Created: latency-svc-zcq26
Feb 24 17:36:30.926: INFO: Got endpoints: latency-svc-rtmnk [750.143763ms]
Feb 24 17:36:30.938: INFO: Created: latency-svc-hxhxq
Feb 24 17:36:30.978: INFO: Got endpoints: latency-svc-x4jp7 [751.417767ms]
Feb 24 17:36:30.989: INFO: Created: latency-svc-5hg2g
Feb 24 17:36:31.025: INFO: Got endpoints: latency-svc-lltxs [744.731912ms]
Feb 24 17:36:31.038: INFO: Created: latency-svc-jwdt6
Feb 24 17:36:31.076: INFO: Got endpoints: latency-svc-wf6ff [749.547198ms]
Feb 24 17:36:31.087: INFO: Created: latency-svc-lpz6j
Feb 24 17:36:31.126: INFO: Got endpoints: latency-svc-dpx9x [750.481674ms]
Feb 24 17:36:31.205: INFO: Got endpoints: latency-svc-sgr8f [777.218314ms]
Feb 24 17:36:31.213: INFO: Created: latency-svc-l76t2
Feb 24 17:36:31.227: INFO: Got endpoints: latency-svc-sfql9 [750.11868ms]
Feb 24 17:36:31.278: INFO: Got endpoints: latency-svc-fg5ld [750.830384ms]
Feb 24 17:36:31.327: INFO: Got endpoints: latency-svc-dbc8f [751.020315ms]
Feb 24 17:36:31.376: INFO: Got endpoints: latency-svc-nsbxm [750.192008ms]
Feb 24 17:36:31.427: INFO: Got endpoints: latency-svc-rf75j [750.274367ms]
Feb 24 17:36:31.477: INFO: Got endpoints: latency-svc-v8pc9 [751.002951ms]
Feb 24 17:36:31.526: INFO: Got endpoints: latency-svc-8b77t [746.11881ms]
Feb 24 17:36:31.575: INFO: Got endpoints: latency-svc-zcq26 [750.062132ms]
Feb 24 17:36:31.626: INFO: Got endpoints: latency-svc-hxhxq [747.886715ms]
Feb 24 17:36:31.676: INFO: Got endpoints: latency-svc-5hg2g [750.085334ms]
Feb 24 17:36:31.732: INFO: Got endpoints: latency-svc-jwdt6 [753.318568ms]
Feb 24 17:36:31.780: INFO: Got endpoints: latency-svc-lpz6j [754.589284ms]
Feb 24 17:36:31.826: INFO: Got endpoints: latency-svc-l76t2 [750.173439ms]
Feb 24 17:36:31.826: INFO: Latencies: [78.32348ms 87.238091ms 111.126876ms 121.612989ms 144.29791ms 153.530839ms 170.877376ms 181.725227ms 199.555437ms 202.147836ms 211.640229ms 214.133575ms 222.455391ms 225.901547ms 225.962357ms 229.309089ms 231.723404ms 234.401608ms 236.745736ms 238.496007ms 239.431956ms 239.438356ms 240.835557ms 241.103085ms 244.124434ms 261.492876ms 263.821983ms 270.661475ms 273.691564ms 276.779306ms 282.838969ms 283.063288ms 283.192527ms 288.638052ms 304.531904ms 329.055807ms 337.239911ms 339.991029ms 345.133256ms 351.612765ms 374.820458ms 379.413744ms 392.376694ms 397.638959ms 421.530022ms 438.878388ms 439.615522ms 459.203688ms 473.538148ms 485.990925ms 506.438611ms 511.580022ms 514.140053ms 524.491017ms 540.058237ms 545.815753ms 547.313996ms 557.6727ms 562.900249ms 563.69632ms 575.900819ms 582.255833ms 607.88088ms 636.768013ms 670.266611ms 701.991456ms 714.069697ms 726.803927ms 731.400488ms 734.346255ms 736.67502ms 737.005229ms 738.729244ms 740.709804ms 743.246849ms 743.351847ms 743.538651ms 743.544979ms 743.625748ms 743.808366ms 743.964129ms 744.121493ms 744.165153ms 744.517894ms 744.731912ms 745.465104ms 745.478713ms 746.03554ms 746.078531ms 746.088691ms 746.11881ms 746.122083ms 746.214612ms 746.282146ms 746.36234ms 746.500147ms 746.52377ms 746.881394ms 747.23855ms 747.460058ms 747.617218ms 747.668032ms 747.847499ms 747.886715ms 747.928974ms 747.938339ms 748.03717ms 748.054355ms 748.071646ms 748.090124ms 748.227066ms 748.282478ms 748.412716ms 748.419824ms 748.443245ms 748.487112ms 748.530874ms 748.57885ms 748.621861ms 748.792736ms 749.035627ms 749.166792ms 749.252171ms 749.405827ms 749.416787ms 749.476403ms 749.547198ms 749.606896ms 749.633336ms 749.81139ms 749.872156ms 749.907561ms 749.920277ms 750.062132ms 750.085334ms 750.11868ms 750.122375ms 750.143763ms 750.173439ms 750.192008ms 750.240331ms 750.251582ms 750.255171ms 750.274367ms 750.320062ms 750.323741ms 750.481674ms 750.499801ms 750.513761ms 750.514633ms 750.523909ms 750.53202ms 750.716533ms 750.807765ms 750.830384ms 750.830435ms 750.861525ms 751.002951ms 751.020315ms 751.042333ms 751.078721ms 751.09502ms 751.13808ms 751.207817ms 751.226896ms 751.291259ms 751.417767ms 751.517957ms 751.704435ms 752.27559ms 752.275632ms 752.411959ms 752.449163ms 752.478448ms 752.55095ms 752.987246ms 753.094697ms 753.234766ms 753.27681ms 753.318568ms 753.599958ms 753.695654ms 753.884526ms 754.072805ms 754.247959ms 754.505895ms 754.589284ms 754.9893ms 755.139141ms 755.834461ms 756.15303ms 757.768667ms 757.859183ms 758.815709ms 758.930147ms 760.584523ms 762.270693ms 773.590983ms 777.218314ms 804.59751ms]
Feb 24 17:36:31.826: INFO: 50 %ile: 747.617218ms
Feb 24 17:36:31.826: INFO: 90 %ile: 753.599958ms
Feb 24 17:36:31.826: INFO: 99 %ile: 777.218314ms
Feb 24 17:36:31.826: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:31.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3688" for this suite.
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":94,"skipped":1638,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:31.943: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 24 17:36:32.258: INFO: Waiting up to 5m0s for pod "pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e" in namespace "emptydir-1625" to be "Succeeded or Failed"
Feb 24 17:36:32.311: INFO: Pod "pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.341243ms
Feb 24 17:36:34.362: INFO: Pod "pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103959435s
STEP: Saw pod success
Feb 24 17:36:34.362: INFO: Pod "pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e" satisfied condition "Succeeded or Failed"
Feb 24 17:36:34.415: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e container test-container: <nil>
STEP: delete the pod
Feb 24 17:36:34.550: INFO: Waiting for pod pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e to disappear
Feb 24 17:36:34.601: INFO: Pod pod-9ad18b7f-eeb9-46f5-bea5-2f028b0ad40e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:34.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1625" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:34.715: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:36:35.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d" in namespace "projected-9940" to be "Succeeded or Failed"
Feb 24 17:36:35.080: INFO: Pod "downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d": Phase="Pending", Reason="", readiness=false. Elapsed: 51.039747ms
Feb 24 17:36:37.147: INFO: Pod "downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.1176198s
STEP: Saw pod success
Feb 24 17:36:37.147: INFO: Pod "downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d" satisfied condition "Succeeded or Failed"
Feb 24 17:36:37.204: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d container client-container: <nil>
STEP: delete the pod
Feb 24 17:36:37.335: INFO: Waiting for pod downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d to disappear
Feb 24 17:36:37.389: INFO: Pod downwardapi-volume-e73debcc-25e4-4fb4-814d-d761bb43974d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:37.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9940" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1680,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:37.516: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 24 17:36:38.156: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9220 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:36:38.156: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9242 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:36:38.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9252 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 24 17:36:48.635: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9947 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:36:48.635: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9948 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:36:48.635: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-768  15f21ba9-de42-4d34-aa3b-23e1ed58f070 9949 0 2021-02-24 17:36:37 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-24 17:36:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:48.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-768" for this suite.
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":97,"skipped":1683,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:48.748: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:36:51.388: INFO: Waiting up to 5m0s for pod "client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca" in namespace "pods-88" to be "Succeeded or Failed"
Feb 24 17:36:51.439: INFO: Pod "client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca": Phase="Pending", Reason="", readiness=false. Elapsed: 51.298816ms
Feb 24 17:36:53.491: INFO: Pod "client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103250648s
STEP: Saw pod success
Feb 24 17:36:53.491: INFO: Pod "client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca" satisfied condition "Succeeded or Failed"
Feb 24 17:36:53.543: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca container env3cont: <nil>
STEP: delete the pod
Feb 24 17:36:53.684: INFO: Waiting for pod client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca to disappear
Feb 24 17:36:53.739: INFO: Pod client-envvars-40563f95-77ad-4b5e-a816-9a81252b92ca no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:36:53.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-88" for this suite.
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1693,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:36:53.859: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7764" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":99,"skipped":1696,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:11.032: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Feb 24 17:37:11.290: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 create -f -'
Feb 24 17:37:11.898: INFO: stderr: ""
Feb 24 17:37:11.898: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 24 17:37:11.898: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 17:37:12.160: INFO: stderr: ""
Feb 24 17:37:12.160: INFO: stdout: "update-demo-nautilus-gtl87 update-demo-nautilus-jvxt2 "
Feb 24 17:37:12.160: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods update-demo-nautilus-gtl87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 17:37:12.394: INFO: stderr: ""
Feb 24 17:37:12.394: INFO: stdout: ""
Feb 24 17:37:12.394: INFO: update-demo-nautilus-gtl87 is created but not running
Feb 24 17:37:17.394: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 17:37:17.629: INFO: stderr: ""
Feb 24 17:37:17.629: INFO: stdout: "update-demo-nautilus-gtl87 update-demo-nautilus-jvxt2 "
Feb 24 17:37:17.629: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods update-demo-nautilus-gtl87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 17:37:17.903: INFO: stderr: ""
Feb 24 17:37:17.903: INFO: stdout: "true"
Feb 24 17:37:17.903: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods update-demo-nautilus-gtl87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 17:37:18.153: INFO: stderr: ""
Feb 24 17:37:18.153: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 17:37:18.153: INFO: validating pod update-demo-nautilus-gtl87
Feb 24 17:37:18.209: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 17:37:18.209: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 17:37:18.209: INFO: update-demo-nautilus-gtl87 is verified up and running
Feb 24 17:37:18.209: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods update-demo-nautilus-jvxt2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 17:37:18.498: INFO: stderr: ""
Feb 24 17:37:18.498: INFO: stdout: "true"
Feb 24 17:37:18.498: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods update-demo-nautilus-jvxt2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 17:37:18.742: INFO: stderr: ""
Feb 24 17:37:18.742: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 17:37:18.742: INFO: validating pod update-demo-nautilus-jvxt2
Feb 24 17:37:18.798: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 17:37:18.798: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 17:37:18.798: INFO: update-demo-nautilus-jvxt2 is verified up and running
STEP: using delete to clean up resources
Feb 24 17:37:18.798: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 delete --grace-period=0 --force -f -'
Feb 24 17:37:19.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:37:19.124: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 17:37:19.124: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get rc,svc -l name=update-demo --no-headers'
Feb 24 17:37:19.442: INFO: stderr: "No resources found in kubectl-381 namespace.\n"
Feb 24 17:37:19.442: INFO: stdout: ""
Feb 24 17:37:19.442: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 17:37:19.709: INFO: stderr: ""
Feb 24 17:37:19.709: INFO: stdout: "update-demo-nautilus-gtl87\nupdate-demo-nautilus-jvxt2\n"
Feb 24 17:37:20.209: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get rc,svc -l name=update-demo --no-headers'
Feb 24 17:37:20.557: INFO: stderr: "No resources found in kubectl-381 namespace.\n"
Feb 24 17:37:20.557: INFO: stdout: ""
Feb 24 17:37:20.558: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-381 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 17:37:20.804: INFO: stderr: ""
Feb 24 17:37:20.804: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:20.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-381" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":100,"skipped":1704,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:20.918: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:21.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3459" for this suite.
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":101,"skipped":1708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:21.994: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:37:22.250: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 24 17:37:22.575: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:22.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4311" for this suite.
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":102,"skipped":1742,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:22.740: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-34faa03c-d5f2-4a9e-baa6-d2902813c9a2
STEP: Creating a pod to test consume secrets
Feb 24 17:37:23.106: INFO: Waiting up to 5m0s for pod "pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb" in namespace "secrets-7384" to be "Succeeded or Failed"
Feb 24 17:37:23.157: INFO: Pod "pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb": Phase="Pending", Reason="", readiness=false. Elapsed: 51.208617ms
Feb 24 17:37:25.211: INFO: Pod "pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104779158s
STEP: Saw pod success
Feb 24 17:37:25.211: INFO: Pod "pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb" satisfied condition "Succeeded or Failed"
Feb 24 17:37:25.263: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:37:25.379: INFO: Waiting for pod pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb to disappear
Feb 24 17:37:25.431: INFO: Pod pod-secrets-975eed28-c786-4cd3-afc8-e3814b4479eb no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:25.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7384" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:25.544: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-4848ee07-505f-4770-8e4d-85842708e285
STEP: Creating a pod to test consume configMaps
Feb 24 17:37:25.909: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784" in namespace "projected-2734" to be "Succeeded or Failed"
Feb 24 17:37:25.963: INFO: Pod "pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784": Phase="Pending", Reason="", readiness=false. Elapsed: 53.164094ms
Feb 24 17:37:28.016: INFO: Pod "pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106322624s
STEP: Saw pod success
Feb 24 17:37:28.016: INFO: Pod "pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784" satisfied condition "Succeeded or Failed"
Feb 24 17:37:28.068: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:37:28.185: INFO: Waiting for pod pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784 to disappear
Feb 24 17:37:28.236: INFO: Pod pod-projected-configmaps-403cebff-077f-41fe-99b4-11fedd628784 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:28.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2734" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1870,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:28.348: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Feb 24 17:37:28.606: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:37:53.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5862" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":105,"skipped":1873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:37:53.602: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0224 17:38:04.179528   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 17:38:06.408: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:38:06.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1782" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":106,"skipped":1915,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:38:06.516: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:38:24.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-219" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":107,"skipped":1935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:38:24.306: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3486
Feb 24 17:38:26.775: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 24 17:38:27.512: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 24 17:38:27.512: INFO: stdout: "iptables"
Feb 24 17:38:27.512: INFO: proxyMode: iptables
Feb 24 17:38:27.586: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 24 17:38:27.722: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3486
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3486
I0224 17:38:27.834806   10144 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3486, replica count: 3
I0224 17:38:30.935435   10144 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:38:31.038: INFO: Creating new exec pod
Feb 24 17:38:34.245: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Feb 24 17:38:35.947: INFO: rc: 1
Feb 24 17:38:35.947: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip-timeout 80
nc: connect to affinity-clusterip-timeout port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 24 17:38:36.947: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Feb 24 17:38:38.641: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 24 17:38:38.641: INFO: stdout: ""
Feb 24 17:38:38.641: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c nc -zv -t -w 2 10.0.228.70 80'
Feb 24 17:38:39.270: INFO: stderr: "+ nc -zv -t -w 2 10.0.228.70 80\nConnection to 10.0.228.70 80 port [tcp/http] succeeded!\n"
Feb 24 17:38:39.270: INFO: stdout: ""
Feb 24 17:38:39.271: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.228.70:80/ ; done'
Feb 24 17:38:39.956: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n"
Feb 24 17:38:39.956: INFO: stdout: "\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr\naffinity-clusterip-timeout-p26hr"
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Received response from host: affinity-clusterip-timeout-p26hr
Feb 24 17:38:39.956: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.228.70:80/'
Feb 24 17:38:40.610: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n"
Feb 24 17:38:40.610: INFO: stdout: "affinity-clusterip-timeout-p26hr"
Feb 24 17:39:00.610: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3486 exec execpod-affinity5kvb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.228.70:80/'
Feb 24 17:39:01.249: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.228.70:80/\n"
Feb 24 17:39:01.250: INFO: stdout: "affinity-clusterip-timeout-cwr6z"
Feb 24 17:39:01.250: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3486, will wait for the garbage collector to delete the pods
Feb 24 17:39:01.535: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 59.140548ms
Feb 24 17:39:02.335: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 800.334184ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:39:12.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3486" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":108,"skipped":1974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:39:12.691: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 24 17:39:13.418: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 24 17:39:13.519: INFO: starting watch
STEP: patching
STEP: updating
Feb 24 17:39:13.731: INFO: waiting for watch events with expected annotations
Feb 24 17:39:13.731: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:39:14.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1035" for this suite.
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":109,"skipped":2004,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:39:14.350: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-eed2cbb3-bdac-4ce6-a578-164bbb17c013
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:39:14.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3147" for this suite.
{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":110,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:39:14.766: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:39:16.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785156, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785156, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785156, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785156, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:39:19.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:39:20.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1000" for this suite.
STEP: Destroying namespace "webhook-1000-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":111,"skipped":2042,"failed":0}

------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:39:20.914: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-9557
STEP: creating replication controller nodeport-test in namespace services-9557
I0224 17:39:21.286739   10144 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9557, replica count: 2
Feb 24 17:39:24.387: INFO: Creating new exec pod
I0224 17:39:24.387237   10144 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 17:39:27.649: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 24 17:39:28.310: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 24 17:39:28.310: INFO: stdout: ""
Feb 24 17:39:28.311: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 10.0.103.166 80'
Feb 24 17:39:28.917: INFO: stderr: "+ nc -zv -t -w 2 10.0.103.166 80\nConnection to 10.0.103.166 80 port [tcp/http] succeeded!\n"
Feb 24 17:39:28.917: INFO: stdout: ""
Feb 24 17:39:28.917: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.4 31833'
Feb 24 17:39:29.578: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.4 31833\nConnection to 10.138.0.4 31833 port [tcp/31833] succeeded!\n"
Feb 24 17:39:29.578: INFO: stdout: ""
Feb 24 17:39:29.578: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.3 31833'
Feb 24 17:39:30.175: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.3 31833\nConnection to 10.138.0.3 31833 port [tcp/31833] succeeded!\n"
Feb 24 17:39:30.175: INFO: stdout: ""
Feb 24 17:39:30.175: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 104.198.10.242 31833'
Feb 24 17:39:30.789: INFO: stderr: "+ nc -zv -t -w 2 104.198.10.242 31833\nConnection to 104.198.10.242 31833 port [tcp/31833] succeeded!\n"
Feb 24 17:39:30.789: INFO: stdout: ""
Feb 24 17:39:30.789: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-9557 exec execpod2vtvc -- /bin/sh -x -c nc -zv -t -w 2 34.82.218.97 31833'
Feb 24 17:39:31.430: INFO: stderr: "+ nc -zv -t -w 2 34.82.218.97 31833\nConnection to 34.82.218.97 31833 port [tcp/31833] succeeded!\n"
Feb 24 17:39:31.430: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:39:31.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9557" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":112,"skipped":2042,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:39:31.536: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 24 17:39:31.987: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 17:40:32.405: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Feb 24 17:40:32.574: INFO: Created pod: pod0-sched-preemption-low-priority
Feb 24 17:40:32.687: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb 24 17:40:32.799: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:40:55.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6734" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":113,"skipped":2059,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:40:55.997: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:40:56.311: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-12f4619b-b17e-4468-98bc-3eca69babffa" in namespace "security-context-test-1416" to be "Succeeded or Failed"
Feb 24 17:40:56.362: INFO: Pod "busybox-privileged-false-12f4619b-b17e-4468-98bc-3eca69babffa": Phase="Pending", Reason="", readiness=false. Elapsed: 51.24649ms
Feb 24 17:40:58.416: INFO: Pod "busybox-privileged-false-12f4619b-b17e-4468-98bc-3eca69babffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104839089s
Feb 24 17:40:58.416: INFO: Pod "busybox-privileged-false-12f4619b-b17e-4468-98bc-3eca69babffa" satisfied condition "Succeeded or Failed"
Feb 24 17:40:58.479: INFO: Got logs for pod "busybox-privileged-false-12f4619b-b17e-4468-98bc-3eca69babffa": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:40:58.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1416" for this suite.
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":2061,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:40:58.587: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b
Feb 24 17:40:58.995: INFO: Pod name my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b: Found 1 pods out of 1
Feb 24 17:40:58.995: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b" are running
Feb 24 17:41:01.100: INFO: Pod "my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b-9std6" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-24 17:40:58 +0000 UTC Reason: Message:}])
Feb 24 17:41:01.100: INFO: Trying to dial the pod
Feb 24 17:41:06.257: INFO: Controller my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b: Got expected result from replica 1 [my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b-9std6]: "my-hostname-basic-304b74aa-41b0-4b38-b665-621079bd701b-9std6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:41:06.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5116" for this suite.
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":115,"skipped":2075,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:41:06.365: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 24 17:41:09.502: INFO: Pod name wrapped-volume-race-2ccb7651-ae14-44fd-8ea3-13c4eeb5f949: Found 3 pods out of 5
Feb 24 17:41:14.561: INFO: Pod name wrapped-volume-race-2ccb7651-ae14-44fd-8ea3-13c4eeb5f949: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ccb7651-ae14-44fd-8ea3-13c4eeb5f949 in namespace emptydir-wrapper-3349, will wait for the garbage collector to delete the pods
Feb 24 17:41:25.089: INFO: Deleting ReplicationController wrapped-volume-race-2ccb7651-ae14-44fd-8ea3-13c4eeb5f949 took: 54.86188ms
Feb 24 17:41:25.889: INFO: Terminating ReplicationController wrapped-volume-race-2ccb7651-ae14-44fd-8ea3-13c4eeb5f949 pods took: 800.381368ms
STEP: Creating RC which spawns configmap-volume pods
Feb 24 17:41:32.523: INFO: Pod name wrapped-volume-race-a0e4fc24-c23a-4a4d-be3b-405924e2b2cc: Found 3 pods out of 5
Feb 24 17:41:37.579: INFO: Pod name wrapped-volume-race-a0e4fc24-c23a-4a4d-be3b-405924e2b2cc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a0e4fc24-c23a-4a4d-be3b-405924e2b2cc in namespace emptydir-wrapper-3349, will wait for the garbage collector to delete the pods
Feb 24 17:41:50.107: INFO: Deleting ReplicationController wrapped-volume-race-a0e4fc24-c23a-4a4d-be3b-405924e2b2cc took: 54.259232ms
Feb 24 17:41:50.907: INFO: Terminating ReplicationController wrapped-volume-race-a0e4fc24-c23a-4a4d-be3b-405924e2b2cc pods took: 800.31426ms
STEP: Creating RC which spawns configmap-volume pods
Feb 24 17:42:02.681: INFO: Pod name wrapped-volume-race-8a95093d-aad8-42d0-9c3e-8df65713a644: Found 1 pods out of 5
Feb 24 17:42:07.738: INFO: Pod name wrapped-volume-race-8a95093d-aad8-42d0-9c3e-8df65713a644: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8a95093d-aad8-42d0-9c3e-8df65713a644 in namespace emptydir-wrapper-3349, will wait for the garbage collector to delete the pods
Feb 24 17:42:18.267: INFO: Deleting ReplicationController wrapped-volume-race-8a95093d-aad8-42d0-9c3e-8df65713a644 took: 55.54318ms
Feb 24 17:42:19.067: INFO: Terminating ReplicationController wrapped-volume-race-8a95093d-aad8-42d0-9c3e-8df65713a644 pods took: 800.273272ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:42:35.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3349" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":116,"skipped":2091,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:42:35.317: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-91070c3b-d1ee-485a-9c42-3d2beccd4eff
STEP: Creating a pod to test consume configMaps
Feb 24 17:42:35.681: INFO: Waiting up to 5m0s for pod "pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b" in namespace "configmap-838" to be "Succeeded or Failed"
Feb 24 17:42:35.732: INFO: Pod "pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.239334ms
Feb 24 17:42:37.789: INFO: Pod "pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107849701s
STEP: Saw pod success
Feb 24 17:42:37.789: INFO: Pod "pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b" satisfied condition "Succeeded or Failed"
Feb 24 17:42:37.840: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:42:37.974: INFO: Waiting for pod pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b to disappear
Feb 24 17:42:38.025: INFO: Pod pod-configmaps-a00c56e2-eabb-4f65-a348-f5df4dc6449b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:42:38.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-838" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:42:38.130: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-ef9c172c-f832-43cb-858b-ff34328febb8
STEP: Creating a pod to test consume secrets
Feb 24 17:42:38.496: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047" in namespace "projected-8761" to be "Succeeded or Failed"
Feb 24 17:42:38.548: INFO: Pod "pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047": Phase="Pending", Reason="", readiness=false. Elapsed: 52.401088ms
Feb 24 17:42:40.600: INFO: Pod "pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104189717s
STEP: Saw pod success
Feb 24 17:42:40.600: INFO: Pod "pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047" satisfied condition "Succeeded or Failed"
Feb 24 17:42:40.665: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:42:40.785: INFO: Waiting for pod pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047 to disappear
Feb 24 17:42:40.836: INFO: Pod pod-projected-secrets-4b38737e-4a95-48d6-80a2-80107105d047 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:42:40.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8761" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:42:41.030: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:42:41.288: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 24 17:42:41.418: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 24 17:42:45.522: INFO: Creating deployment "test-rolling-update-deployment"
Feb 24 17:42:45.578: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 24 17:42:45.702: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 24 17:42:45.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785365, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785365, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785365, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749785365, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:42:47.805: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 17:42:47.962: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6807  aeade31a-bcde-49fe-b472-f7604e306bfe 11624 1 2021-02-24 17:42:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-02-24 17:42:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 17:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003805cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-24 17:42:45 +0000 UTC,LastTransitionTime:2021-02-24 17:42:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-02-24 17:42:47 +0000 UTC,LastTransitionTime:2021-02-24 17:42:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 17:42:48.014: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-6807  68a4eac0-5674-4488-ab12-1a667ad0a43c 11617 1 2021-02-24 17:42:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment aeade31a-bcde-49fe-b472-f7604e306bfe 0xc0008342c7 0xc0008342c8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeade31a-bcde-49fe-b472-f7604e306bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000834368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:42:48.014: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 24 17:42:48.014: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6807  d0ab66a3-3700-4aae-bbb9-a5659f78b3cc 11623 2 2021-02-24 17:42:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment aeade31a-bcde-49fe-b472-f7604e306bfe 0xc000834107 0xc000834108}] []  [{e2e.test Update apps/v1 2021-02-24 17:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 17:42:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aeade31a-bcde-49fe-b472-f7604e306bfe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000834248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:42:48.065: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-z2srq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-z2srq test-rolling-update-deployment-6b6bf9df46- deployment-6807  61affe6d-9efc-4623-b8eb-c1660cdf9bbf 11616 0 2021-02-24 17:42:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 68a4eac0-5674-4488-ab12-1a667ad0a43c 0xc000834a07 0xc000834a08}] []  [{kube-controller-manager Update v1 2021-02-24 17:42:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68a4eac0-5674-4488-ab12-1a667ad0a43c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:42:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m8x62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m8x62,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m8x62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:42:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:42:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:42:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:42:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.66,StartTime:2021-02-24 17:42:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:42:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://57f75329629fad0e40926c35cb5828d88312f5c298e55e7001385c5faa7d13da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:42:48.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6807" for this suite.
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":119,"skipped":2165,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:42:48.173: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:42:50.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5998" for this suite.
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":120,"skipped":2177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:42:50.856: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0224 17:43:31.500137   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 17:43:33.728: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 17:43:33.728: INFO: Deleting pod "simpletest.rc-2nt6r" in namespace "gc-7230"
Feb 24 17:43:33.794: INFO: Deleting pod "simpletest.rc-blznc" in namespace "gc-7230"
Feb 24 17:43:33.857: INFO: Deleting pod "simpletest.rc-cstk5" in namespace "gc-7230"
Feb 24 17:43:33.921: INFO: Deleting pod "simpletest.rc-hcfkb" in namespace "gc-7230"
Feb 24 17:43:33.984: INFO: Deleting pod "simpletest.rc-jv7km" in namespace "gc-7230"
Feb 24 17:43:34.044: INFO: Deleting pod "simpletest.rc-k9884" in namespace "gc-7230"
Feb 24 17:43:34.113: INFO: Deleting pod "simpletest.rc-kvx4j" in namespace "gc-7230"
Feb 24 17:43:34.178: INFO: Deleting pod "simpletest.rc-px4tf" in namespace "gc-7230"
Feb 24 17:43:34.239: INFO: Deleting pod "simpletest.rc-qq5qc" in namespace "gc-7230"
Feb 24 17:43:34.302: INFO: Deleting pod "simpletest.rc-z2b7s" in namespace "gc-7230"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:34.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7230" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":121,"skipped":2201,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:34.475: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-0d974309-d4ec-4819-a709-63bc02b26304
STEP: Creating a pod to test consume configMaps
Feb 24 17:43:34.840: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964" in namespace "projected-5477" to be "Succeeded or Failed"
Feb 24 17:43:34.892: INFO: Pod "pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964": Phase="Pending", Reason="", readiness=false. Elapsed: 51.564664ms
Feb 24 17:43:36.943: INFO: Pod "pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103144442s
STEP: Saw pod success
Feb 24 17:43:36.943: INFO: Pod "pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964" satisfied condition "Succeeded or Failed"
Feb 24 17:43:36.995: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:43:37.112: INFO: Waiting for pod pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964 to disappear
Feb 24 17:43:37.163: INFO: Pod pod-projected-configmaps-cd57a934-7cac-4693-934b-7dbb438f4964 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:37.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5477" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2201,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:37.375: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Feb 24 17:43:37.658: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 24 17:43:38.064: INFO: stderr: ""
Feb 24 17:43:38.064: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Feb 24 17:43:38.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 24 17:43:38.064: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3814" to be "running and ready, or succeeded"
Feb 24 17:43:38.130: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 65.828671ms
Feb 24 17:43:40.182: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.117984537s
Feb 24 17:43:40.182: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 24 17:43:40.182: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 24 17:43:40.182: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator'
Feb 24 17:43:40.490: INFO: stderr: ""
Feb 24 17:43:40.490: INFO: stdout: "I0224 17:43:38.958001       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5qq 362\nI0224 17:43:39.158233       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/lkj 487\nI0224 17:43:39.358292       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/hbs5 554\nI0224 17:43:39.558068       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/97n 370\nI0224 17:43:39.758140       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/m5vc 386\nI0224 17:43:39.958133       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/tk2j 428\nI0224 17:43:40.158132       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/8jlx 502\nI0224 17:43:40.358164       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nhtx 231\n"
STEP: limiting log lines
Feb 24 17:43:40.490: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator --tail=1'
Feb 24 17:43:40.784: INFO: stderr: ""
Feb 24 17:43:40.784: INFO: stdout: "I0224 17:43:40.558141       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/jg6q 232\n"
Feb 24 17:43:40.784: INFO: got output "I0224 17:43:40.558141       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/jg6q 232\n"
STEP: limiting log bytes
Feb 24 17:43:40.784: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator --limit-bytes=1'
Feb 24 17:43:41.090: INFO: stderr: ""
Feb 24 17:43:41.090: INFO: stdout: "I"
Feb 24 17:43:41.090: INFO: got output "I"
STEP: exposing timestamps
Feb 24 17:43:41.090: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 24 17:43:41.383: INFO: stderr: ""
Feb 24 17:43:41.383: INFO: stdout: "2021-02-24T17:43:41.158262616Z I0224 17:43:41.158033       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/dwq 496\n"
Feb 24 17:43:41.383: INFO: got output "2021-02-24T17:43:41.158262616Z I0224 17:43:41.158033       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/dwq 496\n"
STEP: restricting to a time range
Feb 24 17:43:43.884: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator --since=1s'
Feb 24 17:43:44.293: INFO: stderr: ""
Feb 24 17:43:44.293: INFO: stdout: "I0224 17:43:43.358165       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/9tvc 410\nI0224 17:43:43.558155       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/wct 313\nI0224 17:43:43.758215       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/gfmz 445\nI0224 17:43:43.958216       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/p2l 503\nI0224 17:43:44.158183       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/f9rc 288\n"
Feb 24 17:43:44.293: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 logs logs-generator logs-generator --since=24h'
Feb 24 17:43:44.677: INFO: stderr: ""
Feb 24 17:43:44.677: INFO: stdout: "I0224 17:43:38.958001       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/5qq 362\nI0224 17:43:39.158233       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/lkj 487\nI0224 17:43:39.358292       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/hbs5 554\nI0224 17:43:39.558068       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/97n 370\nI0224 17:43:39.758140       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/m5vc 386\nI0224 17:43:39.958133       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/tk2j 428\nI0224 17:43:40.158132       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/8jlx 502\nI0224 17:43:40.358164       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nhtx 231\nI0224 17:43:40.558141       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/jg6q 232\nI0224 17:43:40.758201       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/45vh 291\nI0224 17:43:40.958201       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/ct9 209\nI0224 17:43:41.158033       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/dwq 496\nI0224 17:43:41.358157       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/5s6 238\nI0224 17:43:41.558166       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/j46 330\nI0224 17:43:41.758251       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/sxp 274\nI0224 17:43:41.958180       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/qp5l 235\nI0224 17:43:42.158184       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/tdxx 320\nI0224 17:43:42.358281       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/x9m 537\nI0224 17:43:42.558156       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/tc9h 364\nI0224 17:43:42.758342       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/sb5v 393\nI0224 17:43:42.958166       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/s7z 404\nI0224 17:43:43.158193       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/6z6x 229\nI0224 17:43:43.358165       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/9tvc 410\nI0224 17:43:43.558155       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/wct 313\nI0224 17:43:43.758215       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/gfmz 445\nI0224 17:43:43.958216       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/p2l 503\nI0224 17:43:44.158183       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/f9rc 288\nI0224 17:43:44.358179       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/m75 420\nI0224 17:43:44.558133       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/7ql7 489\n"
[AfterEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Feb 24 17:43:44.677: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3814 delete pod logs-generator'
Feb 24 17:43:46.806: INFO: stderr: ""
Feb 24 17:43:46.806: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:46.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3814" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":123,"skipped":2212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:46.913: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0224 17:43:48.759625   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 17:43:50.997: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7653" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":124,"skipped":2281,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:51.103: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 17:43:51.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c" in namespace "projected-6005" to be "Succeeded or Failed"
Feb 24 17:43:51.494: INFO: Pod "downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c": Phase="Pending", Reason="", readiness=false. Elapsed: 51.302537ms
Feb 24 17:43:53.546: INFO: Pod "downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103299932s
STEP: Saw pod success
Feb 24 17:43:53.546: INFO: Pod "downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c" satisfied condition "Succeeded or Failed"
Feb 24 17:43:53.598: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c container client-container: <nil>
STEP: delete the pod
Feb 24 17:43:53.735: INFO: Waiting for pod downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c to disappear
Feb 24 17:43:53.787: INFO: Pod downwardapi-volume-18a4dcbf-ceb7-4dba-bdfa-fb8e9f1ec00c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:53.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6005" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2291,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:53.897: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Feb 24 17:43:54.518: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Feb 24 17:43:54.830: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:43:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7347" for this suite.
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":126,"skipped":2292,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:43:55.208: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 24 17:43:58.333: INFO: Successfully updated pod "pod-update-activedeadlineseconds-03d4437d-06b0-45e1-9bf6-9511c8d2289d"
Feb 24 17:43:58.333: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-03d4437d-06b0-45e1-9bf6-9511c8d2289d" in namespace "pods-4100" to be "terminated due to deadline exceeded"
Feb 24 17:43:58.384: INFO: Pod "pod-update-activedeadlineseconds-03d4437d-06b0-45e1-9bf6-9511c8d2289d": Phase="Running", Reason="", readiness=true. Elapsed: 51.1053ms
Feb 24 17:44:00.436: INFO: Pod "pod-update-activedeadlineseconds-03d4437d-06b0-45e1-9bf6-9511c8d2289d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.102745635s
Feb 24 17:44:00.436: INFO: Pod "pod-update-activedeadlineseconds-03d4437d-06b0-45e1-9bf6-9511c8d2289d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:44:00.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4100" for this suite.
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2298,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:44:00.544: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5419
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-5419
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5419
Feb 24 17:44:00.978: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 24 17:44:11.032: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 24 17:44:11.084: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:44:11.734: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:44:11.734: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:44:11.734: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:44:11.786: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 17:44:21.840: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:44:21.840: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:44:22.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999583s
Feb 24 17:44:23.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.928757976s
Feb 24 17:44:24.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.875192678s
Feb 24 17:44:25.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.821748512s
Feb 24 17:44:26.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.767706278s
Feb 24 17:44:27.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.713798033s
Feb 24 17:44:28.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.660482241s
Feb 24 17:44:29.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.60659543s
Feb 24 17:44:30.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.554303695s
Feb 24 17:44:31.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 500.904539ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5419
Feb 24 17:44:32.607: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:44:33.280: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 17:44:33.280: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:44:33.280: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:44:33.280: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:44:33.992: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 17:44:33.992: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:44:33.992: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:44:33.992: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 17:44:34.620: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 17:44:34.620: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 17:44:34.620: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 17:44:34.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:44:34.672: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 17:44:34.672: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 24 17:44:34.727: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:44:35.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:44:35.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:44:35.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:44:35.392: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:44:36.010: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:44:36.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:44:36.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:44:36.011: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-5419 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 17:44:36.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 17:44:36.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 17:44:36.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 17:44:36.689: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:44:36.750: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 24 17:44:46.855: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:44:46.855: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:44:46.855: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 17:44:47.022: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:47.022: INFO: ss-0  bootstrap-e2e-minion-group-2qjz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:00 +0000 UTC  }]
Feb 24 17:44:47.022: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:47.022: INFO: ss-2  bootstrap-e2e-minion-group-9mrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  }]
Feb 24 17:44:47.022: INFO: 
Feb 24 17:44:47.022: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 24 17:44:48.076: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:48.076: INFO: ss-0  bootstrap-e2e-minion-group-2qjz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:00 +0000 UTC  }]
Feb 24 17:44:48.076: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:48.076: INFO: ss-2  bootstrap-e2e-minion-group-9mrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  }]
Feb 24 17:44:48.076: INFO: 
Feb 24 17:44:48.076: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 24 17:44:49.128: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:49.128: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:49.128: INFO: ss-2  bootstrap-e2e-minion-group-9mrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  }]
Feb 24 17:44:49.128: INFO: 
Feb 24 17:44:49.128: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 24 17:44:50.180: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:50.180: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:50.180: INFO: ss-2  bootstrap-e2e-minion-group-9mrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  }]
Feb 24 17:44:50.180: INFO: 
Feb 24 17:44:50.180: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 24 17:44:51.233: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:51.233: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:51.233: INFO: ss-2  bootstrap-e2e-minion-group-9mrc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  }]
Feb 24 17:44:51.233: INFO: 
Feb 24 17:44:51.233: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 24 17:44:52.287: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 24 17:44:52.287: INFO: ss-1  bootstrap-e2e-minion-group-bthl  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-24 17:44:21 +0000 UTC  }]
Feb 24 17:44:52.287: INFO: 
Feb 24 17:44:52.287: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 24 17:44:53.339: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.6734218s
Feb 24 17:44:54.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.621454444s
Feb 24 17:44:55.443: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.569395412s
Feb 24 17:44:56.495: INFO: Verifying statefulset ss doesn't scale past 0 for another 517.372129ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5419
Feb 24 17:44:57.547: INFO: Scaling statefulset ss to 0
Feb 24 17:44:57.704: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 17:44:57.755: INFO: Deleting all statefulset in ns statefulset-5419
Feb 24 17:44:57.807: INFO: Scaling statefulset ss to 0
Feb 24 17:44:57.962: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 17:44:58.014: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:44:58.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5419" for this suite.
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":128,"skipped":2308,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:44:58.279: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-3499cbeb-6be9-47dc-9376-1f33be54904a in namespace container-probe-3837
Feb 24 17:45:00.701: INFO: Started pod busybox-3499cbeb-6be9-47dc-9376-1f33be54904a in namespace container-probe-3837
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 17:45:00.753: INFO: Initial restart count of pod busybox-3499cbeb-6be9-47dc-9376-1f33be54904a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:49:01.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3837" for this suite.
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:49:01.573: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7412 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7412;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7412 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7412;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7412.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7412.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7412.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7412.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7412.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7412.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7412.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 56.96.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.96.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.96.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.96.56_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7412 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7412;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7412 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7412;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7412.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7412.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7412.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7412.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7412.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7412.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7412.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7412.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7412.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 56.96.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.96.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.96.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.96.56_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 17:49:12.267: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.325: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.382: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.440: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.497: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.555: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.613: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:12.670: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.071: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.129: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.186: INFO: Unable to read jessie_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.302: INFO: Unable to read jessie_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.359: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.428: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.485: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:13.831: INFO: Lookups using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7412 wheezy_tcp@dns-test-service.dns-7412 wheezy_udp@dns-test-service.dns-7412.svc wheezy_tcp@dns-test-service.dns-7412.svc wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7412 jessie_tcp@dns-test-service.dns-7412 jessie_udp@dns-test-service.dns-7412.svc jessie_tcp@dns-test-service.dns-7412.svc jessie_udp@_http._tcp.dns-test-service.dns-7412.svc jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc]

Feb 24 17:49:18.889: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:18.947: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.121: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.178: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.236: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.295: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.709: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.824: INFO: Unable to read jessie_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.938: INFO: Unable to read jessie_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:19.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:20.053: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:20.110: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:20.455: INFO: Lookups using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7412 wheezy_tcp@dns-test-service.dns-7412 wheezy_udp@dns-test-service.dns-7412.svc wheezy_tcp@dns-test-service.dns-7412.svc wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7412 jessie_tcp@dns-test-service.dns-7412 jessie_udp@dns-test-service.dns-7412.svc jessie_tcp@dns-test-service.dns-7412.svc jessie_udp@_http._tcp.dns-test-service.dns-7412.svc jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc]

Feb 24 17:49:23.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:23.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.069: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.127: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.184: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.299: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.711: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.769: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.826: INFO: Unable to read jessie_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.884: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:24.942: INFO: Unable to read jessie_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:25.000: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:25.138: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:25.211: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:25.583: INFO: Lookups using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7412 wheezy_tcp@dns-test-service.dns-7412 wheezy_udp@dns-test-service.dns-7412.svc wheezy_tcp@dns-test-service.dns-7412.svc wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7412 jessie_tcp@dns-test-service.dns-7412 jessie_udp@dns-test-service.dns-7412.svc jessie_tcp@dns-test-service.dns-7412.svc jessie_udp@_http._tcp.dns-test-service.dns-7412.svc jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc]

Feb 24 17:49:28.945: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.003: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.060: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.174: INFO: Unable to read wheezy_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.232: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.347: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.765: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.822: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.879: INFO: Unable to read jessie_udp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412 from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:29.993: INFO: Unable to read jessie_udp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:30.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:30.108: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:30.167: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:30.512: INFO: Lookups using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7412 wheezy_tcp@dns-test-service.dns-7412 wheezy_udp@dns-test-service.dns-7412.svc wheezy_tcp@dns-test-service.dns-7412.svc wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7412 jessie_tcp@dns-test-service.dns-7412 jessie_udp@dns-test-service.dns-7412.svc jessie_tcp@dns-test-service.dns-7412.svc jessie_udp@_http._tcp.dns-test-service.dns-7412.svc jessie_tcp@_http._tcp.dns-test-service.dns-7412.svc]

Feb 24 17:49:34.238: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:34.295: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:35.046: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7412.svc from pod dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3: the server could not find the requested resource (get pods dns-test-949ab531-8da8-4c7c-9829-304de79770c3)
Feb 24 17:49:35.485: INFO: Lookups using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-7412.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7412.svc jessie_udp@_http._tcp.dns-test-service.dns-7412.svc]

Feb 24 17:49:40.453: INFO: DNS probes using dns-7412/dns-test-949ab531-8da8-4c7c-9829-304de79770c3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:49:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7412" for this suite.
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":130,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:49:40.859: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-9402/secret-test-ce6402e6-1c04-41fd-b516-e1356ad25324
STEP: Creating a pod to test consume secrets
Feb 24 17:49:41.251: INFO: Waiting up to 5m0s for pod "pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154" in namespace "secrets-9402" to be "Succeeded or Failed"
Feb 24 17:49:41.306: INFO: Pod "pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154": Phase="Pending", Reason="", readiness=false. Elapsed: 55.095068ms
Feb 24 17:49:43.361: INFO: Pod "pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11075339s
STEP: Saw pod success
Feb 24 17:49:43.361: INFO: Pod "pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154" satisfied condition "Succeeded or Failed"
Feb 24 17:49:43.416: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154 container env-test: <nil>
STEP: delete the pod
Feb 24 17:49:43.560: INFO: Waiting for pod pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154 to disappear
Feb 24 17:49:43.615: INFO: Pod pod-configmaps-46e68205-e818-4ac6-bf24-e87ebde92154 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:49:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9402" for this suite.
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2446,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:49:43.744: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 24 17:49:44.195: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 17:50:44.677: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:50:44.733: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:50:45.186: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Feb 24 17:50:45.242: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:50:45.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7916" for this suite.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:50:45.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4275" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":132,"skipped":2450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:50:46.228: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-b88b70bb-3a27-4e8b-9d63-f11cc2340a0e
STEP: Creating a pod to test consume configMaps
Feb 24 17:50:46.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937" in namespace "projected-965" to be "Succeeded or Failed"
Feb 24 17:50:46.679: INFO: Pod "pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937": Phase="Pending", Reason="", readiness=false. Elapsed: 55.062242ms
Feb 24 17:50:48.734: INFO: Pod "pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110488434s
STEP: Saw pod success
Feb 24 17:50:48.734: INFO: Pod "pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937" satisfied condition "Succeeded or Failed"
Feb 24 17:50:48.791: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 24 17:50:48.914: INFO: Waiting for pod pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937 to disappear
Feb 24 17:50:48.969: INFO: Pod pod-projected-configmaps-3cc14c19-609f-4c42-97f0-2eb9310e5937 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:50:48.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-965" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2490,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:50:49.086: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:50:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3937" for this suite.
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":134,"skipped":2497,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:50:53.211: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:51:04.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3270" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":135,"skipped":2511,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:51:05.009: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:51:11.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2135" for this suite.
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":136,"skipped":2523,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:51:11.520: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1dad05c9-9be2-46ad-88fe-66d84008ca2d
STEP: Creating a pod to test consume secrets
Feb 24 17:51:11.915: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250" in namespace "projected-6822" to be "Succeeded or Failed"
Feb 24 17:51:11.970: INFO: Pod "pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250": Phase="Pending", Reason="", readiness=false. Elapsed: 55.253677ms
Feb 24 17:51:14.027: INFO: Pod "pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111433226s
STEP: Saw pod success
Feb 24 17:51:14.027: INFO: Pod "pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250" satisfied condition "Succeeded or Failed"
Feb 24 17:51:14.082: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:51:14.207: INFO: Waiting for pod pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250 to disappear
Feb 24 17:51:14.262: INFO: Pod pod-projected-secrets-ac857326-82a1-416c-8e78-29a62355f250 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:51:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6822" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2523,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:51:14.378: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb 24 17:51:17.569: INFO: Successfully updated pod "labelsupdateac037ec0-4ae5-451d-8ba2-6161f4aaee43"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:51:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-676" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:51:19.817: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-1503a0c6-7862-405c-8ac0-787e195a042c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:51:22.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9351" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:51:22.714: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 in namespace container-probe-7541
Feb 24 17:51:25.162: INFO: Started pod liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 in namespace container-probe-7541
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 17:51:25.218: INFO: Initial restart count of pod liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is 0
Feb 24 17:51:43.791: INFO: Restart count of pod container-probe-7541/liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is now 1 (18.573419796s elapsed)
Feb 24 17:52:04.352: INFO: Restart count of pod container-probe-7541/liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is now 2 (39.134035137s elapsed)
Feb 24 17:52:22.875: INFO: Restart count of pod container-probe-7541/liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is now 3 (57.65755045s elapsed)
Feb 24 17:52:43.433: INFO: Restart count of pod container-probe-7541/liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is now 4 (1m18.215225352s elapsed)
Feb 24 17:53:51.278: INFO: Restart count of pod container-probe-7541/liveness-e4f5df46-5bc2-4883-8ead-6b8aef85cbe8 is now 5 (2m26.060165794s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:53:51.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7541" for this suite.
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2651,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:53:51.458: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:53:51.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3079" for this suite.
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2667,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:53:51.970: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb 24 17:53:52.247: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:53:55.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7875" for this suite.
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":142,"skipped":2688,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:53:55.505: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:53:55.783: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 24 17:53:59.619: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-4265 --namespace=crd-publish-openapi-4265 create -f -'
Feb 24 17:54:00.782: INFO: stderr: ""
Feb 24 17:54:00.782: INFO: stdout: "e2e-test-crd-publish-openapi-6002-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 17:54:00.782: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-4265 --namespace=crd-publish-openapi-4265 delete e2e-test-crd-publish-openapi-6002-crds test-cr'
Feb 24 17:54:01.069: INFO: stderr: ""
Feb 24 17:54:01.069: INFO: stdout: "e2e-test-crd-publish-openapi-6002-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 24 17:54:01.069: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-4265 --namespace=crd-publish-openapi-4265 apply -f -'
Feb 24 17:54:01.590: INFO: stderr: ""
Feb 24 17:54:01.590: INFO: stdout: "e2e-test-crd-publish-openapi-6002-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 17:54:01.590: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-4265 --namespace=crd-publish-openapi-4265 delete e2e-test-crd-publish-openapi-6002-crds test-cr'
Feb 24 17:54:01.893: INFO: stderr: ""
Feb 24 17:54:01.893: INFO: stdout: "e2e-test-crd-publish-openapi-6002-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 24 17:54:01.893: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-4265 explain e2e-test-crd-publish-openapi-6002-crds'
Feb 24 17:54:02.259: INFO: stderr: ""
Feb 24 17:54:02.259: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6002-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:06.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4265" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":143,"skipped":2696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:06.325: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 24 17:54:06.670: INFO: Waiting up to 5m0s for pod "pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95" in namespace "emptydir-8172" to be "Succeeded or Failed"
Feb 24 17:54:06.726: INFO: Pod "pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95": Phase="Pending", Reason="", readiness=false. Elapsed: 55.524054ms
Feb 24 17:54:08.781: INFO: Pod "pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111399777s
STEP: Saw pod success
Feb 24 17:54:08.781: INFO: Pod "pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95" satisfied condition "Succeeded or Failed"
Feb 24 17:54:08.837: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95 container test-container: <nil>
STEP: delete the pod
Feb 24 17:54:08.970: INFO: Waiting for pod pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95 to disappear
Feb 24 17:54:09.025: INFO: Pod pod-b38529cb-815c-41e7-a0bb-ac8c74e02e95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:09.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8172" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2770,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:09.140: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e4fc9064-9df7-4552-a9bd-c1f90c565ab5
STEP: Creating a pod to test consume secrets
Feb 24 17:54:09.536: INFO: Waiting up to 5m0s for pod "pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec" in namespace "secrets-884" to be "Succeeded or Failed"
Feb 24 17:54:09.592: INFO: Pod "pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec": Phase="Pending", Reason="", readiness=false. Elapsed: 55.36756ms
Feb 24 17:54:11.651: INFO: Pod "pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11448472s
STEP: Saw pod success
Feb 24 17:54:11.651: INFO: Pod "pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec" satisfied condition "Succeeded or Failed"
Feb 24 17:54:11.706: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 17:54:11.831: INFO: Waiting for pod pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec to disappear
Feb 24 17:54:11.887: INFO: Pod pod-secrets-36fcb6b5-3c16-43c3-b26f-438c78ddc7ec no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:11.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-884" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2775,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:12.001: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Feb 24 17:54:12.336: INFO: Waiting up to 5m0s for pod "client-containers-efb68c1e-c418-46af-b357-8969278858d7" in namespace "containers-9946" to be "Succeeded or Failed"
Feb 24 17:54:12.392: INFO: Pod "client-containers-efb68c1e-c418-46af-b357-8969278858d7": Phase="Pending", Reason="", readiness=false. Elapsed: 55.458738ms
Feb 24 17:54:14.447: INFO: Pod "client-containers-efb68c1e-c418-46af-b357-8969278858d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111005422s
STEP: Saw pod success
Feb 24 17:54:14.447: INFO: Pod "client-containers-efb68c1e-c418-46af-b357-8969278858d7" satisfied condition "Succeeded or Failed"
Feb 24 17:54:14.503: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod client-containers-efb68c1e-c418-46af-b357-8969278858d7 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:54:14.626: INFO: Waiting for pod client-containers-efb68c1e-c418-46af-b357-8969278858d7 to disappear
Feb 24 17:54:14.681: INFO: Pod client-containers-efb68c1e-c418-46af-b357-8969278858d7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:14.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9946" for this suite.
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:14.796: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 24 17:54:15.186: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:21.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7252" for this suite.
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2798,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:21.156: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 24 17:54:21.435: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8650 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Feb 24 17:54:21.714: INFO: stderr: ""
Feb 24 17:54:21.714: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb 24 17:54:21.714: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8650 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Feb 24 17:54:22.422: INFO: stderr: ""
Feb 24 17:54:22.422: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Feb 24 17:54:22.478: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8650 delete pods e2e-test-httpd-pod'
Feb 24 17:54:32.074: INFO: stderr: ""
Feb 24 17:54:32.074: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8650" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":148,"skipped":2806,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:32.189: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0224 17:54:43.332168   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 17:54:45.576: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 17:54:45.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-58rt2" in namespace "gc-9483"
Feb 24 17:54:45.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g6jd" in namespace "gc-9483"
Feb 24 17:54:45.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-cll55" in namespace "gc-9483"
Feb 24 17:54:45.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-cng8b" in namespace "gc-9483"
Feb 24 17:54:45.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwgtf" in namespace "gc-9483"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:45.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9483" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":149,"skipped":2810,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:46.057: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb 24 17:54:46.394: INFO: Waiting up to 5m0s for pod "downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3" in namespace "downward-api-7729" to be "Succeeded or Failed"
Feb 24 17:54:46.451: INFO: Pod "downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3": Phase="Pending", Reason="", readiness=false. Elapsed: 57.182082ms
Feb 24 17:54:48.507: INFO: Pod "downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.112999426s
STEP: Saw pod success
Feb 24 17:54:48.507: INFO: Pod "downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3" satisfied condition "Succeeded or Failed"
Feb 24 17:54:48.562: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3 container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:54:48.693: INFO: Waiting for pod downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3 to disappear
Feb 24 17:54:48.749: INFO: Pod downward-api-2ebb9b0b-681f-4745-aecf-a76bb05397d3 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:48.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7729" for this suite.
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2821,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:48.863: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:53.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3903" for this suite.
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:53.426: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 24 17:54:53.763: INFO: Waiting up to 5m0s for pod "pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06" in namespace "emptydir-7789" to be "Succeeded or Failed"
Feb 24 17:54:53.818: INFO: Pod "pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06": Phase="Pending", Reason="", readiness=false. Elapsed: 55.191151ms
Feb 24 17:54:55.875: INFO: Pod "pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111508919s
STEP: Saw pod success
Feb 24 17:54:55.875: INFO: Pod "pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06" satisfied condition "Succeeded or Failed"
Feb 24 17:54:55.930: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06 container test-container: <nil>
STEP: delete the pod
Feb 24 17:54:56.056: INFO: Waiting for pod pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06 to disappear
Feb 24 17:54:56.112: INFO: Pod pod-62d2a9d3-1c6a-4310-b786-5b4adf861a06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:54:56.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7789" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2837,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:54:56.226: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-4045def8-c5c6-4600-9aca-58047be16c27
STEP: Creating configMap with name cm-test-opt-upd-a6a6f6b8-1973-4004-9f73-721fe0824c72
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4045def8-c5c6-4600-9aca-58047be16c27
STEP: Updating configmap cm-test-opt-upd-a6a6f6b8-1973-4004-9f73-721fe0824c72
STEP: Creating configMap with name cm-test-opt-create-0f6fa632-ec50-4737-a6b7-5856d33d8532
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:07.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5585" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:07.585: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-f9bbeb20-6c60-4e7a-8474-b47d36ab3545
STEP: Creating a pod to test consume configMaps
Feb 24 17:56:07.985: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c" in namespace "projected-5346" to be "Succeeded or Failed"
Feb 24 17:56:08.040: INFO: Pod "pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c": Phase="Pending", Reason="", readiness=false. Elapsed: 55.161879ms
Feb 24 17:56:10.096: INFO: Pod "pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111149588s
STEP: Saw pod success
Feb 24 17:56:10.096: INFO: Pod "pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c" satisfied condition "Succeeded or Failed"
Feb 24 17:56:10.152: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c container agnhost-container: <nil>
STEP: delete the pod
Feb 24 17:56:10.285: INFO: Waiting for pod pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c to disappear
Feb 24 17:56:10.340: INFO: Pod pod-projected-configmaps-6497570d-b300-482a-b3b0-125d127dc29c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:10.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5346" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2907,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:10.455: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Feb 24 17:56:10.792: INFO: Waiting up to 5m0s for pod "var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07" in namespace "var-expansion-825" to be "Succeeded or Failed"
Feb 24 17:56:10.847: INFO: Pod "var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07": Phase="Pending", Reason="", readiness=false. Elapsed: 54.920118ms
Feb 24 17:56:12.902: INFO: Pod "var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110387062s
STEP: Saw pod success
Feb 24 17:56:12.902: INFO: Pod "var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07" satisfied condition "Succeeded or Failed"
Feb 24 17:56:12.958: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07 container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:56:13.089: INFO: Waiting for pod var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07 to disappear
Feb 24 17:56:13.164: INFO: Pod var-expansion-87009034-2da0-4d39-a5fd-840a6dc11d07 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:13.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-825" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:13.299: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:13.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-495" for this suite.
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":156,"skipped":2946,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:14.037: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:22.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2188" for this suite.
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":157,"skipped":2956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:22.650: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb 24 17:56:22.987: INFO: Waiting up to 5m0s for pod "downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986" in namespace "downward-api-5615" to be "Succeeded or Failed"
Feb 24 17:56:23.042: INFO: Pod "downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986": Phase="Pending", Reason="", readiness=false. Elapsed: 55.280217ms
Feb 24 17:56:25.098: INFO: Pod "downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111138963s
STEP: Saw pod success
Feb 24 17:56:25.098: INFO: Pod "downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986" satisfied condition "Succeeded or Failed"
Feb 24 17:56:25.153: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986 container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:56:25.278: INFO: Waiting for pod downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986 to disappear
Feb 24 17:56:25.340: INFO: Pod downward-api-f1307f47-a68b-411b-bb2f-0ec572eac986 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:25.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5615" for this suite.
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:25.455: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:56:48.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9836" for this suite.
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":3045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:56:48.402: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:56:49.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:56:51.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786209, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:56:54.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3539" for this suite.
STEP: Destroying namespace "webhook-3539-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":160,"skipped":3068,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:08.269: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Feb 24 17:57:08.546: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:33.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1243" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":161,"skipped":3081,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:33.224: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:57:33.561: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-835c7260-eb16-4df8-92f9-712bcf463d90" in namespace "security-context-test-3027" to be "Succeeded or Failed"
Feb 24 17:57:33.617: INFO: Pod "alpine-nnp-false-835c7260-eb16-4df8-92f9-712bcf463d90": Phase="Pending", Reason="", readiness=false. Elapsed: 55.13484ms
Feb 24 17:57:35.673: INFO: Pod "alpine-nnp-false-835c7260-eb16-4df8-92f9-712bcf463d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111810004s
Feb 24 17:57:37.729: INFO: Pod "alpine-nnp-false-835c7260-eb16-4df8-92f9-712bcf463d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.167588577s
Feb 24 17:57:37.729: INFO: Pod "alpine-nnp-false-835c7260-eb16-4df8-92f9-712bcf463d90" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:37.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3027" for this suite.
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":3089,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:37.902: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:57:39.251: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786259, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786259, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786259, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786259, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:57:42.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:53.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9315" for this suite.
STEP: Destroying namespace "webhook-9315-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":163,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:53.755: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:57:54.418: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 24 17:57:54.471: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 24 17:57:54.472: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.472: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 24 17:57:54.472: INFO: Checking APIGroup: apps
Feb 24 17:57:54.525: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 24 17:57:54.525: INFO: Versions found [{apps/v1 v1}]
Feb 24 17:57:54.525: INFO: apps/v1 matches apps/v1
Feb 24 17:57:54.525: INFO: Checking APIGroup: events.k8s.io
Feb 24 17:57:54.579: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 24 17:57:54.579: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.579: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 24 17:57:54.579: INFO: Checking APIGroup: authentication.k8s.io
Feb 24 17:57:54.633: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 24 17:57:54.633: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.633: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 24 17:57:54.633: INFO: Checking APIGroup: authorization.k8s.io
Feb 24 17:57:54.688: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 24 17:57:54.688: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.688: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 24 17:57:54.688: INFO: Checking APIGroup: autoscaling
Feb 24 17:57:54.742: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Feb 24 17:57:54.742: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb 24 17:57:54.742: INFO: autoscaling/v1 matches autoscaling/v1
Feb 24 17:57:54.742: INFO: Checking APIGroup: batch
Feb 24 17:57:54.796: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 24 17:57:54.796: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Feb 24 17:57:54.796: INFO: batch/v1 matches batch/v1
Feb 24 17:57:54.796: INFO: Checking APIGroup: certificates.k8s.io
Feb 24 17:57:54.850: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 24 17:57:54.850: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.850: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 24 17:57:54.850: INFO: Checking APIGroup: networking.k8s.io
Feb 24 17:57:54.904: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 24 17:57:54.904: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:54.904: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 24 17:57:54.904: INFO: Checking APIGroup: extensions
Feb 24 17:57:54.958: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Feb 24 17:57:54.958: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Feb 24 17:57:54.958: INFO: extensions/v1beta1 matches extensions/v1beta1
Feb 24 17:57:54.958: INFO: Checking APIGroup: policy
Feb 24 17:57:55.012: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Feb 24 17:57:55.012: INFO: Versions found [{policy/v1beta1 v1beta1}]
Feb 24 17:57:55.012: INFO: policy/v1beta1 matches policy/v1beta1
Feb 24 17:57:55.012: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 24 17:57:55.066: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 24 17:57:55.066: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.066: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 24 17:57:55.066: INFO: Checking APIGroup: storage.k8s.io
Feb 24 17:57:55.120: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 24 17:57:55.120: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.120: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 24 17:57:55.120: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 24 17:57:55.173: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 24 17:57:55.173: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.173: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 24 17:57:55.173: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 24 17:57:55.236: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 24 17:57:55.236: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.236: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 24 17:57:55.236: INFO: Checking APIGroup: scheduling.k8s.io
Feb 24 17:57:55.305: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 24 17:57:55.305: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Feb 24 17:57:55.305: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 24 17:57:55.305: INFO: Checking APIGroup: coordination.k8s.io
Feb 24 17:57:55.359: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 24 17:57:55.359: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.359: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 24 17:57:55.359: INFO: Checking APIGroup: node.k8s.io
Feb 24 17:57:55.412: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 24 17:57:55.412: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.412: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 24 17:57:55.412: INFO: Checking APIGroup: discovery.k8s.io
Feb 24 17:57:55.466: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Feb 24 17:57:55.466: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.466: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Feb 24 17:57:55.466: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 24 17:57:55.520: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Feb 24 17:57:55.520: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.520: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Feb 24 17:57:55.520: INFO: Checking APIGroup: snapshot.storage.k8s.io
Feb 24 17:57:55.574: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Feb 24 17:57:55.574: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.574: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Feb 24 17:57:55.574: INFO: Checking APIGroup: cloud.google.com
Feb 24 17:57:55.628: INFO: PreferredVersion.GroupVersion: cloud.google.com/v1beta1
Feb 24 17:57:55.628: INFO: Versions found [{cloud.google.com/v1beta1 v1beta1}]
Feb 24 17:57:55.628: INFO: cloud.google.com/v1beta1 matches cloud.google.com/v1beta1
Feb 24 17:57:55.628: INFO: Checking APIGroup: metrics.k8s.io
Feb 24 17:57:55.682: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 24 17:57:55.682: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 24 17:57:55.682: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:55.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9417" for this suite.
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":164,"skipped":3105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:55.798: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Feb 24 17:57:56.135: INFO: Waiting up to 5m0s for pod "var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96" in namespace "var-expansion-7467" to be "Succeeded or Failed"
Feb 24 17:57:56.190: INFO: Pod "var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96": Phase="Pending", Reason="", readiness=false. Elapsed: 55.381083ms
Feb 24 17:57:58.246: INFO: Pod "var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111451067s
STEP: Saw pod success
Feb 24 17:57:58.246: INFO: Pod "var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96" satisfied condition "Succeeded or Failed"
Feb 24 17:57:58.303: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96 container dapi-container: <nil>
STEP: delete the pod
Feb 24 17:57:58.495: INFO: Waiting for pod var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96 to disappear
Feb 24 17:57:58.552: INFO: Pod var-expansion-76410b76-afb6-4a0a-ab01-a193b27f5d96 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:57:58.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7467" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":3154,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:57:58.675: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:57:58.963: INFO: Creating deployment "webserver-deployment"
Feb 24 17:57:59.021: INFO: Waiting for observed generation 1
Feb 24 17:57:59.107: INFO: Waiting for all required pods to come up
Feb 24 17:57:59.200: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 24 17:58:05.343: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 24 17:58:05.478: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 24 17:58:05.594: INFO: Updating deployment webserver-deployment
Feb 24 17:58:05.594: INFO: Waiting for observed generation 2
Feb 24 17:58:07.734: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 24 17:58:07.790: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 24 17:58:07.846: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 17:58:08.011: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 24 17:58:08.011: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 24 17:58:08.067: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 17:58:08.177: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 24 17:58:08.177: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 24 17:58:08.292: INFO: Updating deployment webserver-deployment
Feb 24 17:58:08.292: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 24 17:58:08.509: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 24 17:58:08.597: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 17:58:08.736: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2173  e3d792d4-e7e5-46a6-8325-a7fc012228c6 15139 3 2021-02-24 17:57:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-24 17:57:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fd5108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-24 17:58:08 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-02-24 17:58:08 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 24 17:58:08.806: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2173  11877d53-3ced-47b7-9b68-16185bfcdbdb 15131 3 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e3d792d4-e7e5-46a6-8325-a7fc012228c6 0xc004fd5567 0xc004fd5568}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3d792d4-e7e5-46a6-8325-a7fc012228c6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fd55f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:58:08.806: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 24 17:58:08.806: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2173  590185d3-4f6b-4480-a85d-e3569559ca24 15134 3 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e3d792d4-e7e5-46a6-8325-a7fc012228c6 0xc004fd5657 0xc004fd5658}] []  [{kube-controller-manager Update apps/v1 2021-02-24 17:58:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3d792d4-e7e5-46a6-8325-a7fc012228c6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fd56c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 24 17:58:08.864: INFO: Pod "webserver-deployment-795d758f88-27vvz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-27vvz webserver-deployment-795d758f88- deployment-2173  4389a2c3-5d3f-4417-bbbe-45cdf87d46f6 15144 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc004fd5c17 0xc004fd5c18}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-2tkb4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2tkb4 webserver-deployment-795d758f88- deployment-2173  c6163d70-e59e-4bcc-a9de-37ea5f21b35b 15069 0 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc004fd5e10 0xc004fd5e11}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-2zjgd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2zjgd webserver-deployment-795d758f88- deployment-2173  698c053e-522d-4806-81d9-d7b21aa6b7eb 15133 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc004fd5fb0 0xc004fd5fb1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-67fmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-67fmj webserver-deployment-795d758f88- deployment-2173  c9d4ecb6-4f21-402c-8625-4ef191548681 15130 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068140 0xc005068141}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-8kxct" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8kxct webserver-deployment-795d758f88- deployment-2173  51277846-ea72-442a-b8e3-f745419a4459 15054 0 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068270 0xc005068271}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-blv7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-blv7t webserver-deployment-795d758f88- deployment-2173  dea29710-4e1d-4e3e-a63d-3d4f60d30aa4 15123 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068460 0xc005068461}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.865: INFO: Pod "webserver-deployment-795d758f88-c4xvg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c4xvg webserver-deployment-795d758f88- deployment-2173  b947c731-8281-49cf-972a-4beb160aff50 15145 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068620 0xc005068621}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.866: INFO: Pod "webserver-deployment-795d758f88-ktv82" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ktv82 webserver-deployment-795d758f88- deployment-2173  8e4fed4d-03ce-4a68-a9c1-7ef7c5da7d15 15070 0 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc0050687f0 0xc0050687f1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.866: INFO: Pod "webserver-deployment-795d758f88-nq5s7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nq5s7 webserver-deployment-795d758f88- deployment-2173  36a21681-0e56-4ff6-b6a3-a3f0702942a9 15058 0 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068990 0xc005068991}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.866: INFO: Pod "webserver-deployment-795d758f88-svwnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-svwnj webserver-deployment-795d758f88- deployment-2173  3411c068-0ab4-4665-aa3b-3ee3b7a9dced 15106 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068b70 0xc005068b71}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.866: INFO: Pod "webserver-deployment-795d758f88-trckx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-trckx webserver-deployment-795d758f88- deployment-2173  68f0cef1-4612-43cc-865b-c8dbabd77bb3 15142 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068d50 0xc005068d51}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.869: INFO: Pod "webserver-deployment-795d758f88-w679g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w679g webserver-deployment-795d758f88- deployment-2173  999e0fb8-ff7c-41cc-9846-c069eba3daee 15053 0 2021-02-24 17:58:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc005068ef0 0xc005068ef1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.872: INFO: Pod "webserver-deployment-795d758f88-zkmhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zkmhc webserver-deployment-795d758f88- deployment-2173  09cd4974-7153-4275-ad55-cc0fbe0754a3 15143 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 11877d53-3ced-47b7-9b68-16185bfcdbdb 0xc0050690b0 0xc0050690b1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11877d53-3ced-47b7-9b68-16185bfcdbdb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.878: INFO: Pod "webserver-deployment-dd94f59b7-8xdzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8xdzn webserver-deployment-dd94f59b7- deployment-2173  51a711e4-76b7-4824-809c-f7f1dc499818 15137 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069290 0xc005069291}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.880: INFO: Pod "webserver-deployment-dd94f59b7-9gxzp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9gxzp webserver-deployment-dd94f59b7- deployment-2173  4bff10fd-af0d-495d-850d-a48207433afd 15021 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069410 0xc005069411}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.82,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f703a2405a7fd2032c3214bdb985172bf9543eb765b1e46cc76ec3f04240a8a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.887: INFO: Pod "webserver-deployment-dd94f59b7-9s9vb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9s9vb webserver-deployment-dd94f59b7- deployment-2173  356245d6-5854-4b55-a8c4-a617ec142f07 15126 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc0050695e0 0xc0050695e1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.890: INFO: Pod "webserver-deployment-dd94f59b7-d2ngh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d2ngh webserver-deployment-dd94f59b7- deployment-2173  8130758d-f359-458c-a59b-02cf0bee8b74 15149 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069730 0xc005069731}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.897: INFO: Pod "webserver-deployment-dd94f59b7-jrz2z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jrz2z webserver-deployment-dd94f59b7- deployment-2173  8eb2ae9d-4ebb-429a-a891-2933b785e3ae 14994 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc0050698e0 0xc0050698e1}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.2.54,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://965ffdac1b53a341f12f3bbde5754f46ff8379dabce926b247656fa9aab49d5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.900: INFO: Pod "webserver-deployment-dd94f59b7-lnrgr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lnrgr webserver-deployment-dd94f59b7- deployment-2173  62765d3f-43ae-4363-a179-d6487b3692ca 15008 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069ab0 0xc005069ab1}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.128,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4f00ecc139f70865c769b1c639e61d9f8ea93c0235545cf2c6593b22634a9e6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.907: INFO: Pod "webserver-deployment-dd94f59b7-lqlxp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lqlxp webserver-deployment-dd94f59b7- deployment-2173  093648db-f9da-4868-a1fd-2b138600fd64 15129 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069c50 0xc005069c51}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.910: INFO: Pod "webserver-deployment-dd94f59b7-mx8s5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mx8s5 webserver-deployment-dd94f59b7- deployment-2173  3b9d1021-4eff-4ea2-b1e9-fcfaf849916c 15140 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069d70 0xc005069d71}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.917: INFO: Pod "webserver-deployment-dd94f59b7-qlxll" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qlxll webserver-deployment-dd94f59b7- deployment-2173  539ec2ef-5ef1-4380-a727-448520c99d4a 15124 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc005069f40 0xc005069f41}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.920: INFO: Pod "webserver-deployment-dd94f59b7-rdbf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rdbf5 webserver-deployment-dd94f59b7- deployment-2173  e2abda14-04d7-4a32-8ed0-628868a03822 15136 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508a0e0 0xc00508a0e1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.927: INFO: Pod "webserver-deployment-dd94f59b7-srhmv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-srhmv webserver-deployment-dd94f59b7- deployment-2173  56efacc9-b5b9-4166-921a-a3611eaf174d 15006 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508a310 0xc00508a311}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.130,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a019ea40a5aa2b703b918d4be0dad27dbf01aba0bc9b79bf841976cab2d6130c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.930: INFO: Pod "webserver-deployment-dd94f59b7-sv74h" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sv74h webserver-deployment-dd94f59b7- deployment-2173  5bc656c9-f482-4dee-88e1-e3d511559633 15147 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508a510 0xc00508a511}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.937: INFO: Pod "webserver-deployment-dd94f59b7-vfsqf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vfsqf webserver-deployment-dd94f59b7- deployment-2173  41927e3a-c5e5-4a83-b1dc-34c7bcf97bbc 15125 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508a690 0xc00508a691}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.939: INFO: Pod "webserver-deployment-dd94f59b7-vrwpt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vrwpt webserver-deployment-dd94f59b7- deployment-2173  a3e575e5-1a4d-45d1-a1b9-b4e94da0a585 15015 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508a890 0xc00508a891}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.84,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9390202cb8873ac907659ed61a9b385a7df07b1853bfcc4e2151f580c7aa8ad1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.947: INFO: Pod "webserver-deployment-dd94f59b7-vzqxm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vzqxm webserver-deployment-dd94f59b7- deployment-2173  4f467609-79d3-4e68-8fb5-afe6160c6901 15104 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508aac0 0xc00508aac1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.950: INFO: Pod "webserver-deployment-dd94f59b7-w5lbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5lbz webserver-deployment-dd94f59b7- deployment-2173  2ca2fe90-e26c-4fcb-a3ba-f9adf01029f5 15146 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508acc0 0xc00508acc1}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.957: INFO: Pod "webserver-deployment-dd94f59b7-x2k2k" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x2k2k webserver-deployment-dd94f59b7- deployment-2173  ca4f9027-f11d-496e-98bf-f2b36a672edd 14996 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508ae90 0xc00508ae91}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.2.55,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6ce085904b1f58d886a8abb1381be2ae063b53a4cba6492affc4c310b68ad7df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.959: INFO: Pod "webserver-deployment-dd94f59b7-xrqk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xrqk5 webserver-deployment-dd94f59b7- deployment-2173  86c45625-0aeb-43d4-a4f9-38bc3a2948e7 15141 0 2021-02-24 17:58:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508b020 0xc00508b021}] []  [{kube-controller-manager Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2021-02-24 17:58:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.966: INFO: Pod "webserver-deployment-dd94f59b7-z2hd4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z2hd4 webserver-deployment-dd94f59b7- deployment-2173  10fbbf39-0ce9-4cda-b484-102be2e12314 15000 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508b270 0xc00508b271}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.2.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-9mrc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.2.53,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bcdeae3d7daca03157f1eecaae7d95b539e609c229935cf9beb47ba404155256,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 17:58:08.970: INFO: Pod "webserver-deployment-dd94f59b7-z9dg4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z9dg4 webserver-deployment-dd94f59b7- deployment-2173  88010845-00de-4671-b36e-1751070a9cd7 15018 0 2021-02-24 17:57:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 590185d3-4f6b-4480-a85d-e3569559ca24 0xc00508b440 0xc00508b441}] []  [{kube-controller-manager Update v1 2021-02-24 17:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"590185d3-4f6b-4480-a85d-e3569559ca24\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 17:58:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ff7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ff7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ff7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:58:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 17:57:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.83,StartTime:2021-02-24 17:57:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 17:58:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7c1f20a0e727e2487857b9b12fdebb7258718ffcece255061ffe7cb58b555a5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:58:08.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2173" for this suite.
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":166,"skipped":3171,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:58:09.093: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 24 17:58:09.596: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15162 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:58:09.596: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15162 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 24 17:58:19.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15333 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:58:19.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15333 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 24 17:58:29.826: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15354 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:58:29.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15354 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 24 17:58:39.888: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15375 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:58:39.890: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3252  faff6e41-982a-4997-a52a-c26a5183e8c3 15375 0 2021-02-24 17:58:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 24 17:58:49.950: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3252  ae1046ef-ed96-46d0-bdd4-6b4f56fb8f6d 15396 0 2021-02-24 17:58:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:58:49.950: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3252  ae1046ef-ed96-46d0-bdd4-6b4f56fb8f6d 15396 0 2021-02-24 17:58:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 24 17:59:00.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3252  ae1046ef-ed96-46d0-bdd4-6b4f56fb8f6d 15417 0 2021-02-24 17:58:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 17:59:00.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3252  ae1046ef-ed96-46d0-bdd4-6b4f56fb8f6d 15417 0 2021-02-24 17:58:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-24 17:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:59:10.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3252" for this suite.
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":167,"skipped":3173,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:59:10.129: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-7dpv
STEP: Creating a pod to test atomic-volume-subpath
Feb 24 17:59:10.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7dpv" in namespace "subpath-9839" to be "Succeeded or Failed"
Feb 24 17:59:10.638: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Pending", Reason="", readiness=false. Elapsed: 55.101665ms
Feb 24 17:59:12.694: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.110910515s
Feb 24 17:59:14.750: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 4.166697767s
Feb 24 17:59:16.806: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 6.222451932s
Feb 24 17:59:18.862: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 8.278279327s
Feb 24 17:59:20.917: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 10.334075821s
Feb 24 17:59:22.973: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 12.389770667s
Feb 24 17:59:25.029: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 14.4457867s
Feb 24 17:59:27.093: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 16.509643088s
Feb 24 17:59:29.149: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 18.565729315s
Feb 24 17:59:31.205: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Running", Reason="", readiness=true. Elapsed: 20.621420936s
Feb 24 17:59:33.277: INFO: Pod "pod-subpath-test-downwardapi-7dpv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.693352438s
STEP: Saw pod success
Feb 24 17:59:33.277: INFO: Pod "pod-subpath-test-downwardapi-7dpv" satisfied condition "Succeeded or Failed"
Feb 24 17:59:33.332: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-subpath-test-downwardapi-7dpv container test-container-subpath-downwardapi-7dpv: <nil>
STEP: delete the pod
Feb 24 17:59:33.479: INFO: Waiting for pod pod-subpath-test-downwardapi-7dpv to disappear
Feb 24 17:59:33.534: INFO: Pod pod-subpath-test-downwardapi-7dpv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7dpv
Feb 24 17:59:33.534: INFO: Deleting pod "pod-subpath-test-downwardapi-7dpv" in namespace "subpath-9839"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:59:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9839" for this suite.
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":168,"skipped":3182,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:59:33.707: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 17:59:35.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 17:59:37.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786375, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 17:59:41.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 17:59:41.073: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2988-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:59:41.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-388" for this suite.
STEP: Destroying namespace "webhook-388-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":169,"skipped":3184,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:59:42.074: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Feb 24 17:59:42.352: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 24 17:59:42.352: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:43.073: INFO: stderr: ""
Feb 24 17:59:43.073: INFO: stdout: "service/agnhost-replica created\n"
Feb 24 17:59:43.073: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 24 17:59:43.074: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:43.581: INFO: stderr: ""
Feb 24 17:59:43.581: INFO: stdout: "service/agnhost-primary created\n"
Feb 24 17:59:43.581: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 24 17:59:43.581: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:44.064: INFO: stderr: ""
Feb 24 17:59:44.064: INFO: stdout: "service/frontend created\n"
Feb 24 17:59:44.064: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 24 17:59:44.064: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:44.620: INFO: stderr: ""
Feb 24 17:59:44.620: INFO: stdout: "deployment.apps/frontend created\n"
Feb 24 17:59:44.620: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 17:59:44.620: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:45.096: INFO: stderr: ""
Feb 24 17:59:45.096: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 24 17:59:45.096: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 17:59:45.096: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 create -f -'
Feb 24 17:59:45.619: INFO: stderr: ""
Feb 24 17:59:45.619: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb 24 17:59:45.619: INFO: Waiting for all frontend pods to be Running.
Feb 24 17:59:50.720: INFO: Waiting for frontend to serve content.
Feb 24 17:59:50.786: INFO: Trying to add a new entry to the guestbook.
Feb 24 17:59:50.850: INFO: Verifying that added entry can be retrieved.
Feb 24 17:59:50.913: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Feb 24 17:59:55.976: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:56.295: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:56.295: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb 24 17:59:56.296: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:56.624: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:56.624: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 24 17:59:56.624: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:57.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:57.059: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 24 17:59:57.059: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:57.385: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:57.385: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 24 17:59:57.385: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:57.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:57.719: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 24 17:59:57.719: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-2637 delete --grace-period=0 --force -f -'
Feb 24 17:59:58.042: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 17:59:58.042: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 17:59:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2637" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":170,"skipped":3189,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 17:59:58.192: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6555
STEP: creating service affinity-nodeport in namespace services-6555
STEP: creating replication controller affinity-nodeport in namespace services-6555
I0224 17:59:58.588506   10144 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6555, replica count: 3
I0224 18:00:01.688974   10144 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 18:00:01.857: INFO: Creating new exec pod
Feb 24 18:00:05.138: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Feb 24 18:00:05.902: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 24 18:00:05.902: INFO: stdout: ""
Feb 24 18:00:05.902: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 10.0.5.16 80'
Feb 24 18:00:06.551: INFO: stderr: "+ nc -zv -t -w 2 10.0.5.16 80\nConnection to 10.0.5.16 80 port [tcp/http] succeeded!\n"
Feb 24 18:00:06.551: INFO: stdout: ""
Feb 24 18:00:06.551: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.5 31557'
Feb 24 18:00:07.170: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.5 31557\nConnection to 10.138.0.5 31557 port [tcp/31557] succeeded!\n"
Feb 24 18:00:07.170: INFO: stdout: ""
Feb 24 18:00:07.170: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.3 31557'
Feb 24 18:00:07.773: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.3 31557\nConnection to 10.138.0.3 31557 port [tcp/31557] succeeded!\n"
Feb 24 18:00:07.773: INFO: stdout: ""
Feb 24 18:00:07.773: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 34.105.126.161 31557'
Feb 24 18:00:08.436: INFO: stderr: "+ nc -zv -t -w 2 34.105.126.161 31557\nConnection to 34.105.126.161 31557 port [tcp/31557] succeeded!\n"
Feb 24 18:00:08.436: INFO: stdout: ""
Feb 24 18:00:08.436: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c nc -zv -t -w 2 34.82.218.97 31557'
Feb 24 18:00:09.100: INFO: stderr: "+ nc -zv -t -w 2 34.82.218.97 31557\nConnection to 34.82.218.97 31557 port [tcp/31557] succeeded!\n"
Feb 24 18:00:09.100: INFO: stdout: ""
Feb 24 18:00:09.100: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-6555 exec execpod-affinityzrjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.0.4:31557/ ; done'
Feb 24 18:00:09.860: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:31557/\n"
Feb 24 18:00:09.860: INFO: stdout: "\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h\naffinity-nodeport-6dh6h"
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Received response from host: affinity-nodeport-6dh6h
Feb 24 18:00:09.860: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6555, will wait for the garbage collector to delete the pods
Feb 24 18:00:10.150: INFO: Deleting ReplicationController affinity-nodeport took: 58.241444ms
Feb 24 18:00:10.950: INFO: Terminating ReplicationController affinity-nodeport pods took: 800.278835ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:00:22.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6555" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":171,"skipped":3198,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:00:22.547: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-532
Feb 24 18:00:25.051: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 24 18:00:25.693: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 24 18:00:25.693: INFO: stdout: "iptables"
Feb 24 18:00:25.693: INFO: proxyMode: iptables
Feb 24 18:00:25.758: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 24 18:00:25.814: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-532
STEP: creating replication controller affinity-nodeport-timeout in namespace services-532
I0224 18:00:25.940942   10144 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-532, replica count: 3
I0224 18:00:29.041540   10144 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 18:00:29.210: INFO: Creating new exec pod
Feb 24 18:00:32.492: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Feb 24 18:00:33.088: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 24 18:00:33.088: INFO: stdout: ""
Feb 24 18:00:33.088: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 10.0.64.190 80'
Feb 24 18:00:33.712: INFO: stderr: "+ nc -zv -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
Feb 24 18:00:33.712: INFO: stdout: ""
Feb 24 18:00:33.712: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.4 30442'
Feb 24 18:00:34.337: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.4 30442\nConnection to 10.138.0.4 30442 port [tcp/30442] succeeded!\n"
Feb 24 18:00:34.337: INFO: stdout: ""
Feb 24 18:00:34.337: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.5 30442'
Feb 24 18:00:34.983: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.5 30442\nConnection to 10.138.0.5 30442 port [tcp/30442] succeeded!\n"
Feb 24 18:00:34.983: INFO: stdout: ""
Feb 24 18:00:34.983: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 104.198.10.242 30442'
Feb 24 18:00:35.606: INFO: stderr: "+ nc -zv -t -w 2 104.198.10.242 30442\nConnection to 104.198.10.242 30442 port [tcp/30442] succeeded!\n"
Feb 24 18:00:35.606: INFO: stdout: ""
Feb 24 18:00:35.606: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c nc -zv -t -w 2 34.105.126.161 30442'
Feb 24 18:00:36.280: INFO: stderr: "+ nc -zv -t -w 2 34.105.126.161 30442\nConnection to 34.105.126.161 30442 port [tcp/30442] succeeded!\n"
Feb 24 18:00:36.280: INFO: stdout: ""
Feb 24 18:00:36.280: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.138.0.4:30442/ ; done'
Feb 24 18:00:36.983: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n"
Feb 24 18:00:36.983: INFO: stdout: "\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h\naffinity-nodeport-timeout-z7s4h"
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Received response from host: affinity-nodeport-timeout-z7s4h
Feb 24 18:00:36.983: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.0.4:30442/'
Feb 24 18:00:37.633: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n"
Feb 24 18:00:37.633: INFO: stdout: "affinity-nodeport-timeout-z7s4h"
Feb 24 18:00:57.634: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-532 exec execpod-affinityjrfpt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.138.0.4:30442/'
Feb 24 18:00:58.308: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.138.0.4:30442/\n"
Feb 24 18:00:58.309: INFO: stdout: "affinity-nodeport-timeout-qbs7g"
Feb 24 18:00:58.309: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-532, will wait for the garbage collector to delete the pods
Feb 24 18:00:58.616: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 58.102304ms
Feb 24 18:00:59.416: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 800.242146ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:01:12.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-532" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":172,"skipped":3203,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:01:12.630: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:01:13.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3880" for this suite.
{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":173,"skipped":3208,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:01:13.364: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:01:13.765: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 24 18:01:17.878: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 24 18:01:19.934: INFO: Creating deployment "test-rollover-deployment"
Feb 24 18:01:20.062: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 24 18:01:20.117: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 24 18:01:20.230: INFO: Ensure that both replica sets have 1 created replica
Feb 24 18:01:20.343: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 24 18:01:20.457: INFO: Updating deployment test-rollover-deployment
Feb 24 18:01:20.457: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 24 18:01:22.590: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 24 18:01:22.705: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 24 18:01:22.828: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 18:01:22.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786482, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786479, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:01:24.940: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 18:01:24.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786482, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786479, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:01:26.940: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 18:01:26.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786482, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786479, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:01:28.941: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 18:01:28.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786482, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786479, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:01:30.941: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 18:01:30.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786480, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786482, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786479, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:01:32.940: INFO: 
Feb 24 18:01:32.940: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 18:01:33.108: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8690  8f4deb87-fe1f-4b84-b822-0baf4a5794f9 16171 2 2021-02-24 18:01:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-24 18:01:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 18:01:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fadfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-24 18:01:20 +0000 UTC,LastTransitionTime:2021-02-24 18:01:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-02-24 18:01:32 +0000 UTC,LastTransitionTime:2021-02-24 18:01:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 18:01:33.164: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-8690  1747a420-5040-44dd-b9df-a4dff6f33660 16162 2 2021-02-24 18:01:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8f4deb87-fe1f-4b84-b822-0baf4a5794f9 0xc0020a44b7 0xc0020a44b8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 18:01:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4deb87-fe1f-4b84-b822-0baf4a5794f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020a4558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 18:01:33.164: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 24 18:01:33.164: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8690  31863007-4cc8-4d0c-9bb1-503be0151462 16170 2 2021-02-24 18:01:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8f4deb87-fe1f-4b84-b822-0baf4a5794f9 0xc0020a4387 0xc0020a4388}] []  [{e2e.test Update apps/v1 2021-02-24 18:01:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 18:01:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4deb87-fe1f-4b84-b822-0baf4a5794f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0020a4438 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 18:01:33.164: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8690  ed2a1784-0cf1-4570-8912-72091cc3000d 16133 2 2021-02-24 18:01:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8f4deb87-fe1f-4b84-b822-0baf4a5794f9 0xc0020a45d7 0xc0020a45d8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 18:01:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4deb87-fe1f-4b84-b822-0baf4a5794f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020a4688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 18:01:33.220: INFO: Pod "test-rollover-deployment-668db69979-smbfg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-smbfg test-rollover-deployment-668db69979- deployment-8690  45b67af9-ce55-4ead-a88b-9afba27c3c8c 16141 0 2021-02-24 18:01:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 1747a420-5040-44dd-b9df-a4dff6f33660 0xc0020a4c67 0xc0020a4c68}] []  [{kube-controller-manager Update v1 2021-02-24 18:01:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1747a420-5040-44dd-b9df-a4dff6f33660\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 18:01:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.3.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j4xdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j4xdl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j4xdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-bthl,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:01:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:01:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:01:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:01:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.3.98,StartTime:2021-02-24 18:01:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 18:01:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://0f891924c7cc9bcada271b75a11fd1aea966b2da781500d206994ff408fc7d7b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.3.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:01:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8690" for this suite.
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":174,"skipped":3212,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:01:33.335: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Feb 24 18:01:40.075: INFO: 0 pods remaining
Feb 24 18:01:40.075: INFO: 0 pods has nil DeletionTimestamp
Feb 24 18:01:40.075: INFO: 
STEP: Gathering metrics
W0224 18:01:41.015586   10144 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb 24 18:01:43.279: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:01:43.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8017" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":175,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:01:43.394: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:01:43.791: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Pending, waiting for it to be Running (with Ready = true)
Feb 24 18:01:45.914: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:47.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:49.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:51.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:53.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:55.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:57.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:01:59.895: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:02:01.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:02:03.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:02:05.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:02:07.847: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = false)
Feb 24 18:02:09.848: INFO: The status of Pod test-webserver-e3cbceb4-db3f-4e04-93c1-7fe9a5a63a5a is Running (Ready = true)
Feb 24 18:02:09.903: INFO: Container started at 2021-02-24 18:01:44 +0000 UTC, pod became ready at 2021-02-24 18:02:08 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:09.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-89" for this suite.
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3261,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:10.018: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-7861
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7861 to expose endpoints map[]
Feb 24 18:02:10.527: INFO: successfully validated that service endpoint-test2 in namespace services-7861 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7861
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7861 to expose endpoints map[pod1:[80]]
Feb 24 18:02:12.904: INFO: successfully validated that service endpoint-test2 in namespace services-7861 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7861
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7861 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 24 18:02:15.297: INFO: successfully validated that service endpoint-test2 in namespace services-7861 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7861
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7861 to expose endpoints map[pod2:[80]]
Feb 24 18:02:15.593: INFO: successfully validated that service endpoint-test2 in namespace services-7861 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7861
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7861 to expose endpoints map[]
Feb 24 18:02:15.831: INFO: successfully validated that service endpoint-test2 in namespace services-7861 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:15.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7861" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":177,"skipped":3275,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:16.034: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:02:16.313: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:18.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7292" for this suite.
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:18.889: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb 24 18:02:22.082: INFO: Successfully updated pod "labelsupdateb7340b2c-11af-4a21-be3e-815e5d6df1d6"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:24.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8536" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3330,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:24.313: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4152" for this suite.
STEP: Destroying namespace "nspatchtest-ab8354e4-5590-4054-b288-4f1c4661dd07-7810" for this suite.
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":180,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:25.049: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c3e84aea-03d0-4c8e-9cb6-b749cefaa516
STEP: Creating a pod to test consume configMaps
Feb 24 18:02:25.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4" in namespace "configmap-54" to be "Succeeded or Failed"
Feb 24 18:02:25.502: INFO: Pod "pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4": Phase="Pending", Reason="", readiness=false. Elapsed: 55.176158ms
Feb 24 18:02:27.558: INFO: Pod "pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.111356897s
STEP: Saw pod success
Feb 24 18:02:27.558: INFO: Pod "pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4" satisfied condition "Succeeded or Failed"
Feb 24 18:02:27.613: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:02:27.754: INFO: Waiting for pod pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4 to disappear
Feb 24 18:02:27.809: INFO: Pod pod-configmaps-ce6c1945-cc29-4ea5-8a2a-901edf59dfb4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:02:27.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-54" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3364,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:02:27.923: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 24 18:02:28.372: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 18:03:28.801: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Feb 24 18:03:28.987: INFO: Created pod: pod0-sched-preemption-low-priority
Feb 24 18:03:29.164: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb 24 18:03:29.284: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:03:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9399" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":182,"skipped":3374,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:03:54.565: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:03:55.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4922" for this suite.
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":183,"skipped":3382,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:03:55.354: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-0f243678-75bb-42a3-99f4-9974e863ab89 in namespace container-probe-5993
Feb 24 18:03:57.808: INFO: Started pod test-webserver-0f243678-75bb-42a3-99f4-9974e863ab89 in namespace container-probe-5993
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 18:03:57.863: INFO: Initial restart count of pod test-webserver-0f243678-75bb-42a3-99f4-9974e863ab89 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:07:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5993" for this suite.
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":3382,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:07:58.710: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:07:59.385: INFO: Create a RollingUpdate DaemonSet
Feb 24 18:07:59.444: INFO: Check that daemon pods launch on every node of the cluster
Feb 24 18:07:59.556: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:07:59.613: INFO: Number of nodes with available pods: 0
Feb 24 18:07:59.613: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 18:08:00.670: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:08:00.726: INFO: Number of nodes with available pods: 1
Feb 24 18:08:00.726: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:08:01.670: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:08:01.726: INFO: Number of nodes with available pods: 3
Feb 24 18:08:01.726: INFO: Number of running nodes: 3, number of available pods: 3
Feb 24 18:08:01.726: INFO: Update the DaemonSet to trigger a rollout
Feb 24 18:08:01.840: INFO: Updating DaemonSet daemon-set
Feb 24 18:08:05.069: INFO: Roll back the DaemonSet before rollout is complete
Feb 24 18:08:05.184: INFO: Updating DaemonSet daemon-set
Feb 24 18:08:05.184: INFO: Make sure DaemonSet rollback is complete
Feb 24 18:08:05.241: INFO: Wrong image for pod: daemon-set-9cnmc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 24 18:08:05.241: INFO: Pod daemon-set-9cnmc is not available
Feb 24 18:08:05.299: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:08:06.356: INFO: Wrong image for pod: daemon-set-9cnmc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 24 18:08:06.356: INFO: Pod daemon-set-9cnmc is not available
Feb 24 18:08:06.414: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:08:07.358: INFO: Pod daemon-set-hwxpw is not available
Feb 24 18:08:07.429: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7587, will wait for the garbage collector to delete the pods
Feb 24 18:08:07.886: INFO: Deleting DaemonSet.extensions daemon-set took: 59.036005ms
Feb 24 18:08:08.686: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.327931ms
Feb 24 18:08:22.142: INFO: Number of nodes with available pods: 0
Feb 24 18:08:22.142: INFO: Number of running nodes: 0, number of available pods: 0
Feb 24 18:08:22.197: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17404"},"items":null}

Feb 24 18:08:22.252: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17404"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:22.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7587" for this suite.
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":185,"skipped":3383,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:22.594: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:08:23.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786903, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786903, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786903, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786903, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:08:27.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:08:27.071: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8761-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-965" for this suite.
STEP: Destroying namespace "webhook-965-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":186,"skipped":3387,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:28.725: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-1aaddce8-b84b-4e44-ad48-83bb8fb72808
STEP: Creating a pod to test consume configMaps
Feb 24 18:08:29.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc" in namespace "configmap-2025" to be "Succeeded or Failed"
Feb 24 18:08:29.174: INFO: Pod "pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc": Phase="Pending", Reason="", readiness=false. Elapsed: 55.072086ms
Feb 24 18:08:31.230: INFO: Pod "pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.11073168s
STEP: Saw pod success
Feb 24 18:08:31.230: INFO: Pod "pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc" satisfied condition "Succeeded or Failed"
Feb 24 18:08:31.285: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:08:31.427: INFO: Waiting for pod pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc to disappear
Feb 24 18:08:31.482: INFO: Pod pod-configmaps-87f9da48-8701-4a3e-90f1-85a56a898adc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:31.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2025" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:31.597: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:08:33.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786912, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786912, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786912, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749786912, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:08:36.245: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:08:36.301: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9141" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":188,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:38.794: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:08:39.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475" in namespace "downward-api-2470" to be "Succeeded or Failed"
Feb 24 18:08:39.225: INFO: Pod "downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475": Phase="Pending", Reason="", readiness=false. Elapsed: 55.968689ms
Feb 24 18:08:41.281: INFO: Pod "downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.112002658s
STEP: Saw pod success
Feb 24 18:08:41.281: INFO: Pod "downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475" satisfied condition "Succeeded or Failed"
Feb 24 18:08:41.336: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475 container client-container: <nil>
STEP: delete the pod
Feb 24 18:08:41.461: INFO: Waiting for pod downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475 to disappear
Feb 24 18:08:41.515: INFO: Pod downwardapi-volume-505ce60d-6d36-4810-8e05-847ecadd3475 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2470" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:41.629: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 24 18:08:41.970: INFO: Waiting up to 5m0s for pod "pod-675b5ebf-0286-410c-995f-3c6515d24a35" in namespace "emptydir-2348" to be "Succeeded or Failed"
Feb 24 18:08:42.025: INFO: Pod "pod-675b5ebf-0286-410c-995f-3c6515d24a35": Phase="Pending", Reason="", readiness=false. Elapsed: 55.192026ms
Feb 24 18:08:44.081: INFO: Pod "pod-675b5ebf-0286-410c-995f-3c6515d24a35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110986128s
STEP: Saw pod success
Feb 24 18:08:44.081: INFO: Pod "pod-675b5ebf-0286-410c-995f-3c6515d24a35" satisfied condition "Succeeded or Failed"
Feb 24 18:08:44.136: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-675b5ebf-0286-410c-995f-3c6515d24a35 container test-container: <nil>
STEP: delete the pod
Feb 24 18:08:44.260: INFO: Waiting for pod pod-675b5ebf-0286-410c-995f-3c6515d24a35 to disappear
Feb 24 18:08:44.316: INFO: Pod pod-675b5ebf-0286-410c-995f-3c6515d24a35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:08:44.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2348" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:08:44.435: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8616.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8616.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8616.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8616.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 39.222.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.222.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.222.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.222.39_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8616.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8616.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8616.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8616.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8616.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8616.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 39.222.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.222.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.222.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.222.39_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:08:47.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.183: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.240: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.299: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.709: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.767: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.824: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:47.881: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:48.224: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:08:53.282: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.340: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.397: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.856: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:53.973: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:54.030: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:54.451: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:08:58.283: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.340: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.397: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.454: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.857: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:58.972: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:59.029: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:08:59.410: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:09:03.282: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.339: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.401: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.458: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.860: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:03.974: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:04.031: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:04.374: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:09:08.282: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:08.340: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:08.398: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:08.523: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:08.926: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:08.983: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:09.041: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:09.099: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:09.496: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:09:13.290: INFO: Unable to read wheezy_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.353: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.411: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.469: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.877: INFO: Unable to read jessie_udp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.934: INFO: Unable to read jessie_tcp@dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:13.992: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:14.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local from pod dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124: the server could not find the requested resource (get pods dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124)
Feb 24 18:09:14.397: INFO: Lookups using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 failed for: [wheezy_udp@dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@dns-test-service.dns-8616.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_udp@dns-test-service.dns-8616.svc.cluster.local jessie_tcp@dns-test-service.dns-8616.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8616.svc.cluster.local]

Feb 24 18:09:19.375: INFO: DNS probes using dns-8616/dns-test-d44349a6-7668-4e9f-9468-f8b72cc53124 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:09:19.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8616" for this suite.
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":191,"skipped":3503,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:09:19.965: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:09:22.414: INFO: Deleting pod "var-expansion-7066ab85-eda7-4256-8b34-e9fab59099ab" in namespace "var-expansion-6312"
Feb 24 18:09:22.472: INFO: Wait up to 5m0s for pod "var-expansion-7066ab85-eda7-4256-8b34-e9fab59099ab" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:09:26.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6312" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":192,"skipped":3510,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:09:26.697: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-mk8f
STEP: Creating a pod to test atomic-volume-subpath
Feb 24 18:09:27.146: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mk8f" in namespace "subpath-2158" to be "Succeeded or Failed"
Feb 24 18:09:27.201: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Pending", Reason="", readiness=false. Elapsed: 55.124868ms
Feb 24 18:09:29.257: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.111206734s
Feb 24 18:09:31.313: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.166978514s
Feb 24 18:09:33.370: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 6.223526495s
Feb 24 18:09:35.426: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 8.279840913s
Feb 24 18:09:37.487: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 10.340743299s
Feb 24 18:09:39.543: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.396589912s
Feb 24 18:09:41.599: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 14.452424081s
Feb 24 18:09:43.654: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 16.507601094s
Feb 24 18:09:45.709: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 18.563293758s
Feb 24 18:09:47.765: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Running", Reason="", readiness=true. Elapsed: 20.619201637s
Feb 24 18:09:49.821: INFO: Pod "pod-subpath-test-configmap-mk8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.6748123s
STEP: Saw pod success
Feb 24 18:09:49.821: INFO: Pod "pod-subpath-test-configmap-mk8f" satisfied condition "Succeeded or Failed"
Feb 24 18:09:49.877: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-subpath-test-configmap-mk8f container test-container-subpath-configmap-mk8f: <nil>
STEP: delete the pod
Feb 24 18:09:50.002: INFO: Waiting for pod pod-subpath-test-configmap-mk8f to disappear
Feb 24 18:09:50.057: INFO: Pod pod-subpath-test-configmap-mk8f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mk8f
Feb 24 18:09:50.058: INFO: Deleting pod "pod-subpath-test-configmap-mk8f" in namespace "subpath-2158"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:09:50.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2158" for this suite.
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":193,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:09:50.235: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Feb 24 18:09:50.542: INFO: namespace kubectl-8701
Feb 24 18:09:50.542: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8701 create -f -'
Feb 24 18:09:51.905: INFO: stderr: ""
Feb 24 18:09:51.905: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 24 18:09:52.961: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:09:52.962: INFO: Found 0 / 1
Feb 24 18:09:53.961: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:09:53.961: INFO: Found 1 / 1
Feb 24 18:09:53.961: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 18:09:54.016: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:09:54.016: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 18:09:54.016: INFO: wait on agnhost-primary startup in kubectl-8701 
Feb 24 18:09:54.016: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8701 logs agnhost-primary-ddq8r agnhost-primary'
Feb 24 18:09:54.364: INFO: stderr: ""
Feb 24 18:09:54.364: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 24 18:09:54.364: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8701 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 24 18:09:54.747: INFO: stderr: ""
Feb 24 18:09:54.747: INFO: stdout: "service/rm2 exposed\n"
Feb 24 18:09:54.802: INFO: Service rm2 in namespace kubectl-8701 found.
STEP: exposing service
Feb 24 18:09:56.913: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8701 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 24 18:09:57.268: INFO: stderr: ""
Feb 24 18:09:57.268: INFO: stdout: "service/rm3 exposed\n"
Feb 24 18:09:57.323: INFO: Service rm3 in namespace kubectl-8701 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:09:59.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8701" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":194,"skipped":3532,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:09:59.551: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:10:00.062: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6f72fc62-da39-4e60-b4b4-4a184634dd1d", Controller:(*bool)(0xc00175085e), BlockOwnerDeletion:(*bool)(0xc00175085f)}}
Feb 24 18:10:00.120: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5baf5fc4-bae8-48ae-953e-81fc10057e90", Controller:(*bool)(0xc007680cbe), BlockOwnerDeletion:(*bool)(0xc007680cbf)}}
Feb 24 18:10:00.182: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"91bdbd9b-d582-44b6-b5d4-6bfc29c83192", Controller:(*bool)(0xc004f059a6), BlockOwnerDeletion:(*bool)(0xc004f059a7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:10:05.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4397" for this suite.
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":195,"skipped":3543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:10:05.413: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:10:06.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8094" for this suite.
{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":196,"skipped":3566,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:10:06.222: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1981
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1981
I0224 18:10:06.732265   10144 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1981, replica count: 2
Feb 24 18:10:09.832: INFO: Creating new exec pod
I0224 18:10:09.832821   10144 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 18:10:13.129: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 24 18:10:15.818: INFO: rc: 1
Feb 24 18:10:15.818: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 24 18:10:16.818: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 24 18:10:17.547: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 18:10:17.547: INFO: stdout: ""
Feb 24 18:10:17.548: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 10.0.146.16 80'
Feb 24 18:10:18.223: INFO: stderr: "+ nc -zv -t -w 2 10.0.146.16 80\nConnection to 10.0.146.16 80 port [tcp/http] succeeded!\n"
Feb 24 18:10:18.223: INFO: stdout: ""
Feb 24 18:10:18.223: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.4 30163'
Feb 24 18:10:18.913: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.4 30163\nConnection to 10.138.0.4 30163 port [tcp/30163] succeeded!\n"
Feb 24 18:10:18.913: INFO: stdout: ""
Feb 24 18:10:18.913: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.3 30163'
Feb 24 18:10:19.596: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.3 30163\nConnection to 10.138.0.3 30163 port [tcp/30163] succeeded!\n"
Feb 24 18:10:19.596: INFO: stdout: ""
Feb 24 18:10:19.596: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 104.198.10.242 30163'
Feb 24 18:10:20.261: INFO: stderr: "+ nc -zv -t -w 2 104.198.10.242 30163\nConnection to 104.198.10.242 30163 port [tcp/30163] succeeded!\n"
Feb 24 18:10:20.261: INFO: stdout: ""
Feb 24 18:10:20.261: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-1981 exec execpodgvmlq -- /bin/sh -x -c nc -zv -t -w 2 34.82.218.97 30163'
Feb 24 18:10:20.911: INFO: stderr: "+ nc -zv -t -w 2 34.82.218.97 30163\nConnection to 34.82.218.97 30163 port [tcp/30163] succeeded!\n"
Feb 24 18:10:20.911: INFO: stdout: ""
Feb 24 18:10:20.911: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:10:21.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1981" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":197,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:10:21.184: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:11:21.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6946" for this suite.
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3583,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:11:21.798: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:11:24.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787083, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787083, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787083, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787083, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:11:27.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 24 18:11:29.533: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=webhook-2814 attach --namespace=webhook-2814 to-be-attached-pod -i -c=container1'
Feb 24 18:11:30.025: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:11:30.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2814" for this suite.
STEP: Destroying namespace "webhook-2814-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":199,"skipped":3602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:11:30.533: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-8a3d927e-f0c5-4a12-808a-0059585b4cfd in namespace container-probe-3985
Feb 24 18:11:32.995: INFO: Started pod busybox-8a3d927e-f0c5-4a12-808a-0059585b4cfd in namespace container-probe-3985
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 18:11:33.051: INFO: Initial restart count of pod busybox-8a3d927e-f0c5-4a12-808a-0059585b4cfd is 0
Feb 24 18:12:24.521: INFO: Restart count of pod container-probe-3985/busybox-8a3d927e-f0c5-4a12-808a-0059585b4cfd is now 1 (51.470184441s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:12:24.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3985" for this suite.
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:12:24.713: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-25021b94-9ac4-4501-a42f-505034d2d1ab
STEP: Creating secret with name s-test-opt-upd-67ae1504-e5bd-4b0c-be6f-fbd90d857776
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-25021b94-9ac4-4501-a42f-505034d2d1ab
STEP: Updating secret s-test-opt-upd-67ae1504-e5bd-4b0c-be6f-fbd90d857776
STEP: Creating secret with name s-test-opt-create-f5e734f1-ab9e-4f1e-a977-d3a6859c148d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:12:32.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7018" for this suite.
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3664,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:12:32.229: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 24 18:12:33.738: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:12:33.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2425" for this suite.
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3682,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:12:33.999: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7881
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Feb 24 18:12:34.467: INFO: Found 1 stateful pods, waiting for 3
Feb 24 18:12:44.526: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 18:12:44.526: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 18:12:44.526: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 18:12:44.698: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-7881 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 18:12:45.351: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 18:12:45.351: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 18:12:45.351: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 24 18:12:55.717: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 24 18:12:55.889: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-7881 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 18:12:56.558: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 18:12:56.558: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 18:12:56.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 18:13:06.899: INFO: Waiting for StatefulSet statefulset-7881/ss2 to complete update
Feb 24 18:13:06.899: INFO: Waiting for Pod statefulset-7881/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Feb 24 18:13:17.014: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-7881 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 18:13:17.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 18:13:17.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 18:13:17.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 18:13:28.053: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 24 18:13:28.224: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=statefulset-7881 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 18:13:28.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 18:13:28.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 18:13:28.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 18:13:29.134: INFO: Waiting for StatefulSet statefulset-7881/ss2 to complete update
Feb 24 18:13:29.135: INFO: Waiting for Pod statefulset-7881/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 24 18:13:29.135: INFO: Waiting for Pod statefulset-7881/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 24 18:13:29.135: INFO: Waiting for Pod statefulset-7881/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 24 18:13:39.248: INFO: Waiting for StatefulSet statefulset-7881/ss2 to complete update
Feb 24 18:13:39.248: INFO: Waiting for Pod statefulset-7881/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 24 18:13:39.248: INFO: Waiting for Pod statefulset-7881/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 18:13:49.250: INFO: Deleting all statefulset in ns statefulset-7881
Feb 24 18:13:49.306: INFO: Scaling statefulset ss2 to 0
Feb 24 18:14:29.535: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 18:14:29.588: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:14:29.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7881" for this suite.
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":203,"skipped":3690,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:14:29.916: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Feb 24 18:14:32.479: INFO: Pod pod-hostip-daf5a3dd-4975-4ec5-b590-b97458eaa8de has hostIP: 10.138.0.4
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:14:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7550" for this suite.
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3703,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:14:32.593: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 24 18:14:32.856: INFO: >>> kubeConfig: /workspace/.kube/config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Feb 24 18:14:34.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:36.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:38.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:40.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:42.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:44.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787273, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 18:14:47.522: INFO: Waited 1.346479264s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:14:50.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6439" for this suite.
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":205,"skipped":3705,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:14:50.258: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3f87c40d-ef81-4c60-bf6b-931c62a066dc
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3f87c40d-ef81-4c60-bf6b-931c62a066dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:14:55.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7463" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3724,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:14:55.216: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-11120693-43a5-444e-8d91-65f1ec361486
STEP: Creating a pod to test consume secrets
Feb 24 18:14:55.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4" in namespace "projected-1140" to be "Succeeded or Failed"
Feb 24 18:14:55.708: INFO: Pod "pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.049541ms
Feb 24 18:14:57.762: INFO: Pod "pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106178857s
STEP: Saw pod success
Feb 24 18:14:57.762: INFO: Pod "pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4" satisfied condition "Succeeded or Failed"
Feb 24 18:14:57.816: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 24 18:14:57.954: INFO: Waiting for pod pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4 to disappear
Feb 24 18:14:58.006: INFO: Pod pod-projected-secrets-6f13076c-c5ef-4bd6-ac96-0b06b91346f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:14:58.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1140" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:14:58.121: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8366
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 24 18:14:58.382: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 18:14:58.716: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 18:15:00.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:02.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:04.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:06.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:08.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:10.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:15:12.769: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 24 18:15:12.876: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 24 18:15:14.931: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 24 18:15:16.930: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 24 18:15:18.931: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 24 18:15:19.037: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 24 18:15:21.331: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 18:15:21.331: INFO: Breadth first check of 10.64.1.181 on host 10.138.0.4...
Feb 24 18:15:21.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.182:9080/dial?request=hostname&protocol=http&host=10.64.1.181&port=8080&tries=1'] Namespace:pod-network-test-8366 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:15:21.383: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:15:21.750: INFO: Waiting for responses: map[]
Feb 24 18:15:21.750: INFO: reached 10.64.1.181 after 0/1 tries
Feb 24 18:15:21.750: INFO: Breadth first check of 10.64.2.81 on host 10.138.0.3...
Feb 24 18:15:21.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.182:9080/dial?request=hostname&protocol=http&host=10.64.2.81&port=8080&tries=1'] Namespace:pod-network-test-8366 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:15:21.803: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:15:22.165: INFO: Waiting for responses: map[]
Feb 24 18:15:22.165: INFO: reached 10.64.2.81 after 0/1 tries
Feb 24 18:15:22.165: INFO: Breadth first check of 10.64.3.115 on host 10.138.0.5...
Feb 24 18:15:22.217: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.182:9080/dial?request=hostname&protocol=http&host=10.64.3.115&port=8080&tries=1'] Namespace:pod-network-test-8366 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:15:22.217: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:15:22.663: INFO: Waiting for responses: map[]
Feb 24 18:15:22.663: INFO: reached 10.64.3.115 after 0/1 tries
Feb 24 18:15:22.663: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:15:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8366" for this suite.
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:15:22.779: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:15:39.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5537" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":209,"skipped":3799,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:15:39.952: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 24 18:15:40.271: INFO: Waiting up to 5m0s for pod "pod-65682f87-3c43-4d0e-8bed-53ed4379add9" in namespace "emptydir-7682" to be "Succeeded or Failed"
Feb 24 18:15:40.323: INFO: Pod "pod-65682f87-3c43-4d0e-8bed-53ed4379add9": Phase="Pending", Reason="", readiness=false. Elapsed: 51.833095ms
Feb 24 18:15:42.375: INFO: Pod "pod-65682f87-3c43-4d0e-8bed-53ed4379add9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103823529s
STEP: Saw pod success
Feb 24 18:15:42.375: INFO: Pod "pod-65682f87-3c43-4d0e-8bed-53ed4379add9" satisfied condition "Succeeded or Failed"
Feb 24 18:15:42.428: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-65682f87-3c43-4d0e-8bed-53ed4379add9 container test-container: <nil>
STEP: delete the pod
Feb 24 18:15:42.545: INFO: Waiting for pod pod-65682f87-3c43-4d0e-8bed-53ed4379add9 to disappear
Feb 24 18:15:42.598: INFO: Pod pod-65682f87-3c43-4d0e-8bed-53ed4379add9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:15:42.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7682" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3811,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:15:42.749: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 24 18:15:43.222: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5662  a2ca5f46-0320-4350-bedf-6c8735700272 19330 0 2021-02-24 18:15:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-24 18:15:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 18:15:43.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5662  a2ca5f46-0320-4350-bedf-6c8735700272 19331 0 2021-02-24 18:15:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-24 18:15:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 24 18:15:43.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5662  a2ca5f46-0320-4350-bedf-6c8735700272 19332 0 2021-02-24 18:15:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-24 18:15:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 18:15:43.432: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5662  a2ca5f46-0320-4350-bedf-6c8735700272 19333 0 2021-02-24 18:15:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-24 18:15:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:15:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5662" for this suite.
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":211,"skipped":3813,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:15:43.545: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:15:43.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf" in namespace "projected-9645" to be "Succeeded or Failed"
Feb 24 18:15:43.917: INFO: Pod "downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 51.914253ms
Feb 24 18:15:45.969: INFO: Pod "downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104301998s
STEP: Saw pod success
Feb 24 18:15:45.969: INFO: Pod "downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf" satisfied condition "Succeeded or Failed"
Feb 24 18:15:46.023: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf container client-container: <nil>
STEP: delete the pod
Feb 24 18:15:46.142: INFO: Waiting for pod downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf to disappear
Feb 24 18:15:46.193: INFO: Pod downwardapi-volume-6f8b5702-3aa2-4852-af0b-b37a30c97cbf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:15:46.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9645" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3823,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:15:46.307: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:16:16.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1782" for this suite.
STEP: Destroying namespace "nsdeletetest-2919" for this suite.
Feb 24 18:16:16.474: INFO: Namespace nsdeletetest-2919 was already deleted
STEP: Destroying namespace "nsdeletetest-1883" for this suite.
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":213,"skipped":3828,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:16:16.528: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:16:19.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2213" for this suite.
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3838,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:16:19.175: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 24 18:16:19.598: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 18:17:20.046: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:17:20.097: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb 24 18:17:22.681: INFO: found a healthy node: bootstrap-e2e-minion-group-2qjz
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:17:31.491: INFO: pods created so far: [1 1 1]
Feb 24 18:17:31.491: INFO: length of pods created so far: 3
Feb 24 18:17:47.598: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:17:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4869" for this suite.
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:17:54.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7795" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":215,"skipped":3842,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:17:55.440: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-3ec11e24-b367-44c5-8588-0c8157ccdab7
STEP: Creating a pod to test consume secrets
Feb 24 18:17:55.816: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59" in namespace "projected-2755" to be "Succeeded or Failed"
Feb 24 18:17:55.868: INFO: Pod "pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59": Phase="Pending", Reason="", readiness=false. Elapsed: 52.068437ms
Feb 24 18:17:57.921: INFO: Pod "pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104399136s
STEP: Saw pod success
Feb 24 18:17:57.921: INFO: Pod "pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59" satisfied condition "Succeeded or Failed"
Feb 24 18:17:57.973: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 24 18:17:58.099: INFO: Waiting for pod pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59 to disappear
Feb 24 18:17:58.151: INFO: Pod pod-projected-secrets-7ef88852-9f9f-4aeb-ac75-80c7bfd42f59 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:17:58.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2755" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":216,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:17:58.261: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 24 18:18:03.049: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:03.101: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 18:18:05.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:05.156: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 18:18:07.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:07.154: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 18:18:09.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:09.155: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 18:18:11.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:11.154: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 18:18:13.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 18:18:13.155: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:18:13.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3189" for this suite.
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3901,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:18:13.320: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 24 18:18:13.693: INFO: Waiting up to 5m0s for pod "pod-4cbf217d-c586-413a-ad31-928f214eb9b7" in namespace "emptydir-9263" to be "Succeeded or Failed"
Feb 24 18:18:13.745: INFO: Pod "pod-4cbf217d-c586-413a-ad31-928f214eb9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.096545ms
Feb 24 18:18:15.798: INFO: Pod "pod-4cbf217d-c586-413a-ad31-928f214eb9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104648392s
STEP: Saw pod success
Feb 24 18:18:15.798: INFO: Pod "pod-4cbf217d-c586-413a-ad31-928f214eb9b7" satisfied condition "Succeeded or Failed"
Feb 24 18:18:15.850: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-4cbf217d-c586-413a-ad31-928f214eb9b7 container test-container: <nil>
STEP: delete the pod
Feb 24 18:18:16.007: INFO: Waiting for pod pod-4cbf217d-c586-413a-ad31-928f214eb9b7 to disappear
Feb 24 18:18:16.059: INFO: Pod pod-4cbf217d-c586-413a-ad31-928f214eb9b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:18:16.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9263" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3907,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:18:16.169: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:18:16.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2008" for this suite.
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":219,"skipped":3909,"failed":0}
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:18:16.832: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:18:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2990" for this suite.
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":220,"skipped":3912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:18:17.304: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:18:19.945: INFO: DNS probes using dns-test-dc06f528-fb30-4de2-8319-4d66d43a4994 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:18:22.406: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:22.461: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:22.461: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:27.517: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:27.572: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:27.572: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:32.517: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:32.571: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:32.571: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:37.517: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:37.572: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:37.572: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:42.516: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:42.571: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:42.571: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:47.516: INFO: File wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:47.570: INFO: File jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local from pod  dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 18:18:47.570: INFO: Lookups using dns-8622/dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 failed for: [wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local]

Feb 24 18:18:52.570: INFO: DNS probes using dns-test-90535fdc-79bf-45a2-920c-44c371bd4660 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8622.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8622.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:18:55.123: INFO: DNS probes using dns-test-677a07d6-5aaf-46c9-a0ba-f338a643212c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:18:55.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8622" for this suite.
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":221,"skipped":3955,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:18:55.358: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-c08c83ce-0a85-41eb-930e-0208d2dfe082
STEP: Creating configMap with name cm-test-opt-upd-c21e390f-30c4-40f2-ace0-9a9513495abb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c08c83ce-0a85-41eb-930e-0208d2dfe082
STEP: Updating configmap cm-test-opt-upd-c21e390f-30c4-40f2-ace0-9a9513495abb
STEP: Creating configMap with name cm-test-opt-create-5f5266e6-c14e-4e89-ad00-89df3749b851
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:02.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1058" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3959,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:02.725: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 24 18:19:05.834: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5538 pod-service-account-1296ab09-f62d-4bb1-a56d-3968fbbc0722 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 24 18:19:06.582: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5538 pod-service-account-1296ab09-f62d-4bb1-a56d-3968fbbc0722 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 24 18:19:07.239: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5538 pod-service-account-1296ab09-f62d-4bb1-a56d-3968fbbc0722 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5538" for this suite.
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":223,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:08.135: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a32940a0-7685-45e2-b3a0-0edbc13c3de7
STEP: Creating a pod to test consume configMaps
Feb 24 18:19:08.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55" in namespace "configmap-6557" to be "Succeeded or Failed"
Feb 24 18:19:08.563: INFO: Pod "pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55": Phase="Pending", Reason="", readiness=false. Elapsed: 57.585296ms
Feb 24 18:19:10.616: INFO: Pod "pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.110316433s
STEP: Saw pod success
Feb 24 18:19:10.616: INFO: Pod "pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55" satisfied condition "Succeeded or Failed"
Feb 24 18:19:10.668: INFO: Trying to get logs from node bootstrap-e2e-minion-group-9mrc pod pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 24 18:19:10.804: INFO: Waiting for pod pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55 to disappear
Feb 24 18:19:10.856: INFO: Pod pod-configmaps-287e30c3-6575-4012-aacc-3f6701e60e55 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:10.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6557" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:10.964: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:19:12.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787551, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787551, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787552, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787551, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:19:15.235: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:19:15.287: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:17.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6383" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":225,"skipped":4067,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:19.005: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:19:19.324: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5" in namespace "projected-4295" to be "Succeeded or Failed"
Feb 24 18:19:19.378: INFO: Pod "downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.682173ms
Feb 24 18:19:21.431: INFO: Pod "downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106434296s
STEP: Saw pod success
Feb 24 18:19:21.432: INFO: Pod "downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5" satisfied condition "Succeeded or Failed"
Feb 24 18:19:21.484: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5 container client-container: <nil>
STEP: delete the pod
Feb 24 18:19:21.602: INFO: Waiting for pod downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5 to disappear
Feb 24 18:19:21.653: INFO: Pod downwardapi-volume-5c3d3956-9c0c-4bcf-bfbc-7ddc75be36d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:21.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4295" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":4084,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:21.761: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-151272b5-5a74-4dae-be7a-d19562dcb384
STEP: Creating a pod to test consume configMaps
Feb 24 18:19:22.133: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b" in namespace "projected-7599" to be "Succeeded or Failed"
Feb 24 18:19:22.185: INFO: Pod "pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.959256ms
Feb 24 18:19:24.238: INFO: Pod "pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104559224s
STEP: Saw pod success
Feb 24 18:19:24.238: INFO: Pod "pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b" satisfied condition "Succeeded or Failed"
Feb 24 18:19:24.291: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:19:24.408: INFO: Waiting for pod pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b to disappear
Feb 24 18:19:24.459: INFO: Pod pod-projected-configmaps-34ed7888-ed7d-4995-93f0-2e1dca6b7f5b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:24.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7599" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":4088,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:24.567: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:24.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8791" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":228,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:25.090: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:19:27.677: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:27.732: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:27.786: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:27.928: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:28.091: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:28.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:28.199: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:28.253: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:28.362: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:33.416: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.580: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.836: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.897: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:33.951: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:34.060: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:38.416: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.471: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.580: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.743: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.797: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.851: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:38.906: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:39.015: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:43.418: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.526: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.580: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.742: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.857: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.911: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:43.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:44.079: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:48.418: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.527: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.582: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.744: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.798: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.853: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:48.907: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:49.016: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:53.417: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.471: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.528: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.600: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.769: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.824: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.918: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:53.972: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local from pod dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91: the server could not find the requested resource (get pods dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91)
Feb 24 18:19:54.080: INFO: Lookups using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6481.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6481.svc.cluster.local jessie_udp@dns-test-service-2.dns-6481.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6481.svc.cluster.local]

Feb 24 18:19:59.014: INFO: DNS probes using dns-6481/dns-test-1746d468-3c65-4e4d-8111-784aa6e83b91 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:19:59.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6481" for this suite.
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":229,"skipped":4120,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:19:59.282: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Feb 24 18:19:59.604: INFO: Waiting up to 5m0s for pod "var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c" in namespace "var-expansion-2204" to be "Succeeded or Failed"
Feb 24 18:19:59.665: INFO: Pod "var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c": Phase="Pending", Reason="", readiness=false. Elapsed: 60.894598ms
Feb 24 18:20:01.722: INFO: Pod "var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118228367s
STEP: Saw pod success
Feb 24 18:20:01.722: INFO: Pod "var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c" satisfied condition "Succeeded or Failed"
Feb 24 18:20:01.775: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c container dapi-container: <nil>
STEP: delete the pod
Feb 24 18:20:01.902: INFO: Waiting for pod var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c to disappear
Feb 24 18:20:01.953: INFO: Pod var-expansion-61005c07-8f21-4670-b6c2-9a163e945a3c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2204" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":230,"skipped":4134,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:02.063: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Feb 24 18:20:02.328: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3640 create -f -'
Feb 24 18:20:03.798: INFO: stderr: ""
Feb 24 18:20:03.798: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb 24 18:20:03.798: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3640 diff -f -'
Feb 24 18:20:04.782: INFO: rc: 1
Feb 24 18:20:04.782: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3640 delete -f -'
Feb 24 18:20:05.139: INFO: stderr: ""
Feb 24 18:20:05.139: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:05.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3640" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":231,"skipped":4143,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:05.247: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:20:05.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093" in namespace "projected-351" to be "Succeeded or Failed"
Feb 24 18:20:05.619: INFO: Pod "downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093": Phase="Pending", Reason="", readiness=false. Elapsed: 51.982497ms
Feb 24 18:20:07.672: INFO: Pod "downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104235331s
STEP: Saw pod success
Feb 24 18:20:07.672: INFO: Pod "downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093" satisfied condition "Succeeded or Failed"
Feb 24 18:20:07.724: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093 container client-container: <nil>
STEP: delete the pod
Feb 24 18:20:07.855: INFO: Waiting for pod downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093 to disappear
Feb 24 18:20:07.907: INFO: Pod downwardapi-volume-aed6f3fa-f8cf-4b3a-9e99-f1f0e2ac0093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:07.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-351" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4143,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:08.015: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Feb 24 18:20:08.329: INFO: created test-podtemplate-1
Feb 24 18:20:08.383: INFO: created test-podtemplate-2
Feb 24 18:20:08.436: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb 24 18:20:08.488: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb 24 18:20:08.551: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:08.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2751" for this suite.
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":233,"skipped":4156,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:08.710: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-5ca717ec-0344-4185-960c-d3f7fffe0559
STEP: Creating a pod to test consume configMaps
Feb 24 18:20:09.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8650463-3976-4636-b558-c382133ecf33" in namespace "configmap-2776" to be "Succeeded or Failed"
Feb 24 18:20:09.136: INFO: Pod "pod-configmaps-f8650463-3976-4636-b558-c382133ecf33": Phase="Pending", Reason="", readiness=false. Elapsed: 51.904818ms
Feb 24 18:20:11.254: INFO: Pod "pod-configmaps-f8650463-3976-4636-b558-c382133ecf33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.169581794s
STEP: Saw pod success
Feb 24 18:20:11.254: INFO: Pod "pod-configmaps-f8650463-3976-4636-b558-c382133ecf33" satisfied condition "Succeeded or Failed"
Feb 24 18:20:11.305: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-configmaps-f8650463-3976-4636-b558-c382133ecf33 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:20:11.421: INFO: Waiting for pod pod-configmaps-f8650463-3976-4636-b558-c382133ecf33 to disappear
Feb 24 18:20:11.473: INFO: Pod pod-configmaps-f8650463-3976-4636-b558-c382133ecf33 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:11.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2776" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4166,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:11.582: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:20:11.950: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 24 18:20:14.153: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb 24 18:20:17.091: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7816  7b691cf9-3c0b-4c0b-9e94-cba7e4877256 20572 1 2021-02-24 18:20:14 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-02-24 18:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-24 18:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00307f858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-24 18:20:14 +0000 UTC,LastTransitionTime:2021-02-24 18:20:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-02-24 18:20:16 +0000 UTC,LastTransitionTime:2021-02-24 18:20:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 18:20:17.145: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-7816  72bbd570-7ddd-4e71-9ef1-1670c568c631 20565 1 2021-02-24 18:20:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7b691cf9-3c0b-4c0b-9e94-cba7e4877256 0xc00307fbf7 0xc00307fbf8}] []  [{kube-controller-manager Update apps/v1 2021-02-24 18:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b691cf9-3c0b-4c0b-9e94-cba7e4877256\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00307fc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 18:20:17.197: INFO: Pod "test-cleanup-deployment-685c4f8568-8qh88" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-8qh88 test-cleanup-deployment-685c4f8568- deployment-7816  e51cb9f3-08df-425f-9d41-281138ee5729 20564 0 2021-02-24 18:20:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 72bbd570-7ddd-4e71-9ef1-1670c568c631 0xc005059f47 0xc005059f48}] []  [{kube-controller-manager Update v1 2021-02-24 18:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72bbd570-7ddd-4e71-9ef1-1670c568c631\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 18:20:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7sq76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7sq76,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7sq76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:20:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.203,StartTime:2021-02-24 18:20:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 18:20:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://3fca804b82be5ad81ea4df7bbc09198d7a8c7c7db344caf602710fca05adf846,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:17.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7816" for this suite.
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":235,"skipped":4182,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:17.306: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3936
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3936
STEP: creating replication controller externalsvc in namespace services-3936
I0224 18:20:17.743353   10144 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3936, replica count: 2
I0224 18:20:20.843836   10144 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 24 18:20:21.012: INFO: Creating new exec pod
Feb 24 18:20:23.170: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-3936 exec execpodx2r27 -- /bin/sh -x -c nslookup clusterip-service.services-3936.svc.cluster.local'
Feb 24 18:20:23.887: INFO: stderr: "+ nslookup clusterip-service.services-3936.svc.cluster.local\n"
Feb 24 18:20:23.887: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-3936.svc.cluster.local\tcanonical name = externalsvc.services-3936.svc.cluster.local.\nName:\texternalsvc.services-3936.svc.cluster.local\nAddress: 10.0.70.245\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3936, will wait for the garbage collector to delete the pods
Feb 24 18:20:24.094: INFO: Deleting ReplicationController externalsvc took: 54.591965ms
Feb 24 18:20:24.894: INFO: Terminating ReplicationController externalsvc pods took: 800.322637ms
Feb 24 18:20:32.279: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:32.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3936" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":236,"skipped":4194,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:32.477: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:20:33.060: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 24 18:20:33.167: INFO: Number of nodes with available pods: 0
Feb 24 18:20:33.167: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 24 18:20:33.384: INFO: Number of nodes with available pods: 0
Feb 24 18:20:33.384: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:34.437: INFO: Number of nodes with available pods: 0
Feb 24 18:20:34.437: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:35.437: INFO: Number of nodes with available pods: 1
Feb 24 18:20:35.437: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 24 18:20:35.652: INFO: Number of nodes with available pods: 0
Feb 24 18:20:35.652: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 24 18:20:35.760: INFO: Number of nodes with available pods: 0
Feb 24 18:20:35.760: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:36.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:36.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:37.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:37.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:38.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:38.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:39.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:39.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:40.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:40.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:41.813: INFO: Number of nodes with available pods: 0
Feb 24 18:20:41.813: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:42.812: INFO: Number of nodes with available pods: 0
Feb 24 18:20:42.812: INFO: Node bootstrap-e2e-minion-group-9mrc is running more than one daemon pod
Feb 24 18:20:43.813: INFO: Number of nodes with available pods: 1
Feb 24 18:20:43.813: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4131, will wait for the garbage collector to delete the pods
Feb 24 18:20:44.128: INFO: Deleting DaemonSet.extensions daemon-set took: 58.765847ms
Feb 24 18:20:44.928: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.254709ms
Feb 24 18:20:52.282: INFO: Number of nodes with available pods: 0
Feb 24 18:20:52.282: INFO: Number of running nodes: 0, number of available pods: 0
Feb 24 18:20:52.334: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20773"},"items":null}

Feb 24 18:20:52.386: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20773"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:20:52.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4131" for this suite.
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":237,"skipped":4200,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:20:52.768: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7010
[It] should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-7010
Feb 24 18:20:53.193: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 24 18:21:03.248: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 18:21:03.527: INFO: Deleting all statefulset in ns statefulset-7010
Feb 24 18:21:03.579: INFO: Scaling statefulset ss to 0
Feb 24 18:21:13.793: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 18:21:13.845: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:21:14.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7010" for this suite.
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":238,"skipped":4206,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:21:14.124: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:21:14.418: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating first CR 
Feb 24 18:21:14.962: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:14Z]] name:name1 resourceVersion:20898 uid:4b5e3b3a-1b23-4347-9d64-1eec23a7a5d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 24 18:21:25.018: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:24Z]] name:name2 resourceVersion:20928 uid:c804586d-0279-48bb-b133-f5ba57cd12a4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 24 18:21:35.074: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:35Z]] name:name1 resourceVersion:20949 uid:4b5e3b3a-1b23-4347-9d64-1eec23a7a5d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 24 18:21:45.130: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:45Z]] name:name2 resourceVersion:20970 uid:c804586d-0279-48bb-b133-f5ba57cd12a4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 24 18:21:55.190: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:35Z]] name:name1 resourceVersion:20991 uid:4b5e3b3a-1b23-4347-9d64-1eec23a7a5d1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 24 18:22:05.247: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-24T18:21:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-24T18:21:45Z]] name:name2 resourceVersion:21012 uid:c804586d-0279-48bb-b133-f5ba57cd12a4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:15.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-502" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":239,"skipped":4218,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:15.489: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:16.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6947" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":240,"skipped":4221,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:17.152: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6337" for this suite.
STEP: Destroying namespace "nsdeletetest-1705" for this suite.
Feb 24 18:22:24.606: INFO: Namespace nsdeletetest-1705 was already deleted
STEP: Destroying namespace "nsdeletetest-3171" for this suite.
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":241,"skipped":4237,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:24.660: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:22:26.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787746, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787746, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787746, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787746, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:22:29.446: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:29.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9825" for this suite.
STEP: Destroying namespace "webhook-9825-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":242,"skipped":4253,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:30.294: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Feb 24 18:22:30.622: INFO: Waiting up to 5m0s for pod "client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d" in namespace "containers-93" to be "Succeeded or Failed"
Feb 24 18:22:30.674: INFO: Pod "client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d": Phase="Pending", Reason="", readiness=false. Elapsed: 51.357982ms
Feb 24 18:22:32.726: INFO: Pod "client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103595431s
STEP: Saw pod success
Feb 24 18:22:32.726: INFO: Pod "client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d" satisfied condition "Succeeded or Failed"
Feb 24 18:22:32.778: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:22:32.903: INFO: Waiting for pod client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d to disappear
Feb 24 18:22:32.954: INFO: Pod client-containers-c4182a22-ea7c-4785-97eb-c90b1254b74d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:32.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-93" for this suite.
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4260,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:33.063: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8294.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8294.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8294.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8294.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:22:36.037: INFO: DNS probes using dns-8294/dns-test-da27eef4-d396-488e-811b-7f518aba48c3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:22:36.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8294" for this suite.
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":244,"skipped":4261,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:22:36.284: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Feb 24 18:22:36.547: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 create -f -'
Feb 24 18:22:37.291: INFO: stderr: ""
Feb 24 18:22:37.291: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 24 18:22:37.291: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:37.615: INFO: stderr: ""
Feb 24 18:22:37.615: INFO: stdout: "update-demo-nautilus-2t6df update-demo-nautilus-cgq9w "
Feb 24 18:22:37.615: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-2t6df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:37.941: INFO: stderr: ""
Feb 24 18:22:37.941: INFO: stdout: ""
Feb 24 18:22:37.941: INFO: update-demo-nautilus-2t6df is created but not running
Feb 24 18:22:42.942: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:43.223: INFO: stderr: ""
Feb 24 18:22:43.223: INFO: stdout: "update-demo-nautilus-2t6df update-demo-nautilus-cgq9w "
Feb 24 18:22:43.223: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-2t6df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:43.530: INFO: stderr: ""
Feb 24 18:22:43.530: INFO: stdout: "true"
Feb 24 18:22:43.530: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-2t6df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:43.847: INFO: stderr: ""
Feb 24 18:22:43.847: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:43.847: INFO: validating pod update-demo-nautilus-2t6df
Feb 24 18:22:43.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:43.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:43.905: INFO: update-demo-nautilus-2t6df is verified up and running
Feb 24 18:22:43.905: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:44.197: INFO: stderr: ""
Feb 24 18:22:44.197: INFO: stdout: "true"
Feb 24 18:22:44.197: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:44.476: INFO: stderr: ""
Feb 24 18:22:44.476: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:44.476: INFO: validating pod update-demo-nautilus-cgq9w
Feb 24 18:22:44.533: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:44.533: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:44.533: INFO: update-demo-nautilus-cgq9w is verified up and running
STEP: scaling down the replication controller
Feb 24 18:22:44.536: INFO: scanned /workspace for discovery docs: <nil>
Feb 24 18:22:44.536: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 24 18:22:44.952: INFO: stderr: ""
Feb 24 18:22:44.952: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 24 18:22:44.953: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:45.256: INFO: stderr: ""
Feb 24 18:22:45.256: INFO: stdout: "update-demo-nautilus-2t6df update-demo-nautilus-cgq9w "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 24 18:22:50.256: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:50.577: INFO: stderr: ""
Feb 24 18:22:50.577: INFO: stdout: "update-demo-nautilus-cgq9w "
Feb 24 18:22:50.577: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:50.868: INFO: stderr: ""
Feb 24 18:22:50.868: INFO: stdout: "true"
Feb 24 18:22:50.868: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:51.139: INFO: stderr: ""
Feb 24 18:22:51.139: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:51.139: INFO: validating pod update-demo-nautilus-cgq9w
Feb 24 18:22:51.195: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:51.195: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:51.195: INFO: update-demo-nautilus-cgq9w is verified up and running
STEP: scaling up the replication controller
Feb 24 18:22:51.197: INFO: scanned /workspace for discovery docs: <nil>
Feb 24 18:22:51.197: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 24 18:22:51.662: INFO: stderr: ""
Feb 24 18:22:51.662: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 24 18:22:51.662: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:51.964: INFO: stderr: ""
Feb 24 18:22:51.964: INFO: stdout: "update-demo-nautilus-cgq9w update-demo-nautilus-wb8gh "
Feb 24 18:22:51.964: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:52.243: INFO: stderr: ""
Feb 24 18:22:52.243: INFO: stdout: "true"
Feb 24 18:22:52.243: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:52.544: INFO: stderr: ""
Feb 24 18:22:52.544: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:52.544: INFO: validating pod update-demo-nautilus-cgq9w
Feb 24 18:22:52.598: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:52.598: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:52.598: INFO: update-demo-nautilus-cgq9w is verified up and running
Feb 24 18:22:52.598: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-wb8gh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:52.886: INFO: stderr: ""
Feb 24 18:22:52.886: INFO: stdout: ""
Feb 24 18:22:52.886: INFO: update-demo-nautilus-wb8gh is created but not running
Feb 24 18:22:57.886: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 18:22:58.175: INFO: stderr: ""
Feb 24 18:22:58.175: INFO: stdout: "update-demo-nautilus-cgq9w update-demo-nautilus-wb8gh "
Feb 24 18:22:58.175: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:58.461: INFO: stderr: ""
Feb 24 18:22:58.461: INFO: stdout: "true"
Feb 24 18:22:58.461: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-cgq9w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:58.734: INFO: stderr: ""
Feb 24 18:22:58.734: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:58.734: INFO: validating pod update-demo-nautilus-cgq9w
Feb 24 18:22:58.791: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:58.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:58.791: INFO: update-demo-nautilus-cgq9w is verified up and running
Feb 24 18:22:58.791: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-wb8gh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 18:22:59.081: INFO: stderr: ""
Feb 24 18:22:59.081: INFO: stdout: "true"
Feb 24 18:22:59.082: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods update-demo-nautilus-wb8gh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 18:22:59.350: INFO: stderr: ""
Feb 24 18:22:59.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 24 18:22:59.351: INFO: validating pod update-demo-nautilus-wb8gh
Feb 24 18:22:59.407: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 18:22:59.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 18:22:59.408: INFO: update-demo-nautilus-wb8gh is verified up and running
STEP: using delete to clean up resources
Feb 24 18:22:59.408: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 delete --grace-period=0 --force -f -'
Feb 24 18:22:59.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 18:22:59.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 18:22:59.745: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get rc,svc -l name=update-demo --no-headers'
Feb 24 18:23:00.104: INFO: stderr: "No resources found in kubectl-8478 namespace.\n"
Feb 24 18:23:00.104: INFO: stdout: ""
Feb 24 18:23:00.104: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-8478 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 18:23:00.466: INFO: stderr: ""
Feb 24 18:23:00.466: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8478" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":245,"skipped":4269,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:00.574: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:00.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3769" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":246,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:00.998: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:29.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6606" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":247,"skipped":4295,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:29.811: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2849.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2849.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2849.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2849.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2849.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2849.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 24 18:23:32.725: INFO: DNS probes using dns-2849/dns-test-5dcbbc9b-1510-486f-a927-9fe83375d3c0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:32.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2849" for this suite.
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":248,"skipped":4305,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:32.905: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:23:34.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787813, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787813, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787814, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787813, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:23:37.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:38.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9328" for this suite.
STEP: Destroying namespace "webhook-9328-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":249,"skipped":4309,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:38.510: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Feb 24 18:23:38.770: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4047 cluster-info'
Feb 24 18:23:39.050: INFO: stderr: ""
Feb 24 18:23:39.050: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://34.83.69.28\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:39.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4047" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":250,"skipped":4328,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:39.160: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:23:39.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594" in namespace "projected-4031" to be "Succeeded or Failed"
Feb 24 18:23:39.600: INFO: Pod "downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594": Phase="Pending", Reason="", readiness=false. Elapsed: 52.320461ms
Feb 24 18:23:41.652: INFO: Pod "downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103995448s
STEP: Saw pod success
Feb 24 18:23:41.652: INFO: Pod "downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594" satisfied condition "Succeeded or Failed"
Feb 24 18:23:41.705: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594 container client-container: <nil>
STEP: delete the pod
Feb 24 18:23:41.830: INFO: Waiting for pod downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594 to disappear
Feb 24 18:23:41.882: INFO: Pod downwardapi-volume-93fbfd9d-b58a-4b4e-b277-199fadbe0594 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:41.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4031" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":251,"skipped":4334,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:41.990: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 24 18:23:44.516: INFO: &Pod{ObjectMeta:{send-events-8f537c7c-67dd-4250-8720-2666d72eaf40  events-1097  e86e7446-5952-4ce9-8865-45281d8eab90 21575 0 2021-02-24 18:23:42 +0000 UTC <nil> <nil> map[name:foo time:252538795] map[] [] []  [{e2e.test Update v1 2021-02-24 18:23:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-24 18:23:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.64.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rfq9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rfq9m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rfq9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-2qjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:23:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:23:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:23:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-24 18:23:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.213,StartTime:2021-02-24 18:23:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-24 18:23:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://5d5058c77c033cb0cee3520f33f02e44b5585517e2a95e79c8372381bdff6eb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 24 18:23:46.568: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 24 18:23:48.621: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:48.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1097" for this suite.
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":252,"skipped":4354,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:48.785: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:23:50.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787830, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787830, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787830, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787830, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:23:53.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:53.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1489" for this suite.
STEP: Destroying namespace "webhook-1489-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":253,"skipped":4355,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:54.077: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-e4714a88-4d9f-44c1-8549-dd02b9f452fc
STEP: Creating a pod to test consume configMaps
Feb 24 18:23:54.445: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217" in namespace "projected-8441" to be "Succeeded or Failed"
Feb 24 18:23:54.501: INFO: Pod "pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217": Phase="Pending", Reason="", readiness=false. Elapsed: 56.145699ms
Feb 24 18:23:56.554: INFO: Pod "pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.108829514s
STEP: Saw pod success
Feb 24 18:23:56.554: INFO: Pod "pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217" satisfied condition "Succeeded or Failed"
Feb 24 18:23:56.606: INFO: Trying to get logs from node bootstrap-e2e-minion-group-bthl pod pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:23:56.729: INFO: Waiting for pod pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217 to disappear
Feb 24 18:23:56.781: INFO: Pod pod-projected-configmaps-253dc43a-9e14-4ce1-821c-ecdeeaf47217 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:23:56.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8441" for this suite.
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:23:56.890: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb 24 18:23:57.256: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb 24 18:23:57.360: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 18:23:57.360: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb 24 18:23:57.465: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 18:23:57.466: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb 24 18:23:57.572: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 24 18:23:57.572: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb 24 18:24:05.003: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:05.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2826" for this suite.
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":255,"skipped":4410,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:05.207: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 24 18:24:05.947: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:06.005: INFO: Number of nodes with available pods: 0
Feb 24 18:24:06.005: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 18:24:07.060: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:07.114: INFO: Number of nodes with available pods: 1
Feb 24 18:24:07.114: INFO: Node bootstrap-e2e-minion-group-2qjz is running more than one daemon pod
Feb 24 18:24:08.060: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:08.114: INFO: Number of nodes with available pods: 3
Feb 24 18:24:08.114: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 24 18:24:08.329: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:08.383: INFO: Number of nodes with available pods: 2
Feb 24 18:24:08.383: INFO: Node bootstrap-e2e-minion-group-bthl is running more than one daemon pod
Feb 24 18:24:09.438: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:09.492: INFO: Number of nodes with available pods: 2
Feb 24 18:24:09.492: INFO: Node bootstrap-e2e-minion-group-bthl is running more than one daemon pod
Feb 24 18:24:10.439: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:10.494: INFO: Number of nodes with available pods: 2
Feb 24 18:24:10.494: INFO: Node bootstrap-e2e-minion-group-bthl is running more than one daemon pod
Feb 24 18:24:11.438: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:11.492: INFO: Number of nodes with available pods: 2
Feb 24 18:24:11.493: INFO: Node bootstrap-e2e-minion-group-bthl is running more than one daemon pod
Feb 24 18:24:12.439: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 18:24:12.492: INFO: Number of nodes with available pods: 3
Feb 24 18:24:12.492: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2684, will wait for the garbage collector to delete the pods
Feb 24 18:24:12.752: INFO: Deleting DaemonSet.extensions daemon-set took: 55.642883ms
Feb 24 18:24:13.553: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.262707ms
Feb 24 18:24:22.405: INFO: Number of nodes with available pods: 0
Feb 24 18:24:22.405: INFO: Number of running nodes: 0, number of available pods: 0
Feb 24 18:24:22.456: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21845"},"items":null}

Feb 24 18:24:22.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21845"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:22.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2684" for this suite.
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":256,"skipped":4420,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:22.832: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-9b41ee57-4daa-45d0-9f4b-8a684a9ae294
STEP: Creating a pod to test consume configMaps
Feb 24 18:24:23.202: INFO: Waiting up to 5m0s for pod "pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3" in namespace "configmap-336" to be "Succeeded or Failed"
Feb 24 18:24:23.254: INFO: Pod "pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 51.992147ms
Feb 24 18:24:25.307: INFO: Pod "pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104887966s
STEP: Saw pod success
Feb 24 18:24:25.307: INFO: Pod "pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3" satisfied condition "Succeeded or Failed"
Feb 24 18:24:25.361: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:24:25.478: INFO: Waiting for pod pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3 to disappear
Feb 24 18:24:25.530: INFO: Pod pod-configmaps-aeba28c7-293a-4495-b4f2-8c04f005d2f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:25.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-336" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4431,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:25.637: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-455
STEP: creating service affinity-clusterip in namespace services-455
STEP: creating replication controller affinity-clusterip in namespace services-455
I0224 18:24:26.013120   10144 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-455, replica count: 3
I0224 18:24:29.113567   10144 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 18:24:29.217: INFO: Creating new exec pod
Feb 24 18:24:32.429: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-455 exec execpod-affinityqrnff -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Feb 24 18:24:34.155: INFO: rc: 1
Feb 24 18:24:34.155: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-455 exec execpod-affinityqrnff -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 24 18:24:35.155: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-455 exec execpod-affinityqrnff -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Feb 24 18:24:36.845: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 24 18:24:36.845: INFO: stdout: ""
Feb 24 18:24:36.846: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-455 exec execpod-affinityqrnff -- /bin/sh -x -c nc -zv -t -w 2 10.0.32.70 80'
Feb 24 18:24:37.563: INFO: stderr: "+ nc -zv -t -w 2 10.0.32.70 80\nConnection to 10.0.32.70 80 port [tcp/http] succeeded!\n"
Feb 24 18:24:37.564: INFO: stdout: ""
Feb 24 18:24:37.564: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=services-455 exec execpod-affinityqrnff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.32.70:80/ ; done'
Feb 24 18:24:38.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.32.70:80/\n"
Feb 24 18:24:38.295: INFO: stdout: "\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh\naffinity-clusterip-g2gqh"
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Received response from host: affinity-clusterip-g2gqh
Feb 24 18:24:38.295: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-455, will wait for the garbage collector to delete the pods
Feb 24 18:24:38.609: INFO: Deleting ReplicationController affinity-clusterip took: 54.910407ms
Feb 24 18:24:39.409: INFO: Terminating ReplicationController affinity-clusterip pods took: 800.254997ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-455" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":258,"skipped":4443,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:52.303: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Feb 24 18:24:52.619: INFO: created test-pod-1
Feb 24 18:24:52.672: INFO: created test-pod-2
Feb 24 18:24:52.727: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:52.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5211" for this suite.
{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":259,"skipped":4455,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:53.024: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:24:53.342: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a893c06f-94d8-44a4-8225-7cbe962fb06e" in namespace "security-context-test-1763" to be "Succeeded or Failed"
Feb 24 18:24:53.394: INFO: Pod "busybox-user-65534-a893c06f-94d8-44a4-8225-7cbe962fb06e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.100402ms
Feb 24 18:24:55.447: INFO: Pod "busybox-user-65534-a893c06f-94d8-44a4-8225-7cbe962fb06e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104615803s
Feb 24 18:24:55.447: INFO: Pod "busybox-user-65534-a893c06f-94d8-44a4-8225-7cbe962fb06e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:55.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1763" for this suite.
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4461,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:55.555: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:24:55.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9" in namespace "downward-api-8049" to be "Succeeded or Failed"
Feb 24 18:24:55.928: INFO: Pod "downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.366968ms
Feb 24 18:24:57.981: INFO: Pod "downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106933812s
STEP: Saw pod success
Feb 24 18:24:57.981: INFO: Pod "downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9" satisfied condition "Succeeded or Failed"
Feb 24 18:24:58.036: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9 container client-container: <nil>
STEP: delete the pod
Feb 24 18:24:58.159: INFO: Waiting for pod downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9 to disappear
Feb 24 18:24:58.213: INFO: Pod downwardapi-volume-84df315c-fa1b-4353-b44e-829f8fb446f9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:24:58.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8049" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4464,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:24:58.323: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8223, will wait for the garbage collector to delete the pods
Feb 24 18:25:00.959: INFO: Deleting Job.batch foo took: 99.089994ms
Feb 24 18:25:01.059: INFO: Terminating Job.batch foo pods took: 100.231296ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:42.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8223" for this suite.
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":262,"skipped":4478,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:42.519: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb 24 18:25:42.919: INFO: Waiting up to 5m0s for pod "downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad" in namespace "downward-api-412" to be "Succeeded or Failed"
Feb 24 18:25:42.972: INFO: Pod "downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad": Phase="Pending", Reason="", readiness=false. Elapsed: 52.941268ms
Feb 24 18:25:45.025: INFO: Pod "downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105599675s
STEP: Saw pod success
Feb 24 18:25:45.025: INFO: Pod "downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad" satisfied condition "Succeeded or Failed"
Feb 24 18:25:45.077: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad container dapi-container: <nil>
STEP: delete the pod
Feb 24 18:25:45.201: INFO: Waiting for pod downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad to disappear
Feb 24 18:25:45.253: INFO: Pod downward-api-67b2e255-71d9-49e4-afcb-fe707bebd1ad no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:45.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-412" for this suite.
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":263,"skipped":4498,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:45.362: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:25:45.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc" in namespace "downward-api-2859" to be "Succeeded or Failed"
Feb 24 18:25:45.756: INFO: Pod "downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 51.756006ms
Feb 24 18:25:47.809: INFO: Pod "downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10467772s
STEP: Saw pod success
Feb 24 18:25:47.809: INFO: Pod "downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc" satisfied condition "Succeeded or Failed"
Feb 24 18:25:47.861: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc container client-container: <nil>
STEP: delete the pod
Feb 24 18:25:47.978: INFO: Waiting for pod downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc to disappear
Feb 24 18:25:48.030: INFO: Pod downwardapi-volume-9a49e300-42d4-4808-9109-c4baf24a0fbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:48.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2859" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4508,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:48.138: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 24 18:25:48.457: INFO: Waiting up to 5m0s for pod "pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac" in namespace "emptydir-6638" to be "Succeeded or Failed"
Feb 24 18:25:48.508: INFO: Pod "pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 51.72271ms
Feb 24 18:25:50.562: INFO: Pod "pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105457098s
STEP: Saw pod success
Feb 24 18:25:50.562: INFO: Pod "pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac" satisfied condition "Succeeded or Failed"
Feb 24 18:25:50.614: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac container test-container: <nil>
STEP: delete the pod
Feb 24 18:25:50.731: INFO: Waiting for pod pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac to disappear
Feb 24 18:25:50.782: INFO: Pod pod-c28287cb-99c7-42c4-9455-6aa8c067e5ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:50.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6638" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4512,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:50.891: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Feb 24 18:25:51.152: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-788 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:51.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-788" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":266,"skipped":4529,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:51.547: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:51.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6632" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":267,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:51.969: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:25:53.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787953, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787953, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787953, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749787953, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:25:56.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:25:57.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9679" for this suite.
STEP: Destroying namespace "webhook-9679-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":268,"skipped":4617,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:25:57.486: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:00.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8533" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":269,"skipped":4638,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:00.349: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7310
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7310
STEP: Creating statefulset with conflicting port in namespace statefulset-7310
STEP: Waiting until pod test-pod will start running in namespace statefulset-7310
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7310
Feb 24 18:26:05.011: INFO: Observed stateful pod in namespace: statefulset-7310, name: ss-0, uid: 0ca5ebc2-1f74-4ce1-98fe-deaaa93e35f7, status phase: Pending. Waiting for statefulset controller to delete.
Feb 24 18:26:05.438: INFO: Observed stateful pod in namespace: statefulset-7310, name: ss-0, uid: 0ca5ebc2-1f74-4ce1-98fe-deaaa93e35f7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 18:26:05.447: INFO: Observed stateful pod in namespace: statefulset-7310, name: ss-0, uid: 0ca5ebc2-1f74-4ce1-98fe-deaaa93e35f7, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 18:26:05.457: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7310
STEP: Removing pod with conflicting port in namespace statefulset-7310
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7310 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb 24 18:26:09.685: INFO: Deleting all statefulset in ns statefulset-7310
Feb 24 18:26:09.737: INFO: Scaling statefulset ss to 0
Feb 24 18:26:29.950: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 18:26:30.002: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:30.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7310" for this suite.
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":270,"skipped":4641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:30.271: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 24 18:26:32.797: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:32.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4108" for this suite.
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4663,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:33.019: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb 24 18:26:33.283: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:38.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9796" for this suite.
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":272,"skipped":4670,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:38.977: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-05fd81bc-1c62-4049-a15a-94b98699b128
STEP: Creating a pod to test consume secrets
Feb 24 18:26:39.349: INFO: Waiting up to 5m0s for pod "pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a" in namespace "secrets-746" to be "Succeeded or Failed"
Feb 24 18:26:39.401: INFO: Pod "pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a": Phase="Pending", Reason="", readiness=false. Elapsed: 51.824286ms
Feb 24 18:26:41.453: INFO: Pod "pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104259414s
STEP: Saw pod success
Feb 24 18:26:41.453: INFO: Pod "pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a" satisfied condition "Succeeded or Failed"
Feb 24 18:26:41.506: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 18:26:41.622: INFO: Waiting for pod pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a to disappear
Feb 24 18:26:41.673: INFO: Pod pod-secrets-70e2f748-92c3-42ca-a5ca-9f827243f84a no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:41.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-746" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4670,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:41.781: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 24 18:26:42.099: INFO: Waiting up to 5m0s for pod "pod-ed437794-eb3e-406a-b112-6f32432591d2" in namespace "emptydir-6556" to be "Succeeded or Failed"
Feb 24 18:26:42.151: INFO: Pod "pod-ed437794-eb3e-406a-b112-6f32432591d2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01406ms
Feb 24 18:26:44.204: INFO: Pod "pod-ed437794-eb3e-406a-b112-6f32432591d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104890437s
STEP: Saw pod success
Feb 24 18:26:44.204: INFO: Pod "pod-ed437794-eb3e-406a-b112-6f32432591d2" satisfied condition "Succeeded or Failed"
Feb 24 18:26:44.256: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-ed437794-eb3e-406a-b112-6f32432591d2 container test-container: <nil>
STEP: delete the pod
Feb 24 18:26:44.371: INFO: Waiting for pod pod-ed437794-eb3e-406a-b112-6f32432591d2 to disappear
Feb 24 18:26:44.424: INFO: Pod pod-ed437794-eb3e-406a-b112-6f32432591d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:44.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6556" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:44.532: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Feb 24 18:26:44.793: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 create -f -'
Feb 24 18:26:45.294: INFO: stderr: ""
Feb 24 18:26:45.294: INFO: stdout: "pod/pause created\n"
Feb 24 18:26:45.294: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 24 18:26:45.294: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4938" to be "running and ready"
Feb 24 18:26:45.346: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 52.077365ms
Feb 24 18:26:47.399: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.104518884s
Feb 24 18:26:47.399: INFO: Pod "pause" satisfied condition "running and ready"
Feb 24 18:26:47.399: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 24 18:26:47.399: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 label pods pause testing-label=testing-label-value'
Feb 24 18:26:47.765: INFO: stderr: ""
Feb 24 18:26:47.765: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 24 18:26:47.765: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 get pod pause -L testing-label'
Feb 24 18:26:48.033: INFO: stderr: ""
Feb 24 18:26:48.033: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 24 18:26:48.033: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 label pods pause testing-label-'
Feb 24 18:26:48.395: INFO: stderr: ""
Feb 24 18:26:48.396: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 24 18:26:48.396: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 get pod pause -L testing-label'
Feb 24 18:26:48.667: INFO: stderr: ""
Feb 24 18:26:48.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Feb 24 18:26:48.667: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 delete --grace-period=0 --force -f -'
Feb 24 18:26:49.006: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 18:26:49.006: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 24 18:26:49.006: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 get rc,svc -l name=pause --no-headers'
Feb 24 18:26:49.355: INFO: stderr: "No resources found in kubectl-4938 namespace.\n"
Feb 24 18:26:49.355: INFO: stdout: ""
Feb 24 18:26:49.355: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4938 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 18:26:49.652: INFO: stderr: ""
Feb 24 18:26:49.652: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:49.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4938" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":275,"skipped":4702,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:49.777: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 24 18:26:51.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749788011, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749788011, loc:(*time.Location)(0x7977e40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63749788011, loc:(*time.Location)(0x7977e40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63749788011, loc:(*time.Location)(0x7977e40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 24 18:26:54.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:26:55.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7284" for this suite.
STEP: Destroying namespace "webhook-7284-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":276,"skipped":4714,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:26:55.898: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-pnx72 in namespace proxy-8415
I0224 18:26:56.276839   10144 runners.go:190] Created replication controller with name: proxy-service-pnx72, namespace: proxy-8415, replica count: 1
I0224 18:26:57.377353   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 18:26:58.377660   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0224 18:26:59.377919   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0224 18:27:00.378131   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0224 18:27:01.378384   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0224 18:27:02.378675   10144 runners.go:190] proxy-service-pnx72 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 18:27:02.431: INFO: setup took 6.27047211s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 24 18:27:02.493: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 60.945364ms)
Feb 24 18:27:02.494: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 63.022203ms)
Feb 24 18:27:02.494: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 62.749269ms)
Feb 24 18:27:02.499: INFO: (0) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 67.15718ms)
Feb 24 18:27:02.499: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 66.969911ms)
Feb 24 18:27:02.499: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 67.008227ms)
Feb 24 18:27:02.501: INFO: (0) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 68.680233ms)
Feb 24 18:27:02.501: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 69.438046ms)
Feb 24 18:27:02.501: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 68.86976ms)
Feb 24 18:27:02.506: INFO: (0) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 73.93156ms)
Feb 24 18:27:02.538: INFO: (0) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 105.486562ms)
Feb 24 18:27:02.538: INFO: (0) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 106.024436ms)
Feb 24 18:27:02.538: INFO: (0) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 106.00293ms)
Feb 24 18:27:02.541: INFO: (0) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 108.467771ms)
Feb 24 18:27:02.541: INFO: (0) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 108.548181ms)
Feb 24 18:27:02.541: INFO: (0) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 109.083384ms)
Feb 24 18:27:02.604: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 61.357329ms)
Feb 24 18:27:02.604: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 62.250766ms)
Feb 24 18:27:02.606: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 64.234734ms)
Feb 24 18:27:02.607: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 65.458968ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 72.038594ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 72.021379ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 71.887687ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 72.085785ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 71.810252ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 72.019853ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 72.377836ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 71.971272ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.669599ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 71.750028ms)
Feb 24 18:27:02.614: INFO: (1) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 71.920923ms)
Feb 24 18:27:02.616: INFO: (1) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 73.707826ms)
Feb 24 18:27:02.682: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 65.787658ms)
Feb 24 18:27:02.683: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 66.516211ms)
Feb 24 18:27:02.683: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 66.441422ms)
Feb 24 18:27:02.683: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 66.431653ms)
Feb 24 18:27:02.683: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 66.519896ms)
Feb 24 18:27:02.685: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 68.755173ms)
Feb 24 18:27:02.687: INFO: (2) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 70.914306ms)
Feb 24 18:27:02.691: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 74.191578ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 76.242475ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 76.250283ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 76.350326ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 76.3876ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 76.270449ms)
Feb 24 18:27:02.693: INFO: (2) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 76.277369ms)
Feb 24 18:27:02.694: INFO: (2) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 77.220171ms)
Feb 24 18:27:02.696: INFO: (2) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 79.30922ms)
Feb 24 18:27:02.761: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 64.515449ms)
Feb 24 18:27:02.761: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 64.571921ms)
Feb 24 18:27:02.761: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 64.765896ms)
Feb 24 18:27:02.763: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 66.618314ms)
Feb 24 18:27:02.763: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 66.648812ms)
Feb 24 18:27:02.763: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 66.850177ms)
Feb 24 18:27:02.763: INFO: (3) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 67.092149ms)
Feb 24 18:27:02.766: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 69.830662ms)
Feb 24 18:27:02.766: INFO: (3) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 69.753761ms)
Feb 24 18:27:02.767: INFO: (3) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 71.137086ms)
Feb 24 18:27:02.770: INFO: (3) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 74.369711ms)
Feb 24 18:27:02.770: INFO: (3) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 74.349198ms)
Feb 24 18:27:02.770: INFO: (3) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 74.399351ms)
Feb 24 18:27:02.770: INFO: (3) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 74.427986ms)
Feb 24 18:27:02.771: INFO: (3) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 75.19662ms)
Feb 24 18:27:02.772: INFO: (3) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 76.25589ms)
Feb 24 18:27:02.837: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 64.902827ms)
Feb 24 18:27:02.837: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 64.904705ms)
Feb 24 18:27:02.847: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 74.827707ms)
Feb 24 18:27:02.849: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 76.298359ms)
Feb 24 18:27:02.879: INFO: (4) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 106.567237ms)
Feb 24 18:27:02.879: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 106.543185ms)
Feb 24 18:27:02.880: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 107.76346ms)
Feb 24 18:27:02.880: INFO: (4) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 107.721771ms)
Feb 24 18:27:02.880: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 107.710158ms)
Feb 24 18:27:02.880: INFO: (4) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 107.797031ms)
Feb 24 18:27:02.880: INFO: (4) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 107.727231ms)
Feb 24 18:27:02.888: INFO: (4) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 115.520708ms)
Feb 24 18:27:02.888: INFO: (4) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 115.633036ms)
Feb 24 18:27:02.888: INFO: (4) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 115.732857ms)
Feb 24 18:27:02.890: INFO: (4) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 117.741826ms)
Feb 24 18:27:02.892: INFO: (4) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 119.80831ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 96.625395ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 96.452836ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 96.505732ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 96.455546ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 96.510207ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 96.572293ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 96.604862ms)
Feb 24 18:27:02.989: INFO: (5) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 96.572742ms)
Feb 24 18:27:02.990: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 97.745745ms)
Feb 24 18:27:02.991: INFO: (5) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 98.37069ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 100.083701ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 100.343179ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 100.247582ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 100.391066ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 100.473199ms)
Feb 24 18:27:02.993: INFO: (5) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 100.501212ms)
Feb 24 18:27:03.058: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 64.476313ms)
Feb 24 18:27:03.058: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 64.502329ms)
Feb 24 18:27:03.060: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 67.083746ms)
Feb 24 18:27:03.061: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 67.858518ms)
Feb 24 18:27:03.061: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 68.038744ms)
Feb 24 18:27:03.061: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 67.919633ms)
Feb 24 18:27:03.061: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 68.221338ms)
Feb 24 18:27:03.065: INFO: (6) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 71.379955ms)
Feb 24 18:27:03.065: INFO: (6) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 71.847458ms)
Feb 24 18:27:03.068: INFO: (6) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 74.90824ms)
Feb 24 18:27:03.068: INFO: (6) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 75.060126ms)
Feb 24 18:27:03.069: INFO: (6) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 75.928347ms)
Feb 24 18:27:03.069: INFO: (6) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 75.923743ms)
Feb 24 18:27:03.069: INFO: (6) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 75.787777ms)
Feb 24 18:27:03.069: INFO: (6) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 76.005728ms)
Feb 24 18:27:03.071: INFO: (6) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 77.512634ms)
Feb 24 18:27:03.132: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 61.228836ms)
Feb 24 18:27:03.134: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 62.809672ms)
Feb 24 18:27:03.134: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 62.778434ms)
Feb 24 18:27:03.135: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 64.041844ms)
Feb 24 18:27:03.140: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 68.078251ms)
Feb 24 18:27:03.140: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 68.396184ms)
Feb 24 18:27:03.140: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 68.283429ms)
Feb 24 18:27:03.140: INFO: (7) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 68.634599ms)
Feb 24 18:27:03.140: INFO: (7) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 68.517204ms)
Feb 24 18:27:03.141: INFO: (7) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 69.732733ms)
Feb 24 18:27:03.143: INFO: (7) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.820383ms)
Feb 24 18:27:03.143: INFO: (7) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 71.9433ms)
Feb 24 18:27:03.143: INFO: (7) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 72.04148ms)
Feb 24 18:27:03.144: INFO: (7) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 72.77675ms)
Feb 24 18:27:03.144: INFO: (7) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 72.934001ms)
Feb 24 18:27:03.146: INFO: (7) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 74.959769ms)
Feb 24 18:27:03.206: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 59.430062ms)
Feb 24 18:27:03.206: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 59.660561ms)
Feb 24 18:27:03.212: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.125182ms)
Feb 24 18:27:03.212: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 65.084656ms)
Feb 24 18:27:03.212: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 65.12308ms)
Feb 24 18:27:03.213: INFO: (8) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 65.08022ms)
Feb 24 18:27:03.213: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 65.423354ms)
Feb 24 18:27:03.213: INFO: (8) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.69254ms)
Feb 24 18:27:03.214: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 67.181441ms)
Feb 24 18:27:03.215: INFO: (8) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 67.23259ms)
Feb 24 18:27:03.218: INFO: (8) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 70.620227ms)
Feb 24 18:27:03.218: INFO: (8) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 71.181456ms)
Feb 24 18:27:03.218: INFO: (8) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 70.278732ms)
Feb 24 18:27:03.218: INFO: (8) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 70.71745ms)
Feb 24 18:27:03.219: INFO: (8) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.532731ms)
Feb 24 18:27:03.221: INFO: (8) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 73.538589ms)
Feb 24 18:27:03.309: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 88.268781ms)
Feb 24 18:27:03.311: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 90.106074ms)
Feb 24 18:27:03.311: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 90.037606ms)
Feb 24 18:27:03.317: INFO: (9) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 95.715935ms)
Feb 24 18:27:03.317: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 95.713398ms)
Feb 24 18:27:03.319: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 97.817354ms)
Feb 24 18:27:03.319: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 97.883373ms)
Feb 24 18:27:03.319: INFO: (9) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 97.989759ms)
Feb 24 18:27:03.319: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 97.96661ms)
Feb 24 18:27:03.319: INFO: (9) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 97.885288ms)
Feb 24 18:27:03.322: INFO: (9) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 101.057266ms)
Feb 24 18:27:03.322: INFO: (9) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 101.022516ms)
Feb 24 18:27:03.323: INFO: (9) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 101.873204ms)
Feb 24 18:27:03.323: INFO: (9) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 101.908317ms)
Feb 24 18:27:03.323: INFO: (9) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 101.905078ms)
Feb 24 18:27:03.323: INFO: (9) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 102.135251ms)
Feb 24 18:27:03.386: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 62.522249ms)
Feb 24 18:27:03.386: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 63.289661ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 66.554264ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 66.533962ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 66.557236ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 66.930477ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 66.978668ms)
Feb 24 18:27:03.390: INFO: (10) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 66.951799ms)
Feb 24 18:27:03.393: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 70.009874ms)
Feb 24 18:27:03.394: INFO: (10) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 71.035299ms)
Feb 24 18:27:03.396: INFO: (10) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 73.21102ms)
Feb 24 18:27:03.396: INFO: (10) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 73.23585ms)
Feb 24 18:27:03.396: INFO: (10) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 73.173623ms)
Feb 24 18:27:03.397: INFO: (10) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 73.557797ms)
Feb 24 18:27:03.397: INFO: (10) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 73.61526ms)
Feb 24 18:27:03.397: INFO: (10) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 73.675579ms)
Feb 24 18:27:03.462: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 65.309244ms)
Feb 24 18:27:03.462: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.276737ms)
Feb 24 18:27:03.462: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 65.350498ms)
Feb 24 18:27:03.462: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 65.303223ms)
Feb 24 18:27:03.463: INFO: (11) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.666252ms)
Feb 24 18:27:03.463: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 65.590928ms)
Feb 24 18:27:03.463: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 66.05804ms)
Feb 24 18:27:03.464: INFO: (11) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 67.064061ms)
Feb 24 18:27:03.464: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 67.183422ms)
Feb 24 18:27:03.465: INFO: (11) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 68.14324ms)
Feb 24 18:27:03.467: INFO: (11) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 69.998905ms)
Feb 24 18:27:03.467: INFO: (11) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 69.98194ms)
Feb 24 18:27:03.468: INFO: (11) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 71.058842ms)
Feb 24 18:27:03.469: INFO: (11) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.487855ms)
Feb 24 18:27:03.469: INFO: (11) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 71.877676ms)
Feb 24 18:27:03.469: INFO: (11) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 71.949866ms)
Feb 24 18:27:03.548: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 78.743253ms)
Feb 24 18:27:03.548: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 78.677089ms)
Feb 24 18:27:03.548: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 78.752325ms)
Feb 24 18:27:03.548: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 78.818503ms)
Feb 24 18:27:03.550: INFO: (12) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 81.380152ms)
Feb 24 18:27:03.550: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 81.216659ms)
Feb 24 18:27:03.550: INFO: (12) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 81.325662ms)
Feb 24 18:27:03.550: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 81.232581ms)
Feb 24 18:27:03.550: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 81.307433ms)
Feb 24 18:27:03.551: INFO: (12) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 81.348365ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 87.456234ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 87.648713ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 87.762173ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 87.581971ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 87.586356ms)
Feb 24 18:27:03.557: INFO: (12) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 87.868473ms)
Feb 24 18:27:03.618: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 60.623698ms)
Feb 24 18:27:03.619: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 61.386423ms)
Feb 24 18:27:03.620: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 62.862265ms)
Feb 24 18:27:03.624: INFO: (13) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 66.357848ms)
Feb 24 18:27:03.627: INFO: (13) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 69.641085ms)
Feb 24 18:27:03.627: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 69.445087ms)
Feb 24 18:27:03.627: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 69.689192ms)
Feb 24 18:27:03.627: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 69.712292ms)
Feb 24 18:27:03.627: INFO: (13) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 69.258081ms)
Feb 24 18:27:03.629: INFO: (13) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 71.41866ms)
Feb 24 18:27:03.629: INFO: (13) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.377946ms)
Feb 24 18:27:03.629: INFO: (13) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 71.594791ms)
Feb 24 18:27:03.630: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 72.494375ms)
Feb 24 18:27:03.630: INFO: (13) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 72.42425ms)
Feb 24 18:27:03.630: INFO: (13) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 72.520809ms)
Feb 24 18:27:03.632: INFO: (13) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 74.486559ms)
Feb 24 18:27:03.696: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 63.999272ms)
Feb 24 18:27:03.697: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 64.895227ms)
Feb 24 18:27:03.699: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.321263ms)
Feb 24 18:27:03.700: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 66.625328ms)
Feb 24 18:27:03.700: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 67.12358ms)
Feb 24 18:27:03.702: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 68.624048ms)
Feb 24 18:27:03.702: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 68.805747ms)
Feb 24 18:27:03.702: INFO: (14) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 69.573448ms)
Feb 24 18:27:03.702: INFO: (14) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 69.473882ms)
Feb 24 18:27:03.703: INFO: (14) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 70.541456ms)
Feb 24 18:27:03.704: INFO: (14) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 70.934522ms)
Feb 24 18:27:03.704: INFO: (14) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 71.97452ms)
Feb 24 18:27:03.704: INFO: (14) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 71.563654ms)
Feb 24 18:27:03.705: INFO: (14) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 72.161521ms)
Feb 24 18:27:03.705: INFO: (14) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 72.682024ms)
Feb 24 18:27:03.705: INFO: (14) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 72.145217ms)
Feb 24 18:27:03.766: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 61.161996ms)
Feb 24 18:27:03.768: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 62.759135ms)
Feb 24 18:27:03.768: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 62.616921ms)
Feb 24 18:27:03.770: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 65.105687ms)
Feb 24 18:27:03.770: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 65.335875ms)
Feb 24 18:27:03.773: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 67.742136ms)
Feb 24 18:27:03.773: INFO: (15) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 67.969884ms)
Feb 24 18:27:03.773: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 68.049677ms)
Feb 24 18:27:03.775: INFO: (15) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 69.436309ms)
Feb 24 18:27:03.775: INFO: (15) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 69.481748ms)
Feb 24 18:27:03.777: INFO: (15) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 72.277798ms)
Feb 24 18:27:03.778: INFO: (15) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 72.877493ms)
Feb 24 18:27:03.778: INFO: (15) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 73.118517ms)
Feb 24 18:27:03.778: INFO: (15) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 73.138583ms)
Feb 24 18:27:03.779: INFO: (15) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 73.630795ms)
Feb 24 18:27:03.779: INFO: (15) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 73.622276ms)
Feb 24 18:27:03.845: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 65.704761ms)
Feb 24 18:27:03.845: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 65.934928ms)
Feb 24 18:27:03.848: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 68.675835ms)
Feb 24 18:27:03.848: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 68.813839ms)
Feb 24 18:27:03.848: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 68.837005ms)
Feb 24 18:27:03.848: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 68.891178ms)
Feb 24 18:27:03.848: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 68.893955ms)
Feb 24 18:27:03.850: INFO: (16) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 70.696318ms)
Feb 24 18:27:03.850: INFO: (16) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 70.768062ms)
Feb 24 18:27:03.851: INFO: (16) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 71.653763ms)
Feb 24 18:27:03.851: INFO: (16) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 71.729857ms)
Feb 24 18:27:03.851: INFO: (16) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 71.672001ms)
Feb 24 18:27:03.851: INFO: (16) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 71.801202ms)
Feb 24 18:27:03.851: INFO: (16) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 71.685885ms)
Feb 24 18:27:03.852: INFO: (16) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 73.381691ms)
Feb 24 18:27:03.852: INFO: (16) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 73.504898ms)
Feb 24 18:27:03.924: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 71.315157ms)
Feb 24 18:27:03.925: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 72.965599ms)
Feb 24 18:27:03.932: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 78.707202ms)
Feb 24 18:27:03.932: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 78.829703ms)
Feb 24 18:27:03.932: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 78.90884ms)
Feb 24 18:27:03.932: INFO: (17) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 79.151339ms)
Feb 24 18:27:03.932: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 78.227652ms)
Feb 24 18:27:03.937: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 84.240375ms)
Feb 24 18:27:03.937: INFO: (17) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 83.854427ms)
Feb 24 18:27:03.937: INFO: (17) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 83.977772ms)
Feb 24 18:27:03.939: INFO: (17) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 85.574833ms)
Feb 24 18:27:03.948: INFO: (17) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 94.755061ms)
Feb 24 18:27:03.948: INFO: (17) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 94.431524ms)
Feb 24 18:27:03.949: INFO: (17) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 95.691026ms)
Feb 24 18:27:03.949: INFO: (17) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 96.030297ms)
Feb 24 18:27:03.949: INFO: (17) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 95.858928ms)
Feb 24 18:27:04.067: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 117.435259ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 118.272985ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 118.627864ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 118.273073ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 118.411649ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 118.069248ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 118.038981ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 117.772393ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 117.870972ms)
Feb 24 18:27:04.068: INFO: (18) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 118.574936ms)
Feb 24 18:27:04.069: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 119.360238ms)
Feb 24 18:27:04.070: INFO: (18) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 119.890879ms)
Feb 24 18:27:04.071: INFO: (18) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 120.757756ms)
Feb 24 18:27:04.071: INFO: (18) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 120.511073ms)
Feb 24 18:27:04.072: INFO: (18) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 122.631682ms)
Feb 24 18:27:04.073: INFO: (18) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 122.442996ms)
Feb 24 18:27:04.132: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:462/proxy/: tls qux (200; 59.83979ms)
Feb 24 18:27:04.138: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268/proxy/rewriteme">test</a> (200; 64.926205ms)
Feb 24 18:27:04.138: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:460/proxy/: tls baz (200; 65.087185ms)
Feb 24 18:27:04.138: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:1080/proxy/rewriteme">... (200; 64.953955ms)
Feb 24 18:27:04.138: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:162/proxy/: bar (200; 65.411109ms)
Feb 24 18:27:04.140: INFO: (19) /api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/https:proxy-service-pnx72-m7268:443/proxy/tlsrewritem... (200; 67.125081ms)
Feb 24 18:27:04.140: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/: <a href="/api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:1080/proxy/rewriteme">test<... (200; 67.156676ms)
Feb 24 18:27:04.140: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:162/proxy/: bar (200; 67.225264ms)
Feb 24 18:27:04.140: INFO: (19) /api/v1/namespaces/proxy-8415/pods/http:proxy-service-pnx72-m7268:160/proxy/: foo (200; 67.206268ms)
Feb 24 18:27:04.142: INFO: (19) /api/v1/namespaces/proxy-8415/pods/proxy-service-pnx72-m7268:160/proxy/: foo (200; 68.653514ms)
Feb 24 18:27:04.144: INFO: (19) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname1/proxy/: foo (200; 71.523282ms)
Feb 24 18:27:04.145: INFO: (19) /api/v1/namespaces/proxy-8415/services/http:proxy-service-pnx72:portname2/proxy/: bar (200; 72.591326ms)
Feb 24 18:27:04.145: INFO: (19) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname2/proxy/: bar (200; 72.531401ms)
Feb 24 18:27:04.146: INFO: (19) /api/v1/namespaces/proxy-8415/services/proxy-service-pnx72:portname1/proxy/: foo (200; 73.31697ms)
Feb 24 18:27:04.146: INFO: (19) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname1/proxy/: tls baz (200; 73.27578ms)
Feb 24 18:27:04.148: INFO: (19) /api/v1/namespaces/proxy-8415/services/https:proxy-service-pnx72:tlsportname2/proxy/: tls qux (200; 75.395989ms)
STEP: deleting ReplicationController proxy-service-pnx72 in namespace proxy-8415, will wait for the garbage collector to delete the pods
Feb 24 18:27:04.356: INFO: Deleting ReplicationController proxy-service-pnx72 took: 55.295501ms
Feb 24 18:27:04.456: INFO: Terminating ReplicationController proxy-service-pnx72 pods took: 100.233777ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8415" for this suite.
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":277,"skipped":4724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:12.465: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-194a52d8-7112-4177-a80e-8cc0cf354da9
STEP: Creating a pod to test consume secrets
Feb 24 18:27:12.847: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733" in namespace "projected-3199" to be "Succeeded or Failed"
Feb 24 18:27:12.899: INFO: Pod "pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733": Phase="Pending", Reason="", readiness=false. Elapsed: 52.090284ms
Feb 24 18:27:14.953: INFO: Pod "pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106105098s
STEP: Saw pod success
Feb 24 18:27:14.953: INFO: Pod "pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733" satisfied condition "Succeeded or Failed"
Feb 24 18:27:15.005: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733 container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 18:27:15.123: INFO: Waiting for pod pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733 to disappear
Feb 24 18:27:15.175: INFO: Pod pod-projected-secrets-ba3a0d19-211e-43d3-ab4c-ddc69295c733 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:15.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3199" for this suite.
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:15.284: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:22.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1445" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":279,"skipped":4887,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:22.814: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Feb 24 18:27:25.288: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5788 PodName:pod-sharedvolume-98f45f59-180d-4681-83cf-28b5d20a5d88 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:27:25.288: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:27:25.685: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:25.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5788" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":280,"skipped":4896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:25.797: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:27:26.087: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 24 18:27:30.760: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-3256 --namespace=crd-publish-openapi-3256 create -f -'
Feb 24 18:27:32.076: INFO: stderr: ""
Feb 24 18:27:32.076: INFO: stdout: "e2e-test-crd-publish-openapi-1633-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 18:27:32.076: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-3256 --namespace=crd-publish-openapi-3256 delete e2e-test-crd-publish-openapi-1633-crds test-cr'
Feb 24 18:27:32.400: INFO: stderr: ""
Feb 24 18:27:32.400: INFO: stdout: "e2e-test-crd-publish-openapi-1633-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 24 18:27:32.400: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-3256 --namespace=crd-publish-openapi-3256 apply -f -'
Feb 24 18:27:33.038: INFO: stderr: ""
Feb 24 18:27:33.038: INFO: stdout: "e2e-test-crd-publish-openapi-1633-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 18:27:33.038: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-3256 --namespace=crd-publish-openapi-3256 delete e2e-test-crd-publish-openapi-1633-crds test-cr'
Feb 24 18:27:33.414: INFO: stderr: ""
Feb 24 18:27:33.414: INFO: stdout: "e2e-test-crd-publish-openapi-1633-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 24 18:27:33.414: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-3256 explain e2e-test-crd-publish-openapi-1633-crds'
Feb 24 18:27:33.864: INFO: stderr: ""
Feb 24 18:27:33.864: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1633-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3256" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":281,"skipped":4928,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:38.640: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 24 18:27:38.903: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3009 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Feb 24 18:27:39.186: INFO: stderr: ""
Feb 24 18:27:39.186: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 24 18:27:44.286: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3009 get pod e2e-test-httpd-pod -o json'
Feb 24 18:27:44.554: INFO: stderr: ""
Feb 24 18:27:44.554: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-02-24T18:27:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-24T18:27:39Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.64.1.234\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-24T18:27:41Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3009\",\n        \"resourceVersion\": \"22965\",\n        \"uid\": \"df7093bf-c876-4647-af77-d09c3e7df68d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-cfqtb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"bootstrap-e2e-minion-group-2qjz\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-cfqtb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-cfqtb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-24T18:27:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-24T18:27:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-24T18:27:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-24T18:27:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://8f0846adcd4d6284804e0bb64d10a850cb0c1e9138238f6b9348cb027d5abdd6\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-02-24T18:27:40Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.138.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.64.1.234\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.64.1.234\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-02-24T18:27:39Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 24 18:27:44.554: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3009 replace -f -'
Feb 24 18:27:45.476: INFO: stderr: ""
Feb 24 18:27:45.476: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Feb 24 18:27:45.528: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-3009 delete pods e2e-test-httpd-pod'
Feb 24 18:27:52.076: INFO: stderr: ""
Feb 24 18:27:52.076: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:27:52.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3009" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":282,"skipped":4939,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:27:52.186: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:27:52.446: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 24 18:27:57.052: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 create -f -'
Feb 24 18:27:58.896: INFO: stderr: ""
Feb 24 18:27:58.896: INFO: stdout: "e2e-test-crd-publish-openapi-7294-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 18:27:58.896: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 delete e2e-test-crd-publish-openapi-7294-crds test-foo'
Feb 24 18:27:59.208: INFO: stderr: ""
Feb 24 18:27:59.208: INFO: stdout: "e2e-test-crd-publish-openapi-7294-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 24 18:27:59.208: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 apply -f -'
Feb 24 18:27:59.841: INFO: stderr: ""
Feb 24 18:27:59.841: INFO: stdout: "e2e-test-crd-publish-openapi-7294-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 18:27:59.841: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 delete e2e-test-crd-publish-openapi-7294-crds test-foo'
Feb 24 18:28:00.192: INFO: stderr: ""
Feb 24 18:28:00.193: INFO: stdout: "e2e-test-crd-publish-openapi-7294-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 24 18:28:00.193: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 create -f -'
Feb 24 18:28:00.684: INFO: rc: 1
Feb 24 18:28:00.684: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 apply -f -'
Feb 24 18:28:01.291: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 24 18:28:01.291: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 create -f -'
Feb 24 18:28:01.993: INFO: rc: 1
Feb 24 18:28:01.993: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 --namespace=crd-publish-openapi-8745 apply -f -'
Feb 24 18:28:02.692: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 24 18:28:02.692: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 explain e2e-test-crd-publish-openapi-7294-crds'
Feb 24 18:28:03.379: INFO: stderr: ""
Feb 24 18:28:03.379: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7294-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 24 18:28:03.380: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 explain e2e-test-crd-publish-openapi-7294-crds.metadata'
Feb 24 18:28:04.333: INFO: stderr: ""
Feb 24 18:28:04.333: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7294-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 24 18:28:04.336: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 explain e2e-test-crd-publish-openapi-7294-crds.spec'
Feb 24 18:28:05.175: INFO: stderr: ""
Feb 24 18:28:05.175: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7294-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 24 18:28:05.175: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 explain e2e-test-crd-publish-openapi-7294-crds.spec.bars'
Feb 24 18:28:05.791: INFO: stderr: ""
Feb 24 18:28:05.791: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7294-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 24 18:28:05.792: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-8745 explain e2e-test-crd-publish-openapi-7294-crds.spec.bars2'
Feb 24 18:28:06.483: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:11.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8745" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":283,"skipped":4948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:11.274: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 24 18:28:11.592: INFO: Waiting up to 5m0s for pod "pod-71623141-3aea-424d-a15e-d625402197a3" in namespace "emptydir-8933" to be "Succeeded or Failed"
Feb 24 18:28:11.650: INFO: Pod "pod-71623141-3aea-424d-a15e-d625402197a3": Phase="Pending", Reason="", readiness=false. Elapsed: 57.476732ms
Feb 24 18:28:13.760: INFO: Pod "pod-71623141-3aea-424d-a15e-d625402197a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.168211968s
STEP: Saw pod success
Feb 24 18:28:13.760: INFO: Pod "pod-71623141-3aea-424d-a15e-d625402197a3" satisfied condition "Succeeded or Failed"
Feb 24 18:28:13.812: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-71623141-3aea-424d-a15e-d625402197a3 container test-container: <nil>
STEP: delete the pod
Feb 24 18:28:13.931: INFO: Waiting for pod pod-71623141-3aea-424d-a15e-d625402197a3 to disappear
Feb 24 18:28:13.983: INFO: Pod pod-71623141-3aea-424d-a15e-d625402197a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:13.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8933" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:14.092: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:28:14.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f" in namespace "downward-api-1541" to be "Succeeded or Failed"
Feb 24 18:28:14.461: INFO: Pod "downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.158679ms
Feb 24 18:28:16.514: INFO: Pod "downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.10464918s
STEP: Saw pod success
Feb 24 18:28:16.514: INFO: Pod "downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f" satisfied condition "Succeeded or Failed"
Feb 24 18:28:16.566: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f container client-container: <nil>
STEP: delete the pod
Feb 24 18:28:16.681: INFO: Waiting for pod downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f to disappear
Feb 24 18:28:16.733: INFO: Pod downwardapi-volume-e5a524d2-8f3e-432a-9dc7-72a14b69382f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:16.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1541" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":5023,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:16.840: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-dc18103c-83bb-4379-9551-d3676234d86e
STEP: Creating a pod to test consume secrets
Feb 24 18:28:17.212: INFO: Waiting up to 5m0s for pod "pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b" in namespace "secrets-7815" to be "Succeeded or Failed"
Feb 24 18:28:17.264: INFO: Pod "pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.968858ms
Feb 24 18:28:19.319: INFO: Pod "pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106776277s
STEP: Saw pod success
Feb 24 18:28:19.319: INFO: Pod "pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b" satisfied condition "Succeeded or Failed"
Feb 24 18:28:19.371: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b container secret-volume-test: <nil>
STEP: delete the pod
Feb 24 18:28:19.487: INFO: Waiting for pod pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b to disappear
Feb 24 18:28:19.539: INFO: Pod pod-secrets-b471fdd5-b0b2-417f-9abe-5e0fec6a399b no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:19.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7815" for this suite.
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":5026,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:19.652: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Feb 24 18:28:19.968: INFO: Waiting up to 5m0s for pod "test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f" in namespace "svcaccounts-8471" to be "Succeeded or Failed"
Feb 24 18:28:20.020: INFO: Pod "test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.055402ms
Feb 24 18:28:22.073: INFO: Pod "test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104482409s
STEP: Saw pod success
Feb 24 18:28:22.073: INFO: Pod "test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f" satisfied condition "Succeeded or Failed"
Feb 24 18:28:22.125: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:28:22.243: INFO: Waiting for pod test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f to disappear
Feb 24 18:28:22.295: INFO: Pod test-pod-c956c442-e264-48b6-8bc9-7a34a143bd2f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:22.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8471" for this suite.
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":287,"skipped":5030,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:22.403: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Feb 24 18:28:22.665: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4927 create -f -'
Feb 24 18:28:23.389: INFO: stderr: ""
Feb 24 18:28:23.389: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 24 18:28:24.441: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:28:24.442: INFO: Found 0 / 1
Feb 24 18:28:25.442: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:28:25.442: INFO: Found 1 / 1
Feb 24 18:28:25.442: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 24 18:28:25.495: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:28:25.495: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 18:28:25.495: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-4927 patch pod agnhost-primary-tjg5d -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 24 18:28:25.866: INFO: stderr: ""
Feb 24 18:28:25.866: INFO: stdout: "pod/agnhost-primary-tjg5d patched\n"
STEP: checking annotations
Feb 24 18:28:25.918: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 18:28:25.918: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:28:25.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4927" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":288,"skipped":5039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:28:26.029: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb 24 18:28:26.358: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 18:29:26.739: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:29:26.793: INFO: Starting informer...
STEP: Starting pods...
Feb 24 18:29:26.961: INFO: Pod1 is running on bootstrap-e2e-minion-group-2qjz. Tainting Node
Feb 24 18:29:29.229: INFO: Pod2 is running on bootstrap-e2e-minion-group-2qjz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 24 18:29:36.288: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 24 18:29:56.318: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:29:56.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7227" for this suite.
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":289,"skipped":5067,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:29:56.593: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb 24 18:29:56.866: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 18:30:57.250: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:30:57.302: INFO: Starting informer...
STEP: Starting pod...
Feb 24 18:30:57.412: INFO: Pod is running on bootstrap-e2e-minion-group-2qjz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 24 18:30:57.580: INFO: Pod wasn't evicted. Proceeding
Feb 24 18:30:57.580: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 24 18:32:12.747: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:32:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6129" for this suite.
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":290,"skipped":5074,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:32:12.857: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb 24 18:32:13.330: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:32:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6442" for this suite.
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":291,"skipped":5088,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:32:13.549: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 24 18:32:13.810: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 24 18:32:33.853: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:32:38.518: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:32:57.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3571" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":292,"skipped":5108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:32:57.899: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Feb 24 18:32:58.216: INFO: Waiting up to 5m0s for pod "client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9" in namespace "containers-1083" to be "Succeeded or Failed"
Feb 24 18:32:58.268: INFO: Pod "client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.060714ms
Feb 24 18:33:00.321: INFO: Pod "client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104478932s
STEP: Saw pod success
Feb 24 18:33:00.321: INFO: Pod "client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9" satisfied condition "Succeeded or Failed"
Feb 24 18:33:00.373: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9 container agnhost-container: <nil>
STEP: delete the pod
Feb 24 18:33:00.508: INFO: Waiting for pod client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9 to disappear
Feb 24 18:33:00.560: INFO: Pod client-containers-0a9cd999-5a1b-4d1d-b7da-73aaa2647eb9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:33:00.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1083" for this suite.
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5140,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:33:00.668: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:33:00.984: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-947dc947-0f34-4e25-a99d-dcc4ee257d13" in namespace "security-context-test-6339" to be "Succeeded or Failed"
Feb 24 18:33:01.036: INFO: Pod "busybox-readonly-false-947dc947-0f34-4e25-a99d-dcc4ee257d13": Phase="Pending", Reason="", readiness=false. Elapsed: 52.348823ms
Feb 24 18:33:03.090: INFO: Pod "busybox-readonly-false-947dc947-0f34-4e25-a99d-dcc4ee257d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106089602s
Feb 24 18:33:03.090: INFO: Pod "busybox-readonly-false-947dc947-0f34-4e25-a99d-dcc4ee257d13" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:33:03.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6339" for this suite.
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":5141,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:33:03.198: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-0dd72e03-91bc-48b7-801a-cd84b0d3ac1d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0dd72e03-91bc-48b7-801a-cd84b0d3ac1d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:13.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2349" for this suite.
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":5149,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:13.898: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:34:14.215: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563" in namespace "downward-api-956" to be "Succeeded or Failed"
Feb 24 18:34:14.268: INFO: Pod "downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563": Phase="Pending", Reason="", readiness=false. Elapsed: 52.430691ms
Feb 24 18:34:16.321: INFO: Pod "downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.105263786s
STEP: Saw pod success
Feb 24 18:34:16.321: INFO: Pod "downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563" satisfied condition "Succeeded or Failed"
Feb 24 18:34:16.373: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563 container client-container: <nil>
STEP: delete the pod
Feb 24 18:34:16.496: INFO: Waiting for pod downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563 to disappear
Feb 24 18:34:16.548: INFO: Pod downwardapi-volume-89965611-a155-4bf7-83d2-78d0e01a3563 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:16.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-956" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5154,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:16.656: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 24 18:34:16.978: INFO: Waiting up to 5m0s for pod "pod-98aa29be-876b-4744-b153-2b16ac9149c4" in namespace "emptydir-8964" to be "Succeeded or Failed"
Feb 24 18:34:17.029: INFO: Pod "pod-98aa29be-876b-4744-b153-2b16ac9149c4": Phase="Pending", Reason="", readiness=false. Elapsed: 51.678865ms
Feb 24 18:34:19.081: INFO: Pod "pod-98aa29be-876b-4744-b153-2b16ac9149c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103762932s
STEP: Saw pod success
Feb 24 18:34:19.081: INFO: Pod "pod-98aa29be-876b-4744-b153-2b16ac9149c4" satisfied condition "Succeeded or Failed"
Feb 24 18:34:19.134: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-98aa29be-876b-4744-b153-2b16ac9149c4 container test-container: <nil>
STEP: delete the pod
Feb 24 18:34:19.250: INFO: Waiting for pod pod-98aa29be-876b-4744-b153-2b16ac9149c4 to disappear
Feb 24 18:34:19.302: INFO: Pod pod-98aa29be-876b-4744-b153-2b16ac9149c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:19.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8964" for this suite.
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5162,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:19.410: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:34:19.673: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:21.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7475" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":298,"skipped":5168,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:21.257: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb 24 18:34:24.390: INFO: Successfully updated pod "annotationupdate299e27cb-c947-40e3-80d0-6a599c8dbd00"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:26.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3912" for this suite.
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5168,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:26.607: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:34:26.864: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:27.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-305" for this suite.
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":300,"skipped":5169,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:27.899: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2186
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 24 18:34:28.852: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 18:34:29.170: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 18:34:31.222: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:33.222: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:35.222: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:37.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:39.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:41.222: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:43.222: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:45.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 24 18:34:47.222: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 24 18:34:47.327: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 24 18:34:49.379: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 24 18:34:49.482: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 24 18:34:51.742: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 18:34:51.742: INFO: Breadth first check of 10.64.1.249 on host 10.138.0.4...
Feb 24 18:34:51.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.250:9080/dial?request=hostname&protocol=udp&host=10.64.1.249&port=8081&tries=1'] Namespace:pod-network-test-2186 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:34:51.793: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:34:52.202: INFO: Waiting for responses: map[]
Feb 24 18:34:52.202: INFO: reached 10.64.1.249 after 0/1 tries
Feb 24 18:34:52.202: INFO: Breadth first check of 10.64.2.93 on host 10.138.0.3...
Feb 24 18:34:52.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.250:9080/dial?request=hostname&protocol=udp&host=10.64.2.93&port=8081&tries=1'] Namespace:pod-network-test-2186 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:34:52.254: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:34:52.620: INFO: Waiting for responses: map[]
Feb 24 18:34:52.620: INFO: reached 10.64.2.93 after 0/1 tries
Feb 24 18:34:52.620: INFO: Breadth first check of 10.64.3.136 on host 10.138.0.5...
Feb 24 18:34:52.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.250:9080/dial?request=hostname&protocol=udp&host=10.64.3.136&port=8081&tries=1'] Namespace:pod-network-test-2186 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 18:34:52.671: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 24 18:34:53.062: INFO: Waiting for responses: map[]
Feb 24 18:34:53.062: INFO: reached 10.64.3.136 after 0/1 tries
Feb 24 18:34:53.062: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:53.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2186" for this suite.
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5172,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:53.169: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:34:53.426: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.69.28 --kubeconfig=/workspace/.kube/config --namespace=kubectl-1988 version'
Feb 24 18:34:53.678: INFO: stderr: ""
Feb 24 18:34:53.678: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.5-rc.0.10+165e5664b0e65d\", GitCommit:\"165e5664b0e65d2e0f8fbcfec16aa95f2bc3cb19\", GitTreeState:\"clean\", BuildDate:\"2021-02-21T07:33:42Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.5-rc.0.10+165e5664b0e65d\", GitCommit:\"165e5664b0e65d2e0f8fbcfec16aa95f2bc3cb19\", GitTreeState:\"clean\", BuildDate:\"2021-02-21T07:33:42Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:34:53.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1988" for this suite.
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":302,"skipped":5177,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:34:53.837: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb 24 18:34:56.970: INFO: Successfully updated pod "annotationupdate6af53e26-3784-4ab3-be0b-e80ce7b8f5dc"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:01.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4670" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:01.242: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb 24 18:35:01.501: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 18:35:01.611: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 18:35:01.663: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-2qjz before test
Feb 24 18:35:01.723: INFO: kube-proxy-bootstrap-e2e-minion-group-2qjz from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.723: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 18:35:01.723: INFO: metadata-proxy-v0.1-mjnrs from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 18:35:01.723: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 18:35:01.723: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 18:35:01.723: INFO: netserver-0 from pod-network-test-2186 started at 2021-02-24 18:34:28 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.723: INFO: 	Container webserver ready: false, restart count 0
Feb 24 18:35:01.723: INFO: test-container-pod from pod-network-test-2186 started at 2021-02-24 18:34:49 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.723: INFO: 	Container webserver ready: false, restart count 0
Feb 24 18:35:01.723: INFO: annotationupdate6af53e26-3784-4ab3-be0b-e80ce7b8f5dc from projected-4670 started at 2021-02-24 18:34:54 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.723: INFO: 	Container client-container ready: true, restart count 0
Feb 24 18:35:01.723: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-9mrc before test
Feb 24 18:35:01.778: INFO: coredns-6954c77b9b-9rzld from kube-system started at 2021-02-24 17:01:20 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.778: INFO: 	Container coredns ready: true, restart count 0
Feb 24 18:35:01.778: INFO: kube-proxy-bootstrap-e2e-minion-group-9mrc from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.778: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 18:35:01.778: INFO: metadata-proxy-v0.1-rgzg9 from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 18:35:01.778: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 18:35:01.778: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 18:35:01.778: INFO: metrics-server-v0.3.6-8b98f98c9-pvx78 from kube-system started at 2021-02-24 17:01:37 +0000 UTC (2 container statuses recorded)
Feb 24 18:35:01.779: INFO: 	Container metrics-server ready: true, restart count 0
Feb 24 18:35:01.779: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 24 18:35:01.779: INFO: netserver-1 from pod-network-test-2186 started at 2021-02-24 18:34:29 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.779: INFO: 	Container webserver ready: false, restart count 0
Feb 24 18:35:01.779: INFO: 
Logging pods the apiserver thinks is on node bootstrap-e2e-minion-group-bthl before test
Feb 24 18:35:01.834: INFO: coredns-6954c77b9b-4fdxt from kube-system started at 2021-02-24 18:29:29 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container coredns ready: true, restart count 0
Feb 24 18:35:01.834: INFO: kube-dns-autoscaler-76478fcf46-gsgnb from kube-system started at 2021-02-24 17:01:17 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container autoscaler ready: true, restart count 0
Feb 24 18:35:01.834: INFO: kube-proxy-bootstrap-e2e-minion-group-bthl from kube-system started at 2021-02-24 17:00:52 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 18:35:01.834: INFO: l7-default-backend-6f8dd4f4d5-c7mgv from kube-system started at 2021-02-24 17:01:13 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 24 18:35:01.834: INFO: metadata-proxy-v0.1-whmsq from kube-system started at 2021-02-24 17:00:52 +0000 UTC (2 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 24 18:35:01.834: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 24 18:35:01.834: INFO: volume-snapshot-controller-0 from kube-system started at 2021-02-24 17:01:18 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Feb 24 18:35:01.834: INFO: netserver-2 from pod-network-test-2186 started at 2021-02-24 18:34:29 +0000 UTC (1 container statuses recorded)
Feb 24 18:35:01.834: INFO: 	Container webserver ready: false, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node bootstrap-e2e-minion-group-2qjz
STEP: verifying the node has the label node bootstrap-e2e-minion-group-9mrc
STEP: verifying the node has the label node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod coredns-6954c77b9b-4fdxt requesting resource cpu=100m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod coredns-6954c77b9b-9rzld requesting resource cpu=100m on Node bootstrap-e2e-minion-group-9mrc
Feb 24 18:35:02.286: INFO: Pod kube-dns-autoscaler-76478fcf46-gsgnb requesting resource cpu=20m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-2qjz requesting resource cpu=100m on Node bootstrap-e2e-minion-group-2qjz
Feb 24 18:35:02.286: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-9mrc requesting resource cpu=100m on Node bootstrap-e2e-minion-group-9mrc
Feb 24 18:35:02.286: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-bthl requesting resource cpu=100m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod l7-default-backend-6f8dd4f4d5-c7mgv requesting resource cpu=10m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod metadata-proxy-v0.1-mjnrs requesting resource cpu=32m on Node bootstrap-e2e-minion-group-2qjz
Feb 24 18:35:02.286: INFO: Pod metadata-proxy-v0.1-rgzg9 requesting resource cpu=32m on Node bootstrap-e2e-minion-group-9mrc
Feb 24 18:35:02.286: INFO: Pod metadata-proxy-v0.1-whmsq requesting resource cpu=32m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod metrics-server-v0.3.6-8b98f98c9-pvx78 requesting resource cpu=53m on Node bootstrap-e2e-minion-group-9mrc
Feb 24 18:35:02.286: INFO: Pod volume-snapshot-controller-0 requesting resource cpu=0m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod netserver-2 requesting resource cpu=0m on Node bootstrap-e2e-minion-group-bthl
Feb 24 18:35:02.286: INFO: Pod test-container-pod requesting resource cpu=0m on Node bootstrap-e2e-minion-group-2qjz
Feb 24 18:35:02.286: INFO: Pod annotationupdate6af53e26-3784-4ab3-be0b-e80ce7b8f5dc requesting resource cpu=0m on Node bootstrap-e2e-minion-group-2qjz
STEP: Starting Pods to consume most of the cluster CPU.
Feb 24 18:35:02.286: INFO: Creating a pod which consumes cpu=1307m on Node bootstrap-e2e-minion-group-2qjz
Feb 24 18:35:02.346: INFO: Creating a pod which consumes cpu=1200m on Node bootstrap-e2e-minion-group-9mrc
Feb 24 18:35:02.398: INFO: Creating a pod which consumes cpu=1216m on Node bootstrap-e2e-minion-group-bthl
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-276e3789-05f3-40cb-9513-6e58578fc201.1666c2cd08a69783], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1713/filler-pod-276e3789-05f3-40cb-9513-6e58578fc201 to bootstrap-e2e-minion-group-2qjz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-276e3789-05f3-40cb-9513-6e58578fc201.1666c2cd35a49ccd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-276e3789-05f3-40cb-9513-6e58578fc201.1666c2cd383ba8cf], Reason = [Created], Message = [Created container filler-pod-276e3789-05f3-40cb-9513-6e58578fc201]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-276e3789-05f3-40cb-9513-6e58578fc201.1666c2cd3d179984], Reason = [Started], Message = [Started container filler-pod-276e3789-05f3-40cb-9513-6e58578fc201]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d.1666c2cd0ef5061f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1713/filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d to bootstrap-e2e-minion-group-bthl]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d.1666c2cd3c28ce19], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d.1666c2cd403357e0], Reason = [Created], Message = [Created container filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d.1666c2cd4550cb5c], Reason = [Started], Message = [Started container filler-pod-a3caeb98-1a1b-43c0-b7c5-48c9ac8c7c6d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386.1666c2cd0bd467a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1713/filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386 to bootstrap-e2e-minion-group-9mrc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386.1666c2cd387227bd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386.1666c2cd3aa0175f], Reason = [Created], Message = [Created container filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386.1666c2cd3f9a1780], Reason = [Started], Message = [Started container filler-pod-ecbf76b2-b6f3-46f3-8222-4a6628dca386]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1666c2cd9b935cd1], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 Insufficient cpu.]
STEP: removing the label node off the node bootstrap-e2e-minion-group-2qjz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-9mrc
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-bthl
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1713" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":304,"skipped":5215,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:06.469: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 24 18:35:06.727: INFO: Creating ReplicaSet my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a
Feb 24 18:35:06.844: INFO: Pod name my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a: Found 1 pods out of 1
Feb 24 18:35:06.844: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a" is running
Feb 24 18:35:08.950: INFO: Pod "my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a-c5h49" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-24 18:35:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-24 18:35:06 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-24 18:35:06 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-24 18:35:06 +0000 UTC Reason: Message:}])
Feb 24 18:35:08.951: INFO: Trying to dial the pod
Feb 24 18:35:14.107: INFO: Controller my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a: Got expected result from replica 1 [my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a-c5h49]: "my-hostname-basic-8020de17-66ca-4a72-810d-dd4e95a70f9a-c5h49", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:14.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7852" for this suite.
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":305,"skipped":5216,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:14.214: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6290/configmap-test-d2a65309-ed81-4a01-bcde-7ab609d5c9f5
STEP: Creating a pod to test consume configMaps
Feb 24 18:35:14.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f" in namespace "configmap-6290" to be "Succeeded or Failed"
Feb 24 18:35:14.665: INFO: Pod "pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f": Phase="Pending", Reason="", readiness=false. Elapsed: 77.163152ms
Feb 24 18:35:16.718: INFO: Pod "pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.129710312s
STEP: Saw pod success
Feb 24 18:35:16.718: INFO: Pod "pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f" satisfied condition "Succeeded or Failed"
Feb 24 18:35:16.769: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f container env-test: <nil>
STEP: delete the pod
Feb 24 18:35:16.885: INFO: Waiting for pod pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f to disappear
Feb 24 18:35:16.936: INFO: Pod pod-configmaps-7d73ea82-53e3-49e4-8616-a54e9eab607f no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:16.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6290" for this suite.
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:17.044: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:28.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5968" for this suite.
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":307,"skipped":5307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:28.815: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Feb 24 18:35:29.127: INFO: Waiting up to 5m0s for pod "var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba" in namespace "var-expansion-5230" to be "Succeeded or Failed"
Feb 24 18:35:29.179: INFO: Pod "var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 51.494653ms
Feb 24 18:35:31.231: INFO: Pod "var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103703308s
STEP: Saw pod success
Feb 24 18:35:31.231: INFO: Pod "var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba" satisfied condition "Succeeded or Failed"
Feb 24 18:35:31.282: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba container dapi-container: <nil>
STEP: delete the pod
Feb 24 18:35:31.399: INFO: Waiting for pod var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba to disappear
Feb 24 18:35:31.451: INFO: Pod var-expansion-3d84eede-9f55-4063-a1de-cbf3919d21ba no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5230" for this suite.
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5331,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:31.558: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-2f7867db-08eb-4e23-9e9b-b33bc7196ec3 in namespace container-probe-787
Feb 24 18:35:33.973: INFO: Started pod liveness-2f7867db-08eb-4e23-9e9b-b33bc7196ec3 in namespace container-probe-787
STEP: checking the pod's current state and verifying that restartCount is present
Feb 24 18:35:34.027: INFO: Initial restart count of pod liveness-2f7867db-08eb-4e23-9e9b-b33bc7196ec3 is 0
Feb 24 18:35:54.638: INFO: Restart count of pod container-probe-787/liveness-2f7867db-08eb-4e23-9e9b-b33bc7196ec3 is now 1 (20.611632885s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-787" for this suite.
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5334,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:54.805: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 24 18:35:57.326: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:35:57.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-232" for this suite.
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5340,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 24 18:35:57.546: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb 24 18:35:57.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139" in namespace "projected-3614" to be "Succeeded or Failed"
Feb 24 18:35:57.914: INFO: Pod "downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139": Phase="Pending", Reason="", readiness=false. Elapsed: 51.133142ms
Feb 24 18:35:59.972: INFO: Pod "downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109134279s
Feb 24 18:36:02.024: INFO: Pod "downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.161245863s
STEP: Saw pod success
Feb 24 18:36:02.024: INFO: Pod "downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139" satisfied condition "Succeeded or Failed"
Feb 24 18:36:02.075: INFO: Trying to get logs from node bootstrap-e2e-minion-group-2qjz pod downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139 container client-container: <nil>
STEP: delete the pod
Feb 24 18:36:02.193: INFO: Waiting for pod downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139 to disappear
Feb 24 18:36:02.245: INFO: Pod downwardapi-volume-2f51140b-0b70-45aa-bef1-720dabf3b139 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb 24 18:36:02.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3614" for this suite.
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5348,"failed":0}
SSSSSSSSFeb 24 18:36:02.352: INFO: Running AfterSuite actions on all nodes
Feb 24 18:36:02.352: INFO: Running AfterSuite actions on node 1
Feb 24 18:36:02.352: INFO: Skipping dumping logs from cluster

JUnit report was created: /workspace/_artifacts/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5662.344 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
