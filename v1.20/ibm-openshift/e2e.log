I0628 18:29:42.982283      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-179225761
I0628 18:29:42.982312      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0628 18:29:42.982435      24 e2e.go:129] Starting e2e run "6f56c40a-af57-4403-b3ff-18a7f2cc2533" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1624904981 - Will randomize all specs
Will run 311 of 5667 specs

Jun 28 18:29:42.993: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
E0628 18:29:42.995181      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 28 18:29:42.995: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 28 18:29:43.168: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 28 18:29:43.345: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 28 18:29:43.345: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Jun 28 18:29:43.345: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 28 18:29:43.408: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun 28 18:29:43.408: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-vpc-block-csi-node' (0 seconds elapsed)
Jun 28 18:29:43.408: INFO: e2e test version: v1.20.6
Jun 28 18:29:43.446: INFO: kube-apiserver version: v1.20.0+2817867
Jun 28 18:29:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 18:29:43.519: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:29:43.519: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
Jun 28 18:29:43.889: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1153.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1153.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1153.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1153.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1153.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1153.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 18:30:14.801: INFO: DNS probes using dns-1153/dns-test-1845db2c-87d6-4748-840b-c162f57c0789 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:14.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1153" for this suite.

• [SLOW TEST:31.367 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":1,"skipped":21,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:14.887: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:30:15.820: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:30:17.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:30:20.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:30:22.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501815, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:30:25.199: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:26.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9691" for this suite.
STEP: Destroying namespace "webhook-9691-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":2,"skipped":76,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:28.621: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 18:30:32.187: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 18:30:32.809: INFO: waiting for watch events with expected annotations
Jun 28 18:30:32.809: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:35.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7351" for this suite.

• [SLOW TEST:6.826 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":3,"skipped":92,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:30:39.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6756" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":4,"skipped":119,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:30:40.035: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:30:40.689: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 18:30:51.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-950 --namespace=crd-publish-openapi-950 create -f -'
Jun 28 18:30:55.950: INFO: stderr: ""
Jun 28 18:30:55.950: INFO: stdout: "e2e-test-crd-publish-openapi-1744-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 18:30:55.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-950 --namespace=crd-publish-openapi-950 delete e2e-test-crd-publish-openapi-1744-crds test-cr'
Jun 28 18:30:59.412: INFO: stderr: ""
Jun 28 18:30:59.412: INFO: stdout: "e2e-test-crd-publish-openapi-1744-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 28 18:30:59.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-950 --namespace=crd-publish-openapi-950 apply -f -'
Jun 28 18:31:02.650: INFO: stderr: ""
Jun 28 18:31:02.650: INFO: stdout: "e2e-test-crd-publish-openapi-1744-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 18:31:02.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-950 --namespace=crd-publish-openapi-950 delete e2e-test-crd-publish-openapi-1744-crds test-cr'
Jun 28 18:31:03.111: INFO: stderr: ""
Jun 28 18:31:03.111: INFO: stdout: "e2e-test-crd-publish-openapi-1744-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 28 18:31:03.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-950 explain e2e-test-crd-publish-openapi-1744-crds'
Jun 28 18:31:03.586: INFO: stderr: ""
Jun 28 18:31:03.586: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1744-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:18.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-950" for this suite.

• [SLOW TEST:39.592 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":5,"skipped":119,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:19.627: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 18:31:36.806: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:36.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2076" for this suite.

• [SLOW TEST:17.396 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":131,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:37.024: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c66c8f71-e0b2-4961-a0bb-c4f2ab76ba0c
STEP: Creating a pod to test consume configMaps
Jun 28 18:31:37.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c" in namespace "configmap-5270" to be "Succeeded or Failed"
Jun 28 18:31:37.469: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 34.475792ms
Jun 28 18:31:39.501: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066614833s
Jun 28 18:31:41.721: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2860531s
Jun 28 18:31:44.299: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.863724795s
Jun 28 18:31:46.836: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401182491s
Jun 28 18:31:49.366: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.931470599s
STEP: Saw pod success
Jun 28 18:31:49.366: INFO: Pod "pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c" satisfied condition "Succeeded or Failed"
Jun 28 18:31:49.945: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c container agnhost-container: <nil>
STEP: delete the pod
Jun 28 18:31:51.041: INFO: Waiting for pod pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c to disappear
Jun 28 18:31:51.615: INFO: Pod pod-configmaps-7ff52dde-7c57-4a7b-885b-41237c89df6c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:31:51.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5270" for this suite.

• [SLOW TEST:16.490 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":138,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:31:53.514: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:31:58.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:32:00.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:32:02.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:32:05.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:32:07.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:32:08.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760501917, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:32:12.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:32:27.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3259" for this suite.
STEP: Destroying namespace "webhook-3259-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:36.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":8,"skipped":153,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:32:29.539: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:32:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 28 18:32:48.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 create -f -'
Jun 28 18:32:58.251: INFO: stderr: ""
Jun 28 18:32:58.251: INFO: stdout: "e2e-test-crd-publish-openapi-2009-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 18:32:58.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 delete e2e-test-crd-publish-openapi-2009-crds test-foo'
Jun 28 18:33:00.588: INFO: stderr: ""
Jun 28 18:33:00.588: INFO: stdout: "e2e-test-crd-publish-openapi-2009-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 28 18:33:00.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 apply -f -'
Jun 28 18:33:11.133: INFO: stderr: ""
Jun 28 18:33:11.133: INFO: stdout: "e2e-test-crd-publish-openapi-2009-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 18:33:11.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 delete e2e-test-crd-publish-openapi-2009-crds test-foo'
Jun 28 18:33:12.390: INFO: stderr: ""
Jun 28 18:33:12.390: INFO: stdout: "e2e-test-crd-publish-openapi-2009-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 28 18:33:12.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 create -f -'
Jun 28 18:33:13.822: INFO: rc: 1
Jun 28 18:33:13.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 apply -f -'
Jun 28 18:33:14.373: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 28 18:33:14.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 create -f -'
Jun 28 18:33:20.300: INFO: rc: 1
Jun 28 18:33:20.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 --namespace=crd-publish-openapi-1886 apply -f -'
Jun 28 18:33:20.886: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 28 18:33:20.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 explain e2e-test-crd-publish-openapi-2009-crds'
Jun 28 18:33:24.262: INFO: stderr: ""
Jun 28 18:33:24.262: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2009-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 28 18:33:24.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 explain e2e-test-crd-publish-openapi-2009-crds.metadata'
Jun 28 18:33:26.137: INFO: stderr: ""
Jun 28 18:33:26.137: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2009-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 28 18:33:26.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 explain e2e-test-crd-publish-openapi-2009-crds.spec'
Jun 28 18:33:27.960: INFO: stderr: ""
Jun 28 18:33:27.960: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2009-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 28 18:33:27.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 explain e2e-test-crd-publish-openapi-2009-crds.spec.bars'
Jun 28 18:33:29.810: INFO: stderr: ""
Jun 28 18:33:29.810: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2009-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 28 18:33:29.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1886 explain e2e-test-crd-publish-openapi-2009-crds.spec.bars2'
Jun 28 18:33:43.466: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:33:55.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1886" for this suite.

• [SLOW TEST:85.862 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":9,"skipped":154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:33:55.401: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Jun 28 18:33:55.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-1795 create -f -'
Jun 28 18:33:58.552: INFO: stderr: ""
Jun 28 18:33:58.552: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 28 18:33:58.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-1795 diff -f -'
Jun 28 18:34:04.158: INFO: rc: 1
Jun 28 18:34:04.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-1795 delete -f -'
Jun 28 18:34:04.343: INFO: stderr: ""
Jun 28 18:34:04.343: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:04.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1795" for this suite.

• [SLOW TEST:9.045 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl diff
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:878
    should check if kubectl diff finds a difference for Deployments [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":10,"skipped":194,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:04.446: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:34:05.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:34:07.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:34:09.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:34:11.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502045, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:34:14.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6945" for this suite.
STEP: Destroying namespace "webhook-6945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.386 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":11,"skipped":198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:21.833: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 28 18:34:23.054: INFO: Waiting up to 5m0s for pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e" in namespace "emptydir-646" to be "Succeeded or Failed"
Jun 28 18:34:23.158: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 103.432729ms
Jun 28 18:34:25.214: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159371551s
Jun 28 18:34:27.452: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398040006s
Jun 28 18:34:29.590: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.53594833s
Jun 28 18:34:31.611: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.556621593s
Jun 28 18:34:33.643: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588724845s
Jun 28 18:34:35.682: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.627813661s
Jun 28 18:34:37.756: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.701291625s
STEP: Saw pod success
Jun 28 18:34:37.756: INFO: Pod "pod-0241682b-cac5-43a8-8580-a848b5257c1e" satisfied condition "Succeeded or Failed"
Jun 28 18:34:37.800: INFO: Trying to get logs from node 10.244.0.29 pod pod-0241682b-cac5-43a8-8580-a848b5257c1e container test-container: <nil>
STEP: delete the pod
Jun 28 18:34:38.141: INFO: Waiting for pod pod-0241682b-cac5-43a8-8580-a848b5257c1e to disappear
Jun 28 18:34:38.175: INFO: Pod pod-0241682b-cac5-43a8-8580-a848b5257c1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:38.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-646" for this suite.

• [SLOW TEST:16.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":230,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:34:39.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:34:41.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502079, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:34:44.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:34:45.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1583" for this suite.
STEP: Destroying namespace "webhook-1583-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":13,"skipped":233,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:34:46.178: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun 28 18:35:02.873: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7534 PodName:pod-sharedvolume-f1e6e2d3-4f39-4526-bd40-6e4ed4fe2d4a ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 18:35:02.873: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 18:35:04.996: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:04.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7534" for this suite.

• [SLOW TEST:18.955 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":14,"skipped":240,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:05.134: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 28 18:35:05.534: INFO: Waiting up to 5m0s for pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c" in namespace "emptydir-7478" to be "Succeeded or Failed"
Jun 28 18:35:05.584: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 49.745411ms
Jun 28 18:35:07.637: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102899152s
Jun 28 18:35:09.659: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124968849s
Jun 28 18:35:11.698: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164491496s
Jun 28 18:35:13.730: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196449084s
Jun 28 18:35:15.936: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.402661368s
Jun 28 18:35:17.960: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.426019144s
STEP: Saw pod success
Jun 28 18:35:17.960: INFO: Pod "pod-15504f27-cfdc-472e-9a81-99c762b9935c" satisfied condition "Succeeded or Failed"
Jun 28 18:35:17.981: INFO: Trying to get logs from node 10.244.0.30 pod pod-15504f27-cfdc-472e-9a81-99c762b9935c container test-container: <nil>
STEP: delete the pod
Jun 28 18:35:18.164: INFO: Waiting for pod pod-15504f27-cfdc-472e-9a81-99c762b9935c to disappear
Jun 28 18:35:18.193: INFO: Pod pod-15504f27-cfdc-472e-9a81-99c762b9935c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:18.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7478" for this suite.

• [SLOW TEST:13.172 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:18.306: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 18:35:18.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9718 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 28 18:35:19.171: INFO: stderr: ""
Jun 28 18:35:19.171: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 28 18:35:29.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9718 get pod e2e-test-httpd-pod -o json'
Jun 28 18:35:29.654: INFO: stderr: ""
Jun 28 18:35:29.654: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.17.113.182/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.17.113.182/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.113.182\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.113.182\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-06-28T18:35:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {\n                            \".\": {},\n                            \"f:seLinuxOptions\": {\n                                \"f:level\": {}\n                            }\n                        },\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:35:18Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:35:23Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:35:23Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.17.113.182\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-28T18:35:24Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9718\",\n        \"resourceVersion\": \"47040\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9718/pods/e2e-test-httpd-pod\",\n        \"uid\": \"421b2d61-827e-4dfb-89ae-9f0e63400afe\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4kz57\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-5m7vp\"\n            }\n        ],\n        \"nodeName\": \"10.244.0.29\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c31,c20\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4kz57\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4kz57\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:35:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:35:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:35:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-28T18:35:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://026a6aaf3abe970a9be9b046f12135ebc834cd9c557af658b05c8ec1f55044a8\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-28T18:35:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.244.0.29\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.17.113.182\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.17.113.182\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-28T18:35:19Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 28 18:35:29.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9718 replace -f -'
Jun 28 18:35:30.582: INFO: stderr: ""
Jun 28 18:35:30.582: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Jun 28 18:35:30.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9718 delete pods e2e-test-httpd-pod'
Jun 28 18:35:42.815: INFO: stderr: ""
Jun 28 18:35:42.815: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9718" for this suite.

• [SLOW TEST:24.660 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":16,"skipped":282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:42.967: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 18:35:43.332: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca" in namespace "projected-5758" to be "Succeeded or Failed"
Jun 28 18:35:43.358: INFO: Pod "downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca": Phase="Pending", Reason="", readiness=false. Elapsed: 25.84273ms
Jun 28 18:35:45.392: INFO: Pod "downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060052907s
Jun 28 18:35:47.573: INFO: Pod "downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.24104157s
STEP: Saw pod success
Jun 28 18:35:47.573: INFO: Pod "downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca" satisfied condition "Succeeded or Failed"
Jun 28 18:35:47.780: INFO: Trying to get logs from node 10.244.0.30 pod downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca container client-container: <nil>
STEP: delete the pod
Jun 28 18:35:48.294: INFO: Waiting for pod downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca to disappear
Jun 28 18:35:48.487: INFO: Pod downwardapi-volume-87cdf6b2-4fb5-417a-8fac-e8ad2b17eeca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:48.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5758" for this suite.

• [SLOW TEST:6.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":322,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:49.210: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:35:49.751: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653" in namespace "security-context-test-6418" to be "Succeeded or Failed"
Jun 28 18:35:49.827: INFO: Pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653": Phase="Pending", Reason="", readiness=false. Elapsed: 76.174208ms
Jun 28 18:35:51.857: INFO: Pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105975984s
Jun 28 18:35:53.892: INFO: Pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141413578s
Jun 28 18:35:56.031: INFO: Pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.280205703s
Jun 28 18:35:56.031: INFO: Pod "busybox-readonly-false-b42614b5-2474-4203-adc2-103479a74653" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:35:56.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6418" for this suite.

• [SLOW TEST:7.747 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":329,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:35:56.957: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:35:57.762: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 18:36:08.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1464 --namespace=crd-publish-openapi-1464 create -f -'
Jun 28 18:36:11.488: INFO: stderr: ""
Jun 28 18:36:11.488: INFO: stdout: "e2e-test-crd-publish-openapi-2664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 18:36:11.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1464 --namespace=crd-publish-openapi-1464 delete e2e-test-crd-publish-openapi-2664-crds test-cr'
Jun 28 18:36:12.897: INFO: stderr: ""
Jun 28 18:36:12.897: INFO: stdout: "e2e-test-crd-publish-openapi-2664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 28 18:36:12.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1464 --namespace=crd-publish-openapi-1464 apply -f -'
Jun 28 18:36:16.917: INFO: stderr: ""
Jun 28 18:36:16.917: INFO: stdout: "e2e-test-crd-publish-openapi-2664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 18:36:16.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1464 --namespace=crd-publish-openapi-1464 delete e2e-test-crd-publish-openapi-2664-crds test-cr'
Jun 28 18:36:18.361: INFO: stderr: ""
Jun 28 18:36:18.361: INFO: stdout: "e2e-test-crd-publish-openapi-2664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 28 18:36:18.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1464 explain e2e-test-crd-publish-openapi-2664-crds'
Jun 28 18:36:19.189: INFO: stderr: ""
Jun 28 18:36:19.189: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2664-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:36:33.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1464" for this suite.

• [SLOW TEST:37.506 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":19,"skipped":342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:36:34.464: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:37:05.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4829" for this suite.

• [SLOW TEST:32.735 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":20,"skipped":366,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:37:07.199: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 28 18:37:09.878: INFO: Waiting up to 5m0s for pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38" in namespace "emptydir-7058" to be "Succeeded or Failed"
Jun 28 18:37:10.300: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 421.893029ms
Jun 28 18:37:12.521: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642206833s
Jun 28 18:37:14.806: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.927476767s
Jun 28 18:37:17.015: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 7.136978009s
Jun 28 18:37:19.255: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 9.376855925s
Jun 28 18:37:21.553: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 11.67439383s
Jun 28 18:37:23.873: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Pending", Reason="", readiness=false. Elapsed: 13.994069546s
Jun 28 18:37:26.176: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.297917087s
STEP: Saw pod success
Jun 28 18:37:26.176: INFO: Pod "pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38" satisfied condition "Succeeded or Failed"
Jun 28 18:37:26.431: INFO: Trying to get logs from node 10.244.0.30 pod pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38 container test-container: <nil>
STEP: delete the pod
Jun 28 18:37:26.974: INFO: Waiting for pod pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38 to disappear
Jun 28 18:37:27.180: INFO: Pod pod-4d19e5c9-c7b1-4a23-9f0e-0e972162dc38 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:37:27.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7058" for this suite.

• [SLOW TEST:20.793 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":374,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:37:27.992: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 28 18:37:31.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-213 /api/v1/namespaces/watch-213/configmaps/e2e-watch-test-resource-version 7ae2279e-7374-4e14-b176-a2bd753f6f74 48218 0 2021-06-28 18:37:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-28 18:37:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 18:37:31.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-213 /api/v1/namespaces/watch-213/configmaps/e2e-watch-test-resource-version 7ae2279e-7374-4e14-b176-a2bd753f6f74 48221 0 2021-06-28 18:37:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-28 18:37:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:37:31.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-213" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":22,"skipped":376,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:37:32.122: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 28 18:37:33.135: INFO: Waiting up to 5m0s for pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06" in namespace "emptydir-4673" to be "Succeeded or Failed"
Jun 28 18:37:33.287: INFO: Pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06": Phase="Pending", Reason="", readiness=false. Elapsed: 151.804556ms
Jun 28 18:37:35.559: INFO: Pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423774668s
Jun 28 18:37:37.855: INFO: Pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.719943387s
Jun 28 18:37:40.116: INFO: Pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.981017973s
STEP: Saw pod success
Jun 28 18:37:40.116: INFO: Pod "pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06" satisfied condition "Succeeded or Failed"
Jun 28 18:37:40.358: INFO: Trying to get logs from node 10.244.0.30 pod pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06 container test-container: <nil>
STEP: delete the pod
Jun 28 18:37:40.760: INFO: Waiting for pod pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06 to disappear
Jun 28 18:37:40.953: INFO: Pod pod-1d4c37d2-b1f4-4d56-8895-6e158f146d06 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:37:40.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4673" for this suite.

• [SLOW TEST:9.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":376,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:37:41.379: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3197
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun 28 18:37:42.435: INFO: Found 1 stateful pods, waiting for 3
Jun 28 18:37:52.484: INFO: Found 2 stateful pods, waiting for 3
Jun 28 18:38:02.890: INFO: Found 2 stateful pods, waiting for 3
Jun 28 18:38:12.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:12.698: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:12.698: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 18:38:22.650: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:22.650: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:22.650: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 18:38:32.476: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:32.476: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:32.476: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 18:38:32.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3197 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 18:38:32.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 18:38:32.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 18:38:32.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 28 18:38:44.439: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 28 18:38:45.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3197 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 18:38:47.486: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 18:38:47.486: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 18:38:47.486: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 18:38:59.746: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:38:59.746: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:38:59.746: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:10.235: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:39:10.235: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:10.235: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:20.513: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:39:20.513: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:20.513: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:30.383: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:39:30.383: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:30.383: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:40.007: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:39:40.007: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:39:50.068: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:39:50.068: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:40:01.135: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:40:01.135: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 18:40:10.630: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:40:20.459: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
STEP: Rolling back to a previous revision
Jun 28 18:40:30.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3197 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 18:40:31.874: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 18:40:31.874: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 18:40:31.874: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 18:40:34.517: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 28 18:40:35.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3197 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 18:40:37.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 18:40:37.759: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 18:40:37.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 18:40:39.071: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:40:39.071: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:39.071: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:39.071: INFO: Waiting for Pod statefulset-3197/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:49.531: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:40:49.531: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:49.531: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:59.356: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:40:59.356: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:40:59.356: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:41:09.941: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:41:09.941: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:41:09.941: INFO: Waiting for Pod statefulset-3197/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:41:19.663: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:41:19.663: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 28 18:41:30.010: INFO: Waiting for StatefulSet statefulset-3197/ss2 to complete update
Jun 28 18:41:30.010: INFO: Waiting for Pod statefulset-3197/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 18:41:39.596: INFO: Deleting all statefulset in ns statefulset-3197
Jun 28 18:41:39.731: INFO: Scaling statefulset ss2 to 0
Jun 28 18:42:20.563: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 18:42:20.880: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:21.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3197" for this suite.

• [SLOW TEST:282.197 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":24,"skipped":378,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:23.577: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7767" for this suite.

• [SLOW TEST:13.075 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":380,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:36.652: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:42:37.970: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 28 18:42:40.592: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:40.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3368" for this suite.

• [SLOW TEST:5.195 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":26,"skipped":381,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:41.848: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:43.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5228" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":27,"skipped":392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:45.542: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:47.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3533" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":28,"skipped":416,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:48.062: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6729" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":29,"skipped":422,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:42:49.119: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-fe466353-b957-491e-9f98-59502ec02252
STEP: Creating a pod to test consume configMaps
Jun 28 18:42:49.703: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4" in namespace "projected-5160" to be "Succeeded or Failed"
Jun 28 18:42:49.787: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4": Phase="Pending", Reason="", readiness=false. Elapsed: 83.273358ms
Jun 28 18:42:52.023: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319719199s
Jun 28 18:42:54.172: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.469090322s
Jun 28 18:42:56.324: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.620666322s
Jun 28 18:42:58.453: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.749685776s
STEP: Saw pod success
Jun 28 18:42:58.453: INFO: Pod "pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4" satisfied condition "Succeeded or Failed"
Jun 28 18:42:58.581: INFO: Trying to get logs from node 10.244.0.30 pod pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 18:42:59.481: INFO: Waiting for pod pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4 to disappear
Jun 28 18:42:59.646: INFO: Pod pod-projected-configmaps-da306e6e-bcc6-457a-8e72-771f9d3f07b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:42:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5160" for this suite.

• [SLOW TEST:11.278 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":422,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:43:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:43:13.582: INFO: Deleting pod "var-expansion-65c475b4-de13-4220-9cc6-ce77fa6aab48" in namespace "var-expansion-1574"
Jun 28 18:43:13.691: INFO: Wait up to 5m0s for pod "var-expansion-65c475b4-de13-4220-9cc6-ce77fa6aab48" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:43:19.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1574" for this suite.

• [SLOW TEST:20.659 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":31,"skipped":427,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:43:21.057: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-5psw
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 18:43:22.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5psw" in namespace "subpath-8275" to be "Succeeded or Failed"
Jun 28 18:43:22.738: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Pending", Reason="", readiness=false. Elapsed: 150.609796ms
Jun 28 18:43:25.054: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466834885s
Jun 28 18:43:27.346: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.75846019s
Jun 28 18:43:29.691: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 7.103251987s
Jun 28 18:43:31.779: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 9.191696443s
Jun 28 18:43:33.965: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 11.377656829s
Jun 28 18:43:36.294: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 13.705936059s
Jun 28 18:43:38.590: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 16.00260757s
Jun 28 18:43:40.835: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 18.247562445s
Jun 28 18:43:43.350: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 20.762682614s
Jun 28 18:43:45.709: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Running", Reason="", readiness=true. Elapsed: 23.121677617s
Jun 28 18:43:48.117: INFO: Pod "pod-subpath-test-configmap-5psw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.529274146s
STEP: Saw pod success
Jun 28 18:43:48.117: INFO: Pod "pod-subpath-test-configmap-5psw" satisfied condition "Succeeded or Failed"
Jun 28 18:43:48.442: INFO: Trying to get logs from node 10.244.0.30 pod pod-subpath-test-configmap-5psw container test-container-subpath-configmap-5psw: <nil>
STEP: delete the pod
Jun 28 18:43:49.060: INFO: Waiting for pod pod-subpath-test-configmap-5psw to disappear
Jun 28 18:43:49.311: INFO: Pod pod-subpath-test-configmap-5psw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5psw
Jun 28 18:43:49.311: INFO: Deleting pod "pod-subpath-test-configmap-5psw" in namespace "subpath-8275"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:43:49.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8275" for this suite.

• [SLOW TEST:29.246 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":32,"skipped":442,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:43:50.303: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 28 18:43:51.401: INFO: Waiting up to 5m0s for pod "pod-06a16e52-b379-4374-95db-eb77a219c440" in namespace "emptydir-6097" to be "Succeeded or Failed"
Jun 28 18:43:51.572: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440": Phase="Pending", Reason="", readiness=false. Elapsed: 171.006877ms
Jun 28 18:43:53.733: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3315097s
Jun 28 18:43:55.986: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584934645s
Jun 28 18:43:58.385: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440": Phase="Pending", Reason="", readiness=false. Elapsed: 6.983921857s
Jun 28 18:44:00.708: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.306717802s
STEP: Saw pod success
Jun 28 18:44:00.708: INFO: Pod "pod-06a16e52-b379-4374-95db-eb77a219c440" satisfied condition "Succeeded or Failed"
Jun 28 18:44:01.166: INFO: Trying to get logs from node 10.244.0.29 pod pod-06a16e52-b379-4374-95db-eb77a219c440 container test-container: <nil>
STEP: delete the pod
Jun 28 18:44:02.137: INFO: Waiting for pod pod-06a16e52-b379-4374-95db-eb77a219c440 to disappear
Jun 28 18:44:02.556: INFO: Pod pod-06a16e52-b379-4374-95db-eb77a219c440 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:02.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6097" for this suite.

• [SLOW TEST:14.469 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":445,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:04.772: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 28 18:44:07.116: INFO: Waiting up to 5m0s for pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26" in namespace "emptydir-7201" to be "Succeeded or Failed"
Jun 28 18:44:07.270: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 153.528987ms
Jun 28 18:44:09.637: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.520653599s
Jun 28 18:44:11.912: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.795600486s
Jun 28 18:44:14.171: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05507121s
Jun 28 18:44:16.598: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.4812263s
Jun 28 18:44:19.109: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.992204052s
STEP: Saw pod success
Jun 28 18:44:19.109: INFO: Pod "pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26" satisfied condition "Succeeded or Failed"
Jun 28 18:44:19.596: INFO: Trying to get logs from node 10.244.0.29 pod pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26 container test-container: <nil>
STEP: delete the pod
Jun 28 18:44:20.586: INFO: Waiting for pod pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26 to disappear
Jun 28 18:44:21.048: INFO: Pod pod-5dedfd86-94c3-42fb-8a2e-39c87c5a2e26 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:21.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7201" for this suite.

• [SLOW TEST:19.662 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:24.435: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 18:44:38.941: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:44:39.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1047" for this suite.

• [SLOW TEST:16.416 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":491,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:44:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun 28 18:44:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:50.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5626" for this suite.

• [SLOW TEST:70.126 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":36,"skipped":509,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:45:52.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7009" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":37,"skipped":523,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:45:52.731: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 18:45:53.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 18:45:56.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502754, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:45:58.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502754, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502753, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 18:46:02.082: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 28 18:46:02.443: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:46:02.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5784" for this suite.
STEP: Destroying namespace "webhook-5784-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.340 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":38,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:46:04.071: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:46:11.872: INFO: Waiting up to 5m0s for pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b" in namespace "pods-260" to be "Succeeded or Failed"
Jun 28 18:46:12.093: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b": Phase="Pending", Reason="", readiness=false. Elapsed: 220.456324ms
Jun 28 18:46:14.235: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.363109071s
Jun 28 18:46:16.765: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893050312s
Jun 28 18:46:19.135: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.262819183s
Jun 28 18:46:21.428: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.555663177s
STEP: Saw pod success
Jun 28 18:46:21.428: INFO: Pod "client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b" satisfied condition "Succeeded or Failed"
Jun 28 18:46:21.693: INFO: Trying to get logs from node 10.244.0.29 pod client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b container env3cont: <nil>
STEP: delete the pod
Jun 28 18:46:23.875: INFO: Waiting for pod client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b to disappear
Jun 28 18:46:24.148: INFO: Pod client-envvars-75d1403d-fbc3-4fa1-823b-22731ea8754b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:46:24.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-260" for this suite.

• [SLOW TEST:21.013 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":548,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:46:25.084: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:46:44.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6145" for this suite.

• [SLOW TEST:20.951 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":40,"skipped":559,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:46:46.035: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun 28 18:46:47.075: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:47:51.295: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:47:51.938: INFO: Starting informer...
STEP: Starting pod...
Jun 28 18:47:52.268: INFO: Pod is running on 10.244.0.29. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 28 18:47:52.642: INFO: Pod wasn't evicted. Proceeding
Jun 28 18:47:52.642: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 28 18:49:08.120: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:08.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1620" for this suite.

• [SLOW TEST:143.326 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":41,"skipped":580,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:09.362: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:46.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7646" for this suite.
STEP: Destroying namespace "nsdeletetest-9572" for this suite.
Jun 28 18:49:47.825: INFO: Namespace nsdeletetest-9572 was already deleted
STEP: Destroying namespace "nsdeletetest-5646" for this suite.

• [SLOW TEST:38.549 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":42,"skipped":584,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 28 18:49:48.414: INFO: Waiting up to 5m0s for pod "pod-633ffb43-6487-42b4-bbc3-9f3defe21e36" in namespace "emptydir-8893" to be "Succeeded or Failed"
Jun 28 18:49:48.478: INFO: Pod "pod-633ffb43-6487-42b4-bbc3-9f3defe21e36": Phase="Pending", Reason="", readiness=false. Elapsed: 63.85159ms
Jun 28 18:49:50.811: INFO: Pod "pod-633ffb43-6487-42b4-bbc3-9f3defe21e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396921139s
Jun 28 18:49:53.047: INFO: Pod "pod-633ffb43-6487-42b4-bbc3-9f3defe21e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.633322051s
STEP: Saw pod success
Jun 28 18:49:53.047: INFO: Pod "pod-633ffb43-6487-42b4-bbc3-9f3defe21e36" satisfied condition "Succeeded or Failed"
Jun 28 18:49:53.239: INFO: Trying to get logs from node 10.244.0.29 pod pod-633ffb43-6487-42b4-bbc3-9f3defe21e36 container test-container: <nil>
STEP: delete the pod
Jun 28 18:49:53.844: INFO: Waiting for pod pod-633ffb43-6487-42b4-bbc3-9f3defe21e36 to disappear
Jun 28 18:49:54.083: INFO: Pod pod-633ffb43-6487-42b4-bbc3-9f3defe21e36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:49:54.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8893" for this suite.

• [SLOW TEST:6.926 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":588,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:49:54.837: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 28 18:49:56.188: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Jun 28 18:49:59.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:01.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:03.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:05.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:07.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:09.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:11.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:13.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:15.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:17.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:19.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:21.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502998, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760502997, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 18:50:28.507: INFO: Waited 4.50246141s for the sample-apiserver to be ready to handle requests.
I0628 18:50:33.534763      24 request.go:655] Throttling request took 1.048962343s, request: GET:https://172.21.0.1:443/apis/crd.projectcalico.org/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:50:43.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9859" for this suite.

• [SLOW TEST:52.741 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":44,"skipped":592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:50:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-15b70155-cbb2-44dd-bcde-764d7126e927
STEP: Creating a pod to test consume secrets
Jun 28 18:50:49.104: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638" in namespace "projected-450" to be "Succeeded or Failed"
Jun 28 18:50:49.412: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 308.386916ms
Jun 28 18:50:51.738: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634656176s
Jun 28 18:50:54.014: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910812863s
Jun 28 18:50:56.479: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375441333s
Jun 28 18:51:00.548: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 11.444132638s
Jun 28 18:51:02.785: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 13.681367138s
Jun 28 18:51:04.973: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Pending", Reason="", readiness=false. Elapsed: 15.869685122s
Jun 28 18:51:07.163: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.059561422s
STEP: Saw pod success
Jun 28 18:51:07.163: INFO: Pod "pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638" satisfied condition "Succeeded or Failed"
Jun 28 18:51:07.337: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:51:07.712: INFO: Waiting for pod pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638 to disappear
Jun 28 18:51:07.809: INFO: Pod pod-projected-secrets-51712463-ab62-46ee-8496-40f46479f638 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:51:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-450" for this suite.

• [SLOW TEST:20.798 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":616,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:51:08.377: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 28 18:51:09.007: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:52:11.543: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:52:11.752: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 28 18:52:18.497: INFO: found a healthy node: 10.244.0.29
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:52:41.890: INFO: pods created so far: [1 1 1]
Jun 28 18:52:41.890: INFO: length of pods created so far: 3
Jun 28 18:52:58.353: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:53:05.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2032" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:53:09.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-344" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:125.186 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":46,"skipped":633,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:53:13.562: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-58379cdb-f3d7-4e35-907a-ad064b2d3e78
STEP: Creating a pod to test consume secrets
Jun 28 18:53:14.560: INFO: Waiting up to 5m0s for pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c" in namespace "secrets-3454" to be "Succeeded or Failed"
Jun 28 18:53:14.760: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c": Phase="Pending", Reason="", readiness=false. Elapsed: 199.828728ms
Jun 28 18:53:16.841: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281031644s
Jun 28 18:53:19.014: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453878004s
Jun 28 18:53:21.176: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615900073s
Jun 28 18:53:23.342: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.781235781s
STEP: Saw pod success
Jun 28 18:53:23.342: INFO: Pod "pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c" satisfied condition "Succeeded or Failed"
Jun 28 18:53:23.514: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 18:53:25.315: INFO: Waiting for pod pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c to disappear
Jun 28 18:53:25.484: INFO: Pod pod-secrets-6a042db6-a57b-4730-8dc0-c4968786c02c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:53:25.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3454" for this suite.

• [SLOW TEST:12.817 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":639,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:53:26.380: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 28 18:53:29.543: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:54:35.169: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun 28 18:54:36.292: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 28 18:54:36.943: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 28 18:54:38.198: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:55:30.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3777" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:127.874 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":48,"skipped":644,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:55:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 28 18:55:37.180: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 18:56:42.023: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun 28 18:56:44.110: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 28 18:56:44.813: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 28 18:56:45.403: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:57:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3665" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:101.671 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":49,"skipped":644,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:57:15.925: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 28 18:57:41.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 18:57:41.871: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 18:57:43.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 18:57:44.125: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 18:57:45.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 18:57:46.225: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 18:57:47.871: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 18:57:48.245: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 18:57:49.872: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 18:57:50.045: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:57:50.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2447" for this suite.

• [SLOW TEST:35.476 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":654,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:57:51.401: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:57:58.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5992" for this suite.

• [SLOW TEST:7.286 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":51,"skipped":664,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:57:58.687: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 18:57:58.914: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:58:02.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8626" for this suite.

• [SLOW TEST:6.541 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":52,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:58:05.229: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 18:58:09.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2530" for this suite.

• [SLOW TEST:5.563 seconds]
[sig-node] PodTemplates
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/podtemplates.go:41
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":53,"skipped":699,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 18:58:10.792: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-f7ac0dd1-838a-479b-88a6-6dd696839cf8 in namespace container-probe-1801
Jun 28 18:58:18.337: INFO: Started pod busybox-f7ac0dd1-838a-479b-88a6-6dd696839cf8 in namespace container-probe-1801
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 18:58:18.520: INFO: Initial restart count of pod busybox-f7ac0dd1-838a-479b-88a6-6dd696839cf8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:02:20.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1801" for this suite.

• [SLOW TEST:251.109 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":708,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:02:21.902: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:02:23.288: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a5a20e6f-81d7-4b24-af34-06bd2ec3158c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a5a20e6f-81d7-4b24-af34-06bd2ec3158c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:04:04.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4189" for this suite.

• [SLOW TEST:104.618 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":711,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:04:06.520: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 28 19:04:14.714: INFO: Successfully updated pod "labelsupdate01899987-83fa-4569-82bb-388f425b7a16"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:04:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-666" for this suite.

• [SLOW TEST:14.294 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:04:20.815: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9525
STEP: creating service affinity-clusterip-transition in namespace services-9525
STEP: creating replication controller affinity-clusterip-transition in namespace services-9525
I0628 19:04:23.361089      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9525, replica count: 3
I0628 19:04:26.811361      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:04:29.811634      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:04:32.811805      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:04:35.813177      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:04:38.813304      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:04:41.813687      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:04:42.197: INFO: Creating new exec pod
Jun 28 19:04:54.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9525 exec execpod-affinitygtg8j -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun 28 19:05:01.999: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 28 19:05:01.999: INFO: stdout: ""
Jun 28 19:05:02.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9525 exec execpod-affinitygtg8j -- /bin/sh -x -c nc -zv -t -w 2 172.21.194.92 80'
Jun 28 19:05:07.233: INFO: stderr: "+ nc -zv -t -w 2 172.21.194.92 80\nConnection to 172.21.194.92 80 port [tcp/http] succeeded!\n"
Jun 28 19:05:07.233: INFO: stdout: ""
Jun 28 19:05:07.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9525 exec execpod-affinitygtg8j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.194.92:80/ ; done'
Jun 28 19:05:11.747: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n"
Jun 28 19:05:11.747: INFO: stdout: "\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-97b8t\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-97b8t\naffinity-clusterip-transition-97b8t\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-97b8t\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-vkjz8\naffinity-clusterip-transition-nqdlj"
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-97b8t
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-97b8t
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-97b8t
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-97b8t
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-vkjz8
Jun 28 19:05:11.747: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:12.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9525 exec execpod-affinitygtg8j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.194.92:80/ ; done'
Jun 28 19:05:15.979: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.92:80/\n"
Jun 28 19:05:15.979: INFO: stdout: "\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj\naffinity-clusterip-transition-nqdlj"
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Received response from host: affinity-clusterip-transition-nqdlj
Jun 28 19:05:15.979: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9525, will wait for the garbage collector to delete the pods
Jun 28 19:05:17.618: INFO: Deleting ReplicationController affinity-clusterip-transition took: 437.740066ms
Jun 28 19:05:18.118: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 500.162807ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:35.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9525" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:74.977 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":57,"skipped":756,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:35.792: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Jun 28 19:05:36.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 28 19:05:36.872: INFO: stderr: ""
Jun 28 19:05:36.872: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Jun 28 19:05:36.872: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 28 19:05:36.872: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7412" to be "running and ready, or succeeded"
Jun 28 19:05:37.032: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 159.991575ms
Jun 28 19:05:39.312: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439916632s
Jun 28 19:05:41.401: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.528535441s
Jun 28 19:05:41.401: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 28 19:05:41.401: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 28 19:05:41.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator'
Jun 28 19:05:42.990: INFO: stderr: ""
Jun 28 19:05:42.990: INFO: stdout: "I0628 19:05:40.351148       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/bpq 370\nI0628 19:05:40.550900       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/665k 279\nI0628 19:05:40.750920       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gzj 574\nI0628 19:05:40.950847       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cbf 580\nI0628 19:05:41.150858       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kwh 509\nI0628 19:05:41.350921       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kp7z 255\nI0628 19:05:41.550768       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nx4h 276\nI0628 19:05:41.750915       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gvb 302\nI0628 19:05:41.950841       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/7hx 499\nI0628 19:05:42.150916       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/2hqj 295\nI0628 19:05:42.350858       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/xhvp 382\nI0628 19:05:42.550859       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/9sb 204\n"
STEP: limiting log lines
Jun 28 19:05:42.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator --tail=1'
Jun 28 19:05:44.148: INFO: stderr: ""
Jun 28 19:05:44.148: INFO: stdout: "I0628 19:05:43.950838       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5fq 535\n"
Jun 28 19:05:44.148: INFO: got output "I0628 19:05:43.950838       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5fq 535\n"
STEP: limiting log bytes
Jun 28 19:05:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator --limit-bytes=1'
Jun 28 19:05:44.913: INFO: stderr: ""
Jun 28 19:05:44.913: INFO: stdout: "I"
Jun 28 19:05:44.913: INFO: got output "I"
STEP: exposing timestamps
Jun 28 19:05:44.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 28 19:05:45.236: INFO: stderr: ""
Jun 28 19:05:45.236: INFO: stdout: "2021-06-28T14:05:45.150960392-05:00 I0628 19:05:45.150842       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/tpf 329\n"
Jun 28 19:05:45.236: INFO: got output "2021-06-28T14:05:45.150960392-05:00 I0628 19:05:45.150842       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/tpf 329\n"
STEP: restricting to a time range
Jun 28 19:05:47.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator --since=1s'
Jun 28 19:05:48.180: INFO: stderr: ""
Jun 28 19:05:48.180: INFO: stdout: "I0628 19:05:47.150910       1 logs_generator.go:76] 34 GET /api/v1/namespaces/ns/pods/v59r 481\nI0628 19:05:47.350919       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/pj9 296\nI0628 19:05:47.550915       1 logs_generator.go:76] 36 GET /api/v1/namespaces/kube-system/pods/vsv 224\nI0628 19:05:47.750916       1 logs_generator.go:76] 37 GET /api/v1/namespaces/ns/pods/j9xs 440\nI0628 19:05:47.950843       1 logs_generator.go:76] 38 POST /api/v1/namespaces/kube-system/pods/t6s 395\n"
Jun 28 19:05:48.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 logs logs-generator logs-generator --since=24h'
Jun 28 19:05:49.199: INFO: stderr: ""
Jun 28 19:05:49.199: INFO: stdout: "I0628 19:05:40.351148       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/bpq 370\nI0628 19:05:40.550900       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/665k 279\nI0628 19:05:40.750920       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gzj 574\nI0628 19:05:40.950847       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cbf 580\nI0628 19:05:41.150858       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kwh 509\nI0628 19:05:41.350921       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kp7z 255\nI0628 19:05:41.550768       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nx4h 276\nI0628 19:05:41.750915       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/gvb 302\nI0628 19:05:41.950841       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/7hx 499\nI0628 19:05:42.150916       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/2hqj 295\nI0628 19:05:42.350858       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/xhvp 382\nI0628 19:05:42.550859       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/9sb 204\nI0628 19:05:42.750915       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/2x6z 284\nI0628 19:05:42.950852       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/dvwr 365\nI0628 19:05:43.150924       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/hzzw 470\nI0628 19:05:43.350915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/xgh 261\nI0628 19:05:43.550862       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/z75 561\nI0628 19:05:43.750813       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/js8 449\nI0628 19:05:43.950838       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/5fq 535\nI0628 19:05:44.150837       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/qwk 561\nI0628 19:05:44.350843       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/c9g 398\nI0628 19:05:44.550818       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/c9n7 371\nI0628 19:05:44.750833       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/7nb 358\nI0628 19:05:44.950841       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/8jl 548\nI0628 19:05:45.150842       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/tpf 329\nI0628 19:05:45.350839       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/spzq 359\nI0628 19:05:45.550920       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/zzn 365\nI0628 19:05:45.750845       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/t96 311\nI0628 19:05:45.950853       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/l4l6 254\nI0628 19:05:46.150851       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/rc27 375\nI0628 19:05:46.350923       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/mm6 490\nI0628 19:05:46.550816       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/zjtc 301\nI0628 19:05:46.750840       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/96c 309\nI0628 19:05:46.950839       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/default/pods/4ph 266\nI0628 19:05:47.150910       1 logs_generator.go:76] 34 GET /api/v1/namespaces/ns/pods/v59r 481\nI0628 19:05:47.350919       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/pj9 296\nI0628 19:05:47.550915       1 logs_generator.go:76] 36 GET /api/v1/namespaces/kube-system/pods/vsv 224\nI0628 19:05:47.750916       1 logs_generator.go:76] 37 GET /api/v1/namespaces/ns/pods/j9xs 440\nI0628 19:05:47.950843       1 logs_generator.go:76] 38 POST /api/v1/namespaces/kube-system/pods/t6s 395\nI0628 19:05:48.150855       1 logs_generator.go:76] 39 GET /api/v1/namespaces/ns/pods/rrg 267\nI0628 19:05:48.350921       1 logs_generator.go:76] 40 PUT /api/v1/namespaces/kube-system/pods/zgkb 274\nI0628 19:05:48.550914       1 logs_generator.go:76] 41 POST /api/v1/namespaces/kube-system/pods/7tb 581\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Jun 28 19:05:49.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-7412 delete pod logs-generator'
Jun 28 19:05:53.221: INFO: stderr: ""
Jun 28 19:05:53.221: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:05:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7412" for this suite.

• [SLOW TEST:18.312 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":58,"skipped":765,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:05:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:04.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7256" for this suite.

• [SLOW TEST:11.196 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":786,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Jun 28 19:06:07.425: INFO: Waiting up to 5m0s for pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0" in namespace "containers-4490" to be "Succeeded or Failed"
Jun 28 19:06:07.655: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 229.342942ms
Jun 28 19:06:10.018: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592963569s
Jun 28 19:06:14.272: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846654829s
Jun 28 19:06:18.302: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.876749567s
Jun 28 19:06:20.494: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.068730428s
STEP: Saw pod success
Jun 28 19:06:20.494: INFO: Pod "client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0" satisfied condition "Succeeded or Failed"
Jun 28 19:06:20.681: INFO: Trying to get logs from node 10.244.0.29 pod client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 19:06:21.696: INFO: Waiting for pod client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0 to disappear
Jun 28 19:06:22.010: INFO: Pod client-containers-5813818a-957b-4419-8998-aa5b1a5cd5c0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:22.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4490" for this suite.

• [SLOW TEST:18.605 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":788,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:06:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 28 19:06:25.372: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:06:27.994: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:06:28.950: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.29 before test
Jun 28 19:06:30.119: INFO: calico-node-47lkr from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:06:30.119: INFO: calico-typha-664f469f54-58kwx from calico-system started at 2021-06-28 17:02:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:06:30.119: INFO: ibm-keepalived-watcher-s6s5t from kube-system started at 2021-06-28 16:59:22 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:06:30.119: INFO: ibm-master-proxy-static-10.244.0.29 from kube-system started at 2021-06-28 16:58:02 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:06:30.119: INFO: ibm-vpc-block-csi-node-mxp5s from kube-system started at 2021-06-28 16:59:22 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:06:30.119: INFO: tuned-tfswp from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:06:30.119: INFO: dns-default-bsrhg from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: node-ca-4rccc from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:06:30.119: INFO: ingress-canary-wq2tn from openshift-ingress-canary started at 2021-06-28 18:48:12 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:06:30.119: INFO: openshift-kube-proxy-rr8vk from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: certified-operators-tzhzk from openshift-marketplace started at 2021-06-28 18:48:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:06:30.119: INFO: community-operators-7llsv from openshift-marketplace started at 2021-06-28 18:47:59 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:06:30.119: INFO: redhat-operators-v8hsx from openshift-marketplace started at 2021-06-28 18:48:22 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:06:30.119: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 18:48:12 +0000 UTC (5 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: node-exporter-mk7n4 from openshift-monitoring started at 2021-06-28 17:03:59 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:06:30.119: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 18:48:03 +0000 UTC (7 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:06:30.119: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:06:30.119: INFO: multus-admission-controller-gc4nb from openshift-multus started at 2021-06-28 18:49:12 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:06:30.119: INFO: multus-hnlqc from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:06:30.119: INFO: network-metrics-daemon-zvdbr from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:06:30.119: INFO: network-check-target-b65gf from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:06:30.119: INFO: service-ca-7d7647cfdb-s24rp from openshift-service-ca started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 28 19:06:30.119: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:28:51 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:06:30.119: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:30.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:06:30.119: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.30 before test
Jun 28 19:06:31.278: INFO: calico-node-2ql4z from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:06:31.278: INFO: calico-typha-664f469f54-j6kl2 from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:06:31.278: INFO: ibm-keepalived-watcher-5dmkt from kube-system started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:06:31.278: INFO: ibm-master-proxy-static-10.244.0.30 from kube-system started at 2021-06-28 16:57:51 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:06:31.278: INFO: ibm-vpc-block-csi-node-db8gf from kube-system started at 2021-06-28 16:58:42 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:06:31.278: INFO: vpn-74cdbd76f7-2pkzl from kube-system started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:06:31.278: INFO: tuned-ntcvm from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:06:31.278: INFO: csi-snapshot-controller-fc56779c7-68pwj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 28 19:06:31.278: INFO: csi-snapshot-webhook-798674875b-z2zlj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container webhook ready: true, restart count 0
Jun 28 19:06:31.278: INFO: downloads-6d9c464cd4-vjjxk from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:06:31.278: INFO: dns-default-l8wqd from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: image-registry-567d5cff74-k92hq from openshift-image-registry started at 2021-06-28 17:07:17 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:06:31.278: INFO: node-ca-dn9xk from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:06:31.278: INFO: ingress-canary-d47lg from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:06:31.278: INFO: router-default-fdf999c57-m7k7f from openshift-ingress started at 2021-06-28 18:41:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container router ready: true, restart count 0
Jun 28 19:06:31.278: INFO: openshift-kube-proxy-jd62h from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: migrator-744d665879-r687n from openshift-kube-storage-version-migrator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:06:31.278: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: kube-state-metrics-55cbf55cc7-5nhfz from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:06:31.278: INFO: node-exporter-2x4b4 from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:06:31.278: INFO: openshift-state-metrics-8555845c54-fd7l8 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:06:31.278: INFO: prometheus-operator-6b9599d88d-nqprj from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:06:31.278: INFO: telemeter-client-666c78ff9f-28pc9 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:06:31.278: INFO: thanos-querier-8c945b5d4-24xwt from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (5 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:06:31.278: INFO: multus-admission-controller-96snf from openshift-multus started at 2021-06-28 17:03:00 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:06:31.278: INFO: multus-qp427 from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:06:31.278: INFO: network-metrics-daemon-s59ql from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:06:31.278: INFO: network-check-target-pbqc5 from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:06:31.278: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:06:31.278: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:06:31.278: INFO: tigera-operator-9cb9c95c7-vxwg6 from tigera-operator started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:31.278: INFO: 	Container tigera-operator ready: true, restart count 4
Jun 28 19:06:31.278: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.31 before test
Jun 28 19:06:32.483: INFO: calico-kube-controllers-67cd4fd574-fst6j from calico-system started at 2021-06-28 17:02:55 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:06:32.483: INFO: calico-node-swgct from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:06:32.483: INFO: calico-typha-664f469f54-xp4ng from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:06:32.483: INFO: ibm-keepalived-watcher-8r7sd from kube-system started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:06:32.483: INFO: ibm-master-proxy-static-10.244.0.31 from kube-system started at 2021-06-28 16:57:44 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:06:32.483: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:06:32.483: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-28 17:03:01 +0000 UTC (4 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 28 19:06:32.483: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 28 19:06:32.483: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun 28 19:06:32.483: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:06:32.483: INFO: ibm-vpc-block-csi-node-dlgsz from kube-system started at 2021-06-28 16:59:10 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:32.483: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:06:32.483: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:06:32.484: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:06:32.484: INFO: cluster-node-tuning-operator-85b6488455-92rjz from openshift-cluster-node-tuning-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:06:32.484: INFO: tuned-5p6rg from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:06:32.484: INFO: cluster-samples-operator-6979dbb9c5-ljqjf from openshift-cluster-samples-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:06:32.484: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:06:32.484: INFO: cluster-storage-operator-6974bfb5c6-gjrzk from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 28 19:06:32.484: INFO: csi-snapshot-controller-operator-c9886b54b-mbxtf from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 28 19:06:32.484: INFO: console-operator-946dbb485-549nf from openshift-console-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:06:32.484: INFO: console-7d7f484b46-qwgl6 from openshift-console started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container console ready: true, restart count 0
Jun 28 19:06:32.484: INFO: console-7d7f484b46-zql2k from openshift-console started at 2021-06-28 18:43:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container console ready: true, restart count 0
Jun 28 19:06:32.484: INFO: downloads-6d9c464cd4-mjc9h from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.484: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:06:32.484: INFO: dns-operator-74db48c654-lbr6w from openshift-dns-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: dns-default-f2xdn from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: cluster-image-registry-operator-7c675f968-j72bn from openshift-image-registry started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: node-ca-24vfp from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:06:32.485: INFO: ingress-canary-n5kgb from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:06:32.485: INFO: ingress-operator-6557486749-2zshp from openshift-ingress-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: router-default-fdf999c57-487ml from openshift-ingress started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container router ready: true, restart count 0
Jun 28 19:06:32.485: INFO: openshift-kube-proxy-bhm75 from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: kube-storage-version-migrator-operator-bdddd9479-r779m from openshift-kube-storage-version-migrator-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:06:32.485: INFO: marketplace-operator-569986b7f7-25n44 from openshift-marketplace started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container marketplace-operator ready: true, restart count 2
Jun 28 19:06:32.485: INFO: redhat-marketplace-94d2l from openshift-marketplace started at 2021-06-28 17:09:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:06:32.485: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: cluster-monitoring-operator-6c785d75f6-rzfjr from openshift-monitoring started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 28 19:06:32.485: INFO: grafana-66dff7486b-hhwrz from openshift-monitoring started at 2021-06-28 17:07:24 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: node-exporter-7kpmm from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:06:32.485: INFO: prometheus-adapter-864ff8d5d4-4zdg4 from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:06:32.485: INFO: prometheus-adapter-864ff8d5d4-zqh4w from openshift-monitoring started at 2021-06-28 17:09:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:06:32.485: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 17:09:38 +0000 UTC (7 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:06:32.485: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:06:32.485: INFO: thanos-querier-8c945b5d4-9mqvb from openshift-monitoring started at 2021-06-28 17:07:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:06:32.485: INFO: multus-2vt7k from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:06:32.485: INFO: multus-admission-controller-txqk8 from openshift-multus started at 2021-06-28 17:02:50 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:06:32.485: INFO: network-metrics-daemon-wvzj5 from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:06:32.485: INFO: network-check-source-b77d8dcf6-p7rbb from openshift-network-diagnostics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 28 19:06:32.485: INFO: network-check-target-27f5h from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:06:32.485: INFO: network-operator-585847d64b-4ll8c from openshift-network-operator started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: catalog-operator-5d5d46b6dd-qxtpf from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: olm-operator-87677979d-zh4wc from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:06:32.485: INFO: packageserver-dfb6789cb-4gvbs from openshift-operator-lifecycle-manager started at 2021-06-28 17:07:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:06:32.485: INFO: packageserver-dfb6789cb-pbg2h from openshift-operator-lifecycle-manager started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:06:32.485: INFO: metrics-854698b86f-nw92k from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:06:32.485: INFO: push-gateway-6cbcf758df-cf46c from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:06:32.485: INFO: service-ca-operator-bf8bb76b5-sbsbp from openshift-service-ca-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:06:32.485: INFO: sonobuoy-e2e-job-c80b85415c994408 from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:06:32.485: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:06:32.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:06:32.485: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-67875c4b-48ef-47db-9d3b-c7dc597cef95 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-67875c4b-48ef-47db-9d3b-c7dc597cef95 off the node 10.244.0.29
STEP: verifying the node doesn't have the label kubernetes.io/e2e-67875c4b-48ef-47db-9d3b-c7dc597cef95
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:06:59.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7431" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:36.443 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":61,"skipped":795,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:00.349: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:07:04.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:06.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:08.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:10.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:12.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504022, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:07:15.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:07:15.991: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1157-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:21.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7644" for this suite.
STEP: Destroying namespace "webhook-7644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:25.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":62,"skipped":796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 28 19:07:27.083: INFO: Waiting up to 5m0s for pod "pod-485951ce-3583-49ef-bcef-10049bee1adb" in namespace "emptydir-2899" to be "Succeeded or Failed"
Jun 28 19:07:27.186: INFO: Pod "pod-485951ce-3583-49ef-bcef-10049bee1adb": Phase="Pending", Reason="", readiness=false. Elapsed: 103.023739ms
Jun 28 19:07:29.431: INFO: Pod "pod-485951ce-3583-49ef-bcef-10049bee1adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348413153s
Jun 28 19:07:31.629: INFO: Pod "pod-485951ce-3583-49ef-bcef-10049bee1adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545519347s
Jun 28 19:07:33.831: INFO: Pod "pod-485951ce-3583-49ef-bcef-10049bee1adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.74806256s
STEP: Saw pod success
Jun 28 19:07:33.831: INFO: Pod "pod-485951ce-3583-49ef-bcef-10049bee1adb" satisfied condition "Succeeded or Failed"
Jun 28 19:07:34.005: INFO: Trying to get logs from node 10.244.0.29 pod pod-485951ce-3583-49ef-bcef-10049bee1adb container test-container: <nil>
STEP: delete the pod
Jun 28 19:07:34.796: INFO: Waiting for pod pod-485951ce-3583-49ef-bcef-10049bee1adb to disappear
Jun 28 19:07:34.913: INFO: Pod pod-485951ce-3583-49ef-bcef-10049bee1adb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:34.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2899" for this suite.

• [SLOW TEST:10.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:07:37.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:39.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:07:41.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504057, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:07:44.931: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:07:45.161: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:49.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9914" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:14.410 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":64,"skipped":947,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:50.557: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:07:51.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7554" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":952,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:07:52.090: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-56d0a034-fa83-4950-8b3e-edd406ff8796
STEP: Creating a pod to test consume configMaps
Jun 28 19:07:52.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415" in namespace "configmap-3343" to be "Succeeded or Failed"
Jun 28 19:07:52.998: INFO: Pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415": Phase="Pending", Reason="", readiness=false. Elapsed: 184.877577ms
Jun 28 19:07:55.113: INFO: Pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300457569s
Jun 28 19:07:57.301: INFO: Pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488460827s
Jun 28 19:07:59.571: INFO: Pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.758519605s
STEP: Saw pod success
Jun 28 19:07:59.571: INFO: Pod "pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415" satisfied condition "Succeeded or Failed"
Jun 28 19:07:59.791: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 19:08:00.883: INFO: Waiting for pod pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415 to disappear
Jun 28 19:08:01.093: INFO: Pod pod-configmaps-af09c7f1-cb30-4cb2-abb4-e86145754415 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:01.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3343" for this suite.

• [SLOW TEST:9.853 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:01.943: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:08:03.065: INFO: Creating deployment "test-recreate-deployment"
Jun 28 19:08:03.362: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 28 19:08:03.877: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 28 19:08:04.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:08:06.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504083, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:08:08.222: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 28 19:08:08.578: INFO: Updating deployment test-recreate-deployment
Jun 28 19:08:08.578: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 19:08:08.989: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3646 /apis/apps/v1/namespaces/deployment-3646/deployments/test-recreate-deployment 9306a06a-6232-4c56-bd8d-cc1e5ecb7f2c 63809 2 2021-06-28 19:08:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cbd4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-28 19:08:08 +0000 UTC,LastTransitionTime:2021-06-28 19:08:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-28 19:08:08 +0000 UTC,LastTransitionTime:2021-06-28 19:08:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 28 19:08:09.172: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-3646 /apis/apps/v1/namespaces/deployment-3646/replicasets/test-recreate-deployment-f79dd4667 8733752d-fdad-4a20-b0c5-2b89390bbfd8 63808 1 2021-06-28 19:08:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9306a06a-6232-4c56-bd8d-cc1e5ecb7f2c 0xc003cbd930 0xc003cbd931}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9306a06a-6232-4c56-bd8d-cc1e5ecb7f2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cbd9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:08:09.172: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 28 19:08:09.172: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-3646 /apis/apps/v1/namespaces/deployment-3646/replicasets/test-recreate-deployment-786dd7c454 af3e7a38-e14d-4941-a456-828ae13b0219 63798 2 2021-06-28 19:08:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9306a06a-6232-4c56-bd8d-cc1e5ecb7f2c 0xc003cbd837 0xc003cbd838}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9306a06a-6232-4c56-bd8d-cc1e5ecb7f2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cbd8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:08:09.293: INFO: Pod "test-recreate-deployment-f79dd4667-wsdlq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-wsdlq test-recreate-deployment-f79dd4667- deployment-3646 /api/v1/namespaces/deployment-3646/pods/test-recreate-deployment-f79dd4667-wsdlq 34a4e919-4c12-481b-986e-10e9056e580a 63812 0 2021-06-28 19:08:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 8733752d-fdad-4a20-b0c5-2b89390bbfd8 0xc003cbdde7 0xc003cbdde8}] []  [{kube-controller-manager Update v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8733752d-fdad-4a20-b0c5-2b89390bbfd8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ks8sn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ks8sn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ks8sn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-bt7tk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:08:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:08:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:08:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:,StartTime:2021-06-28 19:08:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:08:09.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3646" for this suite.

• [SLOW TEST:8.006 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":67,"skipped":980,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:08:09.949: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:08:12.756: INFO: Create a RollingUpdate DaemonSet
Jun 28 19:08:12.908: INFO: Check that daemon pods launch on every node of the cluster
Jun 28 19:08:13.338: INFO: Number of nodes with available pods: 0
Jun 28 19:08:13.338: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:08:14.995: INFO: Number of nodes with available pods: 0
Jun 28 19:08:14.995: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:08:16.094: INFO: Number of nodes with available pods: 0
Jun 28 19:08:16.094: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:08:17.216: INFO: Number of nodes with available pods: 0
Jun 28 19:08:17.216: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:08:18.138: INFO: Number of nodes with available pods: 1
Jun 28 19:08:18.138: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 19:08:19.527: INFO: Number of nodes with available pods: 1
Jun 28 19:08:19.527: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 19:08:21.008: INFO: Number of nodes with available pods: 2
Jun 28 19:08:21.008: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 19:08:21.588: INFO: Number of nodes with available pods: 2
Jun 28 19:08:21.588: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 19:08:22.728: INFO: Number of nodes with available pods: 3
Jun 28 19:08:22.728: INFO: Number of running nodes: 3, number of available pods: 3
Jun 28 19:08:22.728: INFO: Update the DaemonSet to trigger a rollout
Jun 28 19:08:23.039: INFO: Updating DaemonSet daemon-set
Jun 28 19:08:30.569: INFO: Roll back the DaemonSet before rollout is complete
Jun 28 19:08:31.060: INFO: Updating DaemonSet daemon-set
Jun 28 19:08:31.060: INFO: Make sure DaemonSet rollback is complete
Jun 28 19:08:31.232: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:31.232: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:33.046: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:33.046: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:34.012: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:34.012: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:36.012: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:36.012: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:37.361: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:37.361: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:38.940: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:38.940: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:39.884: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:39.884: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:40.992: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:40.992: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:41.953: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:41.953: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:42.942: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:42.942: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:45.642: INFO: Wrong image for pod: daemon-set-5b4nc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 28 19:08:45.642: INFO: Pod daemon-set-5b4nc is not available
Jun 28 19:08:46.932: INFO: Pod daemon-set-bbxq2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2970, will wait for the garbage collector to delete the pods
Jun 28 19:08:48.035: INFO: Deleting DaemonSet.extensions daemon-set took: 239.730637ms
Jun 28 19:08:48.635: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.124342ms
Jun 28 19:09:01.998: INFO: Number of nodes with available pods: 0
Jun 28 19:09:01.998: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 19:09:02.393: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2970/daemonsets","resourceVersion":"64270"},"items":null}

Jun 28 19:09:02.813: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2970/pods","resourceVersion":"64273"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:04.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2970" for this suite.

• [SLOW TEST:55.493 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":68,"skipped":995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:05.442: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Jun 28 19:09:06.633: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 28 19:09:06.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:09.380: INFO: stderr: ""
Jun 28 19:09:09.380: INFO: stdout: "service/agnhost-replica created\n"
Jun 28 19:09:09.380: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 28 19:09:09.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:12.152: INFO: stderr: ""
Jun 28 19:09:12.152: INFO: stdout: "service/agnhost-primary created\n"
Jun 28 19:09:12.152: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 28 19:09:12.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:13.544: INFO: stderr: ""
Jun 28 19:09:13.544: INFO: stdout: "service/frontend created\n"
Jun 28 19:09:13.544: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 28 19:09:13.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:19.357: INFO: stderr: ""
Jun 28 19:09:19.357: INFO: stdout: "deployment.apps/frontend created\n"
Jun 28 19:09:19.357: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 19:09:19.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:20.047: INFO: stderr: ""
Jun 28 19:09:20.047: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 28 19:09:20.047: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 19:09:20.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 create -f -'
Jun 28 19:09:20.445: INFO: stderr: ""
Jun 28 19:09:20.445: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 28 19:09:20.445: INFO: Waiting for all frontend pods to be Running.
Jun 28 19:09:30.646: INFO: Waiting for frontend to serve content.
Jun 28 19:09:31.814: INFO: Trying to add a new entry to the guestbook.
Jun 28 19:09:32.915: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 28 19:09:33.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:34.137: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:34.137: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 19:09:34.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:35.651: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:35.651: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 19:09:35.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:36.330: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:36.330: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 19:09:36.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:37.214: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:37.214: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 19:09:37.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:37.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:37.383: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 28 19:09:37.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5429 delete --grace-period=0 --force -f -'
Jun 28 19:09:37.585: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:09:37.585: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:37.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5429" for this suite.

• [SLOW TEST:32.516 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":69,"skipped":1038,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:37.958: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:09:39.245: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"643a06d7-96b5-4125-aec2-5d48110eb9eb", Controller:(*bool)(0xc00387d1a2), BlockOwnerDeletion:(*bool)(0xc00387d1a3)}}
Jun 28 19:09:39.403: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"beee5157-7d56-4222-9a65-121c5fe16859", Controller:(*bool)(0xc00254d192), BlockOwnerDeletion:(*bool)(0xc00254d193)}}
Jun 28 19:09:39.590: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ba86c97f-9c64-4653-803d-c79775f664b7", Controller:(*bool)(0xc00a9c0a06), BlockOwnerDeletion:(*bool)(0xc00a9c0a07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:09:44.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2866" for this suite.

• [SLOW TEST:7.640 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":70,"skipped":1042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:09:45.599: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:09:47.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 19:09:49.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:09:51.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504187, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:09:54.788: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:03.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1607" for this suite.
STEP: Destroying namespace "webhook-1607-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:21.349 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":71,"skipped":1075,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d354dfb0-c546-434e-b925-1badf88b4751
STEP: Creating a pod to test consume secrets
Jun 28 19:10:09.199: INFO: Waiting up to 5m0s for pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced" in namespace "secrets-3723" to be "Succeeded or Failed"
Jun 28 19:10:09.467: INFO: Pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced": Phase="Pending", Reason="", readiness=false. Elapsed: 268.680457ms
Jun 28 19:10:11.775: INFO: Pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576226916s
Jun 28 19:10:13.975: INFO: Pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced": Phase="Pending", Reason="", readiness=false. Elapsed: 4.776031861s
Jun 28 19:10:16.206: INFO: Pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.006891669s
STEP: Saw pod success
Jun 28 19:10:16.206: INFO: Pod "pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced" satisfied condition "Succeeded or Failed"
Jun 28 19:10:16.457: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:10:19.359: INFO: Waiting for pod pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced to disappear
Jun 28 19:10:19.505: INFO: Pod pod-secrets-f2b12776-c4e6-4e04-8cee-a188f3917ced no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:19.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3723" for this suite.

• [SLOW TEST:13.226 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1078,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:20.175: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Jun 28 19:10:20.851: INFO: Waiting up to 5m0s for pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166" in namespace "var-expansion-9020" to be "Succeeded or Failed"
Jun 28 19:10:21.051: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 200.605554ms
Jun 28 19:10:23.183: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332454118s
Jun 28 19:10:25.292: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441351399s
Jun 28 19:10:27.440: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 6.589313321s
Jun 28 19:10:29.671: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 8.820777537s
Jun 28 19:10:32.087: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Pending", Reason="", readiness=false. Elapsed: 11.236762995s
Jun 28 19:10:34.319: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.468373665s
STEP: Saw pod success
Jun 28 19:10:34.319: INFO: Pod "var-expansion-d930dc77-637b-45f6-b4e3-07095510f166" satisfied condition "Succeeded or Failed"
Jun 28 19:10:34.566: INFO: Trying to get logs from node 10.244.0.29 pod var-expansion-d930dc77-637b-45f6-b4e3-07095510f166 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:10:35.383: INFO: Waiting for pod var-expansion-d930dc77-637b-45f6-b4e3-07095510f166 to disappear
Jun 28 19:10:35.541: INFO: Pod var-expansion-d930dc77-637b-45f6-b4e3-07095510f166 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:35.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9020" for this suite.

• [SLOW TEST:15.940 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1085,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:36.115: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:10:38.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952" in namespace "downward-api-6661" to be "Succeeded or Failed"
Jun 28 19:10:38.415: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952": Phase="Pending", Reason="", readiness=false. Elapsed: 389.342732ms
Jun 28 19:10:40.823: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796796575s
Jun 28 19:10:43.107: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952": Phase="Pending", Reason="", readiness=false. Elapsed: 5.081312262s
Jun 28 19:10:45.284: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25829648s
Jun 28 19:10:47.494: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.467646577s
STEP: Saw pod success
Jun 28 19:10:47.494: INFO: Pod "downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952" satisfied condition "Succeeded or Failed"
Jun 28 19:10:47.704: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952 container client-container: <nil>
STEP: delete the pod
Jun 28 19:10:48.498: INFO: Waiting for pod downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952 to disappear
Jun 28 19:10:48.723: INFO: Pod downwardapi-volume-d5c52d19-c6a4-4d8c-b364-dba96195b952 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:10:48.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6661" for this suite.

• [SLOW TEST:13.375 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:10:49.491: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 28 19:10:50.730: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 19:10:52.344: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 19:10:53.128: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.29 before test
Jun 28 19:10:53.594: INFO: calico-node-47lkr from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:10:53.594: INFO: calico-typha-664f469f54-58kwx from calico-system started at 2021-06-28 17:02:30 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:10:53.594: INFO: ibm-keepalived-watcher-s6s5t from kube-system started at 2021-06-28 16:59:22 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:10:53.594: INFO: ibm-master-proxy-static-10.244.0.29 from kube-system started at 2021-06-28 16:58:02 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:10:53.594: INFO: ibm-vpc-block-csi-node-mxp5s from kube-system started at 2021-06-28 16:59:22 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:10:53.594: INFO: tuned-tfswp from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:10:53.594: INFO: dns-default-bsrhg from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: node-ca-4rccc from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:10:53.594: INFO: ingress-canary-wq2tn from openshift-ingress-canary started at 2021-06-28 18:48:12 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:10:53.594: INFO: openshift-kube-proxy-rr8vk from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: certified-operators-tzhzk from openshift-marketplace started at 2021-06-28 18:48:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:10:53.594: INFO: community-operators-7llsv from openshift-marketplace started at 2021-06-28 18:47:59 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:10:53.594: INFO: redhat-operators-v8hsx from openshift-marketplace started at 2021-06-28 18:48:22 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:10:53.594: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 18:48:12 +0000 UTC (5 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: node-exporter-mk7n4 from openshift-monitoring started at 2021-06-28 17:03:59 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:10:53.594: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 18:48:03 +0000 UTC (7 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:10:53.594: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:10:53.594: INFO: multus-admission-controller-gc4nb from openshift-multus started at 2021-06-28 18:49:12 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:10:53.594: INFO: multus-hnlqc from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:10:53.594: INFO: network-metrics-daemon-zvdbr from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:10:53.594: INFO: network-check-target-b65gf from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:10:53.594: INFO: service-ca-7d7647cfdb-s24rp from openshift-service-ca started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 28 19:10:53.594: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:28:51 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 19:10:53.594: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:53.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:10:53.594: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.30 before test
Jun 28 19:10:54.181: INFO: calico-node-2ql4z from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:10:54.181: INFO: calico-typha-664f469f54-j6kl2 from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:10:54.181: INFO: ibm-keepalived-watcher-5dmkt from kube-system started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:10:54.181: INFO: ibm-master-proxy-static-10.244.0.30 from kube-system started at 2021-06-28 16:57:51 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:10:54.181: INFO: ibm-vpc-block-csi-node-db8gf from kube-system started at 2021-06-28 16:58:42 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:10:54.181: INFO: vpn-74cdbd76f7-2pkzl from kube-system started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container vpn ready: true, restart count 0
Jun 28 19:10:54.181: INFO: tuned-ntcvm from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:10:54.181: INFO: csi-snapshot-controller-fc56779c7-68pwj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 28 19:10:54.181: INFO: csi-snapshot-webhook-798674875b-z2zlj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container webhook ready: true, restart count 0
Jun 28 19:10:54.181: INFO: downloads-6d9c464cd4-vjjxk from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:10:54.181: INFO: dns-default-l8wqd from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: image-registry-567d5cff74-k92hq from openshift-image-registry started at 2021-06-28 17:07:17 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container registry ready: true, restart count 0
Jun 28 19:10:54.181: INFO: node-ca-dn9xk from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:10:54.181: INFO: ingress-canary-d47lg from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:10:54.181: INFO: router-default-fdf999c57-m7k7f from openshift-ingress started at 2021-06-28 18:41:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container router ready: true, restart count 0
Jun 28 19:10:54.181: INFO: openshift-kube-proxy-jd62h from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: migrator-744d665879-r687n from openshift-kube-storage-version-migrator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container migrator ready: true, restart count 0
Jun 28 19:10:54.181: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: kube-state-metrics-55cbf55cc7-5nhfz from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 19:10:54.181: INFO: node-exporter-2x4b4 from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:10:54.181: INFO: openshift-state-metrics-8555845c54-fd7l8 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 19:10:54.181: INFO: prometheus-operator-6b9599d88d-nqprj from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 19:10:54.181: INFO: telemeter-client-666c78ff9f-28pc9 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container reload ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 19:10:54.181: INFO: thanos-querier-8c945b5d4-24xwt from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (5 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:10:54.181: INFO: multus-admission-controller-96snf from openshift-multus started at 2021-06-28 17:03:00 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:10:54.181: INFO: multus-qp427 from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:10:54.181: INFO: network-metrics-daemon-s59ql from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:10:54.181: INFO: network-check-target-pbqc5 from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:10:54.181: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:10:54.181: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 19:10:54.181: INFO: tigera-operator-9cb9c95c7-vxwg6 from tigera-operator started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.181: INFO: 	Container tigera-operator ready: true, restart count 4
Jun 28 19:10:54.181: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.31 before test
Jun 28 19:10:54.730: INFO: calico-kube-controllers-67cd4fd574-fst6j from calico-system started at 2021-06-28 17:02:55 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.730: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 19:10:54.731: INFO: calico-node-swgct from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 19:10:54.731: INFO: calico-typha-664f469f54-xp4ng from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ibm-keepalived-watcher-8r7sd from kube-system started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ibm-master-proxy-static-10.244.0.31 from kube-system started at 2021-06-28 16:57:44 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container pause ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-28 17:03:01 +0000 UTC (4 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ibm-vpc-block-csi-node-dlgsz from kube-system started at 2021-06-28 16:59:10 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 19:10:54.731: INFO: cluster-node-tuning-operator-85b6488455-92rjz from openshift-cluster-node-tuning-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: tuned-5p6rg from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container tuned ready: true, restart count 0
Jun 28 19:10:54.731: INFO: cluster-samples-operator-6979dbb9c5-ljqjf from openshift-cluster-samples-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 19:10:54.731: INFO: cluster-storage-operator-6974bfb5c6-gjrzk from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 28 19:10:54.731: INFO: csi-snapshot-controller-operator-c9886b54b-mbxtf from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: console-operator-946dbb485-549nf from openshift-console-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 19:10:54.731: INFO: console-7d7f484b46-qwgl6 from openshift-console started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container console ready: true, restart count 0
Jun 28 19:10:54.731: INFO: console-7d7f484b46-zql2k from openshift-console started at 2021-06-28 18:43:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container console ready: true, restart count 0
Jun 28 19:10:54.731: INFO: downloads-6d9c464cd4-mjc9h from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container download-server ready: true, restart count 0
Jun 28 19:10:54.731: INFO: dns-operator-74db48c654-lbr6w from openshift-dns-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: dns-default-f2xdn from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container dns ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: cluster-image-registry-operator-7c675f968-j72bn from openshift-image-registry started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: node-ca-24vfp from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ingress-canary-n5kgb from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 19:10:54.731: INFO: ingress-operator-6557486749-2zshp from openshift-ingress-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: router-default-fdf999c57-487ml from openshift-ingress started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container router ready: true, restart count 0
Jun 28 19:10:54.731: INFO: openshift-kube-proxy-bhm75 from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: kube-storage-version-migrator-operator-bdddd9479-r779m from openshift-kube-storage-version-migrator-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 19:10:54.731: INFO: marketplace-operator-569986b7f7-25n44 from openshift-marketplace started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container marketplace-operator ready: true, restart count 2
Jun 28 19:10:54.731: INFO: redhat-marketplace-94d2l from openshift-marketplace started at 2021-06-28 17:09:28 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 19:10:54.731: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: cluster-monitoring-operator-6c785d75f6-rzfjr from openshift-monitoring started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 28 19:10:54.731: INFO: grafana-66dff7486b-hhwrz from openshift-monitoring started at 2021-06-28 17:07:24 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container grafana ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: node-exporter-7kpmm from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 19:10:54.731: INFO: prometheus-adapter-864ff8d5d4-4zdg4 from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:10:54.731: INFO: prometheus-adapter-864ff8d5d4-zqh4w from openshift-monitoring started at 2021-06-28 17:09:24 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 19:10:54.731: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 17:09:38 +0000 UTC (7 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 19:10:54.731: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 19:10:54.731: INFO: thanos-querier-8c945b5d4-9mqvb from openshift-monitoring started at 2021-06-28 17:07:31 +0000 UTC (5 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 19:10:54.731: INFO: multus-2vt7k from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 19:10:54.731: INFO: multus-admission-controller-txqk8 from openshift-multus started at 2021-06-28 17:02:50 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 19:10:54.731: INFO: network-metrics-daemon-wvzj5 from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 19:10:54.731: INFO: network-check-source-b77d8dcf6-p7rbb from openshift-network-diagnostics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 28 19:10:54.731: INFO: network-check-target-27f5h from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 19:10:54.731: INFO: network-operator-585847d64b-4ll8c from openshift-network-operator started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: catalog-operator-5d5d46b6dd-qxtpf from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: olm-operator-87677979d-zh4wc from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 19:10:54.731: INFO: packageserver-dfb6789cb-4gvbs from openshift-operator-lifecycle-manager started at 2021-06-28 17:07:50 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:10:54.731: INFO: packageserver-dfb6789cb-pbg2h from openshift-operator-lifecycle-manager started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 19:10:54.731: INFO: metrics-854698b86f-nw92k from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container metrics ready: true, restart count 1
Jun 28 19:10:54.731: INFO: push-gateway-6cbcf758df-cf46c from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 19:10:54.731: INFO: service-ca-operator-bf8bb76b5-sbsbp from openshift-service-ca-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 19:10:54.731: INFO: sonobuoy-e2e-job-c80b85415c994408 from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container e2e ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:10:54.731: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 19:10:54.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 19:10:54.731: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ad95416f-6180-4d30-9980-7cfa5ccd9b3f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.244.0.29 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.244.0.29 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 28 19:11:29.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.244.0.29 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321
Jun 28 19:11:34.663: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.244.0.29:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:34.663: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321 UDP
Jun 28 19:11:37.878: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.244.0.29 54321] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 28 19:11:48.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.244.0.29 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321
Jun 28 19:11:49.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.244.0.29:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:49.706: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321 UDP
Jun 28 19:11:51.193: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.244.0.29 54321] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:51.193: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 28 19:11:58.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.244.0.29 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:11:58.211: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321
Jun 28 19:12:03.517: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.244.0.29:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:03.518: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321 UDP
Jun 28 19:12:09.948: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.244.0.29 54321] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:09.948: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 28 19:12:20.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.244.0.29 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:20.207: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321
Jun 28 19:12:22.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.244.0.29:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:22.969: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321 UDP
Jun 28 19:12:26.408: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.244.0.29 54321] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:26.408: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 28 19:12:37.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.244.0.29 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:37.310: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321
Jun 28 19:12:43.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.244.0.29:54321/hostname] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:43.190: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.244.0.29, port: 54321 UDP
Jun 28 19:12:54.228: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.244.0.29 54321] Namespace:sched-pred-7023 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:12:54.228: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: removing the label kubernetes.io/e2e-ad95416f-6180-4d30-9980-7cfa5ccd9b3f off the node 10.244.0.29
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ad95416f-6180-4d30-9980-7cfa5ccd9b3f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:05.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7023" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:136.732 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":75,"skipped":1120,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 28 19:13:08.081: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:13:08.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4686" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":76,"skipped":1138,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:13:09.664: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-922f2cce-4ad3-47cc-a060-30b5926d8a69 in namespace container-probe-7075
Jun 28 19:13:17.215: INFO: Started pod test-webserver-922f2cce-4ad3-47cc-a060-30b5926d8a69 in namespace container-probe-7075
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:13:17.327: INFO: Initial restart count of pod test-webserver-922f2cce-4ad3-47cc-a060-30b5926d8a69 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:17:18.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7075" for this suite.

• [SLOW TEST:252.116 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1159,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:17:21.782: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Jun 28 19:17:22.626: INFO: Waiting up to 5m0s for pod "var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760" in namespace "var-expansion-7461" to be "Succeeded or Failed"
Jun 28 19:17:22.745: INFO: Pod "var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760": Phase="Pending", Reason="", readiness=false. Elapsed: 119.472298ms
Jun 28 19:17:24.899: INFO: Pod "var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273333497s
Jun 28 19:17:27.282: INFO: Pod "var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.655847256s
STEP: Saw pod success
Jun 28 19:17:27.282: INFO: Pod "var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760" satisfied condition "Succeeded or Failed"
Jun 28 19:17:27.584: INFO: Trying to get logs from node 10.244.0.29 pod var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:17:30.594: INFO: Waiting for pod var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760 to disappear
Jun 28 19:17:30.818: INFO: Pod var-expansion-906e0653-0174-4d0f-8d39-55dd004eb760 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:17:30.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7461" for this suite.

• [SLOW TEST:10.716 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:17:32.499: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 28 19:17:40.707: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-172 pod-service-account-0ab24e59-b569-4acc-9d4f-94676fe8cfff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 28 19:17:48.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-172 pod-service-account-0ab24e59-b569-4acc-9d4f-94676fe8cfff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 28 19:17:56.413: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-172 pod-service-account-0ab24e59-b569-4acc-9d4f-94676fe8cfff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:18:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-172" for this suite.

• [SLOW TEST:32.527 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":79,"skipped":1239,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:18:05.026: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:18:06.218: INFO: Waiting up to 5m0s for pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f" in namespace "projected-5960" to be "Succeeded or Failed"
Jun 28 19:18:06.369: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f": Phase="Pending", Reason="", readiness=false. Elapsed: 150.351088ms
Jun 28 19:18:08.687: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468382803s
Jun 28 19:18:10.948: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729451412s
Jun 28 19:18:13.128: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.910027623s
Jun 28 19:18:15.355: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.136946888s
STEP: Saw pod success
Jun 28 19:18:15.355: INFO: Pod "downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f" satisfied condition "Succeeded or Failed"
Jun 28 19:18:15.579: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f container client-container: <nil>
STEP: delete the pod
Jun 28 19:18:16.275: INFO: Waiting for pod downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f to disappear
Jun 28 19:18:16.401: INFO: Pod downwardapi-volume-027c2417-84f4-411f-be69-9816e2cce72f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:18:16.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5960" for this suite.

• [SLOW TEST:12.390 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:18:17.417: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5t45c in namespace proxy-2914
I0628 19:18:18.883613      24 runners.go:190] Created replication controller with name: proxy-service-5t45c, namespace: proxy-2914, replica count: 1
I0628 19:18:21.933984      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:18:22.934156      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:18:23.934315      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:18:24.934487      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:25.934641      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:26.934799      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:27.934980      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:28.935228      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:29.935372      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:30.935570      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:31.935747      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 19:18:32.936011      24 runners.go:190] proxy-service-5t45c Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:18:33.090: INFO: setup took 14.754119832s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 28 19:18:34.110: INFO: (0) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.019171975s)
Jun 28 19:18:34.110: INFO: (0) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.019297621s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.196677323s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.196695341s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.196747054s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.196661084s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 1.196763117s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 1.19680849s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.196763705s)
Jun 28 19:18:34.287: INFO: (0) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.196899459s)
Jun 28 19:18:34.445: INFO: (0) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.354213359s)
Jun 28 19:18:34.549: INFO: (0) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 1.45872551s)
Jun 28 19:18:34.549: INFO: (0) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.458694395s)
Jun 28 19:18:34.787: INFO: (0) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.696558556s)
Jun 28 19:18:34.787: INFO: (0) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.696795618s)
Jun 28 19:18:34.787: INFO: (0) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.696661019s)
Jun 28 19:18:35.442: INFO: (1) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 654.746086ms)
Jun 28 19:18:35.442: INFO: (1) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 654.829182ms)
Jun 28 19:18:35.454: INFO: (1) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 667.018471ms)
Jun 28 19:18:35.719: INFO: (1) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 931.288109ms)
Jun 28 19:18:35.719: INFO: (1) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 931.242819ms)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.109620101s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.109676656s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.109593143s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.109612944s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 1.109759309s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.109637664s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.109768762s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.109699882s)
Jun 28 19:18:35.897: INFO: (1) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.109706902s)
Jun 28 19:18:36.256: INFO: (1) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.468697346s)
Jun 28 19:18:36.256: INFO: (1) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.468750701s)
Jun 28 19:18:37.005: INFO: (2) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 749.168281ms)
Jun 28 19:18:37.005: INFO: (2) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 749.182422ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 985.834878ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 985.788125ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 985.711869ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 985.81116ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 985.725879ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 985.776502ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 985.861791ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 985.720833ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 985.817238ms)
Jun 28 19:18:37.242: INFO: (2) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 985.833034ms)
Jun 28 19:18:37.303: INFO: (2) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.04704659s)
Jun 28 19:18:37.503: INFO: (2) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.246650529s)
Jun 28 19:18:37.600: INFO: (2) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.344117841s)
Jun 28 19:18:37.600: INFO: (2) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.344025246s)
Jun 28 19:18:38.047: INFO: (3) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 446.905726ms)
Jun 28 19:18:38.228: INFO: (3) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 627.38431ms)
Jun 28 19:18:38.228: INFO: (3) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 627.513239ms)
Jun 28 19:18:38.252: INFO: (3) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 651.315279ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 803.854333ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 803.864258ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 803.858295ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 803.780463ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 803.832433ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 803.858488ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 803.786424ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 803.787744ms)
Jun 28 19:18:38.404: INFO: (3) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 803.876965ms)
Jun 28 19:18:38.483: INFO: (3) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 882.452296ms)
Jun 28 19:18:38.483: INFO: (3) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 882.478874ms)
Jun 28 19:18:38.559: INFO: (3) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 958.962828ms)
Jun 28 19:18:38.919: INFO: (4) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 359.127441ms)
Jun 28 19:18:38.931: INFO: (4) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 370.756415ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 483.985399ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 483.584833ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 483.513893ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 483.746423ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 483.418454ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 483.72823ms)
Jun 28 19:18:39.044: INFO: (4) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 483.940693ms)
Jun 28 19:18:39.062: INFO: (4) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 502.053811ms)
Jun 28 19:18:39.062: INFO: (4) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 502.239804ms)
Jun 28 19:18:39.062: INFO: (4) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 502.558653ms)
Jun 28 19:18:39.062: INFO: (4) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 502.326688ms)
Jun 28 19:18:39.062: INFO: (4) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 502.360501ms)
Jun 28 19:18:39.185: INFO: (4) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 624.438877ms)
Jun 28 19:18:39.213: INFO: (4) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 653.546802ms)
Jun 28 19:18:39.437: INFO: (5) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 223.175527ms)
Jun 28 19:18:39.459: INFO: (5) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 245.285715ms)
Jun 28 19:18:39.459: INFO: (5) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 245.199094ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 409.655954ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 410.07276ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 409.924593ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 410.132736ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 409.781431ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 409.741827ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 409.969541ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 409.974815ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 409.953351ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 410.242354ms)
Jun 28 19:18:39.624: INFO: (5) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 409.936654ms)
Jun 28 19:18:39.703: INFO: (5) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 489.574189ms)
Jun 28 19:18:39.850: INFO: (5) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 636.598812ms)
Jun 28 19:18:40.294: INFO: (6) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 443.833388ms)
Jun 28 19:18:40.376: INFO: (6) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 524.812012ms)
Jun 28 19:18:40.594: INFO: (6) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 743.491368ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 831.089738ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 830.815987ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 831.233746ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 831.080278ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 831.642853ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 831.22188ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 831.597178ms)
Jun 28 19:18:40.683: INFO: (6) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 831.54744ms)
Jun 28 19:18:40.884: INFO: (6) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.032502756s)
Jun 28 19:18:40.884: INFO: (6) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.032945643s)
Jun 28 19:18:41.089: INFO: (6) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.238191556s)
Jun 28 19:18:41.133: INFO: (6) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.281308524s)
Jun 28 19:18:41.133: INFO: (6) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.281039114s)
Jun 28 19:18:41.981: INFO: (7) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 848.593216ms)
Jun 28 19:18:41.981: INFO: (7) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 848.517185ms)
Jun 28 19:18:42.376: INFO: (7) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.243140673s)
Jun 28 19:18:42.564: INFO: (7) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.431648107s)
Jun 28 19:18:42.843: INFO: (7) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.710038481s)
Jun 28 19:18:42.894: INFO: (7) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.76155931s)
Jun 28 19:18:43.170: INFO: (7) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 2.036827697s)
Jun 28 19:18:43.170: INFO: (7) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 2.037046739s)
Jun 28 19:18:43.170: INFO: (7) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 2.03668353s)
Jun 28 19:18:43.170: INFO: (7) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 2.036968757s)
Jun 28 19:18:43.170: INFO: (7) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 2.036837746s)
Jun 28 19:18:43.221: INFO: (7) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 2.087922106s)
Jun 28 19:18:43.451: INFO: (7) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 2.317910573s)
Jun 28 19:18:43.451: INFO: (7) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 2.317978656s)
Jun 28 19:18:43.451: INFO: (7) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 2.317876519s)
Jun 28 19:18:43.492: INFO: (7) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 2.358824963s)
Jun 28 19:18:44.170: INFO: (8) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 678.154573ms)
Jun 28 19:18:44.170: INFO: (8) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 678.028107ms)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.410038889s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.41037973s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.409994879s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.410234275s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.410048192s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.410204723s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.410246304s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.410087431s)
Jun 28 19:18:44.902: INFO: (8) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 1.410040682s)
Jun 28 19:18:44.903: INFO: (8) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 1.410363735s)
Jun 28 19:18:44.903: INFO: (8) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.410397769s)
Jun 28 19:18:44.903: INFO: (8) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.410208539s)
Jun 28 19:18:44.903: INFO: (8) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.410154896s)
Jun 28 19:18:45.177: INFO: (8) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.684792786s)
Jun 28 19:18:45.914: INFO: (9) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 736.858603ms)
Jun 28 19:18:46.159: INFO: (9) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 981.973522ms)
Jun 28 19:18:46.159: INFO: (9) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 982.267095ms)
Jun 28 19:18:46.159: INFO: (9) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 982.097716ms)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 1.254560518s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.254551173s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 1.254715071s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.254652391s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.254678528s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.254658604s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.254792485s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.254630037s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.254774413s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.254620943s)
Jun 28 19:18:46.432: INFO: (9) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 1.254739582s)
Jun 28 19:18:46.489: INFO: (9) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.311826208s)
Jun 28 19:18:46.943: INFO: (10) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 453.805409ms)
Jun 28 19:18:47.058: INFO: (10) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 568.966156ms)
Jun 28 19:18:47.058: INFO: (10) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 568.85947ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 598.279897ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 598.323431ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 598.225418ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 598.319892ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 598.260129ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 598.4725ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 598.25206ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 598.438335ms)
Jun 28 19:18:47.088: INFO: (10) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 598.372041ms)
Jun 28 19:18:47.186: INFO: (10) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 696.531882ms)
Jun 28 19:18:47.357: INFO: (10) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 867.968472ms)
Jun 28 19:18:47.406: INFO: (10) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 917.132852ms)
Jun 28 19:18:47.452: INFO: (10) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 962.68088ms)
Jun 28 19:18:47.910: INFO: (11) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 457.796139ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 610.399337ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 610.104037ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 610.042683ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 610.179089ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 609.806307ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 610.000954ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 609.494595ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 609.962633ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 610.116352ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 609.765961ms)
Jun 28 19:18:48.062: INFO: (11) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 609.896073ms)
Jun 28 19:18:48.289: INFO: (11) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 836.628062ms)
Jun 28 19:18:48.456: INFO: (11) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.003374026s)
Jun 28 19:18:48.456: INFO: (11) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.00307471s)
Jun 28 19:18:48.456: INFO: (11) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.002945895s)
Jun 28 19:18:49.041: INFO: (12) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 584.322927ms)
Jun 28 19:18:49.041: INFO: (12) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 584.488449ms)
Jun 28 19:18:49.041: INFO: (12) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 584.355094ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 795.664129ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 795.517788ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 795.703521ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 795.609537ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 795.618464ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 795.58383ms)
Jun 28 19:18:49.252: INFO: (12) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 795.664351ms)
Jun 28 19:18:49.473: INFO: (12) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.016731018s)
Jun 28 19:18:49.473: INFO: (12) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.016716111s)
Jun 28 19:18:49.473: INFO: (12) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.016862569s)
Jun 28 19:18:49.643: INFO: (12) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.18653621s)
Jun 28 19:18:49.643: INFO: (12) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.186707435s)
Jun 28 19:18:49.643: INFO: (12) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.186796934s)
Jun 28 19:18:50.174: INFO: (13) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 530.567027ms)
Jun 28 19:18:50.174: INFO: (13) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 530.622413ms)
Jun 28 19:18:50.174: INFO: (13) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 530.828701ms)
Jun 28 19:18:50.309: INFO: (13) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 665.264587ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 738.58615ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 738.881186ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 738.734758ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 738.81859ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 738.582642ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 738.564737ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 738.896127ms)
Jun 28 19:18:50.382: INFO: (13) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 738.670089ms)
Jun 28 19:18:50.537: INFO: (13) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 893.463555ms)
Jun 28 19:18:50.646: INFO: (13) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.002763499s)
Jun 28 19:18:50.688: INFO: (13) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.044257822s)
Jun 28 19:18:50.688: INFO: (13) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.04443023s)
Jun 28 19:18:51.101: INFO: (14) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 412.921519ms)
Jun 28 19:18:51.101: INFO: (14) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 412.746462ms)
Jun 28 19:18:51.101: INFO: (14) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 412.78855ms)
Jun 28 19:18:51.140: INFO: (14) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 451.800041ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 451.726745ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 452.037031ms)
Jun 28 19:18:51.140: INFO: (14) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 451.942197ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 451.661251ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 451.753519ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 452.209454ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 452.096917ms)
Jun 28 19:18:51.141: INFO: (14) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 451.912281ms)
Jun 28 19:18:51.397: INFO: (14) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 708.713539ms)
Jun 28 19:18:51.397: INFO: (14) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 708.964893ms)
Jun 28 19:18:51.397: INFO: (14) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 708.80571ms)
Jun 28 19:18:51.474: INFO: (14) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 785.32759ms)
Jun 28 19:18:51.912: INFO: (15) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 437.910477ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 572.211013ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 571.937828ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 572.398166ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 572.173751ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 571.826382ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 571.807679ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 571.789045ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 571.851171ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 571.885287ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 572.382386ms)
Jun 28 19:18:52.047: INFO: (15) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 572.275089ms)
Jun 28 19:18:52.219: INFO: (15) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 744.409881ms)
Jun 28 19:18:52.388: INFO: (15) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 913.41174ms)
Jun 28 19:18:52.388: INFO: (15) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 913.441649ms)
Jun 28 19:18:52.388: INFO: (15) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 913.359644ms)
Jun 28 19:18:53.103: INFO: (16) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 713.788383ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 995.333779ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 995.38474ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 995.281976ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 995.333057ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 995.437229ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 995.171301ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 995.097166ms)
Jun 28 19:18:53.384: INFO: (16) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 995.077566ms)
Jun 28 19:18:53.609: INFO: (16) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.220287809s)
Jun 28 19:18:54.386: INFO: (16) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.997462622s)
Jun 28 19:18:54.386: INFO: (16) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.997547097s)
Jun 28 19:18:54.386: INFO: (16) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.997714678s)
Jun 28 19:18:55.197: INFO: (16) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 2.807846496s)
Jun 28 19:18:55.197: INFO: (16) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 2.807665837s)
Jun 28 19:18:55.197: INFO: (16) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 2.807871526s)
Jun 28 19:18:55.933: INFO: (17) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 736.033127ms)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.022402004s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.022520806s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.022426694s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.022552317s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 1.022423352s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 1.022442096s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.022352916s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.022547028s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.022339138s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 1.02242056s)
Jun 28 19:18:56.219: INFO: (17) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.022497165s)
Jun 28 19:18:56.427: INFO: (17) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.229938959s)
Jun 28 19:18:56.514: INFO: (17) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.317424044s)
Jun 28 19:18:56.514: INFO: (17) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.317402643s)
Jun 28 19:18:56.514: INFO: (17) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.317357435s)
Jun 28 19:18:57.265: INFO: (18) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 750.284433ms)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.099751343s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.100580308s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 1.099826172s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 1.100545862s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 1.0999524s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 1.100406944s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 1.100142234s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 1.100370917s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 1.10055406s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 1.100062953s)
Jun 28 19:18:57.615: INFO: (18) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 1.100313594s)
Jun 28 19:18:57.685: INFO: (18) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.170689285s)
Jun 28 19:18:57.685: INFO: (18) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 1.17013934s)
Jun 28 19:18:58.661: INFO: (18) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 2.14581751s)
Jun 28 19:18:58.661: INFO: (18) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 2.145472543s)
Jun 28 19:18:59.332: INFO: (19) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:160/proxy/: foo (200; 670.657314ms)
Jun 28 19:18:59.332: INFO: (19) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:1080/proxy/rewriteme">... (200; 670.794669ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:162/proxy/: bar (200; 816.509901ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql/proxy/rewriteme">test</a> (200; 816.619524ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/http:proxy-service-5t45c-l5lql:162/proxy/: bar (200; 816.359661ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:160/proxy/: foo (200; 816.079394ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/proxy-service-5t45c-l5lql:1080/proxy/rewriteme">test<... (200; 816.339175ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:460/proxy/: tls baz (200; 816.116596ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname2/proxy/: tls qux (200; 816.192358ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:462/proxy/: tls qux (200; 816.322255ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/: <a href="/api/v1/namespaces/proxy-2914/pods/https:proxy-service-5t45c-l5lql:443/proxy/tlsrewritem... (200; 816.185664ms)
Jun 28 19:18:59.478: INFO: (19) /api/v1/namespaces/proxy-2914/services/https:proxy-service-5t45c:tlsportname1/proxy/: tls baz (200; 816.520737ms)
Jun 28 19:18:59.720: INFO: (19) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname1/proxy/: foo (200; 1.058486158s)
Jun 28 19:18:59.919: INFO: (19) /api/v1/namespaces/proxy-2914/services/http:proxy-service-5t45c:portname2/proxy/: bar (200; 1.257612615s)
Jun 28 19:18:59.931: INFO: (19) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname2/proxy/: bar (200; 1.269463891s)
Jun 28 19:19:00.137: INFO: (19) /api/v1/namespaces/proxy-2914/services/proxy-service-5t45c:portname1/proxy/: foo (200; 1.475431145s)
STEP: deleting ReplicationController proxy-service-5t45c in namespace proxy-2914, will wait for the garbage collector to delete the pods
Jun 28 19:19:01.395: INFO: Deleting ReplicationController proxy-service-5t45c took: 388.57108ms
Jun 28 19:19:01.795: INFO: Terminating ReplicationController proxy-service-5t45c pods took: 400.165378ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:19:13.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2914" for this suite.

• [SLOW TEST:58.798 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":81,"skipped":1279,"failed":0}
SSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:19:16.214: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun 28 19:19:20.563: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun 28 19:19:22.255: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:19:23.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5622" for this suite.

• [SLOW TEST:8.587 seconds]
[sig-node] RuntimeClass
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtimeclass.go:42
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":82,"skipped":1284,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:19:24.802: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun 28 19:19:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:20:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8877" for this suite.

• [SLOW TEST:93.094 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":83,"skipped":1298,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:20:57.896: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:21:00.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:21:02.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760504859, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:21:06.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:21:08.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8847" for this suite.
STEP: Destroying namespace "webhook-8847-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.143 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":84,"skipped":1312,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:21:09.039: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:21:09.449: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 19:21:09.585: INFO: Number of nodes with available pods: 0
Jun 28 19:21:09.585: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:11.501: INFO: Number of nodes with available pods: 0
Jun 28 19:21:11.502: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:12.561: INFO: Number of nodes with available pods: 0
Jun 28 19:21:12.561: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:13.353: INFO: Number of nodes with available pods: 0
Jun 28 19:21:13.353: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:14.324: INFO: Number of nodes with available pods: 0
Jun 28 19:21:14.324: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:16.764: INFO: Number of nodes with available pods: 0
Jun 28 19:21:16.764: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:18.398: INFO: Number of nodes with available pods: 2
Jun 28 19:21:18.398: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 19:21:19.386: INFO: Number of nodes with available pods: 3
Jun 28 19:21:19.386: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 28 19:21:21.090: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:21.090: INFO: Wrong image for pod: daemon-set-9tc2w. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:21.090: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:22.758: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:22.758: INFO: Wrong image for pod: daemon-set-9tc2w. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:22.758: INFO: Pod daemon-set-9tc2w is not available
Jun 28 19:21:22.758: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:24.673: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:24.673: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:24.673: INFO: Pod daemon-set-q4454 is not available
Jun 28 19:21:26.582: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:26.582: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:26.582: INFO: Pod daemon-set-q4454 is not available
Jun 28 19:21:27.628: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:27.628: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:27.628: INFO: Pod daemon-set-q4454 is not available
Jun 28 19:21:28.675: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:28.675: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:31.636: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:31.636: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:32.493: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:32.493: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:33.740: INFO: Wrong image for pod: daemon-set-4cqxj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:33.740: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:35.694: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:35.694: INFO: Pod daemon-set-zc7fv is not available
Jun 28 19:21:36.557: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:36.557: INFO: Pod daemon-set-zc7fv is not available
Jun 28 19:21:37.610: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:37.610: INFO: Pod daemon-set-zc7fv is not available
Jun 28 19:21:38.597: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:39.623: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:40.549: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:41.485: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:41.485: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:42.494: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:42.494: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:43.667: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:43.667: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:45.595: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:45.595: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:46.530: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:46.530: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:47.610: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:47.610: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:48.647: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:48.648: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:49.556: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:49.556: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:50.481: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:50.481: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:51.492: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:51.492: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:52.636: INFO: Wrong image for pod: daemon-set-l689d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 28 19:21:52.636: INFO: Pod daemon-set-l689d is not available
Jun 28 19:21:53.475: INFO: Pod daemon-set-ztlck is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 28 19:21:53.713: INFO: Number of nodes with available pods: 2
Jun 28 19:21:53.713: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 19:21:55.632: INFO: Number of nodes with available pods: 3
Jun 28 19:21:55.632: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-265, will wait for the garbage collector to delete the pods
Jun 28 19:21:58.738: INFO: Deleting DaemonSet.extensions daemon-set took: 1.064017202s
Jun 28 19:21:59.138: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.158952ms
Jun 28 19:22:14.811: INFO: Number of nodes with available pods: 0
Jun 28 19:22:14.811: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 19:22:14.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-265/daemonsets","resourceVersion":"70124"},"items":null}

Jun 28 19:22:15.132: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-265/pods","resourceVersion":"70125"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:22:16.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-265" for this suite.

• [SLOW TEST:68.254 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":85,"skipped":1327,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:22:17.294: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:22:19.984: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Creating first CR 
Jun 28 19:22:23.296: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:23Z]] name:name1 resourceVersion:70213 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f04e884-7c81-4fd8-b707-44ed8d8e81b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 28 19:22:33.642: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:33Z]] name:name2 resourceVersion:70373 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b5e19e1f-0fbd-4c8d-ab2d-8ea86537fc48] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 28 19:22:45.184: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:43Z]] name:name1 resourceVersion:70428 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f04e884-7c81-4fd8-b707-44ed8d8e81b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 28 19:22:55.583: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:55Z]] name:name2 resourceVersion:70495 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b5e19e1f-0fbd-4c8d-ab2d-8ea86537fc48] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 28 19:23:06.007: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:43Z]] name:name1 resourceVersion:70550 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f04e884-7c81-4fd8-b707-44ed8d8e81b9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 28 19:23:16.237: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-28T19:22:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-28T19:22:55Z]] name:name2 resourceVersion:70603 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:b5e19e1f-0fbd-4c8d-ab2d-8ea86537fc48] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:23:27.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5024" for this suite.

• [SLOW TEST:73.939 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":86,"skipped":1329,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:23:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7091
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7091
STEP: creating replication controller externalsvc in namespace services-7091
I0628 19:23:33.148438      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7091, replica count: 2
I0628 19:23:36.398845      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:23:39.398987      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:23:42.399238      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:23:45.399429      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:23:48.399597      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:23:51.399766      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 28 19:23:52.577: INFO: Creating new exec pod
Jun 28 19:23:59.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7091 exec execpod8f85c -- /bin/sh -x -c nslookup nodeport-service.services-7091.svc.cluster.local'
Jun 28 19:24:04.691: INFO: stderr: "+ nslookup nodeport-service.services-7091.svc.cluster.local\n"
Jun 28 19:24:04.691: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-7091.svc.cluster.local\tcanonical name = externalsvc.services-7091.svc.cluster.local.\nName:\texternalsvc.services-7091.svc.cluster.local\nAddress: 172.21.80.161\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7091, will wait for the garbage collector to delete the pods
Jun 28 19:24:05.784: INFO: Deleting ReplicationController externalsvc took: 547.341341ms
Jun 28 19:24:06.384: INFO: Terminating ReplicationController externalsvc pods took: 600.219425ms
Jun 28 19:24:24.591: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:24:25.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7091" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:56.386 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":87,"skipped":1344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:24:27.620: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:24:28.888: INFO: Creating deployment "webserver-deployment"
Jun 28 19:24:29.152: INFO: Waiting for observed generation 1
Jun 28 19:24:29.424: INFO: Waiting for all required pods to come up
Jun 28 19:24:30.349: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 28 19:24:43.503: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 28 19:24:43.907: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 28 19:24:44.316: INFO: Updating deployment webserver-deployment
Jun 28 19:24:44.316: INFO: Waiting for observed generation 2
Jun 28 19:24:46.672: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 28 19:24:46.987: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 28 19:24:47.330: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:24:48.241: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 28 19:24:48.241: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 28 19:24:48.524: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:24:49.097: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 28 19:24:49.097: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 28 19:24:49.547: INFO: Updating deployment webserver-deployment
Jun 28 19:24:49.547: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 28 19:24:49.862: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 28 19:24:50.020: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 19:24:50.304: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2593 /apis/apps/v1/namespaces/deployment-2593/deployments/webserver-deployment 76dc3474-a565-46bb-8e8a-cc3def6c5f2e 71603 3 2021-06-28 19:24:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 19:24:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ab615f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-28 19:24:49 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-28 19:24:49 +0000 UTC,LastTransitionTime:2021-06-28 19:24:28 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 28 19:24:50.460: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2593 /apis/apps/v1/namespaces/deployment-2593/replicasets/webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 71601 3 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 76dc3474-a565-46bb-8e8a-cc3def6c5f2e 0xc00ab619c7 0xc00ab619c8}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76dc3474-a565-46bb-8e8a-cc3def6c5f2e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ab61a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:24:50.461: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 28 19:24:50.461: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2593 /apis/apps/v1/namespaces/deployment-2593/replicasets/webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 71556 3 2021-06-28 19:24:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 76dc3474-a565-46bb-8e8a-cc3def6c5f2e 0xc00ab61aa7 0xc00ab61aa8}] []  [{kube-controller-manager Update apps/v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76dc3474-a565-46bb-8e8a-cc3def6c5f2e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ab61b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 28 19:24:53.901: INFO: Pod "webserver-deployment-795d758f88-4qbzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4qbzk webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-4qbzk 6ce595ec-c7cf-4f50-8ae2-53b21406fb8e 71613 0 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.74.253/32 cni.projectcalico.org/podIPs:172.17.74.253/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.74.253"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.74.253"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc008eb1c97 0xc008eb1c98}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.901: INFO: Pod "webserver-deployment-795d758f88-5c6m6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5c6m6 webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-5c6m6 a681f724-33b8-4c2f-93b5-e494233b5f0d 71610 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc008eb1e97 0xc008eb1e98}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-5kxdh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5kxdh webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-5kxdh 79b5777d-698a-447c-bfd4-f0f9fe63d4af 71461 0 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84067 0xc003a84068}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:,StartTime:2021-06-28 19:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-6nlfl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6nlfl webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-6nlfl 3b0da1c8-a14e-4d98-9396-ee261f114f5a 71580 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84237 0xc003a84238}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-8jtjb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8jtjb webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-8jtjb 1a2e9eeb-9497-466a-a983-04c9cb965b75 71607 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a843a7 0xc003a843a8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-d8wgr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d8wgr webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-d8wgr 3b0f2369-4a3a-4bea-8acf-efcb1bdcd4ed 71533 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84577 0xc003a84578}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-fnmdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fnmdr webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-fnmdr ff121112-09a8-46d1-8b3d-a78c11ed0f54 71579 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a846e7 0xc003a846e8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.902: INFO: Pod "webserver-deployment-795d758f88-jlpsb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jlpsb webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-jlpsb a3510107-8858-4875-96f7-8fa6c73a9025 71563 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84857 0xc003a84858}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.903: INFO: Pod "webserver-deployment-795d758f88-jz97z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jz97z webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-jz97z e8ed8dd5-330d-4be1-bbf4-57a68338410d 71585 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a849c7 0xc003a849c8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.903: INFO: Pod "webserver-deployment-795d758f88-ml52s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ml52s webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-ml52s 69ae3158-f391-415a-892c-e5e4d45fb6ef 71596 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84b37 0xc003a84b38}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.903: INFO: Pod "webserver-deployment-795d758f88-nlshk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nlshk webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-nlshk 35070826-6c2b-45b6-8b61-a0316da7712b 71472 0 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84ca7 0xc003a84ca8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.903: INFO: Pod "webserver-deployment-795d758f88-s8ctl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s8ctl webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-s8ctl 1455666e-0c15-44cb-a274-db86d12ed118 71442 0 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a84e77 0xc003a84e78}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:,StartTime:2021-06-28 19:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.903: INFO: Pod "webserver-deployment-795d758f88-twvlq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-twvlq webserver-deployment-795d758f88- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-795d758f88-twvlq 634d0fdd-19de-4f60-891c-f5487fd00493 71615 0 2021-06-28 19:24:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.65.231/32 cni.projectcalico.org/podIPs:172.17.65.231/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 faf65fee-c1b4-46b0-80ba-0c0b350db940 0xc003a85067 0xc003a85068}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf65fee-c1b4-46b0-80ba-0c0b350db940\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:,StartTime:2021-06-28 19:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-4wcx7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4wcx7 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-4wcx7 aa78f5e4-442d-417e-93ef-94b1dcdc743c 71272 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.74.251/32 cni.projectcalico.org/podIPs:172.17.74.251/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.74.251"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.74.251"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003a85277 0xc003a85278}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:172.17.74.251,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b69626b4c26cccf83704e77d3e561863c62c62ef81a1595398e339168a955f0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-68mds" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-68mds webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-68mds 1f52d49a-ff8d-4aeb-a580-0e2e8543b601 71320 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.113.157/32 cni.projectcalico.org/podIPs:172.17.113.157/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.157"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003a85487 0xc003a85488}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.157,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://5be59385aa63e0338537a66bc0cf94c2d7d938c15c1ac98a99c4bc71451fc29d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-6cjsf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6cjsf webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-6cjsf 965327bd-3730-40e7-8943-043311464596 71257 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.113.180/32 cni.projectcalico.org/podIPs:172.17.113.180/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.180"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.180"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003a85687 0xc003a85688}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.180,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4e9d471087c0e778c67aaf7e1ee21c06452a395806ef3227409f47785173dc60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-6dvkw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6dvkw webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-6dvkw e4326064-9be6-4ee7-b500-3bb7a97e82cf 71597 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003a85887 0xc003a85888}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-dn4xj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dn4xj webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-dn4xj 7c1941c6-77c0-415c-8f37-c73398f5d367 71275 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.74.252/32 cni.projectcalico.org/podIPs:172.17.74.252/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.74.252"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.74.252"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003a85b27 0xc003a85b28}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 19:24:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 19:24:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:172.17.74.252,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0271a301e0e457ed66bc96a111dc2539f212fa8dedc4f7b38ae29b971b7cd5eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.904: INFO: Pod "webserver-deployment-dd94f59b7-dwxqj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dwxqj webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-dwxqj 2347c220-4f28-4fda-90c3-752ee59f155b 71617 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bc157 0xc0039bc158}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.905: INFO: Pod "webserver-deployment-dd94f59b7-fzd67" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fzd67 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-fzd67 58216588-c8d6-42b7-bb3d-bf86726f4e95 71357 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.65.254/32 cni.projectcalico.org/podIPs:172.17.65.254/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.65.254"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.65.254"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bc567 0xc0039bc568}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.65.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:172.17.65.254,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://8c20c835302812bc4ea5398f19c7147de14c3b55d334b1870afc4374ee3558eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.65.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.905: INFO: Pod "webserver-deployment-dd94f59b7-kgpt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kgpt9 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-kgpt9 bdb82495-2ae7-45c7-ae04-5e2e69c36576 71583 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bc9f7 0xc0039bc9f8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.905: INFO: Pod "webserver-deployment-dd94f59b7-mgnmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mgnmg webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-mgnmg 338365d1-7886-4911-848d-8d8682669b12 71526 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bcd87 0xc0039bcd88}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.905: INFO: Pod "webserver-deployment-dd94f59b7-nsfz6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nsfz6 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-nsfz6 ae6caf14-2270-4814-9158-284a1a7f7185 71382 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.65.255/32 cni.projectcalico.org/podIPs:172.17.65.255/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.65.255"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.65.255"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bd027 0xc0039bd028}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.65.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:172.17.65.255,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e6c8c386cbbea235a437057364f778dddff048d62c95bfad84fce5dca9d5d096,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.65.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.905: INFO: Pod "webserver-deployment-dd94f59b7-pq6rk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pq6rk webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-pq6rk b9f04388-55d9-4822-a3b6-6f3e2dad621e 71541 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bd457 0xc0039bd458}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-q2tdc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q2tdc webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-q2tdc c94d8fe8-dfd2-495c-b169-46a64b30e288 71227 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.74.250/32 cni.projectcalico.org/podIPs:172.17.74.250/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.74.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.74.250"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bd647 0xc0039bd648}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:172.17.74.250,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://2ec9d881d50fdab06834fb3c6c5d7d19f85dd522fd50f0d4e5e539401ac2ae42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-rv6zl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rv6zl webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-rv6zl 4dd978fd-4ec1-4be7-9cc3-26ab8348baae 71586 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bd847 0xc0039bd848}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-s5x2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-s5x2p webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-s5x2p 02b69d5a-17c1-4b17-9345-786f6578c594 71546 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bd9f7 0xc0039bd9f8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-szsfx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-szsfx webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-szsfx a8af745f-797a-448f-bf0d-8b1140387d8a 71552 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bdb57 0xc0039bdb58}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-xgj94" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xgj94 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-xgj94 3d9a860b-caa2-4940-a810-6e93db6caeae 71571 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0039bdd27 0xc0039bdd28}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.906: INFO: Pod "webserver-deployment-dd94f59b7-xh9ft" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xh9ft webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-xh9ft ecd1d09e-ed41-4acb-872b-aabe4c70583b 71595 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003850017 0xc003850018}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.30,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.907: INFO: Pod "webserver-deployment-dd94f59b7-ztw2b" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ztw2b webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-ztw2b 953af7a3-bfce-4788-81bd-6d50a49f4d12 71616 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003850217 0xc003850218}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-28 19:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:,StartTime:2021-06-28 19:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.907: INFO: Pod "webserver-deployment-dd94f59b7-zvbjp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zvbjp webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-zvbjp 3c858cce-6a33-4d34-ae9f-3a5f47025e91 71373 0 2021-06-28 19:24:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.65.228/32 cni.projectcalico.org/podIPs:172.17.65.228/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.65.228"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.65.228"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc003850437 0xc003850438}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-28 19:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-28 19:24:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.65.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.31,PodIP:172.17.65.228,StartTime:2021-06-28 19:24:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:24:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d684be138f0ef1198cecb60f92648710cf8b7f89a137232f5891367ec0f7e93b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.65.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 19:24:53.907: INFO: Pod "webserver-deployment-dd94f59b7-zw8l4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zw8l4 webserver-deployment-dd94f59b7- deployment-2593 /api/v1/namespaces/deployment-2593/pods/webserver-deployment-dd94f59b7-zw8l4 2f9b37ec-f752-42da-8e0c-47b33ad3db9b 71545 0 2021-06-28 19:24:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e0a6fe89-8be5-42fa-a9fd-6b905516ea98 0xc0038506c7 0xc0038506c8}] []  [{kube-controller-manager Update v1 2021-06-28 19:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a6fe89-8be5-42fa-a9fd-6b905516ea98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7ftqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7ftqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7ftqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.31,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c42,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-85btb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:24:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2593" for this suite.

• [SLOW TEST:28.484 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":88,"skipped":1366,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:24:56.104: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9215
STEP: creating service affinity-clusterip in namespace services-9215
STEP: creating replication controller affinity-clusterip in namespace services-9215
I0628 19:24:59.171116      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9215, replica count: 3
I0628 19:25:03.021541      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:06.021769      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:09.021943      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:12.022109      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:15.022499      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:18.022675      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:21.022848      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:24.023315      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:27.023501      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:30.023674      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:33.023852      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:36.024521      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:25:39.024713      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:25:39.764: INFO: Creating new exec pod
Jun 28 19:25:50.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9215 exec execpod-affinitylmzwp -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun 28 19:25:54.222: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 28 19:25:54.222: INFO: stdout: ""
Jun 28 19:25:54.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9215 exec execpod-affinitylmzwp -- /bin/sh -x -c nc -zv -t -w 2 172.21.2.3 80'
Jun 28 19:25:56.589: INFO: stderr: "+ nc -zv -t -w 2 172.21.2.3 80\nConnection to 172.21.2.3 80 port [tcp/http] succeeded!\n"
Jun 28 19:25:56.589: INFO: stdout: ""
Jun 28 19:25:56.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-9215 exec execpod-affinitylmzwp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.2.3:80/ ; done'
Jun 28 19:25:58.257: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.2.3:80/\n"
Jun 28 19:25:58.257: INFO: stdout: "\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt\naffinity-clusterip-v5rzt"
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Received response from host: affinity-clusterip-v5rzt
Jun 28 19:25:58.257: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9215, will wait for the garbage collector to delete the pods
Jun 28 19:25:59.756: INFO: Deleting ReplicationController affinity-clusterip took: 504.189706ms
Jun 28 19:26:00.556: INFO: Terminating ReplicationController affinity-clusterip pods took: 800.18188ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:26:23.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9215" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:91.055 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":89,"skipped":1374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:26:27.160: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Jun 28 19:26:27.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-4229 cluster-info'
Jun 28 19:26:28.426: INFO: stderr: ""
Jun 28 19:26:28.426: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:26:28.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4229" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":90,"skipped":1421,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:26:29.597: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1219
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-1219
Jun 28 19:26:31.312: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 19:26:41.497: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:26:42.507: INFO: Deleting all statefulset in ns statefulset-1219
Jun 28 19:26:42.638: INFO: Scaling statefulset ss to 0
Jun 28 19:26:53.260: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:26:53.477: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:26:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1219" for this suite.

• [SLOW TEST:25.745 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":91,"skipped":1437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:26:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:11.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-129" for this suite.

• [SLOW TEST:18.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":92,"skipped":1509,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:27:15.423: INFO: Creating ReplicaSet my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6
Jun 28 19:27:16.278: INFO: Pod name my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6: Found 1 pods out of 1
Jun 28 19:27:16.278: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6" is running
Jun 28 19:27:23.052: INFO: Pod "my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6-xsj8d" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 19:27:15 +0000 UTC Reason: Message:}])
Jun 28 19:27:23.052: INFO: Trying to dial the pod
Jun 28 19:27:30.145: INFO: Controller my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6: Got expected result from replica 1 [my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6-xsj8d]: "my-hostname-basic-d43e9164-c2a9-43ff-ac7d-7db30a4538a6-xsj8d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:27:30.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1120" for this suite.

• [SLOW TEST:18.270 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":93,"skipped":1515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:27:31.825: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:27:33.771: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cd854b6d-617d-4022-854e-fc606cbd7d7a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cd854b6d-617d-4022-854e-fc606cbd7d7a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:28:43.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3338" for this suite.

• [SLOW TEST:72.511 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1544,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:28:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:28:46.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:28:48.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:28:50.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505325, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:28:54.100: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:28:54.328: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:09.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9923" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:26.993 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":95,"skipped":1545,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:11.330: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 28 19:29:12.162: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:23.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6760" for this suite.

• [SLOW TEST:13.174 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":96,"skipped":1548,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:24.504: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-fea2b1dd-ac28-48d3-aff0-499cf64f5b37
STEP: Creating a pod to test consume secrets
Jun 28 19:29:27.997: INFO: Waiting up to 5m0s for pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006" in namespace "secrets-1483" to be "Succeeded or Failed"
Jun 28 19:29:28.342: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006": Phase="Pending", Reason="", readiness=false. Elapsed: 344.984068ms
Jun 28 19:29:30.737: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740573843s
Jun 28 19:29:33.013: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006": Phase="Pending", Reason="", readiness=false. Elapsed: 5.016556828s
Jun 28 19:29:35.271: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006": Phase="Pending", Reason="", readiness=false. Elapsed: 7.273915064s
Jun 28 19:29:37.523: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.526063627s
STEP: Saw pod success
Jun 28 19:29:37.523: INFO: Pod "pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006" satisfied condition "Succeeded or Failed"
Jun 28 19:29:37.760: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:29:40.103: INFO: Waiting for pod pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006 to disappear
Jun 28 19:29:43.009: INFO: Pod pod-secrets-44e5f722-f1e0-4812-a79d-574938e12006 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:29:43.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1483" for this suite.

• [SLOW TEST:21.622 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:29:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:29:50.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:29:52.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:29:54.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:29:56.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:29:58.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505389, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:30:02.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:30:02.759: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:30:21.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7877" for this suite.
STEP: Destroying namespace "webhook-7877-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:40.228 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":98,"skipped":1585,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:30:26.354: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3468
STEP: creating service affinity-nodeport-transition in namespace services-3468
STEP: creating replication controller affinity-nodeport-transition in namespace services-3468
I0628 19:30:28.815571      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3468, replica count: 3
I0628 19:30:32.215911      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:30:35.216063      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:30:38.216231      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:30:41.216528      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:30:43.703: INFO: Creating new exec pod
Jun 28 19:30:55.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun 28 19:31:02.628: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 28 19:31:02.628: INFO: stdout: ""
Jun 28 19:31:02.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 172.21.43.7 80'
Jun 28 19:31:05.741: INFO: stderr: "+ nc -zv -t -w 2 172.21.43.7 80\nConnection to 172.21.43.7 80 port [tcp/http] succeeded!\n"
Jun 28 19:31:05.741: INFO: stdout: ""
Jun 28 19:31:05.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30123'
Jun 28 19:31:08.989: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30123\nConnection to 10.244.0.29 30123 port [tcp/30123] succeeded!\n"
Jun 28 19:31:08.989: INFO: stdout: ""
Jun 28 19:31:08.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 30123'
Jun 28 19:31:16.282: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 30123\nConnection to 10.244.0.31 30123 port [tcp/30123] succeeded!\n"
Jun 28 19:31:16.282: INFO: stdout: ""
Jun 28 19:31:16.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30123'
Jun 28 19:31:22.469: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30123\nConnection to 10.244.0.29 30123 port [tcp/30123] succeeded!\n"
Jun 28 19:31:22.469: INFO: stdout: ""
Jun 28 19:31:22.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 30123'
Jun 28 19:31:25.326: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 30123\nConnection to 10.244.0.31 30123 port [tcp/30123] succeeded!\n"
Jun 28 19:31:25.326: INFO: stdout: ""
Jun 28 19:31:26.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.244.0.29:30123/ ; done'
Jun 28 19:31:35.638: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n"
Jun 28 19:31:35.638: INFO: stdout: "\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-zmmsm\naffinity-nodeport-transition-z6rpr\naffinity-nodeport-transition-z6rpr"
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-zmmsm
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:35.638: INFO: Received response from host: affinity-nodeport-transition-z6rpr
Jun 28 19:31:38.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-3468 exec execpod-affinity6bz6g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.244.0.29:30123/ ; done'
Jun 28 19:31:42.101: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30123/\n"
Jun 28 19:31:42.101: INFO: stdout: "\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks\naffinity-nodeport-transition-g87ks"
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Received response from host: affinity-nodeport-transition-g87ks
Jun 28 19:31:42.101: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3468, will wait for the garbage collector to delete the pods
Jun 28 19:31:43.999: INFO: Deleting ReplicationController affinity-nodeport-transition took: 679.797254ms
Jun 28 19:31:44.999: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.000191545s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:05.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3468" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:102.009 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":99,"skipped":1592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:08.364: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2395/secret-test-6a1e3d2d-0cca-4548-9c12-66e39256b832
STEP: Creating a pod to test consume secrets
Jun 28 19:32:09.568: INFO: Waiting up to 5m0s for pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b" in namespace "secrets-2395" to be "Succeeded or Failed"
Jun 28 19:32:09.735: INFO: Pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b": Phase="Pending", Reason="", readiness=false. Elapsed: 167.586665ms
Jun 28 19:32:11.879: INFO: Pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310743509s
Jun 28 19:32:14.073: INFO: Pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505352018s
Jun 28 19:32:16.295: INFO: Pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.726960423s
STEP: Saw pod success
Jun 28 19:32:16.295: INFO: Pod "pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b" satisfied condition "Succeeded or Failed"
Jun 28 19:32:16.543: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b container env-test: <nil>
STEP: delete the pod
Jun 28 19:32:20.172: INFO: Waiting for pod pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b to disappear
Jun 28 19:32:20.298: INFO: Pod pod-configmaps-804a1bf3-29a8-452b-b5e2-67e7c136169b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2395" for this suite.

• [SLOW TEST:12.635 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1616,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:20.999: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Jun 28 19:32:21.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 create -f -'
Jun 28 19:32:22.512: INFO: stderr: ""
Jun 28 19:32:22.512: INFO: stdout: "pod/pause created\n"
Jun 28 19:32:22.512: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 28 19:32:22.512: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8821" to be "running and ready"
Jun 28 19:32:22.641: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 128.442955ms
Jun 28 19:32:24.795: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2829152s
Jun 28 19:32:27.169: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.65657058s
Jun 28 19:32:27.169: INFO: Pod "pause" satisfied condition "running and ready"
Jun 28 19:32:27.169: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 28 19:32:27.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 label pods pause testing-label=testing-label-value'
Jun 28 19:32:28.257: INFO: stderr: ""
Jun 28 19:32:28.257: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 28 19:32:28.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 get pod pause -L testing-label'
Jun 28 19:32:29.291: INFO: stderr: ""
Jun 28 19:32:29.291: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 28 19:32:29.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 label pods pause testing-label-'
Jun 28 19:32:30.824: INFO: stderr: ""
Jun 28 19:32:30.824: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 28 19:32:30.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 get pod pause -L testing-label'
Jun 28 19:32:31.056: INFO: stderr: ""
Jun 28 19:32:31.056: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Jun 28 19:32:31.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 delete --grace-period=0 --force -f -'
Jun 28 19:32:32.459: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 19:32:32.459: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 28 19:32:32.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 get rc,svc -l name=pause --no-headers'
Jun 28 19:32:32.606: INFO: stderr: "No resources found in kubectl-8821 namespace.\n"
Jun 28 19:32:32.606: INFO: stdout: ""
Jun 28 19:32:32.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8821 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 19:32:32.738: INFO: stderr: ""
Jun 28 19:32:32.738: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:32:32.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8821" for this suite.

• [SLOW TEST:12.209 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":101,"skipped":1628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:32:33.209: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:32:34.310: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-44fcef75-991b-4d0b-800d-568b7c3c54ad
STEP: Creating configMap with name cm-test-opt-upd-83087e0c-5a77-4b93-9da2-1f5bae030e03
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-44fcef75-991b-4d0b-800d-568b7c3c54ad
STEP: Updating configmap cm-test-opt-upd-83087e0c-5a77-4b93-9da2-1f5bae030e03
STEP: Creating configMap with name cm-test-opt-create-e219c4f8-bd02-4fed-b29b-6a0037e85d56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:05.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5431" for this suite.

• [SLOW TEST:94.045 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1713,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:07.254: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Jun 28 19:34:07.926: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2324 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:34:08.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2324" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":103,"skipped":1714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:34:08.618: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:34:09.414: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-64dacd06-2278-4657-a780-996e9c336852
STEP: Creating secret with name s-test-opt-upd-569ad92a-547c-47cf-b0a0-3352efaaa95d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-64dacd06-2278-4657-a780-996e9c336852
STEP: Updating secret s-test-opt-upd-569ad92a-547c-47cf-b0a0-3352efaaa95d
STEP: Creating secret with name s-test-opt-create-3c8da4c0-7a30-4695-a625-49796e07e5a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:35:48.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7127" for this suite.

• [SLOW TEST:100.784 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1754,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:35:49.402: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Jun 28 19:35:50.877: INFO: Waiting up to 5m0s for pod "var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d" in namespace "var-expansion-8830" to be "Succeeded or Failed"
Jun 28 19:35:51.093: INFO: Pod "var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 216.118519ms
Jun 28 19:35:53.234: INFO: Pod "var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357035492s
Jun 28 19:35:55.384: INFO: Pod "var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.507412539s
STEP: Saw pod success
Jun 28 19:35:55.384: INFO: Pod "var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d" satisfied condition "Succeeded or Failed"
Jun 28 19:35:55.500: INFO: Trying to get logs from node 10.244.0.29 pod var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:35:57.323: INFO: Waiting for pod var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d to disappear
Jun 28 19:35:57.528: INFO: Pod var-expansion-68b10705-f1cc-4687-b5a0-4aa633ef2e5d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:35:57.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8830" for this suite.

• [SLOW TEST:9.190 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":105,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:35:58.593: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:35:59.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4" in namespace "downward-api-9661" to be "Succeeded or Failed"
Jun 28 19:35:59.693: INFO: Pod "downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 100.602131ms
Jun 28 19:36:01.931: INFO: Pod "downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.338684487s
Jun 28 19:36:04.176: INFO: Pod "downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.583507743s
STEP: Saw pod success
Jun 28 19:36:04.176: INFO: Pod "downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4" satisfied condition "Succeeded or Failed"
Jun 28 19:36:04.420: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4 container client-container: <nil>
STEP: delete the pod
Jun 28 19:36:05.224: INFO: Waiting for pod downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4 to disappear
Jun 28 19:36:05.547: INFO: Pod downwardapi-volume-728d0593-6cc2-435b-9df2-54b8e0925ef4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:36:05.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9661" for this suite.

• [SLOW TEST:8.498 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:36:07.092: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:36:08.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650" in namespace "downward-api-3968" to be "Succeeded or Failed"
Jun 28 19:36:08.704: INFO: Pod "downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650": Phase="Pending", Reason="", readiness=false. Elapsed: 223.718399ms
Jun 28 19:36:11.104: INFO: Pod "downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624229265s
Jun 28 19:36:13.415: INFO: Pod "downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.935038224s
STEP: Saw pod success
Jun 28 19:36:13.415: INFO: Pod "downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650" satisfied condition "Succeeded or Failed"
Jun 28 19:36:13.720: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650 container client-container: <nil>
STEP: delete the pod
Jun 28 19:36:16.102: INFO: Waiting for pod downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650 to disappear
Jun 28 19:36:16.578: INFO: Pod downwardapi-volume-f97264ae-2cde-4e42-a157-652a836cb650 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:36:16.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3968" for this suite.

• [SLOW TEST:11.476 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1889,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:36:18.568: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:36:25.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:28.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:30.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:32.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:36:34.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760505783, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:36:37.433: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:36:46.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2266" for this suite.
STEP: Destroying namespace "webhook-2266-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:30.914 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":108,"skipped":1895,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:36:49.483: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 28 19:36:50.846: INFO: Waiting up to 5m0s for pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4" in namespace "emptydir-431" to be "Succeeded or Failed"
Jun 28 19:36:51.010: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4": Phase="Pending", Reason="", readiness=false. Elapsed: 163.737115ms
Jun 28 19:36:53.160: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314470901s
Jun 28 19:36:55.353: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507722095s
Jun 28 19:36:57.516: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.669828436s
Jun 28 19:36:59.829: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.983349028s
STEP: Saw pod success
Jun 28 19:36:59.829: INFO: Pod "pod-b2a0e101-fd0d-45de-8769-8e897d5054f4" satisfied condition "Succeeded or Failed"
Jun 28 19:37:00.099: INFO: Trying to get logs from node 10.244.0.29 pod pod-b2a0e101-fd0d-45de-8769-8e897d5054f4 container test-container: <nil>
STEP: delete the pod
Jun 28 19:37:03.290: INFO: Waiting for pod pod-b2a0e101-fd0d-45de-8769-8e897d5054f4 to disappear
Jun 28 19:37:03.637: INFO: Pod pod-b2a0e101-fd0d-45de-8769-8e897d5054f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:37:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-431" for this suite.

• [SLOW TEST:15.903 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1907,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:37:05.386: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-5fd6bda7-971e-4e9a-9781-45ae870c5d97
STEP: Creating a pod to test consume secrets
Jun 28 19:37:06.504: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af" in namespace "projected-6603" to be "Succeeded or Failed"
Jun 28 19:37:06.685: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Pending", Reason="", readiness=false. Elapsed: 181.607605ms
Jun 28 19:37:08.747: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243731464s
Jun 28 19:37:10.857: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352922698s
Jun 28 19:37:13.069: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.565512388s
Jun 28 19:37:15.305: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Pending", Reason="", readiness=false. Elapsed: 8.800956778s
Jun 28 19:37:17.680: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.176540421s
STEP: Saw pod success
Jun 28 19:37:17.680: INFO: Pod "pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af" satisfied condition "Succeeded or Failed"
Jun 28 19:37:18.031: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 19:37:19.078: INFO: Waiting for pod pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af to disappear
Jun 28 19:37:19.408: INFO: Pod pod-projected-secrets-0ef80c90-3018-453a-a7f9-bf0a782c32af no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:37:19.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6603" for this suite.

• [SLOW TEST:15.923 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1921,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:37:21.309: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-ww7l
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 19:37:23.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ww7l" in namespace "subpath-3022" to be "Succeeded or Failed"
Jun 28 19:37:23.675: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Pending", Reason="", readiness=false. Elapsed: 338.39808ms
Jun 28 19:37:26.116: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.77935891s
Jun 28 19:37:28.514: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177472857s
Jun 28 19:37:30.869: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532557826s
Jun 28 19:37:33.285: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 9.948754024s
Jun 28 19:37:35.501: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 12.16468191s
Jun 28 19:37:37.883: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 14.546651043s
Jun 28 19:37:40.064: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 16.727726578s
Jun 28 19:37:42.145: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 18.808963173s
Jun 28 19:37:44.237: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 20.900055676s
Jun 28 19:37:46.352: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 23.015667388s
Jun 28 19:37:48.500: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 25.163718918s
Jun 28 19:37:50.672: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 27.335772228s
Jun 28 19:37:52.932: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Running", Reason="", readiness=true. Elapsed: 29.595589275s
Jun 28 19:37:55.113: INFO: Pod "pod-subpath-test-secret-ww7l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.77680121s
STEP: Saw pod success
Jun 28 19:37:55.113: INFO: Pod "pod-subpath-test-secret-ww7l" satisfied condition "Succeeded or Failed"
Jun 28 19:37:55.319: INFO: Trying to get logs from node 10.244.0.29 pod pod-subpath-test-secret-ww7l container test-container-subpath-secret-ww7l: <nil>
STEP: delete the pod
Jun 28 19:37:56.738: INFO: Waiting for pod pod-subpath-test-secret-ww7l to disappear
Jun 28 19:37:56.860: INFO: Pod pod-subpath-test-secret-ww7l no longer exists
STEP: Deleting pod pod-subpath-test-secret-ww7l
Jun 28 19:37:56.860: INFO: Deleting pod "pod-subpath-test-secret-ww7l" in namespace "subpath-3022"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:37:56.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3022" for this suite.

• [SLOW TEST:36.114 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":111,"skipped":1939,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:37:57.424: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:38:40.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4879" for this suite.

• [SLOW TEST:43.112 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1957,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:38:40.536: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:38:53.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1404" for this suite.

• [SLOW TEST:13.285 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":113,"skipped":1962,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:38:53.821: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-62656840-1bff-483b-a8c4-5bb38f2c483b
STEP: Creating secret with name secret-projected-all-test-volume-396856c7-4668-4618-bc02-b20333308883
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 28 19:38:55.457: INFO: Waiting up to 5m0s for pod "projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac" in namespace "projected-7354" to be "Succeeded or Failed"
Jun 28 19:38:55.721: INFO: Pod "projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac": Phase="Pending", Reason="", readiness=false. Elapsed: 264.475665ms
Jun 28 19:38:57.947: INFO: Pod "projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489670044s
Jun 28 19:39:00.150: INFO: Pod "projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.69347366s
STEP: Saw pod success
Jun 28 19:39:00.150: INFO: Pod "projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac" satisfied condition "Succeeded or Failed"
Jun 28 19:39:00.316: INFO: Trying to get logs from node 10.244.0.29 pod projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 28 19:39:01.087: INFO: Waiting for pod projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac to disappear
Jun 28 19:39:01.225: INFO: Pod projected-volume-03d0c597-85c5-479e-b309-82c4ff0f75ac no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:01.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7354" for this suite.

• [SLOW TEST:7.989 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:01.811: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2582 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2582;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2582 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2582;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2582.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2582.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2582.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2582.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2582.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.188.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.188.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.188.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.188.219_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2582 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2582;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2582 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2582;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2582.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2582.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2582.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2582.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2582.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.188.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.188.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.188.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.188.219_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:39:11.470: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2582/dns-test-8575d39d-2890-4e5f-a94c-523fc6e7c568: the server could not find the requested resource (get pods dns-test-8575d39d-2890-4e5f-a94c-523fc6e7c568)
Jun 28 19:39:29.588: INFO: Lookups using dns-2582/dns-test-8575d39d-2890-4e5f-a94c-523fc6e7c568 failed for: [wheezy_udp@dns-test-service]

Jun 28 19:39:43.709: INFO: DNS probes using dns-2582/dns-test-8575d39d-2890-4e5f-a94c-523fc6e7c568 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:39:45.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2582" for this suite.

• [SLOW TEST:44.420 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":115,"skipped":2076,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:39:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 28 19:40:01.032: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:01.722: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:03.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:04.375: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:05.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:06.203: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:07.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:08.220: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:09.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:10.344: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:11.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:12.334: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 19:40:13.722: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 19:40:15.885: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:16.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4178" for this suite.

• [SLOW TEST:33.813 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:20.045: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 28 19:40:21.476: INFO: Waiting up to 5m0s for pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9" in namespace "downward-api-3840" to be "Succeeded or Failed"
Jun 28 19:40:21.735: INFO: Pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 258.832991ms
Jun 28 19:40:23.922: INFO: Pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446178849s
Jun 28 19:40:26.208: INFO: Pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732537983s
Jun 28 19:40:28.633: INFO: Pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.157494588s
STEP: Saw pod success
Jun 28 19:40:28.633: INFO: Pod "downward-api-857a18c4-47e5-4947-aece-19b6057f4db9" satisfied condition "Succeeded or Failed"
Jun 28 19:40:29.069: INFO: Trying to get logs from node 10.244.0.29 pod downward-api-857a18c4-47e5-4947-aece-19b6057f4db9 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:40:30.470: INFO: Waiting for pod downward-api-857a18c4-47e5-4947-aece-19b6057f4db9 to disappear
Jun 28 19:40:30.967: INFO: Pod downward-api-857a18c4-47e5-4947-aece-19b6057f4db9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:30.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3840" for this suite.

• [SLOW TEST:15.825 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2138,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:35.870: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 28 19:40:37.766: INFO: Waiting up to 5m0s for pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3" in namespace "downward-api-3987" to be "Succeeded or Failed"
Jun 28 19:40:38.055: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 288.497075ms
Jun 28 19:40:40.287: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521318968s
Jun 28 19:40:42.480: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.713994466s
Jun 28 19:40:44.739: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.973205084s
Jun 28 19:40:47.089: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.323319213s
Jun 28 19:40:49.551: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.784516092s
Jun 28 19:40:51.704: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.93811263s
STEP: Saw pod success
Jun 28 19:40:51.704: INFO: Pod "downward-api-ddd30166-11c0-4c52-b05f-59822884edd3" satisfied condition "Succeeded or Failed"
Jun 28 19:40:51.861: INFO: Trying to get logs from node 10.244.0.29 pod downward-api-ddd30166-11c0-4c52-b05f-59822884edd3 container dapi-container: <nil>
STEP: delete the pod
Jun 28 19:40:52.576: INFO: Waiting for pod downward-api-ddd30166-11c0-4c52-b05f-59822884edd3 to disappear
Jun 28 19:40:52.730: INFO: Pod downward-api-ddd30166-11c0-4c52-b05f-59822884edd3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:40:52.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3987" for this suite.

• [SLOW TEST:18.020 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2149,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:40:53.890: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3845
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3845
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3845
Jun 28 19:40:55.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 19:41:06.066: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 28 19:41:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:41:16.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:41:16.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:41:16.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:41:16.868: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:41:16.868: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:41:17.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999665s
Jun 28 19:41:18.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.92344836s
Jun 28 19:41:19.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.820024143s
Jun 28 19:41:21.130: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.616910675s
Jun 28 19:41:22.389: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.375168226s
Jun 28 19:41:23.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.116158787s
Jun 28 19:41:25.029: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.81158541s
Jun 28 19:41:26.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.475688659s
Jun 28 19:41:27.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 161.333786ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3845
Jun 28 19:41:28.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:41:36.469: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:41:36.469: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:41:36.469: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:41:36.621: INFO: Found 1 stateful pods, waiting for 3
Jun 28 19:41:46.812: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:41:46.812: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:41:46.812: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 19:41:57.180: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:41:57.180: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:41:57.180: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 28 19:41:58.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:42:06.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:42:06.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:42:06.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:42:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:42:11.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:42:11.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:42:11.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:42:11.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 19:42:15.332: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 19:42:15.332: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 19:42:15.332: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 19:42:15.332: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:42:15.589: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 28 19:42:26.532: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:42:26.532: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:42:26.532: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 19:42:27.493: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999705s
Jun 28 19:42:28.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.528738879s
Jun 28 19:42:30.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.183452654s
Jun 28 19:42:31.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.690411613s
Jun 28 19:42:33.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.400730872s
Jun 28 19:42:34.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.995963945s
Jun 28 19:42:35.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.487097597s
Jun 28 19:42:37.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 191.159083ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3845
Jun 28 19:42:38.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:42:41.989: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:42:41.989: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:42:41.989: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:42:41.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:42:46.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 19:42:46.642: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 19:42:46.642: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 19:42:46.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:42:47.919: INFO: rc: 1
Jun 28 19:42:47.919: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 28 19:42:57.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:42:58.325: INFO: rc: 1
Jun 28 19:42:58.325: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:43:08.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:43:08.804: INFO: rc: 1
Jun 28 19:43:08.804: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:43:18.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:43:19.497: INFO: rc: 1
Jun 28 19:43:19.497: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:43:29.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:43:29.758: INFO: rc: 1
Jun 28 19:43:29.758: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:43:39.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:43:40.580: INFO: rc: 1
Jun 28 19:43:40.580: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:43:50.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:43:50.721: INFO: rc: 1
Jun 28 19:43:50.721: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:01.251: INFO: rc: 1
Jun 28 19:44:01.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:11.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:12.375: INFO: rc: 1
Jun 28 19:44:12.375: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:22.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:24.337: INFO: rc: 1
Jun 28 19:44:24.337: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:34.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:35.084: INFO: rc: 1
Jun 28 19:44:35.084: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:45.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:45.235: INFO: rc: 1
Jun 28 19:44:45.235: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:44:55.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:44:56.447: INFO: rc: 1
Jun 28 19:44:56.447: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:45:06.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:45:07.200: INFO: rc: 1
Jun 28 19:45:07.200: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:45:17.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:45:18.000: INFO: rc: 1
Jun 28 19:45:18.000: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:45:28.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:45:28.368: INFO: rc: 1
Jun 28 19:45:28.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:45:38.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:45:39.197: INFO: rc: 1
Jun 28 19:45:39.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:45:49.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:45:51.014: INFO: rc: 1
Jun 28 19:45:51.014: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:01.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:01.183: INFO: rc: 1
Jun 28 19:46:01.183: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:11.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:11.436: INFO: rc: 1
Jun 28 19:46:11.436: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:21.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:22.419: INFO: rc: 1
Jun 28 19:46:22.419: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:32.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:33.300: INFO: rc: 1
Jun 28 19:46:33.300: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:43.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:45.466: INFO: rc: 1
Jun 28 19:46:45.466: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:46:55.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:46:56.541: INFO: rc: 1
Jun 28 19:46:56.541: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:47:06.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:47:06.670: INFO: rc: 1
Jun 28 19:47:06.670: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:47:16.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:47:17.363: INFO: rc: 1
Jun 28 19:47:17.363: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:47:27.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:47:27.528: INFO: rc: 1
Jun 28 19:47:27.528: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:47:37.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:47:37.783: INFO: rc: 1
Jun 28 19:47:37.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 28 19:47:47.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-3845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 19:47:48.507: INFO: rc: 1
Jun 28 19:47:48.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jun 28 19:47:48.507: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:47:48.952: INFO: Deleting all statefulset in ns statefulset-3845
Jun 28 19:47:49.099: INFO: Scaling statefulset ss to 0
Jun 28 19:47:49.516: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:47:49.597: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:47:49.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3845" for this suite.

• [SLOW TEST:416.079 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":119,"skipped":2160,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:47:49.969: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:47:50.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608" in namespace "projected-3" to be "Succeeded or Failed"
Jun 28 19:47:50.398: INFO: Pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608": Phase="Pending", Reason="", readiness=false. Elapsed: 38.695468ms
Jun 28 19:47:52.436: INFO: Pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076935552s
Jun 28 19:47:55.625: INFO: Pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265819038s
Jun 28 19:47:59.062: INFO: Pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.703430665s
STEP: Saw pod success
Jun 28 19:47:59.062: INFO: Pod "downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608" satisfied condition "Succeeded or Failed"
Jun 28 19:47:59.376: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608 container client-container: <nil>
STEP: delete the pod
Jun 28 19:48:01.297: INFO: Waiting for pod downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608 to disappear
Jun 28 19:48:01.556: INFO: Pod downwardapi-volume-b8bdf241-7465-4812-b633-98e91246d608 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:48:01.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3" for this suite.

• [SLOW TEST:13.501 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:48:03.483: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 19:48:07.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:48:09.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 19:48:11.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760506486, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 19:48:15.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:48:16.109: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6124-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:48:22.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2475" for this suite.
STEP: Destroying namespace "webhook-2475-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:21.004 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":121,"skipped":2296,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:48:24.487: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3355.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3355.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 138.54.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.54.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.54.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.54.138_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3355.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3355.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3355.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 138.54.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.54.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.54.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.54.138_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 19:48:34.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-3355.svc.cluster.local from pod dns-3355/dns-test-d86fd352-165f-4257-a18f-70442b2e4711: the server could not find the requested resource (get pods dns-test-d86fd352-165f-4257-a18f-70442b2e4711)
Jun 28 19:48:35.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3355.svc.cluster.local from pod dns-3355/dns-test-d86fd352-165f-4257-a18f-70442b2e4711: the server could not find the requested resource (get pods dns-test-d86fd352-165f-4257-a18f-70442b2e4711)
Jun 28 19:48:35.692: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local from pod dns-3355/dns-test-d86fd352-165f-4257-a18f-70442b2e4711: the server could not find the requested resource (get pods dns-test-d86fd352-165f-4257-a18f-70442b2e4711)
Jun 28 19:48:43.406: INFO: Lookups using dns-3355/dns-test-d86fd352-165f-4257-a18f-70442b2e4711 failed for: [wheezy_udp@dns-test-service.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service.dns-3355.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3355.svc.cluster.local]

Jun 28 19:49:01.314: INFO: DNS probes using dns-3355/dns-test-d86fd352-165f-4257-a18f-70442b2e4711 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:49:02.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3355" for this suite.

• [SLOW TEST:41.355 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":122,"skipped":2300,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:49:05.843: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-7ac85a37-c1d4-445b-b375-bc8a361d95d1 in namespace container-probe-2274
Jun 28 19:49:12.833: INFO: Started pod liveness-7ac85a37-c1d4-445b-b375-bc8a361d95d1 in namespace container-probe-2274
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:49:13.093: INFO: Initial restart count of pod liveness-7ac85a37-c1d4-445b-b375-bc8a361d95d1 is 0
Jun 28 19:49:31.044: INFO: Restart count of pod container-probe-2274/liveness-7ac85a37-c1d4-445b-b375-bc8a361d95d1 is now 1 (17.951017434s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:49:31.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2274" for this suite.

• [SLOW TEST:29.120 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:49:34.963: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1082
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun 28 19:49:37.944: INFO: Found 1 stateful pods, waiting for 3
Jun 28 19:49:48.388: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:49:48.388: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:49:48.388: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 19:49:58.253: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:49:58.253: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:49:58.253: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 28 19:49:59.341: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 28 19:49:59.972: INFO: Updating stateful set ss2
Jun 28 19:50:00.162: INFO: Waiting for Pod statefulset-1082/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 28 19:50:11.764: INFO: Found 2 stateful pods, waiting for 3
Jun 28 19:50:22.068: INFO: Found 2 stateful pods, waiting for 3
Jun 28 19:50:31.966: INFO: Found 2 stateful pods, waiting for 3
Jun 28 19:50:42.268: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:50:42.268: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 19:50:42.268: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 28 19:50:43.611: INFO: Updating stateful set ss2
Jun 28 19:50:45.271: INFO: Waiting for Pod statefulset-1082/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 19:50:56.195: INFO: Updating stateful set ss2
Jun 28 19:50:56.839: INFO: Waiting for StatefulSet statefulset-1082/ss2 to complete update
Jun 28 19:50:56.839: INFO: Waiting for Pod statefulset-1082/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 19:51:07.519: INFO: Waiting for StatefulSet statefulset-1082/ss2 to complete update
Jun 28 19:51:07.519: INFO: Waiting for Pod statefulset-1082/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 28 19:51:17.650: INFO: Waiting for StatefulSet statefulset-1082/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 19:51:27.802: INFO: Deleting all statefulset in ns statefulset-1082
Jun 28 19:51:28.047: INFO: Scaling statefulset ss2 to 0
Jun 28 19:52:20.234: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 19:52:20.478: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:52:21.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1082" for this suite.

• [SLOW TEST:169.955 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":124,"skipped":2344,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:52:24.919: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 19:52:26.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c" in namespace "downward-api-50" to be "Succeeded or Failed"
Jun 28 19:52:26.663: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 250.538735ms
Jun 28 19:52:28.775: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362269864s
Jun 28 19:52:30.883: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.469817205s
Jun 28 19:52:32.974: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.561141318s
Jun 28 19:52:35.157: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.74468097s
STEP: Saw pod success
Jun 28 19:52:35.157: INFO: Pod "downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c" satisfied condition "Succeeded or Failed"
Jun 28 19:52:35.359: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c container client-container: <nil>
STEP: delete the pod
Jun 28 19:52:36.612: INFO: Waiting for pod downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c to disappear
Jun 28 19:52:36.809: INFO: Pod downwardapi-volume-ac2842d0-fbac-438b-bac3-4fd78d6b1c0c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:52:36.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-50" for this suite.

• [SLOW TEST:13.015 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2362,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:52:37.934: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 28 19:52:43.477: INFO: &Pod{ObjectMeta:{send-events-a832c329-6618-47f0-9cee-2ad91fa46e5c  events-6796 /api/v1/namespaces/events-6796/pods/send-events-a832c329-6618-47f0-9cee-2ad91fa46e5c 320acb61-3bc3-48f0-bef7-6024070cb2ad 84755 0 2021-06-28 19:52:38 +0000 UTC <nil> <nil> map[name:foo time:681820020] map[cni.projectcalico.org/podIP:172.17.113.165/32 cni.projectcalico.org/podIPs:172.17.113.165/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.165"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.165"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-28 19:52:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 19:52:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 19:52:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 19:52:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mrkvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mrkvf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mrkvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c47,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-24wwm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:52:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:52:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:52:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 19:52:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.165,StartTime:2021-06-28 19:52:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 19:52:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://c55b027b12b8565541008552ce4394ca4d63db7d14873a697db2dde8d0f63dea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 28 19:52:45.623: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 28 19:52:47.654: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:52:47.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6796" for this suite.

• [SLOW TEST:9.949 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":126,"skipped":2370,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:52:47.884: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:52:57.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2163" for this suite.
STEP: Destroying namespace "nsdeletetest-9523" for this suite.
Jun 28 19:52:57.668: INFO: Namespace nsdeletetest-9523 was already deleted
STEP: Destroying namespace "nsdeletetest-6427" for this suite.

• [SLOW TEST:9.885 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":127,"skipped":2387,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:52:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 28 19:53:10.351: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:53:11.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2212" for this suite.

• [SLOW TEST:14.547 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":128,"skipped":2396,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:53:12.316: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4004
STEP: creating service affinity-nodeport in namespace services-4004
STEP: creating replication controller affinity-nodeport in namespace services-4004
I0628 19:53:13.953200      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4004, replica count: 3
I0628 19:53:17.253570      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:53:20.253740      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 19:53:23.253901      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 19:53:24.499: INFO: Creating new exec pod
Jun 28 19:53:33.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun 28 19:53:42.566: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 28 19:53:42.566: INFO: stdout: ""
Jun 28 19:53:42.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 172.21.148.77 80'
Jun 28 19:53:46.631: INFO: stderr: "+ nc -zv -t -w 2 172.21.148.77 80\nConnection to 172.21.148.77 80 port [tcp/http] succeeded!\n"
Jun 28 19:53:46.631: INFO: stdout: ""
Jun 28 19:53:46.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 32021'
Jun 28 19:53:50.038: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 32021\nConnection to 10.244.0.31 32021 port [tcp/32021] succeeded!\n"
Jun 28 19:53:50.038: INFO: stdout: ""
Jun 28 19:53:50.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 32021'
Jun 28 19:53:55.743: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 32021\nConnection to 10.244.0.29 32021 port [tcp/32021] succeeded!\n"
Jun 28 19:53:55.743: INFO: stdout: ""
Jun 28 19:53:55.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 32021'
Jun 28 19:53:58.381: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 32021\nConnection to 10.244.0.31 32021 port [tcp/32021] succeeded!\n"
Jun 28 19:53:58.381: INFO: stdout: ""
Jun 28 19:53:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 32021'
Jun 28 19:54:02.708: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 32021\nConnection to 10.244.0.29 32021 port [tcp/32021] succeeded!\n"
Jun 28 19:54:02.708: INFO: stdout: ""
Jun 28 19:54:02.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-4004 exec execpod-affinityjq5r8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.244.0.29:32021/ ; done'
Jun 28 19:54:04.744: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:32021/\n"
Jun 28 19:54:04.744: INFO: stdout: "\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw\naffinity-nodeport-zb9dw"
Jun 28 19:54:04.744: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.744: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.744: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.744: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.744: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Received response from host: affinity-nodeport-zb9dw
Jun 28 19:54:04.745: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4004, will wait for the garbage collector to delete the pods
Jun 28 19:54:05.895: INFO: Deleting ReplicationController affinity-nodeport took: 438.624544ms
Jun 28 19:54:06.196: INFO: Terminating ReplicationController affinity-nodeport pods took: 300.987198ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:54:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4004" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:71.966 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":129,"skipped":2412,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:54:24.282: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 28 19:54:26.004: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:54:27.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2826" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":130,"skipped":2417,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:54:28.731: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:54:30.096: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 19:54:32.170: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 19:54:34.188: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:36.197: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:38.227: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:40.328: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:42.375: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:44.276: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:46.448: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:48.341: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:50.386: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:52.394: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:54.433: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = false)
Jun 28 19:54:56.339: INFO: The status of Pod test-webserver-7c96373a-8f5a-4ead-8263-d87f8466cc96 is Running (Ready = true)
Jun 28 19:54:56.554: INFO: Container started at 2021-06-28 19:54:31 +0000 UTC, pod became ready at 2021-06-28 19:54:54 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:54:56.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9406" for this suite.

• [SLOW TEST:28.995 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2431,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:54:57.727: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:55:00.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3344" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":132,"skipped":2435,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:55:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-396cec33-c055-4ef0-8be0-70333110bfd0
STEP: Creating a pod to test consume configMaps
Jun 28 19:55:03.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032" in namespace "projected-2329" to be "Succeeded or Failed"
Jun 28 19:55:03.949: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032": Phase="Pending", Reason="", readiness=false. Elapsed: 214.035747ms
Jun 28 19:55:06.190: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455056507s
Jun 28 19:55:08.367: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032": Phase="Pending", Reason="", readiness=false. Elapsed: 4.632701238s
Jun 28 19:55:10.566: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032": Phase="Pending", Reason="", readiness=false. Elapsed: 6.831928545s
Jun 28 19:55:12.780: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.045833971s
STEP: Saw pod success
Jun 28 19:55:12.781: INFO: Pod "pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032" satisfied condition "Succeeded or Failed"
Jun 28 19:55:12.979: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 19:55:14.706: INFO: Waiting for pod pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032 to disappear
Jun 28 19:55:14.849: INFO: Pod pod-projected-configmaps-ab313e8c-d886-4487-be2e-1a8ca4159032 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:55:14.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2329" for this suite.

• [SLOW TEST:13.704 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2441,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:55:15.584: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:55:35.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1278" for this suite.

• [SLOW TEST:20.766 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":134,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:55:36.351: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0628 19:55:50.902956      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 19:55:50.902976      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 19:55:50.902980      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 19:55:50.903: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:55:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6456" for this suite.

• [SLOW TEST:15.170 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":135,"skipped":2480,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:55:51.522: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Jun 28 19:55:52.689: INFO: Waiting up to 5m0s for pod "client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287" in namespace "containers-2342" to be "Succeeded or Failed"
Jun 28 19:55:52.883: INFO: Pod "client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287": Phase="Pending", Reason="", readiness=false. Elapsed: 194.503209ms
Jun 28 19:55:55.016: INFO: Pod "client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327025516s
Jun 28 19:55:58.144: INFO: Pod "client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.454731411s
STEP: Saw pod success
Jun 28 19:55:58.144: INFO: Pod "client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287" satisfied condition "Succeeded or Failed"
Jun 28 19:55:58.387: INFO: Trying to get logs from node 10.244.0.29 pod client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 19:55:59.115: INFO: Waiting for pod client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287 to disappear
Jun 28 19:55:59.231: INFO: Pod client-containers-be3b0747-3197-4430-bd2c-c9ada9e22287 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:55:59.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2342" for this suite.

• [SLOW TEST:8.208 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2497,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:55:59.730: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 28 19:56:10.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:10.345: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:13.229: INFO: Exec stderr: ""
Jun 28 19:56:13.229: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:13.229: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:20.179: INFO: Exec stderr: ""
Jun 28 19:56:20.179: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:24.148: INFO: Exec stderr: ""
Jun 28 19:56:24.148: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:24.148: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:25.998: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 28 19:56:25.998: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:25.998: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:29.233: INFO: Exec stderr: ""
Jun 28 19:56:29.233: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:29.234: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:31.678: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 28 19:56:31.678: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:31.678: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:34.581: INFO: Exec stderr: ""
Jun 28 19:56:34.581: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:34.581: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:36.979: INFO: Exec stderr: ""
Jun 28 19:56:36.979: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:36.979: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:40.534: INFO: Exec stderr: ""
Jun 28 19:56:40.534: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9506 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 19:56:40.534: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 19:56:42.590: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:56:42.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9506" for this suite.

• [SLOW TEST:44.044 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2505,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:56:43.775: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 19:56:45.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2062 version'
Jun 28 19:56:45.337: INFO: stderr: ""
Jun 28 19:56:45.337: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0+2817867\", GitCommit:\"2817867655bb7b68215b4e77873a8facf82bee06\", GitTreeState:\"clean\", BuildDate:\"2021-06-02T22:14:22Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 19:56:45.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2062" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":138,"skipped":2510,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 19:56:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-56135c24-a05e-4cd4-9891-bc36077f7cd7 in namespace container-probe-3480
Jun 28 19:56:50.003: INFO: Started pod liveness-56135c24-a05e-4cd4-9891-bc36077f7cd7 in namespace container-probe-3480
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 19:56:50.365: INFO: Initial restart count of pod liveness-56135c24-a05e-4cd4-9891-bc36077f7cd7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:00:53.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3480" for this suite.

• [SLOW TEST:250.626 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:00:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 20:01:23.721: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:01:24.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2946" for this suite.

• [SLOW TEST:29.712 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2564,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:01:26.712: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 in namespace container-probe-2134
Jun 28 20:01:37.605: INFO: Started pod liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 in namespace container-probe-2134
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 20:01:37.865: INFO: Initial restart count of pod liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is 0
Jun 28 20:02:02.155: INFO: Restart count of pod container-probe-2134/liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is now 1 (24.290351079s elapsed)
Jun 28 20:02:16.222: INFO: Restart count of pod container-probe-2134/liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is now 2 (38.357538287s elapsed)
Jun 28 20:02:36.438: INFO: Restart count of pod container-probe-2134/liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is now 3 (58.573390938s elapsed)
Jun 28 20:02:56.436: INFO: Restart count of pod container-probe-2134/liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is now 4 (1m18.571253632s elapsed)
Jun 28 20:03:58.765: INFO: Restart count of pod container-probe-2134/liveness-d3a1c684-e4cd-49c6-b840-7c5108f166f4 is now 5 (2m20.899978757s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:03:59.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2134" for this suite.

• [SLOW TEST:154.987 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2569,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:04:01.699: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0628 20:04:46.304857      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 20:04:46.304933      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 20:04:46.304950      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 20:04:46.304: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 28 20:04:46.305: INFO: Deleting pod "simpletest.rc-8xmk9" in namespace "gc-6991"
Jun 28 20:04:46.869: INFO: Deleting pod "simpletest.rc-9qb7g" in namespace "gc-6991"
Jun 28 20:04:47.123: INFO: Deleting pod "simpletest.rc-bqlqv" in namespace "gc-6991"
Jun 28 20:04:47.388: INFO: Deleting pod "simpletest.rc-cfcjh" in namespace "gc-6991"
Jun 28 20:04:47.654: INFO: Deleting pod "simpletest.rc-dd94d" in namespace "gc-6991"
Jun 28 20:04:47.879: INFO: Deleting pod "simpletest.rc-gtxzz" in namespace "gc-6991"
Jun 28 20:04:48.195: INFO: Deleting pod "simpletest.rc-nw8n4" in namespace "gc-6991"
Jun 28 20:04:48.472: INFO: Deleting pod "simpletest.rc-svtpt" in namespace "gc-6991"
Jun 28 20:04:48.785: INFO: Deleting pod "simpletest.rc-vrvqg" in namespace "gc-6991"
Jun 28 20:04:49.061: INFO: Deleting pod "simpletest.rc-vsrmb" in namespace "gc-6991"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:04:49.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6991" for this suite.

• [SLOW TEST:49.297 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":142,"skipped":2586,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:04:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-578
Jun 28 20:04:55.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 28 20:05:03.216: INFO: rc: 7
Jun 28 20:05:03.451: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 20:05:03.622: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 28 20:05:03.622: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-578
STEP: creating replication controller affinity-nodeport-timeout in namespace services-578
I0628 20:05:03.968243      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-578, replica count: 3
I0628 20:05:07.118551      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:05:10.118715      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:05:13.118905      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 20:05:14.175: INFO: Creating new exec pod
Jun 28 20:05:22.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun 28 20:05:28.479: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 28 20:05:28.479: INFO: stdout: ""
Jun 28 20:05:28.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 172.21.180.22 80'
Jun 28 20:05:35.908: INFO: stderr: "+ nc -zv -t -w 2 172.21.180.22 80\nConnection to 172.21.180.22 80 port [tcp/http] succeeded!\n"
Jun 28 20:05:35.908: INFO: stdout: ""
Jun 28 20:05:35.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30955'
Jun 28 20:05:39.264: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30955\nConnection to 10.244.0.29 30955 port [tcp/30955] succeeded!\n"
Jun 28 20:05:39.264: INFO: stdout: ""
Jun 28 20:05:39.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 30955'
Jun 28 20:05:42.688: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 30955\nConnection to 10.244.0.31 30955 port [tcp/30955] succeeded!\n"
Jun 28 20:05:42.688: INFO: stdout: ""
Jun 28 20:05:42.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30955'
Jun 28 20:05:45.921: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30955\nConnection to 10.244.0.29 30955 port [tcp/30955] succeeded!\n"
Jun 28 20:05:45.921: INFO: stdout: ""
Jun 28 20:05:45.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 30955'
Jun 28 20:05:48.573: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 30955\nConnection to 10.244.0.31 30955 port [tcp/30955] succeeded!\n"
Jun 28 20:05:48.573: INFO: stdout: ""
Jun 28 20:05:48.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.244.0.29:30955/ ; done'
Jun 28 20:05:51.455: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n"
Jun 28 20:05:51.455: INFO: stdout: "\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq\naffinity-nodeport-timeout-tn4hq"
Jun 28 20:05:51.455: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Received response from host: affinity-nodeport-timeout-tn4hq
Jun 28 20:05:51.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.244.0.29:30955/'
Jun 28 20:05:52.723: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n"
Jun 28 20:05:52.723: INFO: stdout: "affinity-nodeport-timeout-tn4hq"
Jun 28 20:06:12.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.244.0.29:30955/'
Jun 28 20:06:18.503: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n"
Jun 28 20:06:18.503: INFO: stdout: "affinity-nodeport-timeout-tn4hq"
Jun 28 20:06:38.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-578 exec execpod-affinity49vnh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.244.0.29:30955/'
Jun 28 20:06:46.791: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.244.0.29:30955/\n"
Jun 28 20:06:46.791: INFO: stdout: "affinity-nodeport-timeout-czprj"
Jun 28 20:06:46.791: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-578, will wait for the garbage collector to delete the pods
Jun 28 20:06:47.409: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 182.325025ms
Jun 28 20:06:48.009: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.171464ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:07:03.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-578" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:135.655 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":143,"skipped":2606,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:07:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:07:07.910: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 20:07:14.421: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 20:07:24.856: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8796 /apis/apps/v1/namespaces/deployment-8796/deployments/test-cleanup-deployment 69cded71-5901-4945-96fe-6e57d2733cfd 91260 1 2021-06-28 20:07:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-06-28 20:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 20:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036b83d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-28 20:07:16 +0000 UTC,LastTransitionTime:2021-06-28 20:07:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-06-28 20:07:22 +0000 UTC,LastTransitionTime:2021-06-28 20:07:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 20:07:25.274: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-8796 /apis/apps/v1/namespaces/deployment-8796/replicasets/test-cleanup-deployment-685c4f8568 b34c9427-0668-4933-9371-11e96ab68991 91251 1 2021-06-28 20:07:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 69cded71-5901-4945-96fe-6e57d2733cfd 0xc0036b87b7 0xc0036b87b8}] []  [{kube-controller-manager Update apps/v1 2021-06-28 20:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69cded71-5901-4945-96fe-6e57d2733cfd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036b8848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 20:07:25.697: INFO: Pod "test-cleanup-deployment-685c4f8568-48p6j" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-48p6j test-cleanup-deployment-685c4f8568- deployment-8796 /api/v1/namespaces/deployment-8796/pods/test-cleanup-deployment-685c4f8568-48p6j e87c950a-93be-4748-87ca-e3b9144ced78 91249 0 2021-06-28 20:07:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:172.17.113.176/32 cni.projectcalico.org/podIPs:172.17.113.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.176"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 b34c9427-0668-4933-9371-11e96ab68991 0xc0036b8c07 0xc0036b8c08}] []  [{kube-controller-manager Update v1 2021-06-28 20:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b34c9427-0668-4933-9371-11e96ab68991\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 20:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 20:07:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 20:07:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5z2dm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5z2dm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5z2dm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6rh9t,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:07:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:07:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:07:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.176,StartTime:2021-06-28 20:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 20:07:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://7eecadfa293af1eee0ff2f4e363d63a10f0b455090536095d57b4e973db66848,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:07:25.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8796" for this suite.

• [SLOW TEST:22.295 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":144,"skipped":2624,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:07:28.948: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-070a6635-8686-4e4f-83dd-1245b08cadd0
STEP: Creating a pod to test consume configMaps
Jun 28 20:07:31.717: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241" in namespace "projected-6599" to be "Succeeded or Failed"
Jun 28 20:07:32.010: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241": Phase="Pending", Reason="", readiness=false. Elapsed: 292.85037ms
Jun 28 20:07:34.392: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.674872073s
Jun 28 20:07:36.740: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023049957s
Jun 28 20:07:39.221: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503587111s
Jun 28 20:07:41.431: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.713880042s
STEP: Saw pod success
Jun 28 20:07:41.431: INFO: Pod "pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241" satisfied condition "Succeeded or Failed"
Jun 28 20:07:41.665: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:07:42.810: INFO: Waiting for pod pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241 to disappear
Jun 28 20:07:43.011: INFO: Pod pod-projected-configmaps-d89893d8-94a5-45fc-974c-c6905f4bc241 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:07:43.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6599" for this suite.

• [SLOW TEST:15.040 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2629,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:07:43.989: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-rwnq
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 20:07:45.608: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rwnq" in namespace "subpath-9923" to be "Succeeded or Failed"
Jun 28 20:07:45.874: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Pending", Reason="", readiness=false. Elapsed: 266.428183ms
Jun 28 20:07:48.078: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470021698s
Jun 28 20:07:50.252: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64461944s
Jun 28 20:07:52.356: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.748091618s
Jun 28 20:07:54.388: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 8.780060735s
Jun 28 20:07:56.482: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 10.873936264s
Jun 28 20:07:58.769: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 13.161220563s
Jun 28 20:08:01.176: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 15.56770015s
Jun 28 20:08:03.660: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 18.051665081s
Jun 28 20:08:05.819: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 20.211392827s
Jun 28 20:08:08.266: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 22.657761151s
Jun 28 20:08:12.800: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Running", Reason="", readiness=true. Elapsed: 27.191867631s
Jun 28 20:08:15.029: INFO: Pod "pod-subpath-test-projected-rwnq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.420827506s
STEP: Saw pod success
Jun 28 20:08:15.029: INFO: Pod "pod-subpath-test-projected-rwnq" satisfied condition "Succeeded or Failed"
Jun 28 20:08:15.329: INFO: Trying to get logs from node 10.244.0.29 pod pod-subpath-test-projected-rwnq container test-container-subpath-projected-rwnq: <nil>
STEP: delete the pod
Jun 28 20:08:16.827: INFO: Waiting for pod pod-subpath-test-projected-rwnq to disappear
Jun 28 20:08:16.968: INFO: Pod pod-subpath-test-projected-rwnq no longer exists
STEP: Deleting pod pod-subpath-test-projected-rwnq
Jun 28 20:08:16.968: INFO: Deleting pod "pod-subpath-test-projected-rwnq" in namespace "subpath-9923"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:08:17.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9923" for this suite.

• [SLOW TEST:34.617 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":146,"skipped":2641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:08:18.606: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:08:41.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7690" for this suite.

• [SLOW TEST:25.892 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":147,"skipped":2672,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:08:44.498: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 28 20:08:45.649: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 20:08:48.910: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 20:08:50.484: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.29 before test
Jun 28 20:08:51.291: INFO: calico-node-47lkr from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 20:08:51.291: INFO: calico-typha-664f469f54-58kwx from calico-system started at 2021-06-28 17:02:30 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 20:08:51.291: INFO: ibm-keepalived-watcher-s6s5t from kube-system started at 2021-06-28 16:59:22 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 20:08:51.291: INFO: ibm-master-proxy-static-10.244.0.29 from kube-system started at 2021-06-28 16:58:02 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container pause ready: true, restart count 0
Jun 28 20:08:51.291: INFO: ibm-vpc-block-csi-node-mxp5s from kube-system started at 2021-06-28 16:59:22 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 20:08:51.291: INFO: tuned-tfswp from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container tuned ready: true, restart count 0
Jun 28 20:08:51.291: INFO: dns-default-bsrhg from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container dns ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: node-ca-4rccc from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 20:08:51.291: INFO: ingress-canary-wq2tn from openshift-ingress-canary started at 2021-06-28 18:48:12 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 20:08:51.291: INFO: openshift-kube-proxy-rr8vk from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: certified-operators-tzhzk from openshift-marketplace started at 2021-06-28 18:48:35 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 20:08:51.291: INFO: community-operators-7llsv from openshift-marketplace started at 2021-06-28 18:47:59 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 20:08:51.291: INFO: redhat-operators-v8hsx from openshift-marketplace started at 2021-06-28 18:48:22 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 20:08:51.291: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 18:48:12 +0000 UTC (5 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: node-exporter-mk7n4 from openshift-monitoring started at 2021-06-28 17:03:59 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 20:08:51.291: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 18:48:03 +0000 UTC (7 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 20:08:51.291: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 20:08:51.291: INFO: multus-admission-controller-gc4nb from openshift-multus started at 2021-06-28 18:49:12 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 20:08:51.291: INFO: multus-hnlqc from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 20:08:51.291: INFO: network-metrics-daemon-zvdbr from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 20:08:51.291: INFO: network-check-target-b65gf from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 20:08:51.291: INFO: service-ca-7d7647cfdb-s24rp from openshift-service-ca started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 28 20:08:51.291: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:28:51 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 20:08:51.291: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:51.291: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 28 20:08:51.291: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 20:08:51.291: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.30 before test
Jun 28 20:08:52.193: INFO: calico-node-2ql4z from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 20:08:52.193: INFO: calico-typha-664f469f54-j6kl2 from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 20:08:52.193: INFO: ibm-keepalived-watcher-5dmkt from kube-system started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 20:08:52.193: INFO: ibm-master-proxy-static-10.244.0.30 from kube-system started at 2021-06-28 16:57:51 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container pause ready: true, restart count 0
Jun 28 20:08:52.193: INFO: ibm-vpc-block-csi-node-db8gf from kube-system started at 2021-06-28 16:58:42 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 20:08:52.193: INFO: vpn-74cdbd76f7-2pkzl from kube-system started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container vpn ready: true, restart count 0
Jun 28 20:08:52.193: INFO: tuned-ntcvm from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container tuned ready: true, restart count 0
Jun 28 20:08:52.193: INFO: csi-snapshot-controller-fc56779c7-68pwj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container snapshot-controller ready: true, restart count 1
Jun 28 20:08:52.193: INFO: csi-snapshot-webhook-798674875b-z2zlj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container webhook ready: true, restart count 0
Jun 28 20:08:52.193: INFO: downloads-6d9c464cd4-vjjxk from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container download-server ready: true, restart count 0
Jun 28 20:08:52.193: INFO: dns-default-l8wqd from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container dns ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: image-registry-567d5cff74-k92hq from openshift-image-registry started at 2021-06-28 17:07:17 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container registry ready: true, restart count 0
Jun 28 20:08:52.193: INFO: node-ca-dn9xk from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 20:08:52.193: INFO: ingress-canary-d47lg from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 20:08:52.193: INFO: router-default-fdf999c57-m7k7f from openshift-ingress started at 2021-06-28 18:41:24 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container router ready: true, restart count 0
Jun 28 20:08:52.193: INFO: openshift-kube-proxy-jd62h from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: migrator-744d665879-r687n from openshift-kube-storage-version-migrator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container migrator ready: true, restart count 0
Jun 28 20:08:52.193: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: kube-state-metrics-55cbf55cc7-5nhfz from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 20:08:52.193: INFO: node-exporter-2x4b4 from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 20:08:52.193: INFO: openshift-state-metrics-8555845c54-fd7l8 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 20:08:52.193: INFO: prometheus-operator-6b9599d88d-nqprj from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 20:08:52.193: INFO: telemeter-client-666c78ff9f-28pc9 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container reload ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 20:08:52.193: INFO: thanos-querier-8c945b5d4-24xwt from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (5 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 20:08:52.193: INFO: multus-admission-controller-96snf from openshift-multus started at 2021-06-28 17:03:00 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 20:08:52.193: INFO: multus-qp427 from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 20:08:52.193: INFO: network-metrics-daemon-s59ql from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:52.193: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 20:08:52.193: INFO: network-check-target-pbqc5 from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 20:08:52.193: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 28 20:08:52.193: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 20:08:52.193: INFO: tigera-operator-9cb9c95c7-vxwg6 from tigera-operator started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:52.193: INFO: 	Container tigera-operator ready: true, restart count 4
Jun 28 20:08:52.193: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.31 before test
Jun 28 20:08:53.357: INFO: calico-kube-controllers-67cd4fd574-fst6j from calico-system started at 2021-06-28 17:02:55 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 20:08:53.357: INFO: calico-node-swgct from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 20:08:53.357: INFO: calico-typha-664f469f54-xp4ng from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ibm-keepalived-watcher-8r7sd from kube-system started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ibm-master-proxy-static-10.244.0.31 from kube-system started at 2021-06-28 16:57:44 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container pause ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-28 17:03:01 +0000 UTC (4 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ibm-vpc-block-csi-node-dlgsz from kube-system started at 2021-06-28 16:59:10 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 20:08:53.357: INFO: cluster-node-tuning-operator-85b6488455-92rjz from openshift-cluster-node-tuning-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: tuned-5p6rg from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container tuned ready: true, restart count 0
Jun 28 20:08:53.357: INFO: cluster-samples-operator-6979dbb9c5-ljqjf from openshift-cluster-samples-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container cluster-samples-operator ready: true, restart count 1
Jun 28 20:08:53.357: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 20:08:53.357: INFO: cluster-storage-operator-6974bfb5c6-gjrzk from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 28 20:08:53.357: INFO: csi-snapshot-controller-operator-c9886b54b-mbxtf from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: console-operator-946dbb485-549nf from openshift-console-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 20:08:53.357: INFO: console-7d7f484b46-qwgl6 from openshift-console started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container console ready: true, restart count 0
Jun 28 20:08:53.357: INFO: console-7d7f484b46-zql2k from openshift-console started at 2021-06-28 18:43:41 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container console ready: true, restart count 0
Jun 28 20:08:53.357: INFO: downloads-6d9c464cd4-mjc9h from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container download-server ready: true, restart count 0
Jun 28 20:08:53.357: INFO: dns-operator-74db48c654-lbr6w from openshift-dns-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: dns-default-f2xdn from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container dns ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: cluster-image-registry-operator-7c675f968-j72bn from openshift-image-registry started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: node-ca-24vfp from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ingress-canary-n5kgb from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 20:08:53.357: INFO: ingress-operator-6557486749-2zshp from openshift-ingress-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: router-default-fdf999c57-487ml from openshift-ingress started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container router ready: true, restart count 0
Jun 28 20:08:53.357: INFO: openshift-kube-proxy-bhm75 from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: kube-storage-version-migrator-operator-bdddd9479-r779m from openshift-kube-storage-version-migrator-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 20:08:53.357: INFO: marketplace-operator-569986b7f7-25n44 from openshift-marketplace started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container marketplace-operator ready: true, restart count 2
Jun 28 20:08:53.357: INFO: redhat-marketplace-94d2l from openshift-marketplace started at 2021-06-28 17:09:28 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 20:08:53.357: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: cluster-monitoring-operator-6c785d75f6-rzfjr from openshift-monitoring started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 28 20:08:53.357: INFO: grafana-66dff7486b-hhwrz from openshift-monitoring started at 2021-06-28 17:07:24 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container grafana ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: node-exporter-7kpmm from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 20:08:53.357: INFO: prometheus-adapter-864ff8d5d4-4zdg4 from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 20:08:53.357: INFO: prometheus-adapter-864ff8d5d4-zqh4w from openshift-monitoring started at 2021-06-28 17:09:24 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 20:08:53.357: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 17:09:38 +0000 UTC (7 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 20:08:53.357: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 20:08:53.357: INFO: thanos-querier-8c945b5d4-9mqvb from openshift-monitoring started at 2021-06-28 17:07:31 +0000 UTC (5 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 20:08:53.357: INFO: multus-2vt7k from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 20:08:53.357: INFO: multus-admission-controller-txqk8 from openshift-multus started at 2021-06-28 17:02:50 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 20:08:53.357: INFO: network-metrics-daemon-wvzj5 from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 20:08:53.357: INFO: network-check-source-b77d8dcf6-p7rbb from openshift-network-diagnostics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 28 20:08:53.357: INFO: network-check-target-27f5h from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 20:08:53.357: INFO: network-operator-585847d64b-4ll8c from openshift-network-operator started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: catalog-operator-5d5d46b6dd-qxtpf from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: olm-operator-87677979d-zh4wc from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 20:08:53.357: INFO: packageserver-dfb6789cb-4gvbs from openshift-operator-lifecycle-manager started at 2021-06-28 17:07:50 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 20:08:53.357: INFO: packageserver-dfb6789cb-pbg2h from openshift-operator-lifecycle-manager started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 20:08:53.357: INFO: metrics-854698b86f-nw92k from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container metrics ready: true, restart count 1
Jun 28 20:08:53.357: INFO: push-gateway-6cbcf758df-cf46c from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 20:08:53.357: INFO: service-ca-operator-bf8bb76b5-sbsbp from openshift-service-ca-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 20:08:53.357: INFO: sonobuoy-e2e-job-c80b85415c994408 from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container e2e ready: true, restart count 0
Jun 28 20:08:53.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 20:08:53.357: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 20:08:53.357: INFO: 	Container sonobuoy-worker ready: false, restart count 12
Jun 28 20:08:53.357: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 10.244.0.29
STEP: verifying the node has the label node 10.244.0.30
STEP: verifying the node has the label node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod calico-kube-controllers-67cd4fd574-fst6j requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod calico-node-2ql4z requesting resource cpu=250m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod calico-node-47lkr requesting resource cpu=250m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod calico-node-swgct requesting resource cpu=250m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod calico-typha-664f469f54-58kwx requesting resource cpu=250m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod calico-typha-664f469f54-j6kl2 requesting resource cpu=250m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod calico-typha-664f469f54-xp4ng requesting resource cpu=250m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ibm-keepalived-watcher-5dmkt requesting resource cpu=5m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod ibm-keepalived-watcher-8r7sd requesting resource cpu=5m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ibm-keepalived-watcher-s6s5t requesting resource cpu=5m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod ibm-master-proxy-static-10.244.0.29 requesting resource cpu=25m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod ibm-master-proxy-static-10.244.0.30 requesting resource cpu=25m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod ibm-master-proxy-static-10.244.0.31 requesting resource cpu=25m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ibm-vpc-block-csi-controller-0 requesting resource cpu=75m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ibm-vpc-block-csi-node-db8gf requesting resource cpu=35m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod ibm-vpc-block-csi-node-dlgsz requesting resource cpu=35m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ibm-vpc-block-csi-node-mxp5s requesting resource cpu=35m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod vpn-74cdbd76f7-2pkzl requesting resource cpu=5m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod cluster-node-tuning-operator-85b6488455-92rjz requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod tuned-5p6rg requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod tuned-ntcvm requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod tuned-tfswp requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod cluster-samples-operator-6979dbb9c5-ljqjf requesting resource cpu=20m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod cluster-storage-operator-6974bfb5c6-gjrzk requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod csi-snapshot-controller-fc56779c7-68pwj requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod csi-snapshot-controller-operator-c9886b54b-mbxtf requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod csi-snapshot-webhook-798674875b-z2zlj requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod console-operator-946dbb485-549nf requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod console-7d7f484b46-qwgl6 requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod console-7d7f484b46-zql2k requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod downloads-6d9c464cd4-mjc9h requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod downloads-6d9c464cd4-vjjxk requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod dns-operator-74db48c654-lbr6w requesting resource cpu=20m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod dns-default-bsrhg requesting resource cpu=65m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod dns-default-f2xdn requesting resource cpu=65m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod dns-default-l8wqd requesting resource cpu=65m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod cluster-image-registry-operator-7c675f968-j72bn requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod image-registry-567d5cff74-k92hq requesting resource cpu=100m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod node-ca-24vfp requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod node-ca-4rccc requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod node-ca-dn9xk requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod ingress-canary-d47lg requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod ingress-canary-n5kgb requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod ingress-canary-wq2tn requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod ingress-operator-6557486749-2zshp requesting resource cpu=20m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod router-default-fdf999c57-487ml requesting resource cpu=100m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod router-default-fdf999c57-m7k7f requesting resource cpu=100m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod openshift-kube-proxy-bhm75 requesting resource cpu=110m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod openshift-kube-proxy-jd62h requesting resource cpu=110m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod openshift-kube-proxy-rr8vk requesting resource cpu=110m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod kube-storage-version-migrator-operator-bdddd9479-r779m requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod migrator-744d665879-r687n requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod certified-operators-tzhzk requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod community-operators-7llsv requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod marketplace-operator-569986b7f7-25n44 requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod redhat-marketplace-94d2l requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod redhat-operators-v8hsx requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod cluster-monitoring-operator-6c785d75f6-rzfjr requesting resource cpu=11m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod grafana-66dff7486b-hhwrz requesting resource cpu=5m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod kube-state-metrics-55cbf55cc7-5nhfz requesting resource cpu=4m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod node-exporter-2x4b4 requesting resource cpu=9m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod node-exporter-7kpmm requesting resource cpu=9m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod node-exporter-mk7n4 requesting resource cpu=9m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod openshift-state-metrics-8555845c54-fd7l8 requesting resource cpu=3m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod prometheus-adapter-864ff8d5d4-4zdg4 requesting resource cpu=1m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod prometheus-adapter-864ff8d5d4-zqh4w requesting resource cpu=1m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod prometheus-operator-6b9599d88d-nqprj requesting resource cpu=6m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod telemeter-client-666c78ff9f-28pc9 requesting resource cpu=3m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod thanos-querier-8c945b5d4-24xwt requesting resource cpu=9m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod thanos-querier-8c945b5d4-9mqvb requesting resource cpu=9m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod multus-2vt7k requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod multus-admission-controller-96snf requesting resource cpu=20m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod multus-admission-controller-gc4nb requesting resource cpu=20m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod multus-admission-controller-txqk8 requesting resource cpu=20m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod multus-hnlqc requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod multus-qp427 requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod network-metrics-daemon-s59ql requesting resource cpu=20m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod network-metrics-daemon-wvzj5 requesting resource cpu=20m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod network-metrics-daemon-zvdbr requesting resource cpu=20m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod network-check-source-b77d8dcf6-p7rbb requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod network-check-target-27f5h requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod network-check-target-b65gf requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod network-check-target-pbqc5 requesting resource cpu=10m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod network-operator-585847d64b-4ll8c requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod catalog-operator-5d5d46b6dd-qxtpf requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod olm-operator-87677979d-zh4wc requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod packageserver-dfb6789cb-4gvbs requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod packageserver-dfb6789cb-pbg2h requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod metrics-854698b86f-nw92k requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod push-gateway-6cbcf758df-cf46c requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod service-ca-operator-bf8bb76b5-sbsbp requesting resource cpu=10m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod service-ca-7d7647cfdb-s24rp requesting resource cpu=10m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod sonobuoy-e2e-job-c80b85415c994408 requesting resource cpu=0m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq requesting resource cpu=0m on Node 10.244.0.29
Jun 28 20:08:59.255: INFO: Pod sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl requesting resource cpu=0m on Node 10.244.0.30
Jun 28 20:08:59.255: INFO: Pod sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf requesting resource cpu=0m on Node 10.244.0.31
Jun 28 20:08:59.255: INFO: Pod tigera-operator-9cb9c95c7-vxwg6 requesting resource cpu=100m on Node 10.244.0.30
STEP: Starting Pods to consume most of the cluster CPU.
Jun 28 20:08:59.255: INFO: Creating a pod which consumes cpu=1760m on Node 10.244.0.31
Jun 28 20:08:59.770: INFO: Creating a pod which consumes cpu=2062m on Node 10.244.0.29
Jun 28 20:08:59.994: INFO: Creating a pod which consumes cpu=1885m on Node 10.244.0.30
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b.168cd7e3f0f8cec0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3050/filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b to 10.244.0.31]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b.168cd7e47b5e116e], Reason = [AddedInterface], Message = [Add eth0 [172.17.65.255/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b.168cd7e480009247], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b.168cd7e48ab227ca], Reason = [Created], Message = [Created container filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b.168cd7e48c63ecc4], Reason = [Started], Message = [Started container filler-pod-7e6176dc-bccb-4c68-8f1d-6443a340d94b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665.168cd7e40e27bd24], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3050/filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665 to 10.244.0.30]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665.168cd7e549594040], Reason = [AddedInterface], Message = [Add eth0 [172.17.74.223/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665.168cd7e559217e52], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665.168cd7e562aa93cc], Reason = [Created], Message = [Created container filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665.168cd7e5641e1f8f], Reason = [Started], Message = [Started container filler-pod-aa8b7a6a-89fc-4ef6-8c1c-bf842c925665]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3.168cd7e3ffcd28f8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3050/filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3 to 10.244.0.29]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3.168cd7e63f8a3a39], Reason = [AddedInterface], Message = [Add eth0 [172.17.113.179/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3.168cd7e65985cd53], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3.168cd7e661fe075d], Reason = [Created], Message = [Created container filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3.168cd7e66362c59b], Reason = [Started], Message = [Started container filler-pod-c144f3ad-4470-488f-8bdc-43f3ab9a1db3]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-3050.168cd7e077137bef], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168cd7e7cdb8d277], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.244.0.29
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.244.0.30
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.244.0.31
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:09:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3050" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:40.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":148,"skipped":2673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:09:24.732: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 28 20:09:27.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92311 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:09:27.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92311 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 28 20:09:37.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92420 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:09:37.886: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92420 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 28 20:09:48.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92469 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:09:48.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92469 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 28 20:09:58.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92515 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:09:58.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-a d8879075-0650-425c-926f-3b07eddf90cf 92515 0 2021-06-28 20:09:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-28 20:09:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 28 20:10:08.670: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-b f01b9c84-6be4-4e93-aeb7-efbc76fc3640 92559 0 2021-06-28 20:10:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 20:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:10:08.671: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-b f01b9c84-6be4-4e93-aeb7-efbc76fc3640 92559 0 2021-06-28 20:10:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 20:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 28 20:10:19.264: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-b f01b9c84-6be4-4e93-aeb7-efbc76fc3640 92602 0 2021-06-28 20:10:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 20:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:10:19.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3689 /api/v1/namespaces/watch-3689/configmaps/e2e-watch-test-configmap-b f01b9c84-6be4-4e93-aeb7-efbc76fc3640 92602 0 2021-06-28 20:10:08 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-28 20:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:10:29.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3689" for this suite.

• [SLOW TEST:65.397 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":149,"skipped":2708,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:10:30.129: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:10:31.820: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-14383ccd-11f6-492a-86b6-2a5ffd80af8b
STEP: Creating secret with name s-test-opt-upd-b2cf14dc-f385-4357-a882-eec63d462235
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-14383ccd-11f6-492a-86b6-2a5ffd80af8b
STEP: Updating secret s-test-opt-upd-b2cf14dc-f385-4357-a882-eec63d462235
STEP: Creating secret with name s-test-opt-create-7bb6e87e-fb71-4303-9ac4-4166183f9e26
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:12:05.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3276" for this suite.

• [SLOW TEST:97.701 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2714,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:12:07.830: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun 28 20:12:09.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 create -f -'
Jun 28 20:12:12.047: INFO: stderr: ""
Jun 28 20:12:12.047: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 20:12:12.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:12:12.360: INFO: stderr: ""
Jun 28 20:12:12.360: INFO: stdout: "update-demo-nautilus-njngf update-demo-nautilus-z4cdp "
Jun 28 20:12:12.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-njngf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:12:12.544: INFO: stderr: ""
Jun 28 20:12:12.544: INFO: stdout: ""
Jun 28 20:12:12.544: INFO: update-demo-nautilus-njngf is created but not running
Jun 28 20:12:17.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:12:18.678: INFO: stderr: ""
Jun 28 20:12:18.678: INFO: stdout: "update-demo-nautilus-njngf update-demo-nautilus-z4cdp "
Jun 28 20:12:18.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-njngf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:12:19.209: INFO: stderr: ""
Jun 28 20:12:19.209: INFO: stdout: ""
Jun 28 20:12:19.209: INFO: update-demo-nautilus-njngf is created but not running
Jun 28 20:12:24.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:12:24.792: INFO: stderr: ""
Jun 28 20:12:24.792: INFO: stdout: "update-demo-nautilus-njngf update-demo-nautilus-z4cdp "
Jun 28 20:12:24.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-njngf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:12:26.860: INFO: stderr: ""
Jun 28 20:12:26.860: INFO: stdout: "true"
Jun 28 20:12:26.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-njngf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:12:27.761: INFO: stderr: ""
Jun 28 20:12:27.761: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:12:27.761: INFO: validating pod update-demo-nautilus-njngf
Jun 28 20:12:28.881: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:12:28.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:12:28.881: INFO: update-demo-nautilus-njngf is verified up and running
Jun 28 20:12:28.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-z4cdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:12:28.995: INFO: stderr: ""
Jun 28 20:12:28.995: INFO: stdout: "true"
Jun 28 20:12:28.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods update-demo-nautilus-z4cdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:12:29.544: INFO: stderr: ""
Jun 28 20:12:29.544: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:12:29.544: INFO: validating pod update-demo-nautilus-z4cdp
Jun 28 20:12:30.329: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:12:30.329: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:12:30.329: INFO: update-demo-nautilus-z4cdp is verified up and running
STEP: using delete to clean up resources
Jun 28 20:12:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 delete --grace-period=0 --force -f -'
Jun 28 20:12:31.151: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 20:12:31.151: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 20:12:31.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get rc,svc -l name=update-demo --no-headers'
Jun 28 20:12:31.308: INFO: stderr: "No resources found in kubectl-2676 namespace.\n"
Jun 28 20:12:31.308: INFO: stdout: ""
Jun 28 20:12:31.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2676 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 20:12:31.417: INFO: stderr: ""
Jun 28 20:12:31.418: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:12:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2676" for this suite.

• [SLOW TEST:24.201 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":151,"skipped":2722,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:12:32.032: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 28 20:12:37.566: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f4d5e144-596c-468f-bc9b-6316b3f8e03d"
Jun 28 20:12:37.566: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f4d5e144-596c-468f-bc9b-6316b3f8e03d" in namespace "pods-1440" to be "terminated due to deadline exceeded"
Jun 28 20:12:37.647: INFO: Pod "pod-update-activedeadlineseconds-f4d5e144-596c-468f-bc9b-6316b3f8e03d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 80.990894ms
Jun 28 20:12:37.647: INFO: Pod "pod-update-activedeadlineseconds-f4d5e144-596c-468f-bc9b-6316b3f8e03d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:12:37.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1440" for this suite.

• [SLOW TEST:6.049 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2725,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:12:38.081: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-zslj
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 20:12:41.930: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zslj" in namespace "subpath-1053" to be "Succeeded or Failed"
Jun 28 20:12:42.355: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Pending", Reason="", readiness=false. Elapsed: 425.109519ms
Jun 28 20:12:44.580: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649821132s
Jun 28 20:12:46.719: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78868794s
Jun 28 20:12:49.093: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Pending", Reason="", readiness=false. Elapsed: 7.163521983s
Jun 28 20:12:51.215: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Pending", Reason="", readiness=false. Elapsed: 9.285460612s
Jun 28 20:12:53.318: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 11.387679925s
Jun 28 20:12:55.697: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 13.767596732s
Jun 28 20:12:58.806: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 16.876539909s
Jun 28 20:13:00.941: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 19.011354036s
Jun 28 20:13:03.171: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 21.241519098s
Jun 28 20:13:05.511: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 23.581329932s
Jun 28 20:13:07.849: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 25.919247973s
Jun 28 20:13:10.023: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Running", Reason="", readiness=true. Elapsed: 28.092758284s
Jun 28 20:13:12.130: INFO: Pod "pod-subpath-test-configmap-zslj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.20027823s
STEP: Saw pod success
Jun 28 20:13:12.130: INFO: Pod "pod-subpath-test-configmap-zslj" satisfied condition "Succeeded or Failed"
Jun 28 20:13:12.259: INFO: Trying to get logs from node 10.244.0.29 pod pod-subpath-test-configmap-zslj container test-container-subpath-configmap-zslj: <nil>
STEP: delete the pod
Jun 28 20:13:13.714: INFO: Waiting for pod pod-subpath-test-configmap-zslj to disappear
Jun 28 20:13:13.817: INFO: Pod pod-subpath-test-configmap-zslj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zslj
Jun 28 20:13:13.817: INFO: Deleting pod "pod-subpath-test-configmap-zslj" in namespace "subpath-1053"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:13:13.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1053" for this suite.

• [SLOW TEST:36.291 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":153,"skipped":2733,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:13:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:13:16.961: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-159ec5ac-66ef-4618-905e-13f246968033
STEP: Creating configMap with name cm-test-opt-upd-28fcac5c-b015-4f76-a3c6-5e4bca280bb5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-159ec5ac-66ef-4618-905e-13f246968033
STEP: Updating configmap cm-test-opt-upd-28fcac5c-b015-4f76-a3c6-5e4bca280bb5
STEP: Creating configMap with name cm-test-opt-create-b33d1d8d-0bca-44fd-9254-78f9e1a91dbc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:14:54.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9858" for this suite.

• [SLOW TEST:101.687 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2738,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:14:56.059: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:14:57.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609" in namespace "downward-api-6076" to be "Succeeded or Failed"
Jun 28 20:14:57.498: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609": Phase="Pending", Reason="", readiness=false. Elapsed: 135.285827ms
Jun 28 20:14:59.753: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390590305s
Jun 28 20:15:01.991: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628440866s
Jun 28 20:15:04.488: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609": Phase="Pending", Reason="", readiness=false. Elapsed: 7.125478424s
Jun 28 20:15:06.765: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.402475508s
STEP: Saw pod success
Jun 28 20:15:06.765: INFO: Pod "downwardapi-volume-53213cd2-622f-4886-8621-c76322861609" satisfied condition "Succeeded or Failed"
Jun 28 20:15:07.113: INFO: Trying to get logs from node 10.244.0.30 pod downwardapi-volume-53213cd2-622f-4886-8621-c76322861609 container client-container: <nil>
STEP: delete the pod
Jun 28 20:15:09.413: INFO: Waiting for pod downwardapi-volume-53213cd2-622f-4886-8621-c76322861609 to disappear
Jun 28 20:15:09.724: INFO: Pod downwardapi-volume-53213cd2-622f-4886-8621-c76322861609 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:15:09.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6076" for this suite.

• [SLOW TEST:14.958 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2757,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:15:11.017: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 20:15:13.215: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 28 20:15:13.444: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 20:15:14.007: INFO: waiting for watch events with expected annotations
Jun 28 20:15:14.007: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:15:15.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7265" for this suite.

• [SLOW TEST:5.645 seconds]
[sig-network] Ingress API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":156,"skipped":2765,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:15:16.662: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 28 20:15:18.950: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-126 /api/v1/namespaces/watch-126/configmaps/e2e-watch-test-watch-closed 2e3880d1-aee5-49c7-a0b2-ef6176b4d1ba 94673 0 2021-06-28 20:15:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 20:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:15:18.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-126 /api/v1/namespaces/watch-126/configmaps/e2e-watch-test-watch-closed 2e3880d1-aee5-49c7-a0b2-ef6176b4d1ba 94675 0 2021-06-28 20:15:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 20:15:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 28 20:15:19.603: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-126 /api/v1/namespaces/watch-126/configmaps/e2e-watch-test-watch-closed 2e3880d1-aee5-49c7-a0b2-ef6176b4d1ba 94679 0 2021-06-28 20:15:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 20:15:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 20:15:19.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-126 /api/v1/namespaces/watch-126/configmaps/e2e-watch-test-watch-closed 2e3880d1-aee5-49c7-a0b2-ef6176b4d1ba 94682 0 2021-06-28 20:15:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-28 20:15:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:15:19.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-126" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":157,"skipped":2766,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:15:20.489: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 28 20:15:21.901: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 20:16:26.798: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:16:27.045: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:16:29.189: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun 28 20:16:29.427: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:16:30.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1176" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:16:31.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6016" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:73.513 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":158,"skipped":2778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:16:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:16:36.607: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cea8156a-ccaa-47ec-846b-87edcabb74bf
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:16:51.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9439" for this suite.

• [SLOW TEST:18.767 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2806,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:16:52.769: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 20:16:58.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:17:00.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:17:02.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508217, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 20:17:05.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:17:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7144" for this suite.
STEP: Destroying namespace "webhook-7144-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.354 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":160,"skipped":2808,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:17:10.124: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6567/configmap-test-3fe7998b-4c9d-43b2-a8e5-e5e4f5468e1a
STEP: Creating a pod to test consume configMaps
Jun 28 20:17:11.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd" in namespace "configmap-6567" to be "Succeeded or Failed"
Jun 28 20:17:11.896: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 196.142112ms
Jun 28 20:17:14.048: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.34783254s
Jun 28 20:17:16.407: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.707773661s
Jun 28 20:17:18.564: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.864791294s
Jun 28 20:17:20.906: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.206569876s
STEP: Saw pod success
Jun 28 20:17:20.906: INFO: Pod "pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd" satisfied condition "Succeeded or Failed"
Jun 28 20:17:21.262: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd container env-test: <nil>
STEP: delete the pod
Jun 28 20:17:22.599: INFO: Waiting for pod pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd to disappear
Jun 28 20:17:23.013: INFO: Pod pod-configmaps-77b547a0-ade1-4c48-9c4b-8ead72f5cfdd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:17:23.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6567" for this suite.

• [SLOW TEST:14.411 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2814,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:17:24.535: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 28 20:17:27.436: INFO: Waiting up to 5m0s for pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a" in namespace "downward-api-7967" to be "Succeeded or Failed"
Jun 28 20:17:27.652: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a": Phase="Pending", Reason="", readiness=false. Elapsed: 216.612491ms
Jun 28 20:17:29.842: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406566223s
Jun 28 20:17:31.907: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471045311s
Jun 28 20:17:34.107: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.67150007s
Jun 28 20:17:36.187: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.751060109s
STEP: Saw pod success
Jun 28 20:17:36.187: INFO: Pod "downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a" satisfied condition "Succeeded or Failed"
Jun 28 20:17:36.248: INFO: Trying to get logs from node 10.244.0.29 pod downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a container dapi-container: <nil>
STEP: delete the pod
Jun 28 20:17:36.791: INFO: Waiting for pod downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a to disappear
Jun 28 20:17:36.870: INFO: Pod downward-api-de070a1a-5306-4b8c-952f-949b9ba3904a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:17:36.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7967" for this suite.

• [SLOW TEST:12.593 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2832,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:17:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 28 20:17:45.225: INFO: Successfully updated pod "labelsupdate6dcf3fe6-e6d3-43e8-81dd-f2a84e4fdaab"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:17:47.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1264" for this suite.

• [SLOW TEST:10.771 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2840,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:17:47.899: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:17:49.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4162" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":164,"skipped":2900,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:17:50.355: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7319
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7319 to expose endpoints map[]
Jun 28 20:17:51.161: INFO: successfully validated that service multi-endpoint-test in namespace services-7319 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7319
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7319 to expose endpoints map[pod1:[100]]
Jun 28 20:17:55.597: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Jun 28 20:18:00.479: INFO: successfully validated that service multi-endpoint-test in namespace services-7319 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7319
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7319 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 28 20:18:08.339: INFO: Unexpected endpoints: found map[c11ab186-29fd-4003-bbc8-12bc6b96e31a:[100]], expected map[pod1:[100] pod2:[101]], will retry
Jun 28 20:18:10.378: INFO: successfully validated that service multi-endpoint-test in namespace services-7319 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7319
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7319 to expose endpoints map[pod2:[101]]
Jun 28 20:18:11.444: INFO: successfully validated that service multi-endpoint-test in namespace services-7319 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7319
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7319 to expose endpoints map[]
Jun 28 20:18:12.248: INFO: successfully validated that service multi-endpoint-test in namespace services-7319 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:18:12.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7319" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.228 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":165,"skipped":2910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:18:13.583: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun 28 20:18:15.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 create -f -'
Jun 28 20:18:19.699: INFO: stderr: ""
Jun 28 20:18:19.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 20:18:19.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:20.791: INFO: stderr: ""
Jun 28 20:18:20.791: INFO: stdout: "update-demo-nautilus-8qnlm update-demo-nautilus-jgjls "
Jun 28 20:18:20.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-8qnlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:21.054: INFO: stderr: ""
Jun 28 20:18:21.054: INFO: stdout: ""
Jun 28 20:18:21.054: INFO: update-demo-nautilus-8qnlm is created but not running
Jun 28 20:18:26.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:26.380: INFO: stderr: ""
Jun 28 20:18:26.380: INFO: stdout: "update-demo-nautilus-8qnlm update-demo-nautilus-jgjls "
Jun 28 20:18:26.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-8qnlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:27.488: INFO: stderr: ""
Jun 28 20:18:27.488: INFO: stdout: "true"
Jun 28 20:18:27.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-8qnlm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:18:28.272: INFO: stderr: ""
Jun 28 20:18:28.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:18:28.272: INFO: validating pod update-demo-nautilus-8qnlm
Jun 28 20:18:30.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:18:30.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:18:30.627: INFO: update-demo-nautilus-8qnlm is verified up and running
Jun 28 20:18:30.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:31.298: INFO: stderr: ""
Jun 28 20:18:31.298: INFO: stdout: "true"
Jun 28 20:18:31.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:18:31.454: INFO: stderr: ""
Jun 28 20:18:31.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:18:31.454: INFO: validating pod update-demo-nautilus-jgjls
Jun 28 20:18:32.493: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:18:32.493: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:18:32.494: INFO: update-demo-nautilus-jgjls is verified up and running
STEP: scaling down the replication controller
Jun 28 20:18:32.496: INFO: scanned /root for discovery docs: <nil>
Jun 28 20:18:32.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 28 20:18:33.748: INFO: stderr: ""
Jun 28 20:18:33.748: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 20:18:33.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:34.371: INFO: stderr: ""
Jun 28 20:18:34.371: INFO: stdout: "update-demo-nautilus-8qnlm update-demo-nautilus-jgjls "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 28 20:18:39.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:39.523: INFO: stderr: ""
Jun 28 20:18:39.523: INFO: stdout: "update-demo-nautilus-8qnlm update-demo-nautilus-jgjls "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 28 20:18:44.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:44.898: INFO: stderr: ""
Jun 28 20:18:44.898: INFO: stdout: "update-demo-nautilus-jgjls "
Jun 28 20:18:44.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:45.766: INFO: stderr: ""
Jun 28 20:18:45.766: INFO: stdout: "true"
Jun 28 20:18:45.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:18:45.878: INFO: stderr: ""
Jun 28 20:18:45.878: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:18:45.878: INFO: validating pod update-demo-nautilus-jgjls
Jun 28 20:18:46.496: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:18:46.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:18:46.496: INFO: update-demo-nautilus-jgjls is verified up and running
STEP: scaling up the replication controller
Jun 28 20:18:46.499: INFO: scanned /root for discovery docs: <nil>
Jun 28 20:18:46.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 28 20:18:49.929: INFO: stderr: ""
Jun 28 20:18:49.929: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 28 20:18:49.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:50.291: INFO: stderr: ""
Jun 28 20:18:50.291: INFO: stdout: "update-demo-nautilus-2tfq7 update-demo-nautilus-jgjls "
Jun 28 20:18:50.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-2tfq7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:51.079: INFO: stderr: ""
Jun 28 20:18:51.079: INFO: stdout: ""
Jun 28 20:18:51.079: INFO: update-demo-nautilus-2tfq7 is created but not running
Jun 28 20:18:56.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 20:18:57.273: INFO: stderr: ""
Jun 28 20:18:57.273: INFO: stdout: "update-demo-nautilus-2tfq7 update-demo-nautilus-jgjls "
Jun 28 20:18:57.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-2tfq7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:18:57.423: INFO: stderr: ""
Jun 28 20:18:57.423: INFO: stdout: "true"
Jun 28 20:18:57.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-2tfq7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:18:57.575: INFO: stderr: ""
Jun 28 20:18:57.575: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:18:57.575: INFO: validating pod update-demo-nautilus-2tfq7
Jun 28 20:18:59.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:18:59.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:18:59.766: INFO: update-demo-nautilus-2tfq7 is verified up and running
Jun 28 20:18:59.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 20:19:00.800: INFO: stderr: ""
Jun 28 20:19:00.800: INFO: stdout: "true"
Jun 28 20:19:00.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods update-demo-nautilus-jgjls -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 20:19:01.160: INFO: stderr: ""
Jun 28 20:19:01.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 28 20:19:01.160: INFO: validating pod update-demo-nautilus-jgjls
Jun 28 20:19:04.883: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 20:19:04.883: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 20:19:04.883: INFO: update-demo-nautilus-jgjls is verified up and running
STEP: using delete to clean up resources
Jun 28 20:19:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 delete --grace-period=0 --force -f -'
Jun 28 20:19:05.061: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 20:19:05.061: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 20:19:05.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get rc,svc -l name=update-demo --no-headers'
Jun 28 20:19:05.923: INFO: stderr: "No resources found in kubectl-9313 namespace.\n"
Jun 28 20:19:05.923: INFO: stdout: ""
Jun 28 20:19:05.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9313 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 20:19:08.434: INFO: stderr: ""
Jun 28 20:19:08.434: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:19:08.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9313" for this suite.

• [SLOW TEST:56.285 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":166,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:19:09.869: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 28 20:19:17.945: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0628 20:19:17.945007      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 20:19:17.945032      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 20:19:17.945037      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:19:17.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9083" for this suite.

• [SLOW TEST:9.247 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":167,"skipped":2972,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:19:19.116: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:19:33.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9309" for this suite.

• [SLOW TEST:15.296 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":168,"skipped":2992,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:19:34.413: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0628 20:19:53.177523      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 20:19:53.177588      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 20:19:53.177599      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 20:19:53.177: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 28 20:19:53.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p6kq" in namespace "gc-8665"
Jun 28 20:19:53.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdjf6" in namespace "gc-8665"
Jun 28 20:19:54.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwzps" in namespace "gc-8665"
Jun 28 20:19:54.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs6gs" in namespace "gc-8665"
Jun 28 20:19:55.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfmd6" in namespace "gc-8665"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:19:55.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8665" for this suite.

• [SLOW TEST:23.319 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":169,"skipped":2997,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:19:57.732: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 28 20:19:59.782: INFO: Waiting up to 5m0s for pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355" in namespace "emptydir-4826" to be "Succeeded or Failed"
Jun 28 20:20:00.060: INFO: Pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355": Phase="Pending", Reason="", readiness=false. Elapsed: 277.658809ms
Jun 28 20:20:02.483: INFO: Pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700504563s
Jun 28 20:20:04.844: INFO: Pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062131534s
Jun 28 20:20:07.382: INFO: Pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.600118772s
STEP: Saw pod success
Jun 28 20:20:07.382: INFO: Pod "pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355" satisfied condition "Succeeded or Failed"
Jun 28 20:20:07.987: INFO: Trying to get logs from node 10.244.0.29 pod pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355 container test-container: <nil>
STEP: delete the pod
Jun 28 20:20:10.347: INFO: Waiting for pod pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355 to disappear
Jun 28 20:20:10.679: INFO: Pod pod-3f107410-cd31-4c0c-b1a6-23c99c8fb355 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:20:10.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4826" for this suite.

• [SLOW TEST:15.808 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":3005,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:20:13.541: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:21:17.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9771" for this suite.

• [SLOW TEST:66.863 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":3014,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:21:20.404: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 28 20:21:33.087: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:21:33.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6051" for this suite.

• [SLOW TEST:14.552 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:21:34.957: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:21:37.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2846" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":173,"skipped":3051,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:21:39.066: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:21:40.822: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 20:21:45.160: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 28 20:21:47.420: INFO: Creating deployment "test-rollover-deployment"
Jun 28 20:21:47.855: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 28 20:21:48.026: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 28 20:21:48.426: INFO: Ensure that both replica sets have 1 created replica
Jun 28 20:21:48.872: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 28 20:21:49.531: INFO: Updating deployment test-rollover-deployment
Jun 28 20:21:49.531: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 28 20:21:49.866: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 28 20:21:50.693: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 28 20:21:51.648: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:21:51.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508509, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:21:54.071: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:21:54.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508509, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:21:56.080: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:21:56.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508509, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:21:58.044: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:21:58.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508516, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:22:00.040: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:22:00.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508516, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:22:03.837: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:22:03.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508516, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:22:05.971: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 20:22:05.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508516, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760508507, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:22:07.962: INFO: 
Jun 28 20:22:07.962: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 20:22:08.492: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6105 /apis/apps/v1/namespaces/deployment-6105/deployments/test-rollover-deployment 8e72d8de-f90c-4063-b30a-a3b9fa385c83 98624 2 2021-06-28 20:21:47 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-28 20:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 20:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aac5708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-28 20:21:47 +0000 UTC,LastTransitionTime:2021-06-28 20:21:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-06-28 20:22:06 +0000 UTC,LastTransitionTime:2021-06-28 20:21:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 20:22:08.654: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-6105 /apis/apps/v1/namespaces/deployment-6105/replicasets/test-rollover-deployment-668db69979 2053bdf2-100e-4e0b-83d4-db89f983430a 98615 2 2021-06-28 20:21:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8e72d8de-f90c-4063-b30a-a3b9fa385c83 0xc00aac5d77 0xc00aac5d78}] []  [{kube-controller-manager Update apps/v1 2021-06-28 20:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e72d8de-f90c-4063-b30a-a3b9fa385c83\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aac5e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 20:22:08.654: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 28 20:22:08.655: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6105 /apis/apps/v1/namespaces/deployment-6105/replicasets/test-rollover-controller 5936fed1-7a47-4053-9954-08139aed1440 98623 2 2021-06-28 20:21:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8e72d8de-f90c-4063-b30a-a3b9fa385c83 0xc00aac5aa7 0xc00aac5aa8}] []  [{e2e.test Update apps/v1 2021-06-28 20:21:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 20:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e72d8de-f90c-4063-b30a-a3b9fa385c83\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00aac5c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 20:22:08.656: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6105 /apis/apps/v1/namespaces/deployment-6105/replicasets/test-rollover-deployment-78bc8b888c e97f99df-a5ab-44ab-8077-3bece95d5dbc 98513 2 2021-06-28 20:21:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8e72d8de-f90c-4063-b30a-a3b9fa385c83 0xc0037de067 0xc0037de068}] []  [{kube-controller-manager Update apps/v1 2021-06-28 20:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e72d8de-f90c-4063-b30a-a3b9fa385c83\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037de168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 20:22:08.776: INFO: Pod "test-rollover-deployment-668db69979-hdmjr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-hdmjr test-rollover-deployment-668db69979- deployment-6105 /api/v1/namespaces/deployment-6105/pods/test-rollover-deployment-668db69979-hdmjr 74117ca3-9be2-4864-bd4f-70c55131c7ea 98568 0 2021-06-28 20:21:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:172.17.113.152/32 cni.projectcalico.org/podIPs:172.17.113.152/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.152"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.152"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 2053bdf2-100e-4e0b-83d4-db89f983430a 0xc0037de887 0xc0037de888}] []  [{kube-controller-manager Update v1 2021-06-28 20:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2053bdf2-100e-4e0b-83d4-db89f983430a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 20:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 20:21:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 20:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xlgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xlgs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xlgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-q8df5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 20:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.152,StartTime:2021-06-28 20:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 20:21:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://6ee9acf94e36505e3261faabdea2eaa45a4a3b25c23fa9770fe5c7bce474c4b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:22:08.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6105" for this suite.

• [SLOW TEST:30.365 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":174,"skipped":3063,"failed":0}
S
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:22:09.431: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:22:10.588: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3" in namespace "security-context-test-2007" to be "Succeeded or Failed"
Jun 28 20:22:10.882: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 293.46602ms
Jun 28 20:22:14.471: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882766723s
Jun 28 20:22:16.724: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.135217206s
Jun 28 20:22:19.063: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.474247748s
Jun 28 20:22:21.540: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.951317234s
Jun 28 20:22:21.540: INFO: Pod "alpine-nnp-false-c8aebcac-cccf-4671-b00e-3c9d39fb2aa3" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:22:23.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2007" for this suite.

• [SLOW TEST:17.872 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:22:27.304: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6174/configmap-test-80eca0bb-07a6-4881-a331-fc3dafacaac6
STEP: Creating a pod to test consume configMaps
Jun 28 20:22:32.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c" in namespace "configmap-6174" to be "Succeeded or Failed"
Jun 28 20:22:32.486: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 447.878616ms
Jun 28 20:22:34.901: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862179076s
Jun 28 20:22:37.406: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367285443s
Jun 28 20:22:39.594: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555279s
Jun 28 20:22:41.764: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.725372862s
Jun 28 20:22:43.980: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.941320529s
Jun 28 20:22:46.260: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.221891079s
STEP: Saw pod success
Jun 28 20:22:46.260: INFO: Pod "pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c" satisfied condition "Succeeded or Failed"
Jun 28 20:22:46.516: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c container env-test: <nil>
STEP: delete the pod
Jun 28 20:22:48.346: INFO: Waiting for pod pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c to disappear
Jun 28 20:22:48.616: INFO: Pod pod-configmaps-21606aef-1736-4349-8d6e-91897209bf0c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:22:48.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6174" for this suite.

• [SLOW TEST:22.828 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3095,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:22:50.135: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4519.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4519.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 20:23:03.187: INFO: DNS probes using dns-4519/dns-test-9e444073-01e5-42d5-b24f-92fbd0ff03e5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:23:04.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4519" for this suite.

• [SLOW TEST:17.737 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":177,"skipped":3110,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:23:07.873: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:23:16.488: INFO: Deleting pod "var-expansion-694d5fa1-96c7-4477-81d6-11753a5422f4" in namespace "var-expansion-9865"
Jun 28 20:23:16.641: INFO: Wait up to 5m0s for pod "var-expansion-694d5fa1-96c7-4477-81d6-11753a5422f4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:23:22.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9865" for this suite.

• [SLOW TEST:15.858 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":178,"skipped":3127,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:23:23.731: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 28 20:23:32.552: INFO: Successfully updated pod "pod-update-d36881ba-8f50-4257-a591-b28e3df8411b"
STEP: verifying the updated pod is in kubernetes
Jun 28 20:23:32.986: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:23:32.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8183" for this suite.

• [SLOW TEST:10.198 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:23:33.929: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a
Jun 28 20:23:35.143: INFO: Pod name my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a: Found 1 pods out of 1
Jun 28 20:23:35.143: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a" are running
Jun 28 20:23:41.540: INFO: Pod "my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a-srlgw" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-28 20:23:34 +0000 UTC Reason: Message:}])
Jun 28 20:23:41.540: INFO: Trying to dial the pod
Jun 28 20:23:47.895: INFO: Controller my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a: Got expected result from replica 1 [my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a-srlgw]: "my-hostname-basic-27f6c22c-d731-4c74-a77f-16e9b7ab884a-srlgw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:23:47.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4350" for this suite.

• [SLOW TEST:14.486 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":180,"skipped":3170,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:23:48.416: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-c3329f72-8cd0-4153-bb59-aa3e3d4a0cf3
STEP: Creating a pod to test consume configMaps
Jun 28 20:23:49.110: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0" in namespace "projected-8897" to be "Succeeded or Failed"
Jun 28 20:23:49.249: INFO: Pod "pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 138.329617ms
Jun 28 20:23:51.450: INFO: Pod "pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340047375s
Jun 28 20:23:53.578: INFO: Pod "pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.467839956s
STEP: Saw pod success
Jun 28 20:23:53.578: INFO: Pod "pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0" satisfied condition "Succeeded or Failed"
Jun 28 20:23:53.701: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:23:54.290: INFO: Waiting for pod pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0 to disappear
Jun 28 20:23:54.465: INFO: Pod pod-projected-configmaps-931919f2-cfbf-4e5d-85a3-71768f99e0f0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:23:54.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8897" for this suite.

• [SLOW TEST:7.213 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3172,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:23:55.629: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-7686
STEP: creating replication controller nodeport-test in namespace services-7686
I0628 20:23:57.590832      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7686, replica count: 2
I0628 20:24:00.791195      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:24:03.791466      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 20:24:03.791: INFO: Creating new exec pod
Jun 28 20:24:12.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 28 20:24:19.074: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 20:24:19.074: INFO: stdout: ""
Jun 28 20:24:19.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 172.21.65.229 80'
Jun 28 20:24:24.705: INFO: stderr: "+ nc -zv -t -w 2 172.21.65.229 80\nConnection to 172.21.65.229 80 port [tcp/http] succeeded!\n"
Jun 28 20:24:24.705: INFO: stdout: ""
Jun 28 20:24:24.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.30 31272'
Jun 28 20:24:29.305: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.30 31272\nConnection to 10.244.0.30 31272 port [tcp/31272] succeeded!\n"
Jun 28 20:24:29.305: INFO: stdout: ""
Jun 28 20:24:29.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 31272'
Jun 28 20:24:34.246: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 31272\nConnection to 10.244.0.31 31272 port [tcp/31272] succeeded!\n"
Jun 28 20:24:34.246: INFO: stdout: ""
Jun 28 20:24:34.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.30 31272'
Jun 28 20:24:38.070: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.30 31272\nConnection to 10.244.0.30 31272 port [tcp/31272] succeeded!\n"
Jun 28 20:24:38.070: INFO: stdout: ""
Jun 28 20:24:38.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7686 exec execpodwtnmr -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.31 31272'
Jun 28 20:24:41.448: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.31 31272\nConnection to 10.244.0.31 31272 port [tcp/31272] succeeded!\n"
Jun 28 20:24:41.448: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:24:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7686" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:47.207 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":182,"skipped":3189,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:24:42.836: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 28 20:24:54.217: INFO: Pod name wrapped-volume-race-e3e8f3e5-82f9-4773-8eb4-4d4c0b43463b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e3e8f3e5-82f9-4773-8eb4-4d4c0b43463b in namespace emptydir-wrapper-8742, will wait for the garbage collector to delete the pods
Jun 28 20:25:09.833: INFO: Deleting ReplicationController wrapped-volume-race-e3e8f3e5-82f9-4773-8eb4-4d4c0b43463b took: 226.396216ms
Jun 28 20:25:10.533: INFO: Terminating ReplicationController wrapped-volume-race-e3e8f3e5-82f9-4773-8eb4-4d4c0b43463b pods took: 700.17705ms
STEP: Creating RC which spawns configmap-volume pods
Jun 28 20:25:44.406: INFO: Pod name wrapped-volume-race-c137aeaf-6f20-45f1-aedc-81357090fd2d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c137aeaf-6f20-45f1-aedc-81357090fd2d in namespace emptydir-wrapper-8742, will wait for the garbage collector to delete the pods
Jun 28 20:25:58.502: INFO: Deleting ReplicationController wrapped-volume-race-c137aeaf-6f20-45f1-aedc-81357090fd2d took: 364.693149ms
Jun 28 20:26:00.004: INFO: Terminating ReplicationController wrapped-volume-race-c137aeaf-6f20-45f1-aedc-81357090fd2d pods took: 1.501558509s
STEP: Creating RC which spawns configmap-volume pods
Jun 28 20:26:28.901: INFO: Pod name wrapped-volume-race-f3705d17-3f50-4f99-960a-ddd54102b5f8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f3705d17-3f50-4f99-960a-ddd54102b5f8 in namespace emptydir-wrapper-8742, will wait for the garbage collector to delete the pods
Jun 28 20:26:50.540: INFO: Deleting ReplicationController wrapped-volume-race-f3705d17-3f50-4f99-960a-ddd54102b5f8 took: 223.516393ms
Jun 28 20:26:51.540: INFO: Terminating ReplicationController wrapped-volume-race-f3705d17-3f50-4f99-960a-ddd54102b5f8 pods took: 1.000171872s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:27:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8742" for this suite.

• [SLOW TEST:162.778 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":183,"skipped":3201,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:27:25.614: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c11b60fa-4a0e-401e-aedc-a6292e7347cc
STEP: Creating a pod to test consume secrets
Jun 28 20:27:27.170: INFO: Waiting up to 5m0s for pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce" in namespace "secrets-8250" to be "Succeeded or Failed"
Jun 28 20:27:27.398: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 228.675441ms
Jun 28 20:27:30.399: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229253903s
Jun 28 20:27:32.653: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.483641962s
Jun 28 20:27:34.908: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.738138532s
Jun 28 20:27:37.220: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050023124s
Jun 28 20:27:39.356: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 12.186005269s
Jun 28 20:27:41.630: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.460017381s
STEP: Saw pod success
Jun 28 20:27:41.630: INFO: Pod "pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce" satisfied condition "Succeeded or Failed"
Jun 28 20:27:41.874: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 20:27:45.007: INFO: Waiting for pod pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce to disappear
Jun 28 20:27:45.255: INFO: Pod pod-secrets-8a234f7f-e5bc-44e8-b7f6-c695a175e1ce no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:27:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8250" for this suite.

• [SLOW TEST:23.003 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":3201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:27:48.619: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-860d9a3c-cf25-4735-ad26-ba19e3d20d50
STEP: Creating a pod to test consume configMaps
Jun 28 20:27:50.320: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54" in namespace "projected-2606" to be "Succeeded or Failed"
Jun 28 20:27:50.463: INFO: Pod "pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54": Phase="Pending", Reason="", readiness=false. Elapsed: 142.442292ms
Jun 28 20:27:52.684: INFO: Pod "pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.363097134s
Jun 28 20:27:54.980: INFO: Pod "pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.659988978s
STEP: Saw pod success
Jun 28 20:27:54.980: INFO: Pod "pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54" satisfied condition "Succeeded or Failed"
Jun 28 20:27:55.276: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:27:56.180: INFO: Waiting for pod pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54 to disappear
Jun 28 20:27:56.516: INFO: Pod pod-projected-configmaps-0597c990-7ef6-4bf2-8187-54a43d6b1e54 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:27:56.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2606" for this suite.

• [SLOW TEST:9.584 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3240,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:27:58.203: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:27:59.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d" in namespace "projected-6042" to be "Succeeded or Failed"
Jun 28 20:27:59.475: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 229.144891ms
Jun 28 20:28:01.735: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488344453s
Jun 28 20:28:03.913: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666658103s
Jun 28 20:28:06.137: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.891099063s
Jun 28 20:28:08.388: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.141947871s
Jun 28 20:28:10.743: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.497140855s
Jun 28 20:28:12.850: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.603244579s
STEP: Saw pod success
Jun 28 20:28:12.850: INFO: Pod "downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d" satisfied condition "Succeeded or Failed"
Jun 28 20:28:12.987: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d container client-container: <nil>
STEP: delete the pod
Jun 28 20:28:13.606: INFO: Waiting for pod downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d to disappear
Jun 28 20:28:13.741: INFO: Pod downwardapi-volume-9deb1594-148e-43db-ba43-121417dc1d1d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:28:13.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6042" for this suite.

• [SLOW TEST:16.293 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3249,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:28:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:28:15.028: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 28 20:28:27.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 create -f -'
Jun 28 20:28:34.690: INFO: stderr: ""
Jun 28 20:28:34.691: INFO: stdout: "e2e-test-crd-publish-openapi-1153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 20:28:34.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 delete e2e-test-crd-publish-openapi-1153-crds test-cr'
Jun 28 20:28:34.943: INFO: stderr: ""
Jun 28 20:28:34.943: INFO: stdout: "e2e-test-crd-publish-openapi-1153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 28 20:28:34.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 apply -f -'
Jun 28 20:28:39.632: INFO: stderr: ""
Jun 28 20:28:39.632: INFO: stdout: "e2e-test-crd-publish-openapi-1153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 20:28:39.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1596 --namespace=crd-publish-openapi-1596 delete e2e-test-crd-publish-openapi-1153-crds test-cr'
Jun 28 20:28:40.892: INFO: stderr: ""
Jun 28 20:28:40.892: INFO: stdout: "e2e-test-crd-publish-openapi-1153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 28 20:28:40.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=crd-publish-openapi-1596 explain e2e-test-crd-publish-openapi-1153-crds'
Jun 28 20:28:47.172: INFO: stderr: ""
Jun 28 20:28:47.172: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1153-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:29:00.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1596" for this suite.

• [SLOW TEST:46.546 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":187,"skipped":3252,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:29:01.042: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5419
Jun 28 20:29:04.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 28 20:29:11.336: INFO: rc: 7
Jun 28 20:29:11.655: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 28 20:29:11.950: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 28 20:29:11.950: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5419
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5419
I0628 20:29:12.455715      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5419, replica count: 3
I0628 20:29:15.756037      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:29:18.756204      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:29:21.756377      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 20:29:22.461: INFO: Creating new exec pod
Jun 28 20:29:28.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec execpod-affinitybmwht -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun 28 20:29:32.764: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 28 20:29:32.764: INFO: stdout: ""
Jun 28 20:29:32.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec execpod-affinitybmwht -- /bin/sh -x -c nc -zv -t -w 2 172.21.174.153 80'
Jun 28 20:29:35.206: INFO: stderr: "+ nc -zv -t -w 2 172.21.174.153 80\nConnection to 172.21.174.153 80 port [tcp/http] succeeded!\n"
Jun 28 20:29:35.206: INFO: stdout: ""
Jun 28 20:29:35.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec execpod-affinitybmwht -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.174.153:80/ ; done'
Jun 28 20:29:37.662: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n"
Jun 28 20:29:37.663: INFO: stdout: "\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n\naffinity-clusterip-timeout-qcw6n"
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Received response from host: affinity-clusterip-timeout-qcw6n
Jun 28 20:29:37.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec execpod-affinitybmwht -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.174.153:80/'
Jun 28 20:29:39.925: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n"
Jun 28 20:29:39.925: INFO: stdout: "affinity-clusterip-timeout-qcw6n"
Jun 28 20:29:59.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-5419 exec execpod-affinitybmwht -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.174.153:80/'
Jun 28 20:30:08.179: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.174.153:80/\n"
Jun 28 20:30:08.179: INFO: stdout: "affinity-clusterip-timeout-8zbfm"
Jun 28 20:30:08.179: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5419, will wait for the garbage collector to delete the pods
Jun 28 20:30:09.857: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 481.48055ms
Jun 28 20:30:10.357: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 500.173617ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:30:23.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5419" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:83.601 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":188,"skipped":3269,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:30:24.643: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:30:27.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88" in namespace "downward-api-6363" to be "Succeeded or Failed"
Jun 28 20:30:27.669: INFO: Pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88": Phase="Pending", Reason="", readiness=false. Elapsed: 415.521043ms
Jun 28 20:30:29.887: INFO: Pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633750416s
Jun 28 20:30:32.327: INFO: Pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074219595s
Jun 28 20:30:34.649: INFO: Pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.395422292s
STEP: Saw pod success
Jun 28 20:30:34.649: INFO: Pod "downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88" satisfied condition "Succeeded or Failed"
Jun 28 20:30:34.901: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88 container client-container: <nil>
STEP: delete the pod
Jun 28 20:30:36.467: INFO: Waiting for pod downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88 to disappear
Jun 28 20:30:36.556: INFO: Pod downwardapi-volume-7386dabf-fb07-4a12-a895-fec84f061b88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:30:36.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6363" for this suite.

• [SLOW TEST:12.190 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:30:36.833: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:30:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6810" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":190,"skipped":3312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:30:37.961: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 28 20:30:38.766: INFO: PodSpec: initContainers in spec.initContainers
Jun 28 20:31:26.261: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6b7f3a4c-8d5d-4ba5-be60-215353d6f341", GenerateName:"", Namespace:"init-container-9732", SelfLink:"/api/v1/namespaces/init-container-9732/pods/pod-init-6b7f3a4c-8d5d-4ba5-be60-215353d6f341", UID:"5ce529d6-ff05-43e3-a3da-ecbdc548a663", ResourceVersion:"103560", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63760509038, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"766148632"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.17.113.182/32", "cni.projectcalico.org/podIPs":"172.17.113.182/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.17.113.182\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.17.113.182\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003d26080), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d260a0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003d260c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d260e0)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003d26100), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d26120)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003d26140), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d26160)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-djffh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005f20000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-djffh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00650e0c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-djffh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00650e120), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-djffh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00650e000), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00a1e20e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.244.0.29", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00250a000), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-lfvdj"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a1e21a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a1e21c0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00a1e21dc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00a1e21e0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003f48040), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509039, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509039, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509039, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509038, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.244.0.29", PodIP:"172.17.113.182", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.17.113.182"}}, StartTime:(*v1.Time)(0xc003d26180), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00250a0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00250a150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://74edac51459feb0cde23543a5f6101d4f5fc9bf30caa99ac1ec851eea4b3e3f3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d261c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003d261a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00a1e225f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:31:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9732" for this suite.

• [SLOW TEST:49.003 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":191,"skipped":3337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:31:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 28 20:31:38.049: INFO: Successfully updated pod "annotationupdatea52c1469-e0eb-4f5b-8942-9588b56d4a37"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:31:42.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9835" for this suite.

• [SLOW TEST:15.568 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3381,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:31:42.533: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-e6975026-7580-4f63-a593-cfd3583e02eb
STEP: Creating a pod to test consume configMaps
Jun 28 20:31:43.424: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b" in namespace "configmap-2291" to be "Succeeded or Failed"
Jun 28 20:31:43.525: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 100.962659ms
Jun 28 20:31:45.704: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279303307s
Jun 28 20:31:47.978: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5539177s
Jun 28 20:31:50.337: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.912372723s
Jun 28 20:31:52.526: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.101557997s
Jun 28 20:31:54.724: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.299933784s
Jun 28 20:31:58.744: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.320185776s
STEP: Saw pod success
Jun 28 20:31:58.744: INFO: Pod "pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b" satisfied condition "Succeeded or Failed"
Jun 28 20:31:59.516: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:32:01.211: INFO: Waiting for pod pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b to disappear
Jun 28 20:32:01.481: INFO: Pod pod-configmaps-e4216de1-6f93-45b5-b3c0-c2391e3de82b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:32:01.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2291" for this suite.

• [SLOW TEST:21.040 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3397,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:32:03.573: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 28 20:32:15.852: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 20:32:15.987: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 20:32:17.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 20:32:18.356: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 20:32:19.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 20:32:20.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 20:32:21.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 20:32:22.320: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:32:22.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9271" for this suite.

• [SLOW TEST:20.776 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3399,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:32:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 28 20:32:25.201: INFO: Waiting up to 5m0s for pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03" in namespace "emptydir-5527" to be "Succeeded or Failed"
Jun 28 20:32:25.338: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03": Phase="Pending", Reason="", readiness=false. Elapsed: 136.516807ms
Jun 28 20:32:27.472: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271442755s
Jun 28 20:32:29.636: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.43487313s
Jun 28 20:32:32.009: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.808252889s
Jun 28 20:32:34.277: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.075819712s
STEP: Saw pod success
Jun 28 20:32:34.277: INFO: Pod "pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03" satisfied condition "Succeeded or Failed"
Jun 28 20:32:34.547: INFO: Trying to get logs from node 10.244.0.29 pod pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03 container test-container: <nil>
STEP: delete the pod
Jun 28 20:32:35.691: INFO: Waiting for pod pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03 to disappear
Jun 28 20:32:35.902: INFO: Pod pod-a4792d4f-c7a1-4f5b-ba60-f9239ba5ba03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:32:35.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5527" for this suite.

• [SLOW TEST:12.730 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3406,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:32:37.079: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Jun 28 20:32:51.705: INFO: Pod pod-hostip-1d899c65-647a-4508-aa2d-9fb9983724ed has hostIP: 10.244.0.29
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:32:51.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3560" for this suite.

• [SLOW TEST:16.453 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3412,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:32:53.532: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3896
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 20:32:54.719: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 20:32:58.675: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:33:00.790: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:33:02.747: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:04.984: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:07.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:08.931: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:10.959: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:12.917: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:14.970: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:17.195: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:19.036: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:33:20.959: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 20:33:21.626: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 20:33:22.182: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 20:33:33.491: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 20:33:33.491: INFO: Going to poll 172.17.113.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:33:33.677: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.113.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3896 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:33:33.678: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:33:38.278: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 28 20:33:38.278: INFO: Going to poll 172.17.74.243 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:33:38.633: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.74.243 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3896 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:33:38.635: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:33:42.587: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 28 20:33:42.587: INFO: Going to poll 172.17.65.246 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:33:42.862: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.65.246 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3896 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:33:42.862: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:33:49.690: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:33:49.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3896" for this suite.

• [SLOW TEST:58.878 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3414,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:33:52.410: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:34:07.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2583" for this suite.

• [SLOW TEST:16.264 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":198,"skipped":3419,"failed":0}
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:34:08.673: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Jun 28 20:34:09.523: INFO: created test-event-1
Jun 28 20:34:09.648: INFO: created test-event-2
Jun 28 20:34:09.752: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 28 20:34:09.878: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 28 20:34:10.200: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:34:10.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2638" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":199,"skipped":3419,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:34:13.013: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 28 20:34:16.153: INFO: starting watch
STEP: patching
STEP: updating
Jun 28 20:34:16.956: INFO: waiting for watch events with expected annotations
Jun 28 20:34:16.956: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:34:18.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2214" for this suite.

• [SLOW TEST:8.048 seconds]
[sig-network] IngressClass API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":200,"skipped":3421,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:34:21.061: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:34:22.792: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 28 20:34:23.049: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 28 20:34:23.049: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:23.049: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 28 20:34:23.049: INFO: Checking APIGroup: apps
Jun 28 20:34:23.242: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 28 20:34:23.242: INFO: Versions found [{apps/v1 v1}]
Jun 28 20:34:23.242: INFO: apps/v1 matches apps/v1
Jun 28 20:34:23.242: INFO: Checking APIGroup: events.k8s.io
Jun 28 20:34:23.473: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 28 20:34:23.473: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:23.473: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 28 20:34:23.473: INFO: Checking APIGroup: authentication.k8s.io
Jun 28 20:34:23.759: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 28 20:34:23.759: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:23.759: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 28 20:34:23.759: INFO: Checking APIGroup: authorization.k8s.io
Jun 28 20:34:24.063: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 28 20:34:24.063: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:24.063: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 28 20:34:24.063: INFO: Checking APIGroup: autoscaling
Jun 28 20:34:24.417: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 28 20:34:24.417: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 28 20:34:24.417: INFO: autoscaling/v1 matches autoscaling/v1
Jun 28 20:34:24.417: INFO: Checking APIGroup: batch
Jun 28 20:34:24.783: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 28 20:34:24.783: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 28 20:34:24.783: INFO: batch/v1 matches batch/v1
Jun 28 20:34:24.783: INFO: Checking APIGroup: certificates.k8s.io
Jun 28 20:34:25.121: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 28 20:34:25.121: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:25.121: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 28 20:34:25.121: INFO: Checking APIGroup: networking.k8s.io
Jun 28 20:34:26.485: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 28 20:34:26.485: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:26.485: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 28 20:34:26.485: INFO: Checking APIGroup: extensions
Jun 28 20:34:26.823: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun 28 20:34:26.823: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun 28 20:34:26.823: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun 28 20:34:26.823: INFO: Checking APIGroup: policy
Jun 28 20:34:27.137: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun 28 20:34:27.137: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun 28 20:34:27.137: INFO: policy/v1beta1 matches policy/v1beta1
Jun 28 20:34:27.137: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 28 20:34:27.357: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 28 20:34:27.357: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:27.357: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 28 20:34:27.357: INFO: Checking APIGroup: storage.k8s.io
Jun 28 20:34:27.577: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 28 20:34:27.577: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:27.577: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 28 20:34:27.577: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 28 20:34:27.774: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 28 20:34:27.774: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:27.774: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 28 20:34:27.774: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 28 20:34:27.978: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 28 20:34:27.978: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:27.978: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 28 20:34:27.978: INFO: Checking APIGroup: scheduling.k8s.io
Jun 28 20:34:28.162: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 28 20:34:28.162: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:28.162: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 28 20:34:28.162: INFO: Checking APIGroup: coordination.k8s.io
Jun 28 20:34:28.323: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 28 20:34:28.323: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:28.323: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 28 20:34:28.323: INFO: Checking APIGroup: node.k8s.io
Jun 28 20:34:28.469: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 28 20:34:28.469: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:28.469: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 28 20:34:28.469: INFO: Checking APIGroup: discovery.k8s.io
Jun 28 20:34:28.619: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun 28 20:34:28.619: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:28.619: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun 28 20:34:28.619: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 28 20:34:28.770: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jun 28 20:34:28.770: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jun 28 20:34:28.770: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jun 28 20:34:28.770: INFO: Checking APIGroup: apps.openshift.io
Jun 28 20:34:28.932: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jun 28 20:34:28.932: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jun 28 20:34:28.932: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jun 28 20:34:28.932: INFO: Checking APIGroup: authorization.openshift.io
Jun 28 20:34:29.076: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jun 28 20:34:29.076: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jun 28 20:34:29.076: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jun 28 20:34:29.076: INFO: Checking APIGroup: build.openshift.io
Jun 28 20:34:29.224: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jun 28 20:34:29.224: INFO: Versions found [{build.openshift.io/v1 v1}]
Jun 28 20:34:29.224: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jun 28 20:34:29.224: INFO: Checking APIGroup: image.openshift.io
Jun 28 20:34:29.347: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jun 28 20:34:29.347: INFO: Versions found [{image.openshift.io/v1 v1}]
Jun 28 20:34:29.347: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jun 28 20:34:29.347: INFO: Checking APIGroup: oauth.openshift.io
Jun 28 20:34:29.485: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jun 28 20:34:29.485: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jun 28 20:34:29.485: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jun 28 20:34:29.485: INFO: Checking APIGroup: project.openshift.io
Jun 28 20:34:29.675: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jun 28 20:34:29.675: INFO: Versions found [{project.openshift.io/v1 v1}]
Jun 28 20:34:29.675: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jun 28 20:34:29.675: INFO: Checking APIGroup: quota.openshift.io
Jun 28 20:34:29.870: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jun 28 20:34:29.870: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jun 28 20:34:29.870: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jun 28 20:34:29.870: INFO: Checking APIGroup: route.openshift.io
Jun 28 20:34:30.061: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jun 28 20:34:30.061: INFO: Versions found [{route.openshift.io/v1 v1}]
Jun 28 20:34:30.061: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jun 28 20:34:30.061: INFO: Checking APIGroup: security.openshift.io
Jun 28 20:34:30.281: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jun 28 20:34:30.281: INFO: Versions found [{security.openshift.io/v1 v1}]
Jun 28 20:34:30.281: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jun 28 20:34:30.281: INFO: Checking APIGroup: template.openshift.io
Jun 28 20:34:30.505: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jun 28 20:34:30.505: INFO: Versions found [{template.openshift.io/v1 v1}]
Jun 28 20:34:30.505: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jun 28 20:34:30.505: INFO: Checking APIGroup: user.openshift.io
Jun 28 20:34:30.716: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jun 28 20:34:30.716: INFO: Versions found [{user.openshift.io/v1 v1}]
Jun 28 20:34:30.716: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jun 28 20:34:30.716: INFO: Checking APIGroup: packages.operators.coreos.com
Jun 28 20:34:30.939: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jun 28 20:34:30.939: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jun 28 20:34:30.939: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jun 28 20:34:30.940: INFO: Checking APIGroup: config.openshift.io
Jun 28 20:34:31.160: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jun 28 20:34:31.160: INFO: Versions found [{config.openshift.io/v1 v1}]
Jun 28 20:34:31.160: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jun 28 20:34:31.160: INFO: Checking APIGroup: operator.openshift.io
Jun 28 20:34:31.373: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jun 28 20:34:31.373: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jun 28 20:34:31.373: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jun 28 20:34:31.373: INFO: Checking APIGroup: cloudcredential.openshift.io
Jun 28 20:34:31.594: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jun 28 20:34:31.594: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jun 28 20:34:31.594: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jun 28 20:34:31.594: INFO: Checking APIGroup: console.openshift.io
Jun 28 20:34:31.847: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jun 28 20:34:31.847: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jun 28 20:34:31.847: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jun 28 20:34:31.847: INFO: Checking APIGroup: crd.projectcalico.org
Jun 28 20:34:32.078: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 28 20:34:32.078: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 28 20:34:32.078: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 28 20:34:32.078: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jun 28 20:34:32.319: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jun 28 20:34:32.319: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jun 28 20:34:32.319: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jun 28 20:34:32.319: INFO: Checking APIGroup: ingress.operator.openshift.io
Jun 28 20:34:32.571: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jun 28 20:34:32.571: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jun 28 20:34:32.571: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jun 28 20:34:32.571: INFO: Checking APIGroup: k8s.cni.cncf.io
Jun 28 20:34:32.764: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jun 28 20:34:32.764: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jun 28 20:34:32.764: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jun 28 20:34:32.764: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jun 28 20:34:32.924: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jun 28 20:34:32.924: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jun 28 20:34:32.924: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jun 28 20:34:32.924: INFO: Checking APIGroup: monitoring.coreos.com
Jun 28 20:34:33.049: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jun 28 20:34:33.049: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jun 28 20:34:33.049: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jun 28 20:34:33.049: INFO: Checking APIGroup: network.operator.openshift.io
Jun 28 20:34:33.191: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jun 28 20:34:33.197: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jun 28 20:34:33.197: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jun 28 20:34:33.197: INFO: Checking APIGroup: operator.tigera.io
Jun 28 20:34:33.325: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 28 20:34:33.325: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 28 20:34:33.325: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 28 20:34:33.325: INFO: Checking APIGroup: operators.coreos.com
Jun 28 20:34:33.482: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Jun 28 20:34:33.482: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jun 28 20:34:33.482: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Jun 28 20:34:33.482: INFO: Checking APIGroup: samples.operator.openshift.io
Jun 28 20:34:33.624: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jun 28 20:34:33.624: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jun 28 20:34:33.624: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jun 28 20:34:33.624: INFO: Checking APIGroup: security.internal.openshift.io
Jun 28 20:34:33.765: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jun 28 20:34:33.765: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jun 28 20:34:33.765: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jun 28 20:34:33.765: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun 28 20:34:33.901: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun 28 20:34:33.901: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:33.901: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun 28 20:34:33.901: INFO: Checking APIGroup: tuned.openshift.io
Jun 28 20:34:34.045: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jun 28 20:34:34.045: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jun 28 20:34:34.045: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jun 28 20:34:34.045: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jun 28 20:34:34.217: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jun 28 20:34:34.217: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jun 28 20:34:34.217: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jun 28 20:34:34.217: INFO: Checking APIGroup: ibm.com
Jun 28 20:34:34.386: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun 28 20:34:34.386: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun 28 20:34:34.386: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun 28 20:34:34.386: INFO: Checking APIGroup: metal3.io
Jun 28 20:34:34.561: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jun 28 20:34:34.561: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jun 28 20:34:34.561: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jun 28 20:34:34.561: INFO: Checking APIGroup: migration.k8s.io
Jun 28 20:34:34.820: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jun 28 20:34:34.820: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jun 28 20:34:34.820: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jun 28 20:34:34.820: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jun 28 20:34:35.112: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jun 28 20:34:35.112: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jun 28 20:34:35.112: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jun 28 20:34:35.112: INFO: Checking APIGroup: helm.openshift.io
Jun 28 20:34:35.475: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jun 28 20:34:35.475: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jun 28 20:34:35.475: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jun 28 20:34:35.475: INFO: Checking APIGroup: metrics.k8s.io
Jun 28 20:34:35.849: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 28 20:34:35.849: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 28 20:34:35.849: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:34:35.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-272" for this suite.

• [SLOW TEST:18.909 seconds]
[sig-api-machinery] Discovery
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":201,"skipped":3430,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:34:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 20:34:41.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8884 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jun 28 20:34:41.506: INFO: stderr: ""
Jun 28 20:34:41.506: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Jun 28 20:34:41.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-8884 delete pods e2e-test-httpd-pod'
Jun 28 20:34:52.728: INFO: stderr: ""
Jun 28 20:34:52.728: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:34:52.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8884" for this suite.

• [SLOW TEST:14.865 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":202,"skipped":3431,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:34:54.835: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5300
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5300
STEP: Creating statefulset with conflicting port in namespace statefulset-5300
STEP: Waiting until pod test-pod will start running in namespace statefulset-5300
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5300
Jun 28 20:35:06.435: INFO: Observed stateful pod in namespace: statefulset-5300, name: ss-0, uid: a80b6518-8a63-4f42-91e8-8648da480488, status phase: Pending. Waiting for statefulset controller to delete.
Jun 28 20:35:06.649: INFO: Observed stateful pod in namespace: statefulset-5300, name: ss-0, uid: a80b6518-8a63-4f42-91e8-8648da480488, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 20:35:06.649: INFO: Observed stateful pod in namespace: statefulset-5300, name: ss-0, uid: a80b6518-8a63-4f42-91e8-8648da480488, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 20:35:06.649: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5300
STEP: Removing pod with conflicting port in namespace statefulset-5300
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5300 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 20:35:15.858: INFO: Deleting all statefulset in ns statefulset-5300
Jun 28 20:35:15.915: INFO: Scaling statefulset ss to 0
Jun 28 20:35:36.483: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 20:35:36.761: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:35:37.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5300" for this suite.

• [SLOW TEST:44.372 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":203,"skipped":3433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:35:39.208: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2222
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 20:35:40.116: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 20:35:42.106: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:35:44.448: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:35:46.330: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:35:48.357: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 20:35:50.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:35:52.387: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:35:56.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:35:58.432: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 20:36:00.386: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 20:36:00.888: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 20:36:03.107: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 20:36:05.044: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 20:36:07.169: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 20:36:09.019: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 20:36:11.206: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 20:36:11.743: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 20:36:23.287: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 20:36:23.287: INFO: Going to poll 172.17.113.158 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:36:23.406: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.113.158:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2222 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:36:23.406: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:36:27.215: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 28 20:36:27.215: INFO: Going to poll 172.17.74.245 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:36:27.636: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.74.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2222 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:36:27.636: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:36:32.135: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 28 20:36:32.135: INFO: Going to poll 172.17.65.247 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun 28 20:36:32.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.65.247:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2222 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:36:32.572: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:36:37.258: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:36:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2222" for this suite.

• [SLOW TEST:59.787 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3470,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:36:38.995: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1525
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1525
I0628 20:36:41.866738      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1525, replica count: 2
I0628 20:36:45.117056      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:36:48.117221      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:36:51.117409      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 20:36:51.117: INFO: Creating new exec pod
Jun 28 20:37:04.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 28 20:37:09.891: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 20:37:09.891: INFO: stdout: ""
Jun 28 20:37:09.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 172.21.211.200 80'
Jun 28 20:37:13.737: INFO: stderr: "+ nc -zv -t -w 2 172.21.211.200 80\nConnection to 172.21.211.200 80 port [tcp/http] succeeded!\n"
Jun 28 20:37:13.737: INFO: stdout: ""
Jun 28 20:37:13.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30379'
Jun 28 20:37:18.032: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30379\nConnection to 10.244.0.29 30379 port [tcp/30379] succeeded!\n"
Jun 28 20:37:18.032: INFO: stdout: ""
Jun 28 20:37:18.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.30 30379'
Jun 28 20:37:20.853: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.30 30379\nConnection to 10.244.0.30 30379 port [tcp/30379] succeeded!\n"
Jun 28 20:37:20.853: INFO: stdout: ""
Jun 28 20:37:20.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.29 30379'
Jun 28 20:37:23.171: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.29 30379\nConnection to 10.244.0.29 30379 port [tcp/30379] succeeded!\n"
Jun 28 20:37:23.171: INFO: stdout: ""
Jun 28 20:37:23.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-1525 exec execpodrwj6z -- /bin/sh -x -c nc -zv -t -w 2 10.244.0.30 30379'
Jun 28 20:37:26.213: INFO: stderr: "+ nc -zv -t -w 2 10.244.0.30 30379\nConnection to 10.244.0.30 30379 port [tcp/30379] succeeded!\n"
Jun 28 20:37:26.214: INFO: stdout: ""
Jun 28 20:37:26.214: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:37:26.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1525" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:48.666 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":205,"skipped":3471,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:37:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun 28 20:37:28.782: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 20:38:33.905: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:38:34.626: INFO: Starting informer...
STEP: Starting pods...
Jun 28 20:38:35.670: INFO: Pod1 is running on 10.244.0.29. Tainting Node
Jun 28 20:38:51.491: INFO: Pod2 is running on 10.244.0.29. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 28 20:39:22.639: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 28 20:39:32.819: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:39:33.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6306" for this suite.

• [SLOW TEST:126.636 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":206,"skipped":3480,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:39:34.298: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Jun 28 20:39:36.015: INFO: Waiting up to 5m0s for pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf" in namespace "containers-639" to be "Succeeded or Failed"
Jun 28 20:39:36.459: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 443.562068ms
Jun 28 20:39:38.722: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706852412s
Jun 28 20:39:41.040: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025377139s
Jun 28 20:39:43.235: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.2197052s
Jun 28 20:39:45.416: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401291361s
Jun 28 20:39:47.657: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.642084792s
STEP: Saw pod success
Jun 28 20:39:47.657: INFO: Pod "client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf" satisfied condition "Succeeded or Failed"
Jun 28 20:39:47.890: INFO: Trying to get logs from node 10.244.0.29 pod client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:39:49.994: INFO: Waiting for pod client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf to disappear
Jun 28 20:39:50.168: INFO: Pod client-containers-16a9db7a-8040-4f83-8d1e-07414ef3dbbf no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:39:50.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-639" for this suite.

• [SLOW TEST:16.372 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:39:50.671: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 28 20:39:51.607: INFO: Waiting up to 5m0s for pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163" in namespace "emptydir-4543" to be "Succeeded or Failed"
Jun 28 20:39:51.740: INFO: Pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163": Phase="Pending", Reason="", readiness=false. Elapsed: 133.349832ms
Jun 28 20:39:54.025: INFO: Pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418067433s
Jun 28 20:39:56.116: INFO: Pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163": Phase="Pending", Reason="", readiness=false. Elapsed: 4.50890061s
Jun 28 20:39:58.310: INFO: Pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.703011317s
STEP: Saw pod success
Jun 28 20:39:58.310: INFO: Pod "pod-dca82b1c-e277-40a3-9529-d175f0c51163" satisfied condition "Succeeded or Failed"
Jun 28 20:39:58.515: INFO: Trying to get logs from node 10.244.0.29 pod pod-dca82b1c-e277-40a3-9529-d175f0c51163 container test-container: <nil>
STEP: delete the pod
Jun 28 20:39:59.604: INFO: Waiting for pod pod-dca82b1c-e277-40a3-9529-d175f0c51163 to disappear
Jun 28 20:39:59.822: INFO: Pod pod-dca82b1c-e277-40a3-9529-d175f0c51163 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:39:59.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4543" for this suite.

• [SLOW TEST:10.825 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3519,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:40:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:40:04.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc" in namespace "projected-149" to be "Succeeded or Failed"
Jun 28 20:40:04.615: INFO: Pod "downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc": Phase="Pending", Reason="", readiness=false. Elapsed: 295.448283ms
Jun 28 20:40:07.843: INFO: Pod "downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 3.523377509s
STEP: Saw pod success
Jun 28 20:40:07.843: INFO: Pod "downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc" satisfied condition "Succeeded or Failed"
Jun 28 20:40:08.070: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc container client-container: <nil>
STEP: delete the pod
Jun 28 20:40:09.340: INFO: Waiting for pod downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc to disappear
Jun 28 20:40:09.628: INFO: Pod downwardapi-volume-55a87f0b-19a6-473d-9c5a-5497ff0762bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:40:09.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-149" for this suite.

• [SLOW TEST:9.786 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:40:11.283: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:40:12.362: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:40:22.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6806" for this suite.

• [SLOW TEST:12.241 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":210,"skipped":3587,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:40:23.523: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-173
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-173
I0628 20:40:24.947321      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-173, replica count: 2
I0628 20:40:28.147672      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:40:31.147840      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:40:34.147980      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:40:37.148141      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 20:40:40.148492      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 20:40:40.148: INFO: Creating new exec pod
Jun 28 20:40:45.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-173 exec execpod2qrrw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 28 20:40:53.670: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 20:40:53.670: INFO: stdout: ""
Jun 28 20:40:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-173 exec execpod2qrrw -- /bin/sh -x -c nc -zv -t -w 2 172.21.52.97 80'
Jun 28 20:40:56.762: INFO: stderr: "+ nc -zv -t -w 2 172.21.52.97 80\nConnection to 172.21.52.97 80 port [tcp/http] succeeded!\n"
Jun 28 20:40:56.762: INFO: stdout: ""
Jun 28 20:40:56.762: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:40:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-173" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:34.252 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":211,"skipped":3592,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:40:57.776: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-eca4ef00-8b47-4704-9f9e-d5f53e7e9154
STEP: Creating a pod to test consume secrets
Jun 28 20:40:58.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99" in namespace "projected-7396" to be "Succeeded or Failed"
Jun 28 20:40:58.991: INFO: Pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99": Phase="Pending", Reason="", readiness=false. Elapsed: 131.129682ms
Jun 28 20:41:01.213: INFO: Pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352803342s
Jun 28 20:41:03.527: INFO: Pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666967277s
Jun 28 20:41:05.722: INFO: Pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.861295009s
STEP: Saw pod success
Jun 28 20:41:05.722: INFO: Pod "pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99" satisfied condition "Succeeded or Failed"
Jun 28 20:41:05.856: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 20:41:06.826: INFO: Waiting for pod pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99 to disappear
Jun 28 20:41:07.085: INFO: Pod pod-projected-secrets-2284df18-7eff-443b-bf8c-159aeab9dc99 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:41:07.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7396" for this suite.

• [SLOW TEST:11.266 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:41:09.042: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Jun 28 20:41:11.421: INFO: created test-podtemplate-1
Jun 28 20:41:11.805: INFO: created test-podtemplate-2
Jun 28 20:41:12.267: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 28 20:41:12.710: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 28 20:41:13.288: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:41:13.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1495" for this suite.

• [SLOW TEST:7.800 seconds]
[sig-node] PodTemplates
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/podtemplates.go:41
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":213,"skipped":3633,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:41:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-9779b5a9-8297-4766-899e-0d76982497a5
STEP: Creating a pod to test consume secrets
Jun 28 20:41:19.058: INFO: Waiting up to 5m0s for pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8" in namespace "secrets-9182" to be "Succeeded or Failed"
Jun 28 20:41:19.198: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 140.567401ms
Jun 28 20:41:21.503: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.445296748s
Jun 28 20:41:23.717: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.659420753s
Jun 28 20:41:25.980: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.9219461s
Jun 28 20:41:28.098: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.040430784s
Jun 28 20:41:30.232: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.174640102s
STEP: Saw pod success
Jun 28 20:41:30.232: INFO: Pod "pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8" satisfied condition "Succeeded or Failed"
Jun 28 20:41:30.378: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 20:41:31.297: INFO: Waiting for pod pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8 to disappear
Jun 28 20:41:31.442: INFO: Pod pod-secrets-2e015ef6-1a78-4f70-9c93-92610da6d9c8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:41:31.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9182" for this suite.

• [SLOW TEST:15.392 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3654,"failed":0}
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:41:32.235: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-4479
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4479
STEP: Deleting pre-stop pod
Jun 28 20:41:57.153: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:41:57.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4479" for this suite.

• [SLOW TEST:29.402 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":215,"skipped":3655,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:42:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 20:42:05.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:42:07.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:42:09.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:42:11.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:42:13.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:42:16.318: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760509724, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 20:42:18.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:42:18.787: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-834-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:42:33.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3845" for this suite.
STEP: Destroying namespace "webhook-3845-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:34.725 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":216,"skipped":3659,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:42:36.362: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:42:36.663: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:42:42.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1532" for this suite.

• [SLOW TEST:9.033 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:42:45.396: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-e53364f8-ec8b-4fb5-a0ed-0dbaf6b5ad42
STEP: Creating a pod to test consume secrets
Jun 28 20:42:47.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e" in namespace "projected-6073" to be "Succeeded or Failed"
Jun 28 20:42:47.715: INFO: Pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e": Phase="Pending", Reason="", readiness=false. Elapsed: 313.138022ms
Jun 28 20:42:49.946: INFO: Pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.544757364s
Jun 28 20:42:52.101: INFO: Pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699109772s
Jun 28 20:42:54.258: INFO: Pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.856061034s
STEP: Saw pod success
Jun 28 20:42:54.258: INFO: Pod "pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e" satisfied condition "Succeeded or Failed"
Jun 28 20:42:54.435: INFO: Trying to get logs from node 10.244.0.30 pod pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 20:42:56.694: INFO: Waiting for pod pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e to disappear
Jun 28 20:42:57.018: INFO: Pod pod-projected-secrets-69d306c8-c3cc-4d86-b4cd-7d9055a5c05e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:42:57.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6073" for this suite.

• [SLOW TEST:13.855 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3685,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:42:59.252: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-1188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1188 to expose endpoints map[]
Jun 28 20:43:08.320: INFO: successfully validated that service endpoint-test2 in namespace services-1188 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1188 to expose endpoints map[pod1:[80]]
Jun 28 20:43:14.470: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Jun 28 20:43:17.084: INFO: successfully validated that service endpoint-test2 in namespace services-1188 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1188 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 28 20:43:24.280: INFO: Unexpected endpoints: found map[f04d2eff-52ca-4074-9429-72e43b1a3be3:[80]], expected map[pod1:[80] pod2:[80]], will retry
Jun 28 20:43:29.240: INFO: Unexpected endpoints: found map[f04d2eff-52ca-4074-9429-72e43b1a3be3:[80]], expected map[pod1:[80] pod2:[80]], will retry
Jun 28 20:43:30.745: INFO: successfully validated that service endpoint-test2 in namespace services-1188 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1188 to expose endpoints map[pod2:[80]]
Jun 28 20:43:32.852: INFO: successfully validated that service endpoint-test2 in namespace services-1188 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1188 to expose endpoints map[]
Jun 28 20:43:34.174: INFO: successfully validated that service endpoint-test2 in namespace services-1188 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:43:34.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1188" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:37.523 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":219,"skipped":3695,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:43:36.775: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 28 20:43:39.502: INFO: Waiting up to 5m0s for pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae" in namespace "downward-api-4181" to be "Succeeded or Failed"
Jun 28 20:43:39.735: INFO: Pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 233.187754ms
Jun 28 20:43:42.021: INFO: Pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518555949s
Jun 28 20:43:44.146: INFO: Pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64414691s
Jun 28 20:43:46.508: INFO: Pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.005412579s
STEP: Saw pod success
Jun 28 20:43:46.508: INFO: Pod "downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae" satisfied condition "Succeeded or Failed"
Jun 28 20:43:46.915: INFO: Trying to get logs from node 10.244.0.29 pod downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae container dapi-container: <nil>
STEP: delete the pod
Jun 28 20:43:51.183: INFO: Waiting for pod downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae to disappear
Jun 28 20:43:51.301: INFO: Pod downward-api-49d2df56-6b01-484f-aacb-dfe0ed12b8ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:43:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4181" for this suite.

• [SLOW TEST:15.677 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:43:52.453: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:44:58.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5773" for this suite.

• [SLOW TEST:67.055 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":221,"skipped":3750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:44:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 28 20:45:00.519: INFO: Waiting up to 5m0s for pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039" in namespace "emptydir-3733" to be "Succeeded or Failed"
Jun 28 20:45:00.665: INFO: Pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039": Phase="Pending", Reason="", readiness=false. Elapsed: 146.361986ms
Jun 28 20:45:02.960: INFO: Pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441582445s
Jun 28 20:45:05.295: INFO: Pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77665884s
Jun 28 20:45:07.402: INFO: Pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.883531802s
STEP: Saw pod success
Jun 28 20:45:07.402: INFO: Pod "pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039" satisfied condition "Succeeded or Failed"
Jun 28 20:45:07.500: INFO: Trying to get logs from node 10.244.0.29 pod pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039 container test-container: <nil>
STEP: delete the pod
Jun 28 20:45:08.365: INFO: Waiting for pod pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039 to disappear
Jun 28 20:45:08.498: INFO: Pod pod-1e0a6cb7-acd3-487b-8cde-cb0eb0dbd039 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:45:08.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3733" for this suite.

• [SLOW TEST:9.513 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3789,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:45:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:45:09.692: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:45:18.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5149" for this suite.

• [SLOW TEST:10.409 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":223,"skipped":3800,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:45:19.430: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2780.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2780.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2780.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2780.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 20:45:32.280: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local from pod dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1: the server could not find the requested resource (get pods dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1)
Jun 28 20:45:32.902: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local from pod dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1: the server could not find the requested resource (get pods dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1)
Jun 28 20:45:33.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2780.svc.cluster.local from pod dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1: the server could not find the requested resource (get pods dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1)
Jun 28 20:45:34.169: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2780.svc.cluster.local from pod dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1: the server could not find the requested resource (get pods dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1)
Jun 28 20:45:38.954: INFO: Lookups using dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2780.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2780.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2780.svc.cluster.local]

Jun 28 20:45:52.301: INFO: DNS probes using dns-2780/dns-test-51f63219-0f2d-46e0-b71b-1a75d38fafc1 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:45:52.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2780" for this suite.

• [SLOW TEST:34.714 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":224,"skipped":3800,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:45:54.145: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 28 20:48:00.100: INFO: Successfully updated pod "var-expansion-6db925dd-d087-48b8-aaad-4fab3c829d74"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 28 20:48:00.316: INFO: Deleting pod "var-expansion-6db925dd-d087-48b8-aaad-4fab3c829d74" in namespace "var-expansion-2471"
Jun 28 20:48:00.481: INFO: Wait up to 5m0s for pod "var-expansion-6db925dd-d087-48b8-aaad-4fab3c829d74" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:48:43.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2471" for this suite.

• [SLOW TEST:175.221 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":225,"skipped":3808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:48:49.366: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:48:56.693: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 28 20:48:57.239: INFO: Number of nodes with available pods: 0
Jun 28 20:48:57.239: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 28 20:48:59.935: INFO: Number of nodes with available pods: 0
Jun 28 20:48:59.935: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:01.348: INFO: Number of nodes with available pods: 0
Jun 28 20:49:01.348: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:02.398: INFO: Number of nodes with available pods: 0
Jun 28 20:49:02.398: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:03.250: INFO: Number of nodes with available pods: 0
Jun 28 20:49:03.250: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:04.338: INFO: Number of nodes with available pods: 0
Jun 28 20:49:04.338: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:05.260: INFO: Number of nodes with available pods: 0
Jun 28 20:49:05.260: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:06.228: INFO: Number of nodes with available pods: 0
Jun 28 20:49:06.228: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:07.196: INFO: Number of nodes with available pods: 0
Jun 28 20:49:07.196: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:08.185: INFO: Number of nodes with available pods: 0
Jun 28 20:49:08.185: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:09.255: INFO: Number of nodes with available pods: 0
Jun 28 20:49:09.255: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:10.212: INFO: Number of nodes with available pods: 0
Jun 28 20:49:10.212: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:11.169: INFO: Number of nodes with available pods: 0
Jun 28 20:49:11.169: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:12.138: INFO: Number of nodes with available pods: 0
Jun 28 20:49:12.138: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:13.212: INFO: Number of nodes with available pods: 0
Jun 28 20:49:13.212: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:14.401: INFO: Number of nodes with available pods: 0
Jun 28 20:49:14.401: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:15.458: INFO: Number of nodes with available pods: 0
Jun 28 20:49:15.458: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:16.490: INFO: Number of nodes with available pods: 1
Jun 28 20:49:16.490: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 28 20:49:26.737: INFO: Number of nodes with available pods: 0
Jun 28 20:49:26.737: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 28 20:49:28.454: INFO: Number of nodes with available pods: 0
Jun 28 20:49:28.454: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:30.143: INFO: Number of nodes with available pods: 0
Jun 28 20:49:30.143: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:30.964: INFO: Number of nodes with available pods: 0
Jun 28 20:49:30.964: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:31.651: INFO: Number of nodes with available pods: 0
Jun 28 20:49:31.651: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:32.641: INFO: Number of nodes with available pods: 0
Jun 28 20:49:32.641: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:33.718: INFO: Number of nodes with available pods: 0
Jun 28 20:49:33.718: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:34.690: INFO: Number of nodes with available pods: 0
Jun 28 20:49:34.690: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:35.799: INFO: Number of nodes with available pods: 0
Jun 28 20:49:35.799: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:36.842: INFO: Number of nodes with available pods: 0
Jun 28 20:49:36.842: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:37.730: INFO: Number of nodes with available pods: 0
Jun 28 20:49:37.730: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 20:49:38.677: INFO: Number of nodes with available pods: 1
Jun 28 20:49:38.677: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-354, will wait for the garbage collector to delete the pods
Jun 28 20:49:39.789: INFO: Deleting DaemonSet.extensions daemon-set took: 192.652443ms
Jun 28 20:49:40.289: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.176465ms
Jun 28 20:49:45.820: INFO: Number of nodes with available pods: 0
Jun 28 20:49:45.820: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 20:49:45.937: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-354/daemonsets","resourceVersion":"113203"},"items":null}

Jun 28 20:49:46.045: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-354/pods","resourceVersion":"113203"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:49:47.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-354" for this suite.

• [SLOW TEST:58.750 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":226,"skipped":3832,"failed":0}
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:49:48.117: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Jun 28 20:49:51.196: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0628 20:49:51.196944      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 20:49:51.196969      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 20:49:51.196972      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:49:51.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9595" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":227,"skipped":3832,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:49:51.713: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 28 20:49:59.228: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-376 PodName:var-expansion-4f09c21b-a90a-441b-bc2d-249501a48c5f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:49:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: test for file in mounted path
Jun 28 20:50:03.271: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-376 PodName:var-expansion-4f09c21b-a90a-441b-bc2d-249501a48c5f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 20:50:03.271: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: updating the annotation value
Jun 28 20:50:11.454: INFO: Successfully updated pod "var-expansion-4f09c21b-a90a-441b-bc2d-249501a48c5f"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 28 20:50:11.788: INFO: Deleting pod "var-expansion-4f09c21b-a90a-441b-bc2d-249501a48c5f" in namespace "var-expansion-376"
Jun 28 20:50:12.158: INFO: Wait up to 5m0s for pod "var-expansion-4f09c21b-a90a-441b-bc2d-249501a48c5f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:50:54.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-376" for this suite.

• [SLOW TEST:64.201 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":228,"skipped":3832,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:50:55.914: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Jun 28 20:50:56.899: INFO: Waiting up to 5m0s for pod "var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2" in namespace "var-expansion-5616" to be "Succeeded or Failed"
Jun 28 20:50:56.999: INFO: Pod "var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 99.406015ms
Jun 28 20:50:59.059: INFO: Pod "var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159175872s
Jun 28 20:51:01.303: INFO: Pod "var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.403651566s
STEP: Saw pod success
Jun 28 20:51:01.303: INFO: Pod "var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2" satisfied condition "Succeeded or Failed"
Jun 28 20:51:01.517: INFO: Trying to get logs from node 10.244.0.29 pod var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2 container dapi-container: <nil>
STEP: delete the pod
Jun 28 20:51:02.722: INFO: Waiting for pod var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2 to disappear
Jun 28 20:51:03.047: INFO: Pod var-expansion-83850d81-dd94-418e-b312-fa09e6a84dd2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:51:03.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5616" for this suite.

• [SLOW TEST:8.435 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":3843,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:51:04.349: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:51:05.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c" in namespace "projected-1235" to be "Succeeded or Failed"
Jun 28 20:51:05.471: INFO: Pod "downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c": Phase="Pending", Reason="", readiness=false. Elapsed: 116.12438ms
Jun 28 20:51:07.747: INFO: Pod "downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392107508s
Jun 28 20:51:10.165: INFO: Pod "downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.810175005s
STEP: Saw pod success
Jun 28 20:51:10.165: INFO: Pod "downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c" satisfied condition "Succeeded or Failed"
Jun 28 20:51:10.529: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c container client-container: <nil>
STEP: delete the pod
Jun 28 20:51:12.476: INFO: Waiting for pod downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c to disappear
Jun 28 20:51:12.781: INFO: Pod downwardapi-volume-45dd801a-0c1a-4284-b249-105b85edf42c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:51:12.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1235" for this suite.

• [SLOW TEST:9.999 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:51:14.348: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:51:28.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3593" for this suite.

• [SLOW TEST:15.325 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3870,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:51:29.673: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-325, will wait for the garbage collector to delete the pods
Jun 28 20:51:46.752: INFO: Deleting Job.batch foo took: 719.702513ms
Jun 28 20:51:48.052: INFO: Terminating Job.batch foo pods took: 1.300185257s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:52:31.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-325" for this suite.

• [SLOW TEST:62.944 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":232,"skipped":3878,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:52:32.618: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:52:33.338: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:52:34.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8676" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":233,"skipped":3887,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:52:35.766: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:52:36.888: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb" in namespace "downward-api-4360" to be "Succeeded or Failed"
Jun 28 20:52:37.155: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 267.004841ms
Jun 28 20:52:39.484: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595293252s
Jun 28 20:52:41.763: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.874389781s
Jun 28 20:52:44.152: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.263669363s
Jun 28 20:52:46.407: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.518160602s
Jun 28 20:52:48.641: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.752862799s
STEP: Saw pod success
Jun 28 20:52:48.642: INFO: Pod "downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb" satisfied condition "Succeeded or Failed"
Jun 28 20:52:48.837: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb container client-container: <nil>
STEP: delete the pod
Jun 28 20:52:49.980: INFO: Waiting for pod downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb to disappear
Jun 28 20:52:50.218: INFO: Pod downwardapi-volume-21df7f0d-7384-45f8-89a9-69db1f4dcfbb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:52:50.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4360" for this suite.

• [SLOW TEST:15.792 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":3889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:52:51.563: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:53:08.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5557" for this suite.

• [SLOW TEST:17.545 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":235,"skipped":3944,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:53:09.108: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 28 20:53:09.433: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:53:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8401" for this suite.

• [SLOW TEST:14.957 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":3949,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:53:24.065: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5de16486-ee76-4535-b6e0-3ba23ed7d3ab
STEP: Creating a pod to test consume configMaps
Jun 28 20:53:26.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0" in namespace "configmap-962" to be "Succeeded or Failed"
Jun 28 20:53:26.467: INFO: Pod "pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0": Phase="Pending", Reason="", readiness=false. Elapsed: 325.406505ms
Jun 28 20:53:28.756: INFO: Pod "pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.614026525s
Jun 28 20:53:31.066: INFO: Pod "pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.924566043s
STEP: Saw pod success
Jun 28 20:53:31.066: INFO: Pod "pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0" satisfied condition "Succeeded or Failed"
Jun 28 20:53:31.385: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:53:32.180: INFO: Waiting for pod pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0 to disappear
Jun 28 20:53:32.406: INFO: Pod pod-configmaps-19b0cd19-9296-499d-bf8f-23467b3303c0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:53:32.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-962" for this suite.

• [SLOW TEST:9.624 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":3974,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:53:33.689: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun 28 20:53:35.483: INFO: observed Pod pod-test in namespace pods-6026 in phase Pending conditions []
Jun 28 20:53:35.487: INFO: observed Pod pod-test in namespace pods-6026 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  }]
Jun 28 20:53:35.493: INFO: observed Pod pod-test in namespace pods-6026 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  }]
Jun 28 20:53:39.158: INFO: observed Pod pod-test in namespace pods-6026 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  }]
Jun 28 20:53:39.511: INFO: observed Pod pod-test in namespace pods-6026 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 20:53:34 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun 28 20:53:40.734: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun 28 20:53:43.188: INFO: observed event type ADDED
Jun 28 20:53:43.211: INFO: observed event type MODIFIED
Jun 28 20:53:43.296: INFO: observed event type MODIFIED
Jun 28 20:53:43.296: INFO: observed event type MODIFIED
Jun 28 20:53:43.297: INFO: observed event type MODIFIED
Jun 28 20:53:43.501: INFO: observed event type MODIFIED
Jun 28 20:53:43.501: INFO: observed event type MODIFIED
Jun 28 20:53:43.501: INFO: observed event type MODIFIED
Jun 28 20:53:43.501: INFO: observed event type MODIFIED
Jun 28 20:53:43.827: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:53:43.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6026" for this suite.

• [SLOW TEST:11.730 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":238,"skipped":3982,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:53:45.419: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-jh66
STEP: Creating a pod to test atomic-volume-subpath
Jun 28 20:53:47.594: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jh66" in namespace "subpath-1437" to be "Succeeded or Failed"
Jun 28 20:53:47.849: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Pending", Reason="", readiness=false. Elapsed: 254.936157ms
Jun 28 20:53:50.058: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464108564s
Jun 28 20:53:52.404: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810028896s
Jun 28 20:53:54.941: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 7.346988265s
Jun 28 20:53:57.345: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 9.751629214s
Jun 28 20:53:59.806: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 12.212629286s
Jun 28 20:54:02.253: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 14.659337719s
Jun 28 20:54:06.324: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 18.73019419s
Jun 28 20:54:08.560: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 20.966180096s
Jun 28 20:54:10.839: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 23.244749688s
Jun 28 20:54:13.225: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Running", Reason="", readiness=true. Elapsed: 25.631354302s
Jun 28 20:54:15.844: INFO: Pod "pod-subpath-test-downwardapi-jh66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.250109194s
STEP: Saw pod success
Jun 28 20:54:15.844: INFO: Pod "pod-subpath-test-downwardapi-jh66" satisfied condition "Succeeded or Failed"
Jun 28 20:54:16.276: INFO: Trying to get logs from node 10.244.0.29 pod pod-subpath-test-downwardapi-jh66 container test-container-subpath-downwardapi-jh66: <nil>
STEP: delete the pod
Jun 28 20:54:18.498: INFO: Waiting for pod pod-subpath-test-downwardapi-jh66 to disappear
Jun 28 20:54:18.931: INFO: Pod pod-subpath-test-downwardapi-jh66 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jh66
Jun 28 20:54:18.931: INFO: Deleting pod "pod-subpath-test-downwardapi-jh66" in namespace "subpath-1437"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:54:19.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1437" for this suite.

• [SLOW TEST:35.545 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":239,"skipped":4002,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:54:20.965: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 28 20:54:22.435: INFO: Waiting up to 5m0s for pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c" in namespace "downward-api-9152" to be "Succeeded or Failed"
Jun 28 20:54:22.640: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Pending", Reason="", readiness=false. Elapsed: 205.139033ms
Jun 28 20:54:24.851: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415781059s
Jun 28 20:54:27.119: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.684018928s
Jun 28 20:54:29.458: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.022675764s
Jun 28 20:54:31.827: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391303513s
Jun 28 20:54:34.281: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.846198702s
STEP: Saw pod success
Jun 28 20:54:34.281: INFO: Pod "downward-api-fdc12390-4587-4723-9f3d-7694978e868c" satisfied condition "Succeeded or Failed"
Jun 28 20:54:34.790: INFO: Trying to get logs from node 10.244.0.29 pod downward-api-fdc12390-4587-4723-9f3d-7694978e868c container dapi-container: <nil>
STEP: delete the pod
Jun 28 20:54:36.547: INFO: Waiting for pod downward-api-fdc12390-4587-4723-9f3d-7694978e868c to disappear
Jun 28 20:54:37.998: INFO: Pod downward-api-fdc12390-4587-4723-9f3d-7694978e868c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:54:37.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9152" for this suite.

• [SLOW TEST:20.333 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4008,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:54:41.298: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0628 20:54:53.143231      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0628 20:54:53.143251      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0628 20:54:53.143258      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 28 20:54:53.143: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:54:53.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3184" for this suite.

• [SLOW TEST:13.776 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":241,"skipped":4010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:54:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 20:54:58.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35" in namespace "downward-api-4686" to be "Succeeded or Failed"
Jun 28 20:54:58.533: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 462.460224ms
Jun 28 20:55:00.783: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712813068s
Jun 28 20:55:02.890: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819714382s
Jun 28 20:55:05.118: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04749794s
Jun 28 20:55:07.381: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 9.31110644s
Jun 28 20:55:09.735: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Pending", Reason="", readiness=false. Elapsed: 11.664256968s
Jun 28 20:55:12.090: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.019815942s
STEP: Saw pod success
Jun 28 20:55:12.090: INFO: Pod "downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35" satisfied condition "Succeeded or Failed"
Jun 28 20:55:12.282: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35 container client-container: <nil>
STEP: delete the pod
Jun 28 20:55:13.125: INFO: Waiting for pod downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35 to disappear
Jun 28 20:55:13.307: INFO: Pod downwardapi-volume-e190a25b-94c9-45ef-9de2-72fb94198f35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:55:13.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4686" for this suite.

• [SLOW TEST:18.888 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:55:13.963: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-69d18911-58e7-4804-ad40-9ee86f892800
STEP: Creating a pod to test consume configMaps
Jun 28 20:55:14.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6" in namespace "projected-198" to be "Succeeded or Failed"
Jun 28 20:55:14.990: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 44.422988ms
Jun 28 20:55:17.186: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240388185s
Jun 28 20:55:19.440: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493984108s
Jun 28 20:55:21.753: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.807010644s
Jun 28 20:55:23.952: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.0064349s
STEP: Saw pod success
Jun 28 20:55:23.952: INFO: Pod "pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6" satisfied condition "Succeeded or Failed"
Jun 28 20:55:24.123: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 20:55:24.800: INFO: Waiting for pod pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6 to disappear
Jun 28 20:55:24.917: INFO: Pod pod-projected-configmaps-2b8d4fb2-d69f-4c3d-97d3-53a84643d9e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:55:24.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-198" for this suite.

• [SLOW TEST:11.587 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4061,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:55:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:55:34.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8971" for this suite.

• [SLOW TEST:10.105 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":244,"skipped":4070,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:55:35.655: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-31d12c58-8698-4608-aecd-13c88724445b
STEP: Creating a pod to test consume secrets
Jun 28 20:55:37.229: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd" in namespace "projected-6700" to be "Succeeded or Failed"
Jun 28 20:55:37.512: INFO: Pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 282.300854ms
Jun 28 20:55:39.803: INFO: Pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.573871686s
Jun 28 20:55:42.075: INFO: Pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845133304s
Jun 28 20:55:44.543: INFO: Pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.313621s
STEP: Saw pod success
Jun 28 20:55:44.543: INFO: Pod "pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd" satisfied condition "Succeeded or Failed"
Jun 28 20:55:44.985: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 20:55:47.599: INFO: Waiting for pod pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd to disappear
Jun 28 20:55:47.863: INFO: Pod pod-projected-secrets-c9255159-2c4b-4dfe-a7c1-e2f162176ffd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:55:47.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6700" for this suite.

• [SLOW TEST:13.068 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4084,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:55:48.723: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 20:55:53.036: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:55:55.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:55:57.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:55:59.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:56:01.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:56:03.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:56:05.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510552, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510551, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 20:56:08.686: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:56:12.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-879" for this suite.
STEP: Destroying namespace "webhook-879-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:29.616 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":246,"skipped":4094,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:56:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 20:56:26.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510585, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 20:56:30.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510585, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760510584, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 20:56:33.837: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:56:37.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3307" for this suite.
STEP: Destroying namespace "webhook-3307-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:23.579 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":247,"skipped":4108,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:56:41.918: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:56:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-500" for this suite.

• [SLOW TEST:15.190 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4122,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:56:57.108: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 20:56:57.847: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:56:58.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4147" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":249,"skipped":4122,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:57:00.382: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 28 20:57:01.672: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 28 20:58:15.950: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 20:58:27.416: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 20:59:45.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5347" for this suite.

• [SLOW TEST:166.144 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":250,"skipped":4134,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 20:59:46.526: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 28 20:59:47.730: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:00:07.488: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:01:24.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6781" for this suite.

• [SLOW TEST:98.427 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":251,"skipped":4137,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:01:24.953: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7745
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7745
STEP: creating replication controller externalsvc in namespace services-7745
I0628 21:01:26.054279      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7745, replica count: 2
I0628 21:01:29.204728      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 21:01:32.204905      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 28 21:01:32.847: INFO: Creating new exec pod
Jun 28 21:01:41.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=services-7745 exec execpod476fc -- /bin/sh -x -c nslookup clusterip-service.services-7745.svc.cluster.local'
Jun 28 21:01:49.554: INFO: stderr: "+ nslookup clusterip-service.services-7745.svc.cluster.local\n"
Jun 28 21:01:49.554: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7745.svc.cluster.local\tcanonical name = externalsvc.services-7745.svc.cluster.local.\nName:\texternalsvc.services-7745.svc.cluster.local\nAddress: 172.21.61.139\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7745, will wait for the garbage collector to delete the pods
Jun 28 21:01:51.065: INFO: Deleting ReplicationController externalsvc took: 730.076529ms
Jun 28 21:01:52.765: INFO: Terminating ReplicationController externalsvc pods took: 1.700173359s
Jun 28 21:02:12.340: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:02:12.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7745" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:51.352 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":252,"skipped":4140,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:02:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6435
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 21:02:17.549: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 21:02:21.834: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:02:24.109: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:02:26.124: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:02:28.265: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:02:30.192: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:02:31.956: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:02:33.948: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:02:36.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:02:37.956: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 21:02:38.163: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 28 21:02:40.287: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 21:02:40.494: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 28 21:02:42.687: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 21:02:48.077: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 21:02:48.077: INFO: Breadth first check of 172.17.113.144 on host 10.244.0.29...
Jun 28 21:02:48.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.136:9080/dial?request=hostname&protocol=http&host=172.17.113.144&port=8080&tries=1'] Namespace:pod-network-test-6435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:02:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:02:51.441: INFO: Waiting for responses: map[]
Jun 28 21:02:51.441: INFO: reached 172.17.113.144 after 0/1 tries
Jun 28 21:02:51.441: INFO: Breadth first check of 172.17.74.212 on host 10.244.0.30...
Jun 28 21:02:51.545: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.136:9080/dial?request=hostname&protocol=http&host=172.17.74.212&port=8080&tries=1'] Namespace:pod-network-test-6435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:02:51.545: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:02:55.219: INFO: Waiting for responses: map[]
Jun 28 21:02:55.219: INFO: reached 172.17.74.212 after 0/1 tries
Jun 28 21:02:55.219: INFO: Breadth first check of 172.17.65.228 on host 10.244.0.31...
Jun 28 21:02:55.574: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.136:9080/dial?request=hostname&protocol=http&host=172.17.65.228&port=8080&tries=1'] Namespace:pod-network-test-6435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:02:55.574: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:02:59.958: INFO: Waiting for responses: map[]
Jun 28 21:02:59.958: INFO: reached 172.17.65.228 after 0/1 tries
Jun 28 21:02:59.958: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:02:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6435" for this suite.

• [SLOW TEST:45.898 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:03:02.204: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 28 21:03:04.669: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:03:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:04:25.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6700" for this suite.

• [SLOW TEST:84.474 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":254,"skipped":4179,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:04:26.678: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:04:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1988" for this suite.

• [SLOW TEST:5.385 seconds]
[sig-api-machinery] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/events.go:38
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":255,"skipped":4180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:04:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 21:04:34.282: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47" in namespace "security-context-test-2043" to be "Succeeded or Failed"
Jun 28 21:04:34.643: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47": Phase="Pending", Reason="", readiness=false. Elapsed: 361.016633ms
Jun 28 21:04:36.932: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.650420583s
Jun 28 21:04:39.214: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931933137s
Jun 28 21:04:41.500: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218137839s
Jun 28 21:04:43.626: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.3441895s
Jun 28 21:04:43.626: INFO: Pod "busybox-user-65534-e7427dd3-4b55-4e64-adee-5aa176e98e47" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:04:43.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2043" for this suite.

• [SLOW TEST:11.999 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4241,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:04:44.063: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-adc3a362-3498-4345-821b-fed229898bf3
STEP: Creating a pod to test consume secrets
Jun 28 21:04:45.274: INFO: Waiting up to 5m0s for pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd" in namespace "secrets-8284" to be "Succeeded or Failed"
Jun 28 21:04:45.517: INFO: Pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 243.106926ms
Jun 28 21:04:47.644: INFO: Pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370553714s
Jun 28 21:04:49.707: INFO: Pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433168711s
Jun 28 21:04:51.985: INFO: Pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.711663113s
STEP: Saw pod success
Jun 28 21:04:51.985: INFO: Pod "pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd" satisfied condition "Succeeded or Failed"
Jun 28 21:04:52.240: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 21:04:55.340: INFO: Waiting for pod pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd to disappear
Jun 28 21:04:55.590: INFO: Pod pod-secrets-cc9a52d6-3203-4971-a66e-844df0534dbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:04:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8284" for this suite.

• [SLOW TEST:12.084 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:04:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4231
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 28 21:04:56.948: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 21:04:59.023: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:05:01.370: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:05:03.220: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:05:05.443: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:05:07.384: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:05:09.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:11.204: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:13.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:15.298: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:17.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:19.373: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:21.205: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:23.326: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 28 21:05:25.192: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 28 21:05:25.465: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 28 21:05:25.667: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 28 21:05:27.845: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 28 21:05:32.580: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 21:05:32.580: INFO: Breadth first check of 172.17.113.166 on host 10.244.0.29...
Jun 28 21:05:32.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.167:9080/dial?request=hostname&protocol=udp&host=172.17.113.166&port=8081&tries=1'] Namespace:pod-network-test-4231 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:05:32.724: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:05:33.550: INFO: Waiting for responses: map[]
Jun 28 21:05:33.550: INFO: reached 172.17.113.166 after 0/1 tries
Jun 28 21:05:33.550: INFO: Breadth first check of 172.17.74.210 on host 10.244.0.30...
Jun 28 21:05:33.757: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.167:9080/dial?request=hostname&protocol=udp&host=172.17.74.210&port=8081&tries=1'] Namespace:pod-network-test-4231 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:05:33.757: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:05:35.234: INFO: Waiting for responses: map[]
Jun 28 21:05:35.234: INFO: reached 172.17.74.210 after 0/1 tries
Jun 28 21:05:35.234: INFO: Breadth first check of 172.17.65.232 on host 10.244.0.31...
Jun 28 21:05:35.427: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.113.167:9080/dial?request=hostname&protocol=udp&host=172.17.65.232&port=8081&tries=1'] Namespace:pod-network-test-4231 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:05:35.427: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:05:36.823: INFO: Waiting for responses: map[]
Jun 28 21:05:36.824: INFO: reached 172.17.65.232 after 0/1 tries
Jun 28 21:05:36.824: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:05:36.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4231" for this suite.

• [SLOW TEST:41.225 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4281,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:05:37.373: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 21:05:38.207: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9599
I0628 21:05:38.447117      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9599, replica count: 1
I0628 21:05:39.697427      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 21:05:40.697576      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 21:05:41.697734      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 21:05:42.697888      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 21:05:43.015: INFO: Created: latency-svc-9fgxn
Jun 28 21:05:43.053: INFO: Got endpoints: latency-svc-9fgxn [155.259665ms]
Jun 28 21:05:43.215: INFO: Created: latency-svc-g79fr
Jun 28 21:05:43.215: INFO: Got endpoints: latency-svc-g79fr [161.323565ms]
Jun 28 21:05:43.229: INFO: Created: latency-svc-ndsr4
Jun 28 21:05:43.240: INFO: Got endpoints: latency-svc-ndsr4 [186.11209ms]
Jun 28 21:05:43.314: INFO: Created: latency-svc-5kw76
Jun 28 21:05:43.330: INFO: Got endpoints: latency-svc-5kw76 [276.305412ms]
Jun 28 21:05:43.330: INFO: Created: latency-svc-lld8h
Jun 28 21:05:43.340: INFO: Got endpoints: latency-svc-lld8h [285.806747ms]
Jun 28 21:05:43.340: INFO: Created: latency-svc-dfkdk
Jun 28 21:05:43.362: INFO: Created: latency-svc-tl6jw
Jun 28 21:05:43.363: INFO: Got endpoints: latency-svc-dfkdk [308.656735ms]
Jun 28 21:05:43.386: INFO: Created: latency-svc-74hxq
Jun 28 21:05:43.396: INFO: Got endpoints: latency-svc-tl6jw [342.640106ms]
Jun 28 21:05:43.408: INFO: Got endpoints: latency-svc-74hxq [354.204909ms]
Jun 28 21:05:43.421: INFO: Created: latency-svc-7b6mw
Jun 28 21:05:43.421: INFO: Created: latency-svc-2m654
Jun 28 21:05:43.431: INFO: Got endpoints: latency-svc-7b6mw [377.666342ms]
Jun 28 21:05:43.442: INFO: Got endpoints: latency-svc-2m654 [388.179228ms]
Jun 28 21:05:43.442: INFO: Created: latency-svc-fj9lb
Jun 28 21:05:43.453: INFO: Created: latency-svc-6dkrq
Jun 28 21:05:43.453: INFO: Got endpoints: latency-svc-fj9lb [399.710573ms]
Jun 28 21:05:43.454: INFO: Created: latency-svc-7gkbx
Jun 28 21:05:43.464: INFO: Got endpoints: latency-svc-6dkrq [410.733279ms]
Jun 28 21:05:43.478: INFO: Got endpoints: latency-svc-7gkbx [423.754072ms]
Jun 28 21:05:43.478: INFO: Created: latency-svc-shlwm
Jun 28 21:05:43.488: INFO: Created: latency-svc-9486t
Jun 28 21:05:43.488: INFO: Got endpoints: latency-svc-shlwm [434.568785ms]
Jun 28 21:05:43.500: INFO: Got endpoints: latency-svc-9486t [446.10705ms]
Jun 28 21:05:43.500: INFO: Created: latency-svc-nvf8b
Jun 28 21:05:43.500: INFO: Created: latency-svc-9ms6n
Jun 28 21:05:43.524: INFO: Created: latency-svc-fztd4
Jun 28 21:05:43.524: INFO: Got endpoints: latency-svc-9ms6n [469.740159ms]
Jun 28 21:05:43.524: INFO: Got endpoints: latency-svc-nvf8b [308.895276ms]
Jun 28 21:05:43.534: INFO: Created: latency-svc-gtczw
Jun 28 21:05:43.534: INFO: Got endpoints: latency-svc-fztd4 [294.399573ms]
Jun 28 21:05:43.545: INFO: Got endpoints: latency-svc-gtczw [215.205656ms]
Jun 28 21:05:43.569: INFO: Created: latency-svc-nfqj5
Jun 28 21:05:43.594: INFO: Created: latency-svc-hppc4
Jun 28 21:05:43.595: INFO: Got endpoints: latency-svc-nfqj5 [232.146113ms]
Jun 28 21:05:43.615: INFO: Created: latency-svc-7bzb7
Jun 28 21:05:43.615: INFO: Got endpoints: latency-svc-hppc4 [275.723628ms]
Jun 28 21:05:43.626: INFO: Created: latency-svc-rrmc2
Jun 28 21:05:43.649: INFO: Got endpoints: latency-svc-7bzb7 [241.358208ms]
Jun 28 21:05:43.660: INFO: Created: latency-svc-rs9zl
Jun 28 21:05:43.660: INFO: Got endpoints: latency-svc-rrmc2 [263.223522ms]
Jun 28 21:05:43.693: INFO: Created: latency-svc-g2cjr
Jun 28 21:05:43.693: INFO: Got endpoints: latency-svc-rs9zl [251.151838ms]
Jun 28 21:05:43.734: INFO: Created: latency-svc-bbcdc
Jun 28 21:05:43.734: INFO: Got endpoints: latency-svc-g2cjr [303.234345ms]
Jun 28 21:05:43.745: INFO: Created: latency-svc-hxfcj
Jun 28 21:05:43.766: INFO: Created: latency-svc-bvkxd
Jun 28 21:05:43.812: INFO: Created: latency-svc-kw46d
Jun 28 21:05:43.834: INFO: Got endpoints: latency-svc-bbcdc [381.267793ms]
Jun 28 21:05:43.835: INFO: Got endpoints: latency-svc-hxfcj [370.046103ms]
Jun 28 21:05:43.835: INFO: Got endpoints: latency-svc-kw46d [346.113168ms]
Jun 28 21:05:43.835: INFO: Got endpoints: latency-svc-bvkxd [357.10471ms]
Jun 28 21:05:43.844: INFO: Created: latency-svc-5rmrf
Jun 28 21:05:43.874: INFO: Got endpoints: latency-svc-5rmrf [374.144427ms]
Jun 28 21:05:43.874: INFO: Created: latency-svc-wgbzz
Jun 28 21:05:43.894: INFO: Got endpoints: latency-svc-wgbzz [370.21368ms]
Jun 28 21:05:43.917: INFO: Created: latency-svc-x2vhx
Jun 28 21:05:43.927: INFO: Created: latency-svc-ql2g6
Jun 28 21:05:43.927: INFO: Got endpoints: latency-svc-x2vhx [403.471635ms]
Jun 28 21:05:43.937: INFO: Got endpoints: latency-svc-ql2g6 [391.352583ms]
Jun 28 21:05:43.958: INFO: Created: latency-svc-rsj4h
Jun 28 21:05:43.978: INFO: Got endpoints: latency-svc-rsj4h [444.112357ms]
Jun 28 21:05:43.978: INFO: Created: latency-svc-p6nrt
Jun 28 21:05:44.012: INFO: Got endpoints: latency-svc-p6nrt [416.91308ms]
Jun 28 21:05:44.012: INFO: Created: latency-svc-q246t
Jun 28 21:05:44.021: INFO: Created: latency-svc-c4b57
Jun 28 21:05:44.022: INFO: Got endpoints: latency-svc-q246t [406.125603ms]
Jun 28 21:05:44.043: INFO: Got endpoints: latency-svc-c4b57 [388.028065ms]
Jun 28 21:05:44.043: INFO: Created: latency-svc-hcx4v
Jun 28 21:05:44.065: INFO: Created: latency-svc-hx2st
Jun 28 21:05:44.065: INFO: Got endpoints: latency-svc-hcx4v [405.015279ms]
Jun 28 21:05:44.075: INFO: Got endpoints: latency-svc-hx2st [381.49363ms]
Jun 28 21:05:44.085: INFO: Created: latency-svc-hm9j7
Jun 28 21:05:44.104: INFO: Created: latency-svc-5zklt
Jun 28 21:05:44.104: INFO: Got endpoints: latency-svc-hm9j7 [369.433291ms]
Jun 28 21:05:44.113: INFO: Created: latency-svc-rn6m9
Jun 28 21:05:44.121: INFO: Got endpoints: latency-svc-5zklt [286.71769ms]
Jun 28 21:05:44.130: INFO: Created: latency-svc-kwlb4
Jun 28 21:05:44.130: INFO: Got endpoints: latency-svc-rn6m9 [295.739602ms]
Jun 28 21:05:44.140: INFO: Created: latency-svc-q6cwg
Jun 28 21:05:44.140: INFO: Got endpoints: latency-svc-kwlb4 [305.331752ms]
Jun 28 21:05:44.163: INFO: Got endpoints: latency-svc-q6cwg [328.172782ms]
Jun 28 21:05:44.163: INFO: Created: latency-svc-ls524
Jun 28 21:05:44.174: INFO: Got endpoints: latency-svc-ls524 [279.807187ms]
Jun 28 21:05:44.174: INFO: Created: latency-svc-77ggc
Jun 28 21:05:44.186: INFO: Created: latency-svc-46q7t
Jun 28 21:05:44.198: INFO: Got endpoints: latency-svc-77ggc [324.499349ms]
Jun 28 21:05:44.209: INFO: Created: latency-svc-xw225
Jun 28 21:05:44.209: INFO: Got endpoints: latency-svc-46q7t [281.841656ms]
Jun 28 21:05:44.243: INFO: Got endpoints: latency-svc-xw225 [306.060623ms]
Jun 28 21:05:44.252: INFO: Created: latency-svc-jpnnx
Jun 28 21:05:44.262: INFO: Created: latency-svc-scjpm
Jun 28 21:05:44.283: INFO: Created: latency-svc-wrgjq
Jun 28 21:05:44.293: INFO: Got endpoints: latency-svc-scjpm [281.167732ms]
Jun 28 21:05:44.293: INFO: Got endpoints: latency-svc-jpnnx [314.564908ms]
Jun 28 21:05:44.305: INFO: Got endpoints: latency-svc-wrgjq [282.951983ms]
Jun 28 21:05:44.314: INFO: Created: latency-svc-nzhzz
Jun 28 21:05:44.315: INFO: Got endpoints: latency-svc-nzhzz [271.834173ms]
Jun 28 21:05:44.325: INFO: Created: latency-svc-62vvt
Jun 28 21:05:44.357: INFO: Created: latency-svc-2pcww
Jun 28 21:05:44.357: INFO: Got endpoints: latency-svc-62vvt [291.939486ms]
Jun 28 21:05:44.367: INFO: Got endpoints: latency-svc-2pcww [292.526689ms]
Jun 28 21:05:44.399: INFO: Created: latency-svc-4qnhj
Jun 28 21:05:44.411: INFO: Created: latency-svc-9567m
Jun 28 21:05:44.433: INFO: Got endpoints: latency-svc-4qnhj [302.426296ms]
Jun 28 21:05:44.442: INFO: Got endpoints: latency-svc-9567m [337.852677ms]
Jun 28 21:05:44.442: INFO: Created: latency-svc-6hkl2
Jun 28 21:05:44.464: INFO: Created: latency-svc-7w9sp
Jun 28 21:05:44.464: INFO: Got endpoints: latency-svc-6hkl2 [342.89596ms]
Jun 28 21:05:44.490: INFO: Got endpoints: latency-svc-7w9sp [349.52693ms]
Jun 28 21:05:44.490: INFO: Created: latency-svc-8s8qd
Jun 28 21:05:44.500: INFO: Created: latency-svc-5g86n
Jun 28 21:05:44.513: INFO: Got endpoints: latency-svc-8s8qd [349.765055ms]
Jun 28 21:05:44.525: INFO: Created: latency-svc-h7wk9
Jun 28 21:05:44.525: INFO: Got endpoints: latency-svc-5g86n [351.244032ms]
Jun 28 21:05:44.559: INFO: Created: latency-svc-kmzr5
Jun 28 21:05:44.560: INFO: Got endpoints: latency-svc-h7wk9 [361.008474ms]
Jun 28 21:05:44.568: INFO: Created: latency-svc-z6xxs
Jun 28 21:05:44.579: INFO: Got endpoints: latency-svc-kmzr5 [336.349024ms]
Jun 28 21:05:44.579: INFO: Got endpoints: latency-svc-z6xxs [370.114143ms]
Jun 28 21:05:44.591: INFO: Created: latency-svc-jt8kj
Jun 28 21:05:44.603: INFO: Created: latency-svc-b94lk
Jun 28 21:05:44.614: INFO: Got endpoints: latency-svc-jt8kj [321.547917ms]
Jun 28 21:05:44.625: INFO: Created: latency-svc-64cck
Jun 28 21:05:44.625: INFO: Got endpoints: latency-svc-b94lk [320.334746ms]
Jun 28 21:05:44.636: INFO: Got endpoints: latency-svc-64cck [343.096289ms]
Jun 28 21:05:44.636: INFO: Created: latency-svc-4sxgv
Jun 28 21:05:44.659: INFO: Created: latency-svc-5gz59
Jun 28 21:05:44.659: INFO: Got endpoints: latency-svc-4sxgv [344.203754ms]
Jun 28 21:05:44.670: INFO: Got endpoints: latency-svc-5gz59 [313.719916ms]
Jun 28 21:05:44.682: INFO: Created: latency-svc-kh5d9
Jun 28 21:05:44.682: INFO: Got endpoints: latency-svc-kh5d9 [314.436897ms]
Jun 28 21:05:44.771: INFO: Created: latency-svc-rcs4m
Jun 28 21:05:44.797: INFO: Created: latency-svc-4q4hz
Jun 28 21:05:44.797: INFO: Got endpoints: latency-svc-rcs4m [363.850824ms]
Jun 28 21:05:44.808: INFO: Got endpoints: latency-svc-4q4hz [366.022275ms]
Jun 28 21:05:44.808: INFO: Created: latency-svc-59nh7
Jun 28 21:05:44.832: INFO: Created: latency-svc-252r2
Jun 28 21:05:44.832: INFO: Got endpoints: latency-svc-59nh7 [342.232897ms]
Jun 28 21:05:44.843: INFO: Got endpoints: latency-svc-252r2 [378.905111ms]
Jun 28 21:05:44.855: INFO: Created: latency-svc-47t42
Jun 28 21:05:44.867: INFO: Created: latency-svc-jfb7x
Jun 28 21:05:44.867: INFO: Got endpoints: latency-svc-47t42 [307.598645ms]
Jun 28 21:05:44.879: INFO: Created: latency-svc-b7cgz
Jun 28 21:05:44.893: INFO: Created: latency-svc-ftgpk
Jun 28 21:05:44.894: INFO: Got endpoints: latency-svc-jfb7x [369.130517ms]
Jun 28 21:05:44.905: INFO: Got endpoints: latency-svc-b7cgz [391.987629ms]
Jun 28 21:05:44.917: INFO: Created: latency-svc-mk58c
Jun 28 21:05:44.917: INFO: Got endpoints: latency-svc-ftgpk [337.732662ms]
Jun 28 21:05:44.930: INFO: Got endpoints: latency-svc-mk58c [350.916322ms]
Jun 28 21:05:44.956: INFO: Created: latency-svc-sr4mt
Jun 28 21:05:44.968: INFO: Got endpoints: latency-svc-sr4mt [353.015459ms]
Jun 28 21:05:44.979: INFO: Created: latency-svc-xq7wr
Jun 28 21:05:45.015: INFO: Created: latency-svc-7tr8b
Jun 28 21:05:45.016: INFO: Created: latency-svc-4q244
Jun 28 21:05:45.029: INFO: Got endpoints: latency-svc-7tr8b [393.300185ms]
Jun 28 21:05:45.029: INFO: Got endpoints: latency-svc-xq7wr [404.467823ms]
Jun 28 21:05:45.041: INFO: Got endpoints: latency-svc-4q244 [381.68671ms]
Jun 28 21:05:45.082: INFO: Created: latency-svc-hlc8w
Jun 28 21:05:45.082: INFO: Created: latency-svc-bxccs
Jun 28 21:05:45.082: INFO: Got endpoints: latency-svc-bxccs [411.668278ms]
Jun 28 21:05:45.097: INFO: Got endpoints: latency-svc-hlc8w [414.869484ms]
Jun 28 21:05:45.109: INFO: Created: latency-svc-smqpb
Jun 28 21:05:45.125: INFO: Got endpoints: latency-svc-smqpb [328.53636ms]
Jun 28 21:05:45.126: INFO: Created: latency-svc-m6rpb
Jun 28 21:05:45.149: INFO: Got endpoints: latency-svc-m6rpb [341.641152ms]
Jun 28 21:05:45.228: INFO: Created: latency-svc-gtwvw
Jun 28 21:05:45.243: INFO: Created: latency-svc-gsz2r
Jun 28 21:05:45.243: INFO: Got endpoints: latency-svc-gtwvw [411.519035ms]
Jun 28 21:05:45.274: INFO: Created: latency-svc-pp96n
Jun 28 21:05:45.274: INFO: Got endpoints: latency-svc-gsz2r [343.765724ms]
Jun 28 21:05:45.288: INFO: Created: latency-svc-lj2rt
Jun 28 21:05:45.302: INFO: Got endpoints: latency-svc-pp96n [458.691313ms]
Jun 28 21:05:45.331: INFO: Created: latency-svc-m6tnc
Jun 28 21:05:45.344: INFO: Got endpoints: latency-svc-lj2rt [477.192879ms]
Jun 28 21:05:45.358: INFO: Got endpoints: latency-svc-m6tnc [464.218641ms]
Jun 28 21:05:45.371: INFO: Created: latency-svc-qfr9g
Jun 28 21:05:45.384: INFO: Got endpoints: latency-svc-qfr9g [467.492921ms]
Jun 28 21:05:45.384: INFO: Created: latency-svc-c2r9z
Jun 28 21:05:45.385: INFO: Got endpoints: latency-svc-c2r9z [479.865241ms]
Jun 28 21:05:45.425: INFO: Created: latency-svc-98grt
Jun 28 21:05:45.483: INFO: Created: latency-svc-hvg22
Jun 28 21:05:45.483: INFO: Got endpoints: latency-svc-98grt [515.071377ms]
Jun 28 21:05:45.530: INFO: Created: latency-svc-4tx4j
Jun 28 21:05:45.530: INFO: Got endpoints: latency-svc-hvg22 [500.260305ms]
Jun 28 21:05:45.547: INFO: Created: latency-svc-lglmp
Jun 28 21:05:45.563: INFO: Created: latency-svc-llm75
Jun 28 21:05:45.595: INFO: Created: latency-svc-s5rff
Jun 28 21:05:45.610: INFO: Got endpoints: latency-svc-4tx4j [580.113645ms]
Jun 28 21:05:45.610: INFO: Got endpoints: latency-svc-llm75 [527.639645ms]
Jun 28 21:05:45.610: INFO: Got endpoints: latency-svc-lglmp [513.357443ms]
Jun 28 21:05:45.635: INFO: Created: latency-svc-7knbk
Jun 28 21:05:45.635: INFO: Created: latency-svc-4fjbc
Jun 28 21:05:45.648: INFO: Got endpoints: latency-svc-4fjbc [522.567612ms]
Jun 28 21:05:45.648: INFO: Got endpoints: latency-svc-s5rff [607.391957ms]
Jun 28 21:05:45.676: INFO: Created: latency-svc-92s25
Jun 28 21:05:45.676: INFO: Got endpoints: latency-svc-7knbk [526.551497ms]
Jun 28 21:05:45.676: INFO: Created: latency-svc-hxjkv
Jun 28 21:05:45.676: INFO: Got endpoints: latency-svc-hxjkv [432.678484ms]
Jun 28 21:05:45.691: INFO: Created: latency-svc-pwx9r
Jun 28 21:05:45.691: INFO: Got endpoints: latency-svc-92s25 [389.599263ms]
Jun 28 21:05:45.706: INFO: Got endpoints: latency-svc-pwx9r [431.731694ms]
Jun 28 21:05:45.732: INFO: Created: latency-svc-tp7zx
Jun 28 21:05:45.748: INFO: Created: latency-svc-plt96
Jun 28 21:05:45.748: INFO: Created: latency-svc-sm29h
Jun 28 21:05:45.794: INFO: Created: latency-svc-z62qx
Jun 28 21:05:45.794: INFO: Got endpoints: latency-svc-sm29h [409.910839ms]
Jun 28 21:05:45.794: INFO: Got endpoints: latency-svc-tp7zx [449.750706ms]
Jun 28 21:05:45.794: INFO: Got endpoints: latency-svc-plt96 [435.93424ms]
Jun 28 21:05:45.794: INFO: Got endpoints: latency-svc-z62qx [409.670197ms]
Jun 28 21:05:45.863: INFO: Created: latency-svc-5pnm4
Jun 28 21:05:45.877: INFO: Created: latency-svc-pgsqx
Jun 28 21:05:45.877: INFO: Got endpoints: latency-svc-5pnm4 [394.740432ms]
Jun 28 21:05:45.891: INFO: Got endpoints: latency-svc-pgsqx [361.350587ms]
Jun 28 21:05:45.942: INFO: Created: latency-svc-8ld5b
Jun 28 21:05:45.955: INFO: Created: latency-svc-xqchm
Jun 28 21:05:45.955: INFO: Got endpoints: latency-svc-8ld5b [345.126854ms]
Jun 28 21:05:45.966: INFO: Created: latency-svc-bflnz
Jun 28 21:05:45.966: INFO: Got endpoints: latency-svc-xqchm [356.104843ms]
Jun 28 21:05:45.978: INFO: Created: latency-svc-s7ps9
Jun 28 21:05:45.978: INFO: Got endpoints: latency-svc-bflnz [367.66601ms]
Jun 28 21:05:46.003: INFO: Created: latency-svc-cftkp
Jun 28 21:05:46.003: INFO: Got endpoints: latency-svc-s7ps9 [355.393767ms]
Jun 28 21:05:46.029: INFO: Got endpoints: latency-svc-cftkp [380.922415ms]
Jun 28 21:05:46.041: INFO: Created: latency-svc-l9ljd
Jun 28 21:05:46.067: INFO: Got endpoints: latency-svc-l9ljd [390.77959ms]
Jun 28 21:05:46.098: INFO: Created: latency-svc-7kv49
Jun 28 21:05:46.114: INFO: Got endpoints: latency-svc-7kv49 [437.89833ms]
Jun 28 21:05:46.127: INFO: Created: latency-svc-h2czc
Jun 28 21:05:46.141: INFO: Created: latency-svc-7gkpt
Jun 28 21:05:46.141: INFO: Got endpoints: latency-svc-h2czc [449.103012ms]
Jun 28 21:05:46.426: INFO: Got endpoints: latency-svc-7gkpt [720.647056ms]
Jun 28 21:05:46.427: INFO: Created: latency-svc-4xzkr
Jun 28 21:05:46.452: INFO: Created: latency-svc-txlzv
Jun 28 21:05:46.452: INFO: Created: latency-svc-pbgrw
Jun 28 21:05:46.452: INFO: Created: latency-svc-dk4j6
Jun 28 21:05:46.452: INFO: Created: latency-svc-d2dbs
Jun 28 21:05:46.452: INFO: Created: latency-svc-5pwgw
Jun 28 21:05:46.452: INFO: Created: latency-svc-6wc48
Jun 28 21:05:46.452: INFO: Created: latency-svc-t7tkf
Jun 28 21:05:46.452: INFO: Created: latency-svc-cw7k9
Jun 28 21:05:46.452: INFO: Created: latency-svc-bg44n
Jun 28 21:05:46.452: INFO: Got endpoints: latency-svc-4xzkr [658.002484ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-bg44n [658.077146ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-txlzv [658.407207ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-pbgrw [497.91949ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-cw7k9 [575.547728ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-t7tkf [658.761386ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-6wc48 [561.92225ms]
Jun 28 21:05:46.453: INFO: Got endpoints: latency-svc-dk4j6 [475.372435ms]
Jun 28 21:05:46.454: INFO: Got endpoints: latency-svc-d2dbs [487.476397ms]
Jun 28 21:05:46.465: INFO: Created: latency-svc-kvn9p
Jun 28 21:05:46.466: INFO: Got endpoints: latency-svc-5pwgw [462.128136ms]
Jun 28 21:05:46.490: INFO: Created: latency-svc-q7kct
Jun 28 21:05:46.491: INFO: Created: latency-svc-h2n68
Jun 28 21:05:46.491: INFO: Got endpoints: latency-svc-q7kct [350.129859ms]
Jun 28 21:05:46.491: INFO: Got endpoints: latency-svc-kvn9p [461.856306ms]
Jun 28 21:05:46.501: INFO: Created: latency-svc-vwkpm
Jun 28 21:05:46.524: INFO: Got endpoints: latency-svc-vwkpm [409.852368ms]
Jun 28 21:05:46.524: INFO: Got endpoints: latency-svc-h2n68 [457.195732ms]
Jun 28 21:05:46.777: INFO: Created: latency-svc-28wsz
Jun 28 21:05:46.777: INFO: Got endpoints: latency-svc-28wsz [350.65016ms]
Jun 28 21:05:46.945: INFO: Created: latency-svc-m7qbn
Jun 28 21:05:46.957: INFO: Created: latency-svc-4f2q2
Jun 28 21:05:46.970: INFO: Created: latency-svc-nnz9n
Jun 28 21:05:46.971: INFO: Got endpoints: latency-svc-4f2q2 [518.097953ms]
Jun 28 21:05:46.971: INFO: Got endpoints: latency-svc-m7qbn [517.544445ms]
Jun 28 21:05:47.458: INFO: Created: latency-svc-ktkv8
Jun 28 21:05:47.458: INFO: Created: latency-svc-t9spn
Jun 28 21:05:47.458: INFO: Created: latency-svc-zbmrq
Jun 28 21:05:47.458: INFO: Created: latency-svc-nl4fj
Jun 28 21:05:47.458: INFO: Created: latency-svc-l7bjk
Jun 28 21:05:47.458: INFO: Got endpoints: latency-svc-ktkv8 [1.005311586s]
Jun 28 21:05:47.458: INFO: Got endpoints: latency-svc-zbmrq [1.004957727s]
Jun 28 21:05:47.458: INFO: Got endpoints: latency-svc-nnz9n [1.005422669s]
Jun 28 21:05:47.458: INFO: Got endpoints: latency-svc-nl4fj [1.004750532s]
Jun 28 21:05:47.458: INFO: Got endpoints: latency-svc-t9spn [1.005286761s]
Jun 28 21:05:47.470: INFO: Created: latency-svc-nwws9
Jun 28 21:05:47.471: INFO: Created: latency-svc-8gff7
Jun 28 21:05:47.471: INFO: Created: latency-svc-qpgzb
Jun 28 21:05:47.471: INFO: Created: latency-svc-6prhp
Jun 28 21:05:47.471: INFO: Created: latency-svc-zkpvv
Jun 28 21:05:47.471: INFO: Created: latency-svc-vjz9l
Jun 28 21:05:47.471: INFO: Created: latency-svc-tglwr
Jun 28 21:05:47.471: INFO: Got endpoints: latency-svc-qpgzb [980.575702ms]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-l7bjk [1.018467626s]
Jun 28 21:05:47.472: INFO: Created: latency-svc-jfcjn
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-zkpvv [501.162319ms]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-nwws9 [1.018614494s]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-8gff7 [1.006550859s]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-6prhp [948.053373ms]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-vjz9l [981.148558ms]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-jfcjn [948.117512ms]
Jun 28 21:05:47.472: INFO: Got endpoints: latency-svc-tglwr [695.046384ms]
Jun 28 21:05:47.523: INFO: Created: latency-svc-zcl8r
Jun 28 21:05:47.523: INFO: Got endpoints: latency-svc-zcl8r [552.79639ms]
Jun 28 21:05:47.737: INFO: Created: latency-svc-9rsfb
Jun 28 21:05:47.760: INFO: Got endpoints: latency-svc-9rsfb [302.345756ms]
Jun 28 21:05:48.283: INFO: Created: latency-svc-rzv2d
Jun 28 21:05:48.283: INFO: Created: latency-svc-j8h5g
Jun 28 21:05:48.283: INFO: Created: latency-svc-nbkxm
Jun 28 21:05:48.284: INFO: Created: latency-svc-pcnbq
Jun 28 21:05:48.284: INFO: Created: latency-svc-zt7xx
Jun 28 21:05:48.284: INFO: Created: latency-svc-kbmb2
Jun 28 21:05:48.284: INFO: Created: latency-svc-vfg4p
Jun 28 21:05:48.284: INFO: Created: latency-svc-pxmqq
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-pxmqq [812.625996ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-zt7xx [812.005814ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-vfg4p [825.612137ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-kbmb2 [825.486684ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-j8h5g [825.737199ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-nbkxm [812.113802ms]
Jun 28 21:05:48.284: INFO: Got endpoints: latency-svc-pcnbq [812.335761ms]
Jun 28 21:05:48.470: INFO: Created: latency-svc-fbwsq
Jun 28 21:05:48.470: INFO: Got endpoints: latency-svc-rzv2d [1.011970179s]
Jun 28 21:05:48.471: INFO: Created: latency-svc-75hft
Jun 28 21:05:48.842: INFO: Created: latency-svc-nw4lb
Jun 28 21:05:48.842: INFO: Created: latency-svc-47nqg
Jun 28 21:05:48.842: INFO: Created: latency-svc-gt8qh
Jun 28 21:05:48.842: INFO: Created: latency-svc-2btb6
Jun 28 21:05:48.842: INFO: Created: latency-svc-rc6c6
Jun 28 21:05:48.842: INFO: Got endpoints: latency-svc-fbwsq [1.369838708s]
Jun 28 21:05:48.842: INFO: Got endpoints: latency-svc-75hft [1.318632376s]
Jun 28 21:05:48.842: INFO: Got endpoints: latency-svc-nw4lb [1.369984923s]
Jun 28 21:05:48.842: INFO: Got endpoints: latency-svc-47nqg [1.370144653s]
Jun 28 21:05:48.843: INFO: Got endpoints: latency-svc-gt8qh [1.370200354s]
Jun 28 21:05:48.843: INFO: Got endpoints: latency-svc-rc6c6 [1.082353423s]
Jun 28 21:05:48.843: INFO: Got endpoints: latency-svc-2btb6 [1.370416166s]
Jun 28 21:05:49.351: INFO: Created: latency-svc-rmj84
Jun 28 21:05:49.351: INFO: Created: latency-svc-2hxmn
Jun 28 21:05:49.351: INFO: Created: latency-svc-ggjlc
Jun 28 21:05:49.351: INFO: Created: latency-svc-kt9gz
Jun 28 21:05:49.351: INFO: Created: latency-svc-vjblc
Jun 28 21:05:49.351: INFO: Created: latency-svc-k8f2x
Jun 28 21:05:49.351: INFO: Created: latency-svc-fbhcs
Jun 28 21:05:49.351: INFO: Created: latency-svc-ddtdv
Jun 28 21:05:49.351: INFO: Got endpoints: latency-svc-2hxmn [1.067130205s]
Jun 28 21:05:49.351: INFO: Got endpoints: latency-svc-ggjlc [1.067092206s]
Jun 28 21:05:49.352: INFO: Got endpoints: latency-svc-rmj84 [1.067557648s]
Jun 28 21:05:49.352: INFO: Got endpoints: latency-svc-kt9gz [1.067387717s]
Jun 28 21:05:49.352: INFO: Got endpoints: latency-svc-ddtdv [881.945046ms]
Jun 28 21:05:49.352: INFO: Got endpoints: latency-svc-vjblc [1.067989367s]
Jun 28 21:05:49.353: INFO: Got endpoints: latency-svc-k8f2x [1.068530665s]
Jun 28 21:05:49.597: INFO: Created: latency-svc-w7d86
Jun 28 21:05:49.597: INFO: Created: latency-svc-nrht9
Jun 28 21:05:49.597: INFO: Got endpoints: latency-svc-fbhcs [1.312379661s]
Jun 28 21:05:49.597: INFO: Got endpoints: latency-svc-w7d86 [754.885708ms]
Jun 28 21:05:49.597: INFO: Created: latency-svc-bz4hs
Jun 28 21:05:49.597: INFO: Got endpoints: latency-svc-bz4hs [754.266431ms]
Jun 28 21:05:49.598: INFO: Created: latency-svc-c9f49
Jun 28 21:05:49.598: INFO: Got endpoints: latency-svc-c9f49 [754.923537ms]
Jun 28 21:05:49.598: INFO: Created: latency-svc-ctc85
Jun 28 21:05:49.598: INFO: Created: latency-svc-jztgf
Jun 28 21:05:49.598: INFO: Got endpoints: latency-svc-jztgf [755.677591ms]
Jun 28 21:05:49.598: INFO: Got endpoints: latency-svc-ctc85 [755.795714ms]
Jun 28 21:05:49.598: INFO: Created: latency-svc-v75jp
Jun 28 21:05:49.598: INFO: Got endpoints: latency-svc-v75jp [755.50913ms]
Jun 28 21:05:49.914: INFO: Created: latency-svc-lmsk8
Jun 28 21:05:49.914: INFO: Got endpoints: latency-svc-lmsk8 [562.904556ms]
Jun 28 21:05:49.914: INFO: Got endpoints: latency-svc-nrht9 [1.071954033s]
Jun 28 21:05:50.202: INFO: Created: latency-svc-zsfl5
Jun 28 21:05:50.202: INFO: Created: latency-svc-jlsds
Jun 28 21:05:50.202: INFO: Created: latency-svc-h2xzj
Jun 28 21:05:50.202: INFO: Created: latency-svc-89zqh
Jun 28 21:05:50.202: INFO: Created: latency-svc-2n2xr
Jun 28 21:05:50.202: INFO: Created: latency-svc-r5jq7
Jun 28 21:05:50.202: INFO: Got endpoints: latency-svc-zsfl5 [850.987782ms]
Jun 28 21:05:50.203: INFO: Got endpoints: latency-svc-89zqh [850.348361ms]
Jun 28 21:05:50.203: INFO: Got endpoints: latency-svc-jlsds [851.201667ms]
Jun 28 21:05:50.203: INFO: Got endpoints: latency-svc-r5jq7 [851.286642ms]
Jun 28 21:05:50.475: INFO: Created: latency-svc-kmktr
Jun 28 21:05:50.476: INFO: Got endpoints: latency-svc-2n2xr [1.12306743s]
Jun 28 21:05:50.476: INFO: Got endpoints: latency-svc-kmktr [878.712073ms]
Jun 28 21:05:50.476: INFO: Created: latency-svc-mtlkm
Jun 28 21:05:50.476: INFO: Got endpoints: latency-svc-mtlkm [878.030597ms]
Jun 28 21:05:50.476: INFO: Created: latency-svc-gfrbc
Jun 28 21:05:50.476: INFO: Got endpoints: latency-svc-h2xzj [1.123491778s]
Jun 28 21:05:51.166: INFO: Created: latency-svc-lmjzw
Jun 28 21:05:51.166: INFO: Got endpoints: latency-svc-lmjzw [1.569284205s]
Jun 28 21:05:51.166: INFO: Got endpoints: latency-svc-gfrbc [1.568577716s]
Jun 28 21:05:51.167: INFO: Created: latency-svc-4cmxc
Jun 28 21:05:51.167: INFO: Created: latency-svc-kr44b
Jun 28 21:05:51.167: INFO: Created: latency-svc-pvx2w
Jun 28 21:05:51.167: INFO: Created: latency-svc-lwvv4
Jun 28 21:05:51.168: INFO: Got endpoints: latency-svc-lwvv4 [1.569535362s]
Jun 28 21:05:51.168: INFO: Got endpoints: latency-svc-kr44b [1.56972034s]
Jun 28 21:05:51.168: INFO: Got endpoints: latency-svc-pvx2w [1.570567407s]
Jun 28 21:05:51.168: INFO: Created: latency-svc-6q92d
Jun 28 21:05:51.168: INFO: Got endpoints: latency-svc-6q92d [1.253404788s]
Jun 28 21:05:51.168: INFO: Created: latency-svc-rwwj8
Jun 28 21:05:51.168: INFO: Got endpoints: latency-svc-rwwj8 [1.253742793s]
Jun 28 21:05:51.178: INFO: Created: latency-svc-hphb9
Jun 28 21:05:51.179: INFO: Created: latency-svc-jgmlr
Jun 28 21:05:51.179: INFO: Got endpoints: latency-svc-jgmlr [976.163139ms]
Jun 28 21:05:51.179: INFO: Got endpoints: latency-svc-4cmxc [976.376778ms]
Jun 28 21:05:51.549: INFO: Got endpoints: latency-svc-hphb9 [1.346410741s]
Jun 28 21:05:51.549: INFO: Latencies: [161.323565ms 186.11209ms 215.205656ms 232.146113ms 241.358208ms 251.151838ms 263.223522ms 271.834173ms 275.723628ms 276.305412ms 279.807187ms 281.167732ms 281.841656ms 282.951983ms 285.806747ms 286.71769ms 291.939486ms 292.526689ms 294.399573ms 295.739602ms 302.345756ms 302.426296ms 303.234345ms 305.331752ms 306.060623ms 307.598645ms 308.656735ms 308.895276ms 313.719916ms 314.436897ms 314.564908ms 320.334746ms 321.547917ms 324.499349ms 328.172782ms 328.53636ms 336.349024ms 337.732662ms 337.852677ms 341.641152ms 342.232897ms 342.640106ms 342.89596ms 343.096289ms 343.765724ms 344.203754ms 345.126854ms 346.113168ms 349.52693ms 349.765055ms 350.129859ms 350.65016ms 350.916322ms 351.244032ms 353.015459ms 354.204909ms 355.393767ms 356.104843ms 357.10471ms 361.008474ms 361.350587ms 363.850824ms 366.022275ms 367.66601ms 369.130517ms 369.433291ms 370.046103ms 370.114143ms 370.21368ms 374.144427ms 377.666342ms 378.905111ms 380.922415ms 381.267793ms 381.49363ms 381.68671ms 388.028065ms 388.179228ms 389.599263ms 390.77959ms 391.352583ms 391.987629ms 393.300185ms 394.740432ms 399.710573ms 403.471635ms 404.467823ms 405.015279ms 406.125603ms 409.670197ms 409.852368ms 409.910839ms 410.733279ms 411.519035ms 411.668278ms 414.869484ms 416.91308ms 423.754072ms 431.731694ms 432.678484ms 434.568785ms 435.93424ms 437.89833ms 444.112357ms 446.10705ms 449.103012ms 449.750706ms 457.195732ms 458.691313ms 461.856306ms 462.128136ms 464.218641ms 467.492921ms 469.740159ms 475.372435ms 477.192879ms 479.865241ms 487.476397ms 497.91949ms 500.260305ms 501.162319ms 513.357443ms 515.071377ms 517.544445ms 518.097953ms 522.567612ms 526.551497ms 527.639645ms 552.79639ms 561.92225ms 562.904556ms 575.547728ms 580.113645ms 607.391957ms 658.002484ms 658.077146ms 658.407207ms 658.761386ms 695.046384ms 720.647056ms 754.266431ms 754.885708ms 754.923537ms 755.50913ms 755.677591ms 755.795714ms 812.005814ms 812.113802ms 812.335761ms 812.625996ms 825.486684ms 825.612137ms 825.737199ms 850.348361ms 850.987782ms 851.201667ms 851.286642ms 878.030597ms 878.712073ms 881.945046ms 948.053373ms 948.117512ms 976.163139ms 976.376778ms 980.575702ms 981.148558ms 1.004750532s 1.004957727s 1.005286761s 1.005311586s 1.005422669s 1.006550859s 1.011970179s 1.018467626s 1.018614494s 1.067092206s 1.067130205s 1.067387717s 1.067557648s 1.067989367s 1.068530665s 1.071954033s 1.082353423s 1.12306743s 1.123491778s 1.253404788s 1.253742793s 1.312379661s 1.318632376s 1.346410741s 1.369838708s 1.369984923s 1.370144653s 1.370200354s 1.370416166s 1.568577716s 1.569284205s 1.569535362s 1.56972034s 1.570567407s]
Jun 28 21:05:51.549: INFO: 50 %ile: 434.568785ms
Jun 28 21:05:51.549: INFO: 90 %ile: 1.068530665s
Jun 28 21:05:51.549: INFO: 99 %ile: 1.56972034s
Jun 28 21:05:51.549: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:05:51.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9599" for this suite.

• [SLOW TEST:16.227 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":259,"skipped":4282,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:05:53.600: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d39b8c59-94f5-491d-bcd4-e0fee421c60d
STEP: Creating a pod to test consume secrets
Jun 28 21:05:54.660: INFO: Waiting up to 5m0s for pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174" in namespace "secrets-2154" to be "Succeeded or Failed"
Jun 28 21:05:54.730: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174": Phase="Pending", Reason="", readiness=false. Elapsed: 69.562959ms
Jun 28 21:05:57.056: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395367971s
Jun 28 21:05:59.331: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670752459s
Jun 28 21:06:01.475: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174": Phase="Pending", Reason="", readiness=false. Elapsed: 6.814381654s
Jun 28 21:06:03.653: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.993223293s
STEP: Saw pod success
Jun 28 21:06:03.653: INFO: Pod "pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174" satisfied condition "Succeeded or Failed"
Jun 28 21:06:03.835: INFO: Trying to get logs from node 10.244.0.30 pod pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174 container secret-volume-test: <nil>
STEP: delete the pod
Jun 28 21:06:06.166: INFO: Waiting for pod pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174 to disappear
Jun 28 21:06:06.380: INFO: Pod pod-secrets-3139e714-b285-4bfb-aadd-55d9dca8c174 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:06:06.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2154" for this suite.
STEP: Destroying namespace "secret-namespace-30" for this suite.

• [SLOW TEST:14.473 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4284,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:06:08.074: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:06:12.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3698" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.450 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":261,"skipped":4289,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:06:14.524: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 28 21:06:17.102: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123155 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 21:06:17.102: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123166 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 21:06:17.103: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123173 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 28 21:06:29.369: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123243 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 21:06:29.369: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123247 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 21:06:29.369: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6962 /api/v1/namespaces/watch-6962/configmaps/e2e-watch-test-label-changed f8ca5fc4-da25-4862-bc28-69195bfcbdab 123251 0 2021-06-28 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-28 21:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:06:29.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6962" for this suite.

• [SLOW TEST:16.529 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":262,"skipped":4308,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:06:31.053: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-6397b86e-e662-400a-abb7-37b269c3f338 in namespace container-probe-7131
Jun 28 21:06:43.412: INFO: Started pod busybox-6397b86e-e662-400a-abb7-37b269c3f338 in namespace container-probe-7131
STEP: checking the pod's current state and verifying that restartCount is present
Jun 28 21:06:43.817: INFO: Initial restart count of pod busybox-6397b86e-e662-400a-abb7-37b269c3f338 is 0
Jun 28 21:07:38.132: INFO: Restart count of pod container-probe-7131/busybox-6397b86e-e662-400a-abb7-37b269c3f338 is now 1 (54.314557316s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:07:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7131" for this suite.

• [SLOW TEST:68.535 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":263,"skipped":4317,"failed":0}
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:07:39.588: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:08:03.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9641" for this suite.

• [SLOW TEST:24.715 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":264,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:08:04.304: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5d85b899-510a-4cfd-927b-17fcdbc6b54d
STEP: Creating a pod to test consume configMaps
Jun 28 21:08:05.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1" in namespace "configmap-7705" to be "Succeeded or Failed"
Jun 28 21:08:05.201: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1": Phase="Pending", Reason="", readiness=false. Elapsed: 121.804748ms
Jun 28 21:08:07.421: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341956884s
Jun 28 21:08:09.502: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.422616948s
Jun 28 21:08:11.785: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.706245216s
Jun 28 21:08:13.988: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.908639019s
STEP: Saw pod success
Jun 28 21:08:13.988: INFO: Pod "pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1" satisfied condition "Succeeded or Failed"
Jun 28 21:08:14.834: INFO: Trying to get logs from node 10.244.0.29 pod pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1 container agnhost-container: <nil>
STEP: delete the pod
Jun 28 21:08:16.286: INFO: Waiting for pod pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1 to disappear
Jun 28 21:08:16.414: INFO: Pod pod-configmaps-e2025f90-84b9-4ce5-b141-b359a9ce50d1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:08:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7705" for this suite.

• [SLOW TEST:12.876 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4333,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:08:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:08:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-431" for this suite.

• [SLOW TEST:37.524 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":266,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:08:54.705: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 28 21:09:07.019: INFO: Successfully updated pod "annotationupdate22cc8e70-ca2a-4f8f-9df0-375f83a70247"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:10.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8156" for this suite.

• [SLOW TEST:17.772 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4422,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:12.478: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Jun 28 21:09:15.388: INFO: created pod pod-service-account-defaultsa
Jun 28 21:09:15.388: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 28 21:09:15.633: INFO: created pod pod-service-account-mountsa
Jun 28 21:09:15.633: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 28 21:09:15.912: INFO: created pod pod-service-account-nomountsa
Jun 28 21:09:15.912: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 28 21:09:16.184: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 28 21:09:16.184: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 28 21:09:16.471: INFO: created pod pod-service-account-mountsa-mountspec
Jun 28 21:09:16.471: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 28 21:09:16.736: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 28 21:09:16.736: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 28 21:09:16.978: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 28 21:09:16.978: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 28 21:09:17.174: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 28 21:09:17.174: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 28 21:09:17.351: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 28 21:09:17.351: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:17.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-156" for this suite.

• [SLOW TEST:5.679 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":268,"skipped":4423,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-7195444e-fdee-440f-a88a-f47077e66182
STEP: Creating a pod to test consume configMaps
Jun 28 21:09:21.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb" in namespace "configmap-9684" to be "Succeeded or Failed"
Jun 28 21:09:21.952: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 257.599878ms
Jun 28 21:09:24.173: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478613712s
Jun 28 21:09:26.460: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.765237936s
Jun 28 21:09:28.778: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.083851851s
Jun 28 21:09:31.036: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.341182424s
Jun 28 21:09:33.254: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.559818286s
STEP: Saw pod success
Jun 28 21:09:33.254: INFO: Pod "pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb" satisfied condition "Succeeded or Failed"
Jun 28 21:09:33.479: INFO: Trying to get logs from node 10.244.0.31 pod pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb container agnhost-container: <nil>
STEP: delete the pod
Jun 28 21:09:35.104: INFO: Waiting for pod pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb to disappear
Jun 28 21:09:35.395: INFO: Pod pod-configmaps-472553a3-7b5f-4958-a3c6-25c6b3a702fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:35.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9684" for this suite.

• [SLOW TEST:18.736 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4433,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:36.893: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Jun 28 21:09:38.050: INFO: created test-pod-1
Jun 28 21:09:38.266: INFO: created test-pod-2
Jun 28 21:09:38.490: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:39.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1108" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":270,"skipped":4439,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:40.205: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-1621ccae-0bbc-4069-8997-72a04b272dbe
STEP: Creating a pod to test consume configMaps
Jun 28 21:09:41.339: INFO: Waiting up to 5m0s for pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e" in namespace "configmap-1384" to be "Succeeded or Failed"
Jun 28 21:09:41.492: INFO: Pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 152.93167ms
Jun 28 21:09:43.700: INFO: Pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36097366s
Jun 28 21:09:45.842: INFO: Pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503286841s
Jun 28 21:09:48.063: INFO: Pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.724096739s
STEP: Saw pod success
Jun 28 21:09:48.063: INFO: Pod "pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e" satisfied condition "Succeeded or Failed"
Jun 28 21:09:48.305: INFO: Trying to get logs from node 10.244.0.30 pod pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 21:09:49.474: INFO: Waiting for pod pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e to disappear
Jun 28 21:09:49.692: INFO: Pod pod-configmaps-4893365c-b039-4b49-ada4-e5dc93c47d4e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:49.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1384" for this suite.

• [SLOW TEST:10.541 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4456,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:50.745: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-980b2f06-a5b2-4713-922a-b5d9979d3101
STEP: Creating a pod to test consume configMaps
Jun 28 21:09:51.670: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8" in namespace "projected-2884" to be "Succeeded or Failed"
Jun 28 21:09:51.810: INFO: Pod "pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 140.172004ms
Jun 28 21:09:54.055: INFO: Pod "pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385200176s
Jun 28 21:09:56.216: INFO: Pod "pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.545320192s
STEP: Saw pod success
Jun 28 21:09:56.216: INFO: Pod "pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8" satisfied condition "Succeeded or Failed"
Jun 28 21:09:56.393: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 28 21:09:56.753: INFO: Waiting for pod pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8 to disappear
Jun 28 21:09:56.850: INFO: Pod pod-projected-configmaps-c1ac82a5-8256-4127-a1bd-54dd1c36d8f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:09:56.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2884" for this suite.

• [SLOW TEST:6.403 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4463,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:09:57.148: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-f3bbdeaa-064b-4371-b493-9e6bf871e112
STEP: Creating a pod to test consume secrets
Jun 28 21:09:57.807: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13" in namespace "projected-4149" to be "Succeeded or Failed"
Jun 28 21:09:57.890: INFO: Pod "pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13": Phase="Pending", Reason="", readiness=false. Elapsed: 82.844965ms
Jun 28 21:10:00.110: INFO: Pod "pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302545088s
Jun 28 21:10:02.280: INFO: Pod "pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.472782468s
STEP: Saw pod success
Jun 28 21:10:02.280: INFO: Pod "pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13" satisfied condition "Succeeded or Failed"
Jun 28 21:10:02.473: INFO: Trying to get logs from node 10.244.0.29 pod pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 28 21:10:03.534: INFO: Waiting for pod pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13 to disappear
Jun 28 21:10:03.779: INFO: Pod pod-projected-secrets-f3a53331-5dd6-4dad-8a5c-6e8009c2db13 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:10:03.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4149" for this suite.

• [SLOW TEST:7.529 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:10:04.678: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 28 21:10:05.936: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:10:14.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6344" for this suite.

• [SLOW TEST:10.938 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":274,"skipped":4508,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:10:15.617: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:19.969: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 21:10:29.551: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 28 21:10:29.551: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 28 21:10:29.639: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun 28 21:10:30.125: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 0
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.447: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.472: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.472: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.472: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.472: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.586: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.586: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 2
Jun 28 21:10:30.586: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
STEP: listing Deployments
Jun 28 21:10:30.822: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun 28 21:10:31.140: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.455: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.625: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 21:10:31.625: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun 28 21:10:53.474: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.474: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.474: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.474: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.650: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.650: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.650: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
Jun 28 21:10:53.650: INFO: observed Deployment test-deployment in namespace deployment-6814 with ReadyReplicas 1
STEP: deleting the Deployment
Jun 28 21:10:54.087: INFO: observed event type MODIFIED
Jun 28 21:10:54.087: INFO: observed event type MODIFIED
Jun 28 21:10:54.087: INFO: observed event type MODIFIED
Jun 28 21:10:54.087: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
Jun 28 21:10:54.214: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 21:10:54.356: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:10:54.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6814" for this suite.

• [SLOW TEST:39.510 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":275,"skipped":4510,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:10:55.126: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:10:57.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6282" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":276,"skipped":4513,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:10:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun 28 21:10:58.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-4121 create -f -'
Jun 28 21:10:59.416: INFO: stderr: ""
Jun 28 21:10:59.416: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 21:11:00.516: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:00.516: INFO: Found 0 / 1
Jun 28 21:11:01.496: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:01.496: INFO: Found 0 / 1
Jun 28 21:11:02.450: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:02.450: INFO: Found 0 / 1
Jun 28 21:11:03.448: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:03.448: INFO: Found 0 / 1
Jun 28 21:11:04.508: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:04.508: INFO: Found 0 / 1
Jun 28 21:11:05.620: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:05.620: INFO: Found 1 / 1
Jun 28 21:11:05.620: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 28 21:11:05.776: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:05.776: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 21:11:05.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-4121 patch pod agnhost-primary-4mfzc -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 28 21:11:06.187: INFO: stderr: ""
Jun 28 21:11:06.187: INFO: stdout: "pod/agnhost-primary-4mfzc patched\n"
STEP: checking annotations
Jun 28 21:11:06.421: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:11:06.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:11:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4121" for this suite.

• [SLOW TEST:9.677 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1466
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":277,"skipped":4516,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:11:07.625: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 28 21:11:15.699: INFO: Successfully updated pod "adopt-release-4snmn"
STEP: Checking that the Job readopts the Pod
Jun 28 21:11:15.699: INFO: Waiting up to 15m0s for pod "adopt-release-4snmn" in namespace "job-5964" to be "adopted"
Jun 28 21:11:15.891: INFO: Pod "adopt-release-4snmn": Phase="Running", Reason="", readiness=true. Elapsed: 192.435415ms
Jun 28 21:11:15.891: INFO: Pod "adopt-release-4snmn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 28 21:11:18.155: INFO: Successfully updated pod "adopt-release-4snmn"
STEP: Checking that the Job releases the Pod
Jun 28 21:11:18.155: INFO: Waiting up to 15m0s for pod "adopt-release-4snmn" in namespace "job-5964" to be "released"
Jun 28 21:11:20.458: INFO: Pod "adopt-release-4snmn": Phase="Running", Reason="", readiness=true. Elapsed: 2.303570158s
Jun 28 21:11:20.458: INFO: Pod "adopt-release-4snmn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:11:20.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5964" for this suite.

• [SLOW TEST:14.962 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":278,"skipped":4518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:11:22.588: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 21:11:23.537: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 28 21:11:24.050: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 28 21:11:32.672: INFO: Creating deployment "test-rolling-update-deployment"
Jun 28 21:11:33.103: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 28 21:11:33.844: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 28 21:11:34.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:36.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:38.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:40.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:42.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:44.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:46.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511492, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:11:48.458: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 28 21:11:49.298: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4051 /apis/apps/v1/namespaces/deployment-4051/deployments/test-rolling-update-deployment 3c2d1266-7bef-4902-8fe7-3f03265265d6 126689 1 2021-06-28 21:11:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-06-28 21:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 21:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083fabb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-28 21:11:32 +0000 UTC,LastTransitionTime:2021-06-28 21:11:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-06-28 21:11:46 +0000 UTC,LastTransitionTime:2021-06-28 21:11:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 21:11:49.608: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-4051 /apis/apps/v1/namespaces/deployment-4051/replicasets/test-rolling-update-deployment-6b6bf9df46 e938678d-a730-45e5-ada7-2099ecac4718 126678 1 2021-06-28 21:11:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3c2d1266-7bef-4902-8fe7-3f03265265d6 0xc003a25187 0xc003a25188}] []  [{kube-controller-manager Update apps/v1 2021-06-28 21:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c2d1266-7bef-4902-8fe7-3f03265265d6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a252c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 21:11:49.608: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 28 21:11:49.608: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4051 /apis/apps/v1/namespaces/deployment-4051/replicasets/test-rolling-update-controller 5d193dcf-6a26-4ae4-92c2-6ea630d1d321 126687 2 2021-06-28 21:11:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3c2d1266-7bef-4902-8fe7-3f03265265d6 0xc003a24fd7 0xc003a24fd8}] []  [{e2e.test Update apps/v1 2021-06-28 21:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-28 21:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c2d1266-7bef-4902-8fe7-3f03265265d6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a250c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 21:11:49.857: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-7nklz" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-7nklz test-rolling-update-deployment-6b6bf9df46- deployment-4051 /api/v1/namespaces/deployment-4051/pods/test-rolling-update-deployment-6b6bf9df46-7nklz 0f5a0484-cc47-4ef8-b00b-aa26c8b58f16 126677 0 2021-06-28 21:11:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:172.17.113.191/32 cni.projectcalico.org/podIPs:172.17.113.191/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.113.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.113.191"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 e938678d-a730-45e5-ada7-2099ecac4718 0xc003a25777 0xc003a25778}] []  [{kube-controller-manager Update v1 2021-06-28 21:11:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e938678d-a730-45e5-ada7-2099ecac4718\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-28 21:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-28 21:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.113.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-28 21:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cmjk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cmjk8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cmjk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.244.0.29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c62,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kpw46,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 21:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 21:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 21:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-28 21:11:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.244.0.29,PodIP:172.17.113.191,StartTime:2021-06-28 21:11:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-28 21:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://ba4a163b7760ed2a31dd882d99da4bf8720469b6165b13a27500f097a051f5c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.113.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:11:49.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4051" for this suite.

• [SLOW TEST:28.737 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":279,"skipped":4586,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:11:51.325: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 28 21:11:52.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6154 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 28 21:11:53.266: INFO: stderr: ""
Jun 28 21:11:53.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 28 21:11:53.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6154 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jun 28 21:11:54.814: INFO: stderr: ""
Jun 28 21:11:54.814: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun 28 21:11:55.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6154 delete pods e2e-test-httpd-pod'
Jun 28 21:11:58.858: INFO: stderr: ""
Jun 28 21:11:58.858: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:11:58.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6154" for this suite.

• [SLOW TEST:10.052 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":280,"skipped":4588,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:12:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-8ce25599-c10c-42b2-b108-2164dfe1550a
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:12:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5125" for this suite.

• [SLOW TEST:7.398 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":281,"skipped":4596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:12:08.776: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 21:12:09.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c" in namespace "projected-6877" to be "Succeeded or Failed"
Jun 28 21:12:09.961: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 162.236661ms
Jun 28 21:12:12.014: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215007322s
Jun 28 21:12:14.269: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.470740542s
Jun 28 21:12:16.659: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860779508s
Jun 28 21:12:18.771: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.972785686s
STEP: Saw pod success
Jun 28 21:12:18.771: INFO: Pod "downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c" satisfied condition "Succeeded or Failed"
Jun 28 21:12:18.883: INFO: Trying to get logs from node 10.244.0.30 pod downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c container client-container: <nil>
STEP: delete the pod
Jun 28 21:12:21.563: INFO: Waiting for pod downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c to disappear
Jun 28 21:12:21.771: INFO: Pod downwardapi-volume-a6f89ea2-6b63-4171-bf50-65df6b4b4b1c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:12:21.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6877" for this suite.

• [SLOW TEST:14.851 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:12:23.628: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:12:28.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-891" for this suite.

• [SLOW TEST:7.072 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":283,"skipped":4645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:12:30.700: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 21:12:35.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:12:39.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:12:39.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:12:42.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760511554, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 21:12:44.773: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:13:04.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4245" for this suite.
STEP: Destroying namespace "webhook-4245-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:37.727 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":284,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:13:08.428: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-dc71f6db-afd3-477a-8213-b25685fabf21
STEP: Creating a pod to test consume secrets
Jun 28 21:13:10.139: INFO: Waiting up to 5m0s for pod "pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b" in namespace "secrets-2664" to be "Succeeded or Failed"
Jun 28 21:13:10.254: INFO: Pod "pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b": Phase="Pending", Reason="", readiness=false. Elapsed: 115.034433ms
Jun 28 21:13:12.484: INFO: Pod "pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345298802s
Jun 28 21:13:14.602: INFO: Pod "pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.463196089s
STEP: Saw pod success
Jun 28 21:13:14.602: INFO: Pod "pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b" satisfied condition "Succeeded or Failed"
Jun 28 21:13:14.690: INFO: Trying to get logs from node 10.244.0.29 pod pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b container secret-env-test: <nil>
STEP: delete the pod
Jun 28 21:13:15.568: INFO: Waiting for pod pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b to disappear
Jun 28 21:13:15.683: INFO: Pod pod-secrets-3ca95a04-2fd4-483e-8d4e-13a2fc9cee1b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:13:15.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2664" for this suite.

• [SLOW TEST:8.109 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4738,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:13:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 21:13:17.492: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309" in namespace "security-context-test-7335" to be "Succeeded or Failed"
Jun 28 21:13:17.583: INFO: Pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309": Phase="Pending", Reason="", readiness=false. Elapsed: 91.424698ms
Jun 28 21:13:19.670: INFO: Pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178698499s
Jun 28 21:13:21.879: INFO: Pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.387474157s
Jun 28 21:13:21.879: INFO: Pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309" satisfied condition "Succeeded or Failed"
Jun 28 21:13:22.422: INFO: Got logs for pod "busybox-privileged-false-778da3f7-132a-419a-a250-b03a14ffe309": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:13:22.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7335" for this suite.

• [SLOW TEST:6.478 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:13:23.016: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 28 21:13:23.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 create -f -'
Jun 28 21:13:24.447: INFO: stderr: ""
Jun 28 21:13:24.447: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 28 21:13:24.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 create -f -'
Jun 28 21:13:27.869: INFO: stderr: ""
Jun 28 21:13:27.869: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 21:13:28.984: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:13:28.984: INFO: Found 1 / 1
Jun 28 21:13:28.984: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 21:13:29.091: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:13:29.091: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 21:13:29.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 describe pod agnhost-primary-g8cfg'
Jun 28 21:13:29.320: INFO: stderr: ""
Jun 28 21:13:29.320: INFO: stdout: "Name:         agnhost-primary-g8cfg\nNamespace:    kubectl-9603\nPriority:     0\nNode:         10.244.0.29/10.244.0.29\nStart Time:   Mon, 28 Jun 2021 21:13:24 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.17.113.163/32\n              cni.projectcalico.org/podIPs: 172.17.113.163/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.17.113.163\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.17.113.163\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.17.113.163\nIPs:\n  IP:           172.17.113.163\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://e8727f998136de59e982c5546c0e02623a473760dca3d8657b12986518345391\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 28 Jun 2021 21:13:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-klltp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-klltp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-klltp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       5s    default-scheduler  Successfully assigned kubectl-9603/agnhost-primary-g8cfg to 10.244.0.29\n  Normal  AddedInterface  4s    multus             Add eth0 [172.17.113.163/32]\n  Normal  Pulled          4s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         3s    kubelet            Started container agnhost-primary\n"
Jun 28 21:13:29.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 describe rc agnhost-primary'
Jun 28 21:13:30.913: INFO: stderr: ""
Jun 28 21:13:30.913: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9603\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: agnhost-primary-g8cfg\n"
Jun 28 21:13:30.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 describe service agnhost-primary'
Jun 28 21:13:31.554: INFO: stderr: ""
Jun 28 21:13:31.554: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9603\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.52.126\nIPs:               172.21.52.126\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.17.113.163:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 28 21:13:31.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 describe node 10.244.0.29'
Jun 28 21:13:33.347: INFO: stderr: ""
Jun 28 21:13:33.347: INFO: stdout: "Name:               10.244.0.29\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=bx2.4x16\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-tok\n                    failure-domain.beta.kubernetes.io/zone=jp-tok-1\n                    ibm-cloud.kubernetes.io/iaas-provider=g2\n                    ibm-cloud.kubernetes.io/instance-id=02e7_27bd4b48-65dd-4ea1-ba7b-1508e577927c\n                    ibm-cloud.kubernetes.io/internal-ip=10.244.0.29\n                    ibm-cloud.kubernetes.io/machine-type=bx2.4x16\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=jp-tok\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/subnet-id=02e7-9bdde449-84d2-4b3d-b6af-bc432ed4a034\n                    ibm-cloud.kubernetes.io/worker-id=kube-c3cvgu6t0hvooucorl40-kubee2epvge-default-00000266\n                    ibm-cloud.kubernetes.io/worker-pool-id=c3cvgu6t0hvooucorl40-fdb4aaa\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.7.16_1522_openshift\n                    ibm-cloud.kubernetes.io/zone=jp-tok-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.244.0.29\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=bx2.4x16\n                    node.openshift.io/os_id=rhel\n                    topology.kubernetes.io/region=jp-tok\n                    topology.kubernetes.io/zone=jp-tok-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"vpc.block.csi.ibm.io\":\"kube-c3cvgu6t0hvooucorl40-kubee2epvge-default-00000266\"}\n                    projectcalico.org/IPv4Address: 10.244.0.29/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.17.113.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 28 Jun 2021 16:58:50 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.244.0.29\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 28 Jun 2021 21:13:23 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 28 Jun 2021 17:03:05 +0000   Mon, 28 Jun 2021 17:03:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 28 Jun 2021 21:09:06 +0000   Mon, 28 Jun 2021 16:58:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 28 Jun 2021 21:09:06 +0000   Mon, 28 Jun 2021 16:58:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 28 Jun 2021 21:09:06 +0000   Mon, 28 Jun 2021 16:58:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 28 Jun 2021 21:09:06 +0000   Mon, 28 Jun 2021 17:03:06 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.244.0.29\n  ExternalIP:  10.244.0.29\n  Hostname:    10.244.0.29\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102048096Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16266152Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93525038945\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13489064Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             ecebb94067704d8b9c524105f7c0c8c3\n  System UUID:                            ECEBB940-6770-4D8B-9C52-4105F7C0C8C3\n  Boot ID:                                ef5d4165-c229-4501-a31d-90bf755bab80\n  Kernel Version:                         3.10.0-1160.31.1.el7.x86_64\n  OS Image:                               Red Hat\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.20.3-2.rhaos4.7.gitb53fa9d.el7\n  Kubelet Version:                        v1.20.0+2817867\n  Kube-Proxy Version:                     v1.20.0+2817867\nPodCIDR:                                  172.17.66.0/24\nPodCIDRs:                                 172.17.66.0/24\nProviderID:                               ibm://68010fd8df4f467681ddec1e065d7a48///c3cvgu6t0hvooucorl40/kube-c3cvgu6t0hvooucorl40-kubee2epvge-default-00000266\nNon-terminated Pods:                      (18 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-47lkr                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h11m\n  calico-system                           calico-typha-664f469f54-58kwx                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h11m\n  kube-system                             ibm-keepalived-watcher-s6s5t                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h14m\n  kube-system                             ibm-master-proxy-static-10.244.0.29                        25m (0%)      300m (7%)   32M (0%)         512M (3%)      4h14m\n  kube-system                             ibm-vpc-block-csi-node-mxp5s                               35m (0%)      350m (8%)   80Mi (0%)        400Mi (3%)     4h14m\n  kubectl-9603                            agnhost-primary-g8cfg                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  openshift-cluster-node-tuning-operator  tuned-tfswp                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h6m\n  openshift-dns                           dns-default-bsrhg                                          65m (1%)      0 (0%)      131Mi (0%)       0 (0%)         4h5m\n  openshift-image-registry                node-ca-4rccc                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h6m\n  openshift-ingress-canary                ingress-canary-s2tfx                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         34m\n  openshift-kube-proxy                    openshift-kube-proxy-rr8vk                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h13m\n  openshift-monitoring                    node-exporter-mk7n4                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         4h9m\n  openshift-multus                        multus-admission-controller-mf54j                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         34m\n  openshift-multus                        multus-hnlqc                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         4h13m\n  openshift-multus                        network-metrics-daemon-zvdbr                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h13m\n  openshift-network-diagnostics           network-check-target-b65gf                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h12m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         164m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         164m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  839m (21%)      650m (16%)\n  memory               1255954Ki (9%)  931430400 (6%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
Jun 28 21:13:33.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-9603 describe namespace kubectl-9603'
Jun 28 21:13:33.986: INFO: stderr: ""
Jun 28 21:13:33.986: INFO: stdout: "Name:         kubectl-9603\nLabels:       e2e-framework=kubectl\n              e2e-run=6f56c40a-af57-4403-b3ff-18a7f2cc2533\nAnnotations:  openshift.io/sa.scc.mcs: s0:c62,c54\n              openshift.io/sa.scc.supplemental-groups: 1003890000/10000\n              openshift.io/sa.scc.uid-range: 1003890000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:13:33.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9603" for this suite.

• [SLOW TEST:11.382 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":287,"skipped":4812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:13:34.398: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 28 21:13:34.763: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 21:13:35.120: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 21:13:35.616: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.29 before test
Jun 28 21:13:36.028: INFO: calico-node-47lkr from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:13:36.028: INFO: calico-typha-664f469f54-58kwx from calico-system started at 2021-06-28 17:02:30 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:13:36.028: INFO: ibm-keepalived-watcher-s6s5t from kube-system started at 2021-06-28 16:59:22 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:13:36.028: INFO: ibm-master-proxy-static-10.244.0.29 from kube-system started at 2021-06-28 16:58:02 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:13:36.028: INFO: ibm-vpc-block-csi-node-mxp5s from kube-system started at 2021-06-28 16:59:22 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:13:36.028: INFO: agnhost-primary-g8cfg from kubectl-9603 started at 2021-06-28 21:13:24 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container agnhost-primary ready: true, restart count 0
Jun 28 21:13:36.028: INFO: tuned-tfswp from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:13:36.028: INFO: dns-default-bsrhg from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: node-ca-4rccc from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:13:36.028: INFO: ingress-canary-s2tfx from openshift-ingress-canary started at 2021-06-28 20:39:33 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:13:36.028: INFO: openshift-kube-proxy-rr8vk from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: node-exporter-mk7n4 from openshift-monitoring started at 2021-06-28 17:03:59 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:13:36.028: INFO: multus-admission-controller-mf54j from openshift-multus started at 2021-06-28 20:39:33 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:13:36.028: INFO: multus-hnlqc from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:13:36.028: INFO: network-metrics-daemon-zvdbr from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:13:36.028: INFO: network-check-target-b65gf from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:13:36.028: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:28:51 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 21:13:36.028: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:36.028: INFO: 	Container sonobuoy-worker ready: false, restart count 25
Jun 28 21:13:36.028: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 21:13:36.028: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.30 before test
Jun 28 21:13:37.612: INFO: calico-node-2ql4z from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:13:37.612: INFO: calico-typha-664f469f54-j6kl2 from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:13:37.612: INFO: ibm-keepalived-watcher-5dmkt from kube-system started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:13:37.612: INFO: ibm-master-proxy-static-10.244.0.30 from kube-system started at 2021-06-28 16:57:51 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:13:37.612: INFO: ibm-vpc-block-csi-node-db8gf from kube-system started at 2021-06-28 16:58:42 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:13:37.612: INFO: vpn-74cdbd76f7-2pkzl from kube-system started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container vpn ready: true, restart count 0
Jun 28 21:13:37.612: INFO: tuned-ntcvm from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:13:37.612: INFO: csi-snapshot-controller-fc56779c7-68pwj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container snapshot-controller ready: true, restart count 1
Jun 28 21:13:37.612: INFO: csi-snapshot-webhook-798674875b-z2zlj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container webhook ready: true, restart count 0
Jun 28 21:13:37.612: INFO: downloads-6d9c464cd4-vjjxk from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container download-server ready: true, restart count 0
Jun 28 21:13:37.612: INFO: dns-default-l8wqd from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: image-registry-567d5cff74-k92hq from openshift-image-registry started at 2021-06-28 17:07:17 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container registry ready: true, restart count 0
Jun 28 21:13:37.612: INFO: node-ca-dn9xk from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:13:37.612: INFO: ingress-canary-d47lg from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:13:37.612: INFO: router-default-fdf999c57-m7k7f from openshift-ingress started at 2021-06-28 18:41:24 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container router ready: true, restart count 0
Jun 28 21:13:37.612: INFO: openshift-kube-proxy-jd62h from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: migrator-744d665879-r687n from openshift-kube-storage-version-migrator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container migrator ready: true, restart count 0
Jun 28 21:13:37.612: INFO: certified-operators-cdpbc from openshift-marketplace started at 2021-06-28 20:56:48 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:13:37.612: INFO: community-operators-qsbjx from openshift-marketplace started at 2021-06-28 20:39:04 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:13:37.612: INFO: redhat-operators-85gdh from openshift-marketplace started at 2021-06-28 20:38:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:13:37.612: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 20:39:00 +0000 UTC (5 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: kube-state-metrics-55cbf55cc7-5nhfz from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 21:13:37.612: INFO: node-exporter-2x4b4 from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:13:37.612: INFO: openshift-state-metrics-8555845c54-fd7l8 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 21:13:37.612: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 20:39:23 +0000 UTC (7 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 21:13:37.612: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 21:13:37.612: INFO: prometheus-operator-6b9599d88d-nqprj from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 21:13:37.612: INFO: telemeter-client-666c78ff9f-28pc9 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container reload ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 21:13:37.612: INFO: thanos-querier-8c945b5d4-24xwt from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (5 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 21:13:37.612: INFO: multus-admission-controller-96snf from openshift-multus started at 2021-06-28 17:03:00 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:13:37.612: INFO: multus-qp427 from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:13:37.612: INFO: network-metrics-daemon-s59ql from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:37.612: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:13:37.612: INFO: network-check-target-pbqc5 from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:13:37.612: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container sonobuoy-worker ready: false, restart count 25
Jun 28 21:13:37.612: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 21:13:37.612: INFO: tigera-operator-9cb9c95c7-vxwg6 from tigera-operator started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:37.612: INFO: 	Container tigera-operator ready: true, restart count 4
Jun 28 21:13:37.612: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.31 before test
Jun 28 21:13:38.111: INFO: calico-kube-controllers-67cd4fd574-fst6j from calico-system started at 2021-06-28 17:02:55 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 21:13:38.111: INFO: calico-node-swgct from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:13:38.111: INFO: calico-typha-664f469f54-xp4ng from calico-system started at 2021-06-28 17:02:35 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ibm-keepalived-watcher-8r7sd from kube-system started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ibm-master-proxy-static-10.244.0.31 from kube-system started at 2021-06-28 16:57:44 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-28 17:03:01 +0000 UTC (4 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ibm-vpc-block-csi-node-dlgsz from kube-system started at 2021-06-28 16:59:10 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:13:38.111: INFO: cluster-node-tuning-operator-85b6488455-92rjz from openshift-cluster-node-tuning-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: tuned-5p6rg from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:13:38.111: INFO: cluster-samples-operator-6979dbb9c5-ljqjf from openshift-cluster-samples-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container cluster-samples-operator ready: true, restart count 1
Jun 28 21:13:38.111: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 21:13:38.111: INFO: cluster-storage-operator-6974bfb5c6-gjrzk from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 28 21:13:38.111: INFO: csi-snapshot-controller-operator-c9886b54b-mbxtf from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: console-operator-946dbb485-549nf from openshift-console-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 21:13:38.111: INFO: console-7d7f484b46-qwgl6 from openshift-console started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container console ready: true, restart count 0
Jun 28 21:13:38.111: INFO: console-7d7f484b46-zql2k from openshift-console started at 2021-06-28 18:43:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container console ready: true, restart count 0
Jun 28 21:13:38.111: INFO: downloads-6d9c464cd4-mjc9h from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container download-server ready: true, restart count 0
Jun 28 21:13:38.111: INFO: dns-operator-74db48c654-lbr6w from openshift-dns-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: dns-default-f2xdn from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: cluster-image-registry-operator-7c675f968-j72bn from openshift-image-registry started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: node-ca-24vfp from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ingress-canary-n5kgb from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:13:38.111: INFO: ingress-operator-6557486749-2zshp from openshift-ingress-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: router-default-fdf999c57-487ml from openshift-ingress started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container router ready: true, restart count 0
Jun 28 21:13:38.111: INFO: openshift-kube-proxy-bhm75 from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: kube-storage-version-migrator-operator-bdddd9479-r779m from openshift-kube-storage-version-migrator-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 21:13:38.111: INFO: marketplace-operator-569986b7f7-25n44 from openshift-marketplace started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container marketplace-operator ready: true, restart count 2
Jun 28 21:13:38.111: INFO: redhat-marketplace-6vl2f from openshift-marketplace started at 2021-06-28 21:00:40 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:13:38.111: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: cluster-monitoring-operator-6c785d75f6-rzfjr from openshift-monitoring started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 28 21:13:38.111: INFO: grafana-66dff7486b-hhwrz from openshift-monitoring started at 2021-06-28 17:07:24 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container grafana ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: node-exporter-7kpmm from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:13:38.111: INFO: prometheus-adapter-864ff8d5d4-4zdg4 from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 21:13:38.111: INFO: prometheus-adapter-864ff8d5d4-zqh4w from openshift-monitoring started at 2021-06-28 17:09:24 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 21:13:38.111: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 17:09:38 +0000 UTC (7 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 21:13:38.111: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 21:13:38.111: INFO: thanos-querier-8c945b5d4-9mqvb from openshift-monitoring started at 2021-06-28 17:07:31 +0000 UTC (5 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 21:13:38.111: INFO: multus-2vt7k from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:13:38.111: INFO: multus-admission-controller-txqk8 from openshift-multus started at 2021-06-28 17:02:50 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:13:38.111: INFO: network-metrics-daemon-wvzj5 from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:13:38.111: INFO: network-check-source-b77d8dcf6-p7rbb from openshift-network-diagnostics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 28 21:13:38.111: INFO: network-check-target-27f5h from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:13:38.111: INFO: network-operator-585847d64b-4ll8c from openshift-network-operator started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: catalog-operator-5d5d46b6dd-qxtpf from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: olm-operator-87677979d-zh4wc from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 21:13:38.111: INFO: packageserver-dfb6789cb-4gvbs from openshift-operator-lifecycle-manager started at 2021-06-28 17:07:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 21:13:38.111: INFO: packageserver-dfb6789cb-pbg2h from openshift-operator-lifecycle-manager started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 21:13:38.111: INFO: metrics-854698b86f-nw92k from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container metrics ready: true, restart count 1
Jun 28 21:13:38.111: INFO: push-gateway-6cbcf758df-cf46c from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 21:13:38.111: INFO: service-ca-operator-bf8bb76b5-sbsbp from openshift-service-ca-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 21:13:38.111: INFO: service-ca-7d7647cfdb-l8knj from openshift-service-ca started at 2021-06-28 20:38:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 28 21:13:38.111: INFO: sonobuoy-e2e-job-c80b85415c994408 from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container e2e ready: true, restart count 0
Jun 28 21:13:38.111: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 21:13:38.111: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:13:38.111: INFO: 	Container sonobuoy-worker ready: false, restart count 25
Jun 28 21:13:38.111: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cf5aeea6-9f56-4f16-b181-d71f878bb33f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.244.0.29 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-cf5aeea6-9f56-4f16-b181-d71f878bb33f off the node 10.244.0.29
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cf5aeea6-9f56-4f16-b181-d71f878bb33f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:19:07.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9598" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:334.298 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":288,"skipped":4834,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:19:08.717: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 21:19:29.207: INFO: DNS probes using dns-test-d1b9a2b7-b8b9-41c4-8bf7-299dba1c58a5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 21:20:54.876: INFO: DNS probes using dns-test-a522cc4f-b0e9-44cb-8697-507cc130d290 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-575.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-575.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 21:21:07.459: INFO: DNS probes using dns-test-ebf6cd58-fb5f-4dfe-9ccb-8a3f8583c742 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:21:08.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-575" for this suite.

• [SLOW TEST:121.239 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":289,"skipped":4846,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:21:09.956: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:21:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5058" for this suite.
STEP: Destroying namespace "nspatchtest-d714231f-75bf-4228-8c34-85f969018617-3602" for this suite.

• [SLOW TEST:8.078 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":290,"skipped":4857,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:21:18.034: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Jun 28 21:21:19.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-2192 api-versions'
Jun 28 21:21:19.997: INFO: stderr: ""
Jun 28 21:21:19.997: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:21:19.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2192" for this suite.

• [SLOW TEST:6.889 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:771
    should check if v1 is in available api versions  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":291,"skipped":4860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:21:24.923: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-d90da533-712e-47f9-9f78-354a65b13d2a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:21:29.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5126" for this suite.

• [SLOW TEST:7.702 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":292,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:21:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 21:21:35.387: INFO: Waiting up to 5m0s for pod "downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1" in namespace "projected-6415" to be "Succeeded or Failed"
Jun 28 21:21:35.758: INFO: Pod "downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1": Phase="Pending", Reason="", readiness=false. Elapsed: 370.549387ms
Jun 28 21:21:37.997: INFO: Pod "downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609901702s
Jun 28 21:21:40.473: INFO: Pod "downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.085846064s
STEP: Saw pod success
Jun 28 21:21:40.473: INFO: Pod "downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1" satisfied condition "Succeeded or Failed"
Jun 28 21:21:43.189: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1 container client-container: <nil>
STEP: delete the pod
Jun 28 21:21:47.542: INFO: Waiting for pod downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1 to disappear
Jun 28 21:21:47.919: INFO: Pod downwardapi-volume-872d7701-c634-437c-b079-d43ee76579a1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:21:47.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6415" for this suite.

• [SLOW TEST:17.484 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":4932,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:21:50.109: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 21:22:05.222: INFO: Number of nodes with available pods: 0
Jun 28 21:22:05.222: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:22:08.020: INFO: Number of nodes with available pods: 0
Jun 28 21:22:08.020: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:22:09.531: INFO: Number of nodes with available pods: 0
Jun 28 21:22:09.531: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:22:11.037: INFO: Number of nodes with available pods: 0
Jun 28 21:22:11.037: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:22:11.804: INFO: Number of nodes with available pods: 2
Jun 28 21:22:11.804: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 21:22:13.156: INFO: Number of nodes with available pods: 2
Jun 28 21:22:13.156: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 21:22:14.067: INFO: Number of nodes with available pods: 2
Jun 28 21:22:14.067: INFO: Node 10.244.0.30 is running more than one daemon pod
Jun 28 21:22:15.617: INFO: Number of nodes with available pods: 3
Jun 28 21:22:15.617: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 28 21:22:18.399: INFO: Number of nodes with available pods: 2
Jun 28 21:22:18.399: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:20.332: INFO: Number of nodes with available pods: 2
Jun 28 21:22:20.332: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:21.718: INFO: Number of nodes with available pods: 2
Jun 28 21:22:21.718: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:23.179: INFO: Number of nodes with available pods: 2
Jun 28 21:22:23.179: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:23.907: INFO: Number of nodes with available pods: 2
Jun 28 21:22:23.907: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:24.918: INFO: Number of nodes with available pods: 2
Jun 28 21:22:24.918: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:26.230: INFO: Number of nodes with available pods: 2
Jun 28 21:22:26.230: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:27.065: INFO: Number of nodes with available pods: 2
Jun 28 21:22:27.065: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:30.039: INFO: Number of nodes with available pods: 2
Jun 28 21:22:30.039: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:22:32.241: INFO: Number of nodes with available pods: 3
Jun 28 21:22:32.241: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6629, will wait for the garbage collector to delete the pods
Jun 28 21:22:35.297: INFO: Deleting DaemonSet.extensions daemon-set took: 439.438375ms
Jun 28 21:22:35.897: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.189295ms
Jun 28 21:22:53.349: INFO: Number of nodes with available pods: 0
Jun 28 21:22:53.349: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 21:22:53.794: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6629/daemonsets","resourceVersion":"131543"},"items":null}

Jun 28 21:22:54.240: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6629/pods","resourceVersion":"131547"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:22:57.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6629" for this suite.

• [SLOW TEST:68.248 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":294,"skipped":4937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:22:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 28 21:22:59.784: INFO: Waiting up to 5m0s for pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad" in namespace "emptydir-5299" to be "Succeeded or Failed"
Jun 28 21:23:00.032: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad": Phase="Pending", Reason="", readiness=false. Elapsed: 247.663006ms
Jun 28 21:23:02.363: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578414837s
Jun 28 21:23:04.942: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.157784348s
Jun 28 21:23:07.462: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.677412992s
Jun 28 21:23:09.698: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.913316724s
STEP: Saw pod success
Jun 28 21:23:09.698: INFO: Pod "pod-2d7763c9-d88c-4086-bafe-6d73ee084bad" satisfied condition "Succeeded or Failed"
Jun 28 21:23:09.965: INFO: Trying to get logs from node 10.244.0.29 pod pod-2d7763c9-d88c-4086-bafe-6d73ee084bad container test-container: <nil>
STEP: delete the pod
Jun 28 21:23:11.220: INFO: Waiting for pod pod-2d7763c9-d88c-4086-bafe-6d73ee084bad to disappear
Jun 28 21:23:11.559: INFO: Pod pod-2d7763c9-d88c-4086-bafe-6d73ee084bad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:23:11.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5299" for this suite.

• [SLOW TEST:17.659 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":4961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:23:16.017: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 28 21:23:18.369: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1170 /api/v1/namespaces/dns-1170/pods/test-dns-nameservers f1a7762a-4c4b-4120-9ba2-c1612719261e 131890 0 2021-06-28 21:23:18 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-28 21:23:18 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7g49p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7g49p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7g49p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n7xmb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 21:23:18.603: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:20.977: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:23.074: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:24.774: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:26.916: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:28.852: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:30.790: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 21:23:32.802: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 28 21:23:32.803: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1170 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:23:32.803: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Verifying customized DNS server is configured on pod...
Jun 28 21:23:38.352: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1170 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 21:23:38.352: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
Jun 28 21:23:46.736: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:23:47.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1170" for this suite.

• [SLOW TEST:34.777 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":296,"skipped":5011,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:23:50.794: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 21:23:52.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e" in namespace "projected-4101" to be "Succeeded or Failed"
Jun 28 21:23:52.618: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Pending", Reason="", readiness=false. Elapsed: 346.50966ms
Jun 28 21:23:55.080: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.808560661s
Jun 28 21:23:57.549: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277116664s
Jun 28 21:23:59.850: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.578088437s
Jun 28 21:24:01.994: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.721929715s
Jun 28 21:24:04.143: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.871077365s
STEP: Saw pod success
Jun 28 21:24:04.143: INFO: Pod "downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e" satisfied condition "Succeeded or Failed"
Jun 28 21:24:04.296: INFO: Trying to get logs from node 10.244.0.29 pod downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e container client-container: <nil>
STEP: delete the pod
Jun 28 21:24:05.080: INFO: Waiting for pod downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e to disappear
Jun 28 21:24:05.222: INFO: Pod downwardapi-volume-26fb7611-e5ae-4bcf-a509-f7eddbd2e93e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:24:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4101" for this suite.

• [SLOW TEST:15.438 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:24:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun 28 21:24:07.375: INFO: namespace kubectl-6035
Jun 28 21:24:07.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6035 create -f -'
Jun 28 21:24:27.320: INFO: stderr: ""
Jun 28 21:24:27.320: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 28 21:24:28.783: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:28.783: INFO: Found 0 / 1
Jun 28 21:24:29.917: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:29.917: INFO: Found 0 / 1
Jun 28 21:24:30.827: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:30.827: INFO: Found 0 / 1
Jun 28 21:24:31.743: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:31.743: INFO: Found 0 / 1
Jun 28 21:24:32.713: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:32.713: INFO: Found 0 / 1
Jun 28 21:24:33.588: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:33.588: INFO: Found 0 / 1
Jun 28 21:24:34.564: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:34.564: INFO: Found 0 / 1
Jun 28 21:24:35.604: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:35.604: INFO: Found 0 / 1
Jun 28 21:24:36.600: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:36.600: INFO: Found 1 / 1
Jun 28 21:24:36.600: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 21:24:36.873: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 21:24:36.873: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 21:24:36.873: INFO: wait on agnhost-primary startup in kubectl-6035 
Jun 28 21:24:36.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6035 logs agnhost-primary-r98f8 agnhost-primary'
Jun 28 21:24:39.745: INFO: stderr: ""
Jun 28 21:24:39.745: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 28 21:24:39.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6035 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 28 21:24:39.962: INFO: stderr: ""
Jun 28 21:24:39.962: INFO: stdout: "service/rm2 exposed\n"
Jun 28 21:24:40.180: INFO: Service rm2 in namespace kubectl-6035 found.
STEP: exposing service
Jun 28 21:24:43.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-6035 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 28 21:24:44.545: INFO: stderr: ""
Jun 28 21:24:44.545: INFO: stdout: "service/rm3 exposed\n"
Jun 28 21:24:44.707: INFO: Service rm3 in namespace kubectl-6035 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:24:47.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6035" for this suite.

• [SLOW TEST:46.258 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":298,"skipped":5055,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:24:52.491: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 28 21:24:54.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f" in namespace "downward-api-6411" to be "Succeeded or Failed"
Jun 28 21:24:55.240: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 300.337622ms
Jun 28 21:24:59.156: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215723261s
Jun 28 21:25:01.464: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523710572s
Jun 28 21:25:03.989: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.048712426s
Jun 28 21:25:06.364: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.424494318s
Jun 28 21:25:08.860: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.91958336s
STEP: Saw pod success
Jun 28 21:25:08.860: INFO: Pod "downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f" satisfied condition "Succeeded or Failed"
Jun 28 21:25:09.438: INFO: Trying to get logs from node 10.244.0.30 pod downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f container client-container: <nil>
STEP: delete the pod
Jun 28 21:25:12.423: INFO: Waiting for pod downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f to disappear
Jun 28 21:25:12.570: INFO: Pod downwardapi-volume-06907c17-561c-482f-a3e9-ae8d225c3f6f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:25:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6411" for this suite.

• [SLOW TEST:22.010 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:25:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Jun 28 21:25:15.950: INFO: Major version: 1
STEP: Confirm minor version
Jun 28 21:25:15.950: INFO: cleanMinorVersion: 20
Jun 28 21:25:15.950: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:25:15.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8753" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":300,"skipped":5101,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:25:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 28 21:25:17.776: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:25:27.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9739" for this suite.

• [SLOW TEST:12.877 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":301,"skipped":5121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:25:29.788: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 28 21:25:34.443: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 28 21:25:35.383: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 21:25:35.383: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 28 21:25:36.256: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 21:25:36.256: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 28 21:25:36.999: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 28 21:25:36.999: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 28 21:25:46.797: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:25:47.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7477" for this suite.

• [SLOW TEST:20.739 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":302,"skipped":5157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:25:50.529: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Jun 28 21:25:51.801: INFO: Waiting up to 5m0s for pod "test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd" in namespace "svcaccounts-8804" to be "Succeeded or Failed"
Jun 28 21:25:52.027: INFO: Pod "test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 225.198599ms
Jun 28 21:25:54.276: INFO: Pod "test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474462147s
Jun 28 21:25:56.469: INFO: Pod "test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.668026826s
STEP: Saw pod success
Jun 28 21:25:56.469: INFO: Pod "test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd" satisfied condition "Succeeded or Failed"
Jun 28 21:25:56.667: INFO: Trying to get logs from node 10.244.0.29 pod test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd container agnhost-container: <nil>
STEP: delete the pod
Jun 28 21:25:58.213: INFO: Waiting for pod test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd to disappear
Jun 28 21:25:58.332: INFO: Pod test-pod-6a1543e6-8839-4fbb-b90d-badc45a68fcd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:25:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8804" for this suite.

• [SLOW TEST:8.838 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":303,"skipped":5227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:25:59.367: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 28 21:26:00.326: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 21:26:02.506: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 21:26:03.941: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.29 before test
Jun 28 21:26:04.443: INFO: calico-node-47lkr from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:26:04.443: INFO: calico-typha-664f469f54-2h4th from calico-system started at 2021-06-28 21:21:51 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:26:04.443: INFO: ibm-keepalived-watcher-s6s5t from kube-system started at 2021-06-28 16:59:22 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:26:04.443: INFO: ibm-master-proxy-static-10.244.0.29 from kube-system started at 2021-06-28 16:58:02 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:26:04.443: INFO: ibm-vpc-block-csi-node-mxp5s from kube-system started at 2021-06-28 16:59:22 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:26:04.443: INFO: pfpod from limitrange-7477 started at 2021-06-28 21:25:40 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:26:04.443: INFO: tuned-tfswp from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:26:04.443: INFO: dns-default-bsrhg from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: node-ca-4rccc from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:26:04.443: INFO: ingress-canary-s2tfx from openshift-ingress-canary started at 2021-06-28 20:39:33 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:26:04.443: INFO: openshift-kube-proxy-rr8vk from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: node-exporter-mk7n4 from openshift-monitoring started at 2021-06-28 17:03:59 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:26:04.443: INFO: multus-admission-controller-mf54j from openshift-multus started at 2021-06-28 20:39:33 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:26:04.443: INFO: multus-hnlqc from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:26:04.443: INFO: network-metrics-daemon-zvdbr from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:26:04.443: INFO: network-check-target-b65gf from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:26:04.443: INFO: sonobuoy from sonobuoy started at 2021-06-28 18:28:51 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 21:26:04.443: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-7wxfq from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:04.443: INFO: 	Container sonobuoy-worker ready: false, restart count 27
Jun 28 21:26:04.443: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 21:26:04.443: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.30 before test
Jun 28 21:26:05.230: INFO: calico-node-2ql4z from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:26:05.230: INFO: calico-typha-664f469f54-xxwpc from calico-system started at 2021-06-28 21:21:51 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:26:05.230: INFO: ibm-keepalived-watcher-5dmkt from kube-system started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:26:05.230: INFO: ibm-master-proxy-static-10.244.0.30 from kube-system started at 2021-06-28 16:57:51 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:26:05.230: INFO: ibm-vpc-block-csi-node-db8gf from kube-system started at 2021-06-28 16:58:42 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:26:05.230: INFO: vpn-74cdbd76f7-2pkzl from kube-system started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container vpn ready: true, restart count 0
Jun 28 21:26:05.230: INFO: tuned-ntcvm from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:26:05.230: INFO: csi-snapshot-controller-fc56779c7-68pwj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container snapshot-controller ready: true, restart count 1
Jun 28 21:26:05.230: INFO: csi-snapshot-webhook-798674875b-z2zlj from openshift-cluster-storage-operator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container webhook ready: true, restart count 0
Jun 28 21:26:05.230: INFO: downloads-6d9c464cd4-vjjxk from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container download-server ready: true, restart count 0
Jun 28 21:26:05.230: INFO: dns-default-l8wqd from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: image-registry-567d5cff74-k92hq from openshift-image-registry started at 2021-06-28 17:07:17 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container registry ready: true, restart count 0
Jun 28 21:26:05.230: INFO: node-ca-dn9xk from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:26:05.230: INFO: ingress-canary-d47lg from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:26:05.230: INFO: router-default-fdf999c57-m7k7f from openshift-ingress started at 2021-06-28 18:41:24 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container router ready: true, restart count 0
Jun 28 21:26:05.230: INFO: openshift-kube-proxy-jd62h from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: migrator-744d665879-r687n from openshift-kube-storage-version-migrator started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container migrator ready: true, restart count 0
Jun 28 21:26:05.230: INFO: certified-operators-cdpbc from openshift-marketplace started at 2021-06-28 20:56:48 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:26:05.230: INFO: community-operators-qsbjx from openshift-marketplace started at 2021-06-28 20:39:04 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:26:05.230: INFO: redhat-operators-85gdh from openshift-marketplace started at 2021-06-28 20:38:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:26:05.230: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-28 20:39:00 +0000 UTC (5 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: kube-state-metrics-55cbf55cc7-5nhfz from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 28 21:26:05.230: INFO: node-exporter-2x4b4 from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:26:05.230: INFO: openshift-state-metrics-8555845c54-fd7l8 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 28 21:26:05.230: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-28 20:39:23 +0000 UTC (7 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 21:26:05.230: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 21:26:05.230: INFO: prometheus-operator-6b9599d88d-nqprj from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 28 21:26:05.230: INFO: telemeter-client-666c78ff9f-28pc9 from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container reload ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 28 21:26:05.230: INFO: thanos-querier-8c945b5d4-24xwt from openshift-monitoring started at 2021-06-28 18:47:53 +0000 UTC (5 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 21:26:05.230: INFO: multus-admission-controller-96snf from openshift-multus started at 2021-06-28 17:03:00 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:26:05.230: INFO: multus-qp427 from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:26:05.230: INFO: network-metrics-daemon-s59ql from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:05.230: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:26:05.230: INFO: network-check-target-pbqc5 from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:26:05.230: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-b6stl from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container sonobuoy-worker ready: false, restart count 27
Jun 28 21:26:05.230: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 21:26:05.230: INFO: tigera-operator-9cb9c95c7-vxwg6 from tigera-operator started at 2021-06-28 16:58:42 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:05.230: INFO: 	Container tigera-operator ready: true, restart count 6
Jun 28 21:26:05.230: INFO: 
Logging pods the apiserver thinks is on node 10.244.0.31 before test
Jun 28 21:26:06.255: INFO: calico-kube-controllers-67cd4fd574-fst6j from calico-system started at 2021-06-28 17:02:55 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 21:26:06.255: INFO: calico-node-swgct from calico-system started at 2021-06-28 17:02:32 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 21:26:06.255: INFO: calico-typha-664f469f54-757h7 from calico-system started at 2021-06-28 21:21:51 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container calico-typha ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ibm-keepalived-watcher-8r7sd from kube-system started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ibm-master-proxy-static-10.244.0.31 from kube-system started at 2021-06-28 16:57:44 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container pause ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-28 17:03:01 +0000 UTC (4 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container csi-attacher ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ibm-vpc-block-csi-node-dlgsz from kube-system started at 2021-06-28 16:59:10 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 21:26:06.255: INFO: cluster-node-tuning-operator-85b6488455-92rjz from openshift-cluster-node-tuning-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: tuned-5p6rg from openshift-cluster-node-tuning-operator started at 2021-06-28 17:07:28 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container tuned ready: true, restart count 0
Jun 28 21:26:06.255: INFO: cluster-samples-operator-6979dbb9c5-ljqjf from openshift-cluster-samples-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container cluster-samples-operator ready: true, restart count 1
Jun 28 21:26:06.255: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 28 21:26:06.255: INFO: cluster-storage-operator-6974bfb5c6-gjrzk from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 28 21:26:06.255: INFO: csi-snapshot-controller-operator-c9886b54b-mbxtf from openshift-cluster-storage-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: console-operator-946dbb485-549nf from openshift-console-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container console-operator ready: true, restart count 1
Jun 28 21:26:06.255: INFO: console-7d7f484b46-qwgl6 from openshift-console started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container console ready: true, restart count 0
Jun 28 21:26:06.255: INFO: console-7d7f484b46-zql2k from openshift-console started at 2021-06-28 18:43:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container console ready: true, restart count 0
Jun 28 21:26:06.255: INFO: downloads-6d9c464cd4-mjc9h from openshift-console started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container download-server ready: true, restart count 0
Jun 28 21:26:06.255: INFO: dns-operator-74db48c654-lbr6w from openshift-dns-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container dns-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: dns-default-f2xdn from openshift-dns started at 2021-06-28 17:07:59 +0000 UTC (3 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container dns ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: cluster-image-registry-operator-7c675f968-j72bn from openshift-image-registry started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: node-ca-24vfp from openshift-image-registry started at 2021-06-28 17:07:15 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container node-ca ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ingress-canary-n5kgb from openshift-ingress-canary started at 2021-06-28 17:06:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun 28 21:26:06.255: INFO: ingress-operator-6557486749-2zshp from openshift-ingress-operator started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: router-default-fdf999c57-487ml from openshift-ingress started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container router ready: true, restart count 0
Jun 28 21:26:06.255: INFO: openshift-kube-proxy-bhm75 from openshift-kube-proxy started at 2021-06-28 17:00:28 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: kube-storage-version-migrator-operator-bdddd9479-r779m from openshift-kube-storage-version-migrator-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 28 21:26:06.255: INFO: marketplace-operator-569986b7f7-25n44 from openshift-marketplace started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container marketplace-operator ready: true, restart count 2
Jun 28 21:26:06.255: INFO: redhat-marketplace-6vl2f from openshift-marketplace started at 2021-06-28 21:00:40 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container registry-server ready: true, restart count 0
Jun 28 21:26:06.255: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-28 17:09:33 +0000 UTC (5 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container alertmanager ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: cluster-monitoring-operator-6c785d75f6-rzfjr from openshift-monitoring started at 2021-06-28 17:03:01 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 28 21:26:06.255: INFO: grafana-66dff7486b-hhwrz from openshift-monitoring started at 2021-06-28 17:07:24 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container grafana ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: node-exporter-7kpmm from openshift-monitoring started at 2021-06-28 17:03:58 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container node-exporter ready: true, restart count 0
Jun 28 21:26:06.255: INFO: prometheus-adapter-864ff8d5d4-4zdg4 from openshift-monitoring started at 2021-06-28 18:47:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 21:26:06.255: INFO: prometheus-adapter-864ff8d5d4-zqh4w from openshift-monitoring started at 2021-06-28 17:09:24 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 28 21:26:06.255: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-28 17:09:38 +0000 UTC (7 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container config-reloader ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container prometheus ready: true, restart count 1
Jun 28 21:26:06.255: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 28 21:26:06.255: INFO: thanos-querier-8c945b5d4-9mqvb from openshift-monitoring started at 2021-06-28 17:07:31 +0000 UTC (5 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container thanos-query ready: true, restart count 0
Jun 28 21:26:06.255: INFO: multus-2vt7k from openshift-multus started at 2021-06-28 17:00:18 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-multus ready: true, restart count 0
Jun 28 21:26:06.255: INFO: multus-admission-controller-txqk8 from openshift-multus started at 2021-06-28 17:02:50 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 28 21:26:06.255: INFO: network-metrics-daemon-wvzj5 from openshift-multus started at 2021-06-28 17:00:21 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 28 21:26:06.255: INFO: network-check-source-b77d8dcf6-p7rbb from openshift-network-diagnostics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 28 21:26:06.255: INFO: network-check-target-27f5h from openshift-network-diagnostics started at 2021-06-28 17:00:41 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 28 21:26:06.255: INFO: network-operator-585847d64b-4ll8c from openshift-network-operator started at 2021-06-28 16:59:10 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container network-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: catalog-operator-5d5d46b6dd-qxtpf from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: olm-operator-87677979d-zh4wc from openshift-operator-lifecycle-manager started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container olm-operator ready: true, restart count 0
Jun 28 21:26:06.255: INFO: packageserver-dfb6789cb-4gvbs from openshift-operator-lifecycle-manager started at 2021-06-28 17:07:50 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 21:26:06.255: INFO: packageserver-dfb6789cb-pbg2h from openshift-operator-lifecycle-manager started at 2021-06-28 18:47:53 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container packageserver ready: true, restart count 0
Jun 28 21:26:06.255: INFO: metrics-854698b86f-nw92k from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container metrics ready: true, restart count 1
Jun 28 21:26:06.255: INFO: push-gateway-6cbcf758df-cf46c from openshift-roks-metrics started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container push-gateway ready: true, restart count 0
Jun 28 21:26:06.255: INFO: service-ca-operator-bf8bb76b5-sbsbp from openshift-service-ca-operator started at 2021-06-28 17:03:01 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 28 21:26:06.255: INFO: service-ca-7d7647cfdb-l8knj from openshift-service-ca started at 2021-06-28 20:38:52 +0000 UTC (1 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 28 21:26:06.255: INFO: sonobuoy-e2e-job-c80b85415c994408 from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container e2e ready: true, restart count 0
Jun 28 21:26:06.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 21:26:06.255: INFO: sonobuoy-systemd-logs-daemon-set-bcbd11e57a694cf4-dvpqf from sonobuoy started at 2021-06-28 18:29:08 +0000 UTC (2 container statuses recorded)
Jun 28 21:26:06.255: INFO: 	Container sonobuoy-worker ready: false, restart count 27
Jun 28 21:26:06.255: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-8255.168cdc179bde3017], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168cdc1a266914dd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:26:12.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8255" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:13.795 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":304,"skipped":5253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:26:13.162: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7088.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7088.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7088.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7088.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7088.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7088.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 28 21:26:35.315: INFO: DNS probes using dns-7088/dns-test-f61ac5cb-dddb-4eea-abad-dfffaa5055e9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:26:35.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7088" for this suite.

• [SLOW TEST:23.814 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":305,"skipped":5284,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:26:36.977: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7435
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-7435
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7435
Jun 28 21:26:38.849: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jun 28 21:26:48.980: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 28 21:26:49.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 21:26:54.194: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 21:26:54.194: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 21:26:54.194: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 21:26:54.452: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 28 21:27:04.937: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 21:27:04.937: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 21:27:06.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999669s
Jun 28 21:27:08.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.632775249s
Jun 28 21:27:09.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.239379497s
Jun 28 21:27:10.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97619642s
Jun 28 21:27:11.786: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.736299911s
Jun 28 21:27:13.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.602226578s
Jun 28 21:27:14.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.348486585s
Jun 28 21:27:16.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.051819674s
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7435
Jun 28 21:27:17.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:27:24.729: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 21:27:24.729: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 21:27:24.729: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 21:27:24.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:27:28.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 21:27:28.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 21:27:28.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 21:27:28.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:27:33.739: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 21:27:33.739: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 21:27:33.739: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 21:27:33.946: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 21:27:33.946: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 21:27:33.946: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 28 21:27:34.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 21:27:37.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 21:27:37.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 21:27:37.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 21:27:37.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 21:27:40.738: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 21:27:40.738: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 21:27:40.738: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 21:27:40.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 21:27:44.407: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 21:27:44.407: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 21:27:44.407: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 21:27:44.407: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 21:27:44.610: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 28 21:27:54.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 21:27:54.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 21:27:54.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 21:27:55.825: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:27:55.825: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:27:55.825: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:55.826: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:55.826: INFO: 
Jun 28 21:27:55.826: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:27:57.081: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:27:57.081: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:27:57.081: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:57.081: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:57.081: INFO: 
Jun 28 21:27:57.081: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:27:58.260: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:27:58.260: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:27:58.260: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:58.260: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:58.260: INFO: 
Jun 28 21:27:58.260: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:27:59.411: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:27:59.411: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:27:59.411: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:59.411: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:27:59.411: INFO: 
Jun 28 21:27:59.411: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:28:00.650: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:28:00.650: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:28:00.650: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:00.650: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:00.650: INFO: 
Jun 28 21:28:00.650: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:28:01.906: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:28:01.906: INFO: ss-0  10.244.0.29  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:26:38 +0000 UTC  }]
Jun 28 21:28:01.906: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:01.906: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:01.906: INFO: 
Jun 28 21:28:01.906: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 21:28:04.335: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:28:04.335: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:04.335: INFO: ss-2  10.244.0.31  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:04.335: INFO: 
Jun 28 21:28:04.335: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 28 21:28:05.644: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun 28 21:28:05.644: INFO: ss-1  10.244.0.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-28 21:27:06 +0000 UTC  }]
Jun 28 21:28:05.644: INFO: 
Jun 28 21:28:05.644: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7435
Jun 28 21:28:06.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:28:09.640: INFO: rc: 1
Jun 28 21:28:09.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 28 21:28:19.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:28:19.901: INFO: rc: 1
Jun 28 21:28:19.901: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:28:29.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:28:31.928: INFO: rc: 1
Jun 28 21:28:31.928: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:28:41.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:28:42.793: INFO: rc: 1
Jun 28 21:28:42.793: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:28:52.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:28:53.772: INFO: rc: 1
Jun 28 21:28:53.772: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:03.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:29:05.245: INFO: rc: 1
Jun 28 21:29:05.245: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:15.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:29:15.915: INFO: rc: 1
Jun 28 21:29:15.915: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:25.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:29:26.550: INFO: rc: 1
Jun 28 21:29:26.550: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:36.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:29:37.066: INFO: rc: 1
Jun 28 21:29:37.066: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:47.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:29:48.027: INFO: rc: 1
Jun 28 21:29:48.028: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:29:58.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:00.021: INFO: rc: 1
Jun 28 21:30:00.021: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:30:10.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:10.178: INFO: rc: 1
Jun 28 21:30:10.178: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:30:20.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:21.202: INFO: rc: 1
Jun 28 21:30:21.202: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:30:31.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:32.127: INFO: rc: 1
Jun 28 21:30:32.127: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:30:42.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:42.263: INFO: rc: 1
Jun 28 21:30:42.263: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:30:52.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:30:52.426: INFO: rc: 1
Jun 28 21:30:52.426: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:02.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:03.414: INFO: rc: 1
Jun 28 21:31:03.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:13.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:14.131: INFO: rc: 1
Jun 28 21:31:14.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:24.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:24.346: INFO: rc: 1
Jun 28 21:31:24.346: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:34.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:34.486: INFO: rc: 1
Jun 28 21:31:34.486: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:44.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:45.490: INFO: rc: 1
Jun 28 21:31:45.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:31:55.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:31:57.788: INFO: rc: 1
Jun 28 21:31:57.788: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:32:07.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:32:08.553: INFO: rc: 1
Jun 28 21:32:08.553: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:32:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:32:20.131: INFO: rc: 1
Jun 28 21:32:20.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:32:30.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:32:30.291: INFO: rc: 1
Jun 28 21:32:30.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:32:40.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:32:41.581: INFO: rc: 1
Jun 28 21:32:41.581: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:32:51.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:32:52.673: INFO: rc: 1
Jun 28 21:32:52.673: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:33:02.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:33:04.665: INFO: rc: 1
Jun 28 21:33:04.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 28 21:33:14.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=statefulset-7435 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 21:33:16.214: INFO: rc: 1
Jun 28 21:33:16.214: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jun 28 21:33:16.214: INFO: Scaling statefulset ss to 0
Jun 28 21:33:17.310: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 28 21:33:17.550: INFO: Deleting all statefulset in ns statefulset-7435
Jun 28 21:33:17.792: INFO: Scaling statefulset ss to 0
Jun 28 21:33:18.702: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 21:33:19.265: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:33:21.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7435" for this suite.

• [SLOW TEST:406.931 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":306,"skipped":5287,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:33:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:33:44.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1817" for this suite.

• [SLOW TEST:21.398 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:33:45.307: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 28 21:33:49.486: INFO: Number of nodes with available pods: 0
Jun 28 21:33:49.486: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:33:51.410: INFO: Number of nodes with available pods: 0
Jun 28 21:33:51.410: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:33:52.644: INFO: Number of nodes with available pods: 1
Jun 28 21:33:52.644: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:33:55.329: INFO: Number of nodes with available pods: 1
Jun 28 21:33:55.329: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:33:56.936: INFO: Number of nodes with available pods: 1
Jun 28 21:33:56.936: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:33:58.965: INFO: Number of nodes with available pods: 1
Jun 28 21:33:58.965: INFO: Node 10.244.0.29 is running more than one daemon pod
Jun 28 21:34:00.085: INFO: Number of nodes with available pods: 2
Jun 28 21:34:00.085: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:01.018: INFO: Number of nodes with available pods: 3
Jun 28 21:34:01.018: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 28 21:34:02.152: INFO: Number of nodes with available pods: 2
Jun 28 21:34:02.152: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:03.488: INFO: Number of nodes with available pods: 2
Jun 28 21:34:03.488: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:04.905: INFO: Number of nodes with available pods: 2
Jun 28 21:34:04.905: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:05.915: INFO: Number of nodes with available pods: 2
Jun 28 21:34:05.915: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:06.808: INFO: Number of nodes with available pods: 2
Jun 28 21:34:06.808: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:08.068: INFO: Number of nodes with available pods: 2
Jun 28 21:34:08.068: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:09.273: INFO: Number of nodes with available pods: 2
Jun 28 21:34:09.273: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:10.818: INFO: Number of nodes with available pods: 2
Jun 28 21:34:10.818: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:11.809: INFO: Number of nodes with available pods: 2
Jun 28 21:34:11.809: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:13.229: INFO: Number of nodes with available pods: 2
Jun 28 21:34:13.229: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:15.533: INFO: Number of nodes with available pods: 2
Jun 28 21:34:15.533: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:17.258: INFO: Number of nodes with available pods: 2
Jun 28 21:34:17.258: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:19.130: INFO: Number of nodes with available pods: 2
Jun 28 21:34:19.130: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:19.859: INFO: Number of nodes with available pods: 2
Jun 28 21:34:19.859: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:20.713: INFO: Number of nodes with available pods: 2
Jun 28 21:34:20.713: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:21.549: INFO: Number of nodes with available pods: 2
Jun 28 21:34:21.549: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:22.344: INFO: Number of nodes with available pods: 2
Jun 28 21:34:22.344: INFO: Node 10.244.0.31 is running more than one daemon pod
Jun 28 21:34:23.615: INFO: Number of nodes with available pods: 3
Jun 28 21:34:23.615: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8624, will wait for the garbage collector to delete the pods
Jun 28 21:34:24.639: INFO: Deleting DaemonSet.extensions daemon-set took: 364.771446ms
Jun 28 21:34:25.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.126563ms
Jun 28 21:34:37.839: INFO: Number of nodes with available pods: 0
Jun 28 21:34:37.839: INFO: Number of running nodes: 0, number of available pods: 0
Jun 28 21:34:38.063: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8624/daemonsets","resourceVersion":"136516"},"items":null}

Jun 28 21:34:38.261: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8624/pods","resourceVersion":"136517"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:34:39.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8624" for this suite.

• [SLOW TEST:54.498 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":308,"skipped":5329,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:34:39.805: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 28 21:34:58.815: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:34:59.043: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:01.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:01.199: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:03.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:03.160: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:05.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:05.580: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:07.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:07.334: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:09.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:09.389: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:11.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:11.317: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 21:35:13.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 21:35:13.201: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:35:13.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4501" for this suite.

• [SLOW TEST:34.682 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5336,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:35:14.487: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Jun 28 21:35:16.055: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=kubectl-5513 proxy --unix-socket=/tmp/kubectl-proxy-unix122253340/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:35:16.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5513" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":310,"skipped":5339,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 28 21:35:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-179225761
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 28 21:35:22.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512921, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 21:35:25.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512922, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63760512921, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 28 21:35:28.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 28 21:35:38.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-179225761 --namespace=webhook-5051 attach --namespace=webhook-5051 to-be-attached-pod -i -c=container1'
Jun 28 21:35:42.541: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 28 21:35:42.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5051" for this suite.
STEP: Destroying namespace "webhook-5051-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:27.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":311,"skipped":5343,"failed":0}
SSSSSSSSSSSSSJun 28 21:35:44.784: INFO: Running AfterSuite actions on all nodes
Jun 28 21:35:44.784: INFO: Running AfterSuite actions on node 1
Jun 28 21:35:44.784: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 11161.794 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 3h6m2.968698106s
Test Suite Passed
